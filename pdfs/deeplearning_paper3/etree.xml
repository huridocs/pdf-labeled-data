<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="14" family="CMBX12" color="#000000"/>
	<fontspec id="font1" size="21" family="CMBX12" color="#000000"/>
	<fontspec id="font2" size="10" family="CMBXTI10" color="#000000"/>
	<fontspec id="font3" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font4" size="20" family="Times" color="#7f7f7f"/>
<text top="178" left="434" width="99" height="18" font="font1" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">CHAPTER 9</text>
<text top="241" left="79" width="389" height="13" font="font0" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="title">Reinforcement learning for PHY layer communications</text>
<text top="285" left="455" width="72" height="9" font="font2" id="p1_t3" reading_order_no="3" segment_no="2" tag_type="text">Philippe Mary</text>
<text top="296" left="452" width="75" height="9" font="font2" id="p1_t4" reading_order_no="4" segment_no="3" tag_type="text">Visa Koivunen</text>
<text top="308" left="446" width="81" height="9" font="font2" id="p1_t5" reading_order_no="5" segment_no="4" tag_type="text">Christophe Moy</text>
<text top="363" left="79" width="21" height="13" font="font0" id="p1_t6" reading_order_no="6" segment_no="5" tag_type="title">9.1</text>
<text top="363" left="116" width="89" height="13" font="font0" id="p1_t7" reading_order_no="7" segment_no="5" tag_type="title">Introduction</text>
<text top="390" left="79" width="453" height="9" font="font3" id="p1_t8" reading_order_no="8" segment_no="6" tag_type="text">Wireless communication systems have to be designed in order to cope with time-frequency-space varying</text>
<text top="402" left="79" width="453" height="9" font="font3" id="p1_t9" reading_order_no="9" segment_no="6" tag_type="text">channel conditions and variety of interference sources. In cellular wireless systems for instance, channel is</text>
<text top="414" left="79" width="453" height="9" font="font3" id="p1_t10" reading_order_no="10" segment_no="6" tag_type="text">estimated regularly by mobile terminals and base stations (BS) using dedicated pilot signals. This allows</text>
<text top="426" left="79" width="453" height="9" font="font3" id="p1_t11" reading_order_no="11" segment_no="6" tag_type="text">for adapting the transmitters and receivers to the current channel conditions and interference scenario.</text>
<text top="438" left="79" width="453" height="9" font="font3" id="p1_t12" reading_order_no="12" segment_no="6" tag_type="text">Powerful adaptive signal processing algorithms have been developed in the past decades in order to cope</text>
<text top="450" left="79" width="453" height="9" font="font3" id="p1_t13" reading_order_no="13" segment_no="6" tag_type="text">with the dynamic nature of the wireless channel, e.g. the least mean square and recursive least square</text>
<text top="462" left="79" width="453" height="9" font="font3" id="p1_t14" reading_order_no="14" segment_no="6" tag_type="text">algorithms for channel equalization or estimation, the Kalman filtering in multiple-input multiple-output</text>
<text top="474" left="79" width="453" height="9" font="font3" id="p1_t15" reading_order_no="15" segment_no="6" tag_type="text">channel matrix and frequency offset tracking. These techniques rely on very well established mathematical</text>
<text top="486" left="79" width="453" height="9" font="font3" id="p1_t16" reading_order_no="16" segment_no="6" tag_type="text">models of physical phenomena that allow to derive the optimal processing for a given criterion, e.g. mean</text>
<text top="498" left="79" width="301" height="9" font="font3" id="p1_t17" reading_order_no="17" segment_no="6" tag_type="text">square error and assumed noise and interference distribution models.</text>
<text top="510" left="94" width="438" height="9" font="font3" id="p1_t18" reading_order_no="18" segment_no="7" tag_type="text">Any mathematical model trades-off between its complexity and its tractability. A very complete,</text>
<text top="522" left="79" width="453" height="9" font="font3" id="p1_t19" reading_order_no="19" segment_no="7" tag_type="text">and hence complex, model may be useless if any insight on the state of the system cannot be drawn</text>
<text top="534" left="79" width="453" height="9" font="font3" id="p1_t20" reading_order_no="20" segment_no="7" tag_type="text">easily. For instance, the wireless propagation channel is absolutely deterministic and the signal received</text>
<text top="546" left="79" width="453" height="9" font="font3" id="p1_t21" reading_order_no="21" segment_no="7" tag_type="text">at any point of the space at any time can be precisely predicted by the Maxwell equations. However,</text>
<text top="558" left="79" width="453" height="9" font="font3" id="p1_t22" reading_order_no="22" segment_no="7" tag_type="text">this would require a prohibitive amount of computation and memory storage for a receiver to calculate</text>
<text top="570" left="79" width="453" height="9" font="font3" id="p1_t23" reading_order_no="23" segment_no="7" tag_type="text">at any point the value of the electric and magnetic fields using detailed and explicit knowledge of the</text>
<text top="582" left="79" width="453" height="9" font="font3" id="p1_t24" reading_order_no="24" segment_no="7" tag_type="text">physical characteristics of scatterers in the propagation environment, e.g. the dielectric and permittivity</text>
<text top="594" left="79" width="453" height="9" font="font3" id="p1_t25" reading_order_no="25" segment_no="7" tag_type="text">constants of the walls and other obstacles. It is much more efficient to design receivers that perform well</text>
<text top="606" left="79" width="453" height="9" font="font3" id="p1_t26" reading_order_no="26" segment_no="7" tag_type="text">in environments that have been stochastically characterized instead of using explicit deterministic model</text>
<text top="618" left="79" width="192" height="9" font="font3" id="p1_t27" reading_order_no="27" segment_no="7" tag_type="text">of each particular propagation environment.</text>
<text top="630" left="94" width="438" height="9" font="font3" id="p1_t28" reading_order_no="28" segment_no="9" tag_type="text">Modern and emerging wireless systems are characterized by massive amounts of connected mobile de-</text>
<text top="642" left="79" width="453" height="9" font="font3" id="p1_t29" reading_order_no="29" segment_no="9" tag_type="text">vices, BS, sensors and actuators. Modeling such large scale wireless systems has become a formidable task</text>
<text top="654" left="79" width="453" height="9" font="font3" id="p1_t30" reading_order_no="30" segment_no="9" tag_type="text">because of, for example, very small cell sizes, channel aware link adaptation and waveform deployment,</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p1_t31" reading_order_no="31" segment_no="9" tag_type="text">diversity techniques and optimization of the use of different degrees of freedom in tranceivers. Conse-</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p1_t32" reading_order_no="32" segment_no="9" tag_type="text">quently, it may not be feasible to build explicit and detailed mathematical models of wireless systems</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p1_t33" reading_order_no="33" segment_no="9" tag_type="text">and their operational environments. In fact, there is a serious modeling deficit that calls for creating</text>
<text top="702" left="79" width="350" height="9" font="font3" id="p1_t34" reading_order_no="34" segment_no="9" tag_type="text">awareness of the operational wireless environment through sensing and learning.</text>
<text top="714" left="94" width="438" height="9" font="font3" id="p1_t35" reading_order_no="35" segment_no="10" tag_type="text">Machine learning (ML) refers to a large class of algorithms that aim at giving to a machine the ca-</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p1_t36" reading_order_no="36" segment_no="10" tag_type="text">pability to acquire knowledge or behavior. If the machine is a wireless system, which is man-made, then</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p1_t37" reading_order_no="37" segment_no="11" tag_type="text">1</text>
<text top="546" left="32" width="0" height="18" font="font4" id="p1_t38" reading_order_no="0" segment_no="8" tag_type="title">arXiv:2106.11595v1  [cs.AI]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font5" size="10" family="CMTI10" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">the goal of ML in that case is to let the system choose its signal processing techniques and protocols to</text>
<text top="122" left="79" width="454" height="9" font="font3" id="p2_t3" reading_order_no="2" segment_no="1" tag_type="text">perform communication without being programmed a priori . The learning can be supervised , requiring</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p2_t4" reading_order_no="3" segment_no="1" tag_type="text">labeled data, unsupervised or based on reinforcement learning requiring trial-and-error approach. Super-</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p2_t5" reading_order_no="4" segment_no="1" tag_type="text">vised learning refers to methods that learn from a training set for which the desired output is provided</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p2_t6" reading_order_no="5" segment_no="1" tag_type="text">by an external supervisor, i.e. with labeled data, and then perform a task on data that are not present</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p2_t7" reading_order_no="6" segment_no="1" tag_type="text">in the training set. Unsupervised learning refers to methods that attempt to find hidden structures or</text>
<text top="182" left="79" width="453" height="9" font="font3" id="p2_t8" reading_order_no="7" segment_no="1" tag_type="text">patterns in a large data set. Reinforcement learning (RL), is a class of machine learning technique that</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p2_t9" reading_order_no="8" segment_no="1" tag_type="text">aims at maximizing a cumulative reward signal over a finite or infinite time horizon for the selected ac-</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p2_t10" reading_order_no="9" segment_no="1" tag_type="text">tions. RL refers to the interaction through trials and errors between an agent , the entity that learns, and</text>
<text top="218" left="79" width="454" height="9" font="font3" id="p2_t11" reading_order_no="10" segment_no="1" tag_type="text">its operational environment , and will be the focus of this Chapter. This technique is particularly useful</text>
<text top="230" left="79" width="453" height="9" font="font3" id="p2_t12" reading_order_no="11" segment_no="1" tag_type="text">when the agent wants to acquire a knowledge on the characteristics of its environment while making min-</text>
<text top="242" left="79" width="453" height="9" font="font3" id="p2_t13" reading_order_no="12" segment_no="1" tag_type="text">imal assumptions on it. In this Chapter, we will focus on RL for physical layer (PHY) communications.</text>
<text top="253" left="79" width="453" height="9" font="font3" id="p2_t14" reading_order_no="13" segment_no="1" tag_type="text">The rewards in learning are then typically associated with high data rate or high signal-to-interference</text>
<text top="265" left="79" width="454" height="9" font="font3" id="p2_t15" reading_order_no="14" segment_no="1" tag_type="text">and noise ratio (SINR). Even if ML in general and RL in particular are what we may call data driven</text>
<text top="277" left="79" width="453" height="9" font="font3" id="p2_t16" reading_order_no="15" segment_no="1" tag_type="text">approaches, mathematical and physics-based modeling should not be completely abandoned. Indeed,</text>
<text top="289" left="79" width="453" height="9" font="font3" id="p2_t17" reading_order_no="16" segment_no="1" tag_type="text">wireless systems are man-made with plenty of structures that can be exploited to make the learning</text>
<text top="301" left="79" width="158" height="9" font="font3" id="p2_t18" reading_order_no="17" segment_no="1" tag_type="text">faster, transparent, and explainable.</text>
<text top="315" left="94" width="438" height="9" font="font3" id="p2_t19" reading_order_no="18" segment_no="2" tag_type="text">Three main reasons for RL be applied to a wireless communication problem are: i) the mathematical</text>
<text top="327" left="79" width="453" height="9" font="font3" id="p2_t20" reading_order_no="19" segment_no="2" tag_type="text">modelling of the environment is far too complex to be implemented in an agent, or such a mathematical</text>
<text top="339" left="79" width="453" height="9" font="font3" id="p2_t21" reading_order_no="20" segment_no="2" tag_type="text">model does not exist, ii) the signalling for acquiring the useful data needed to properly run the system</text>
<text top="351" left="79" width="453" height="9" font="font3" id="p2_t22" reading_order_no="21" segment_no="2" tag_type="text">is too complex and iii) the desired outcome or goal of the learning can be described as a scalar reward.</text>
<text top="363" left="79" width="453" height="9" font="font3" id="p2_t23" reading_order_no="22" segment_no="2" tag_type="text">For instance, the massive machine type communication or small-cell deployment where detailed network</text>
<text top="375" left="79" width="453" height="9" font="font3" id="p2_t24" reading_order_no="23" segment_no="2" tag_type="text">planning is not feasible are interesting scenarios where RL would be attractive technology. Moreover,</text>
<text top="387" left="79" width="453" height="9" font="font3" id="p2_t25" reading_order_no="24" segment_no="2" tag_type="text">machine type communications typically involve some cheap battery operated devices where complex base</text>
<text top="399" left="79" width="453" height="9" font="font3" id="p2_t26" reading_order_no="25" segment_no="2" tag_type="text">band processing is not possible which is in line with condition i). Moreover, excessive signalling is excluded</text>
<text top="411" left="79" width="453" height="9" font="font3" id="p2_t27" reading_order_no="26" segment_no="2" tag_type="text">because of the huge number of devices to be connected, condition ii). Hence, transmission strategies, e.g.</text>
<text top="422" left="79" width="454" height="9" font="font3" id="p2_t28" reading_order_no="27" segment_no="2" tag_type="text">channel, transmit power, beam patterns, can be learned from scratch or at least with very few a priori</text>
<text top="434" left="79" width="348" height="9" font="font3" id="p2_t29" reading_order_no="28" segment_no="2" tag_type="text">information in order to maximize the number of received packets, condition iii).</text>
<text top="448" left="94" width="439" height="9" font="font5" id="p2_t30" reading_order_no="29" segment_no="3" tag_type="text">Adaptability is the key benefit of RL techniques, as well as classical adaptive signal processing tech-</text>
<text top="460" left="79" width="453" height="9" font="font3" id="p2_t31" reading_order_no="30" segment_no="3" tag_type="text">niques such as the least mean square filter. However, unlike adaptive filtering, RL does not rely on well</text>
<text top="472" left="79" width="453" height="9" font="font3" id="p2_t32" reading_order_no="31" segment_no="3" tag_type="text">established mathematical models of the physical phenomena that occur during a wireless transmission</text>
<text top="484" left="79" width="453" height="9" font="font3" id="p2_t33" reading_order_no="32" segment_no="3" tag_type="text">or very few (cf. Section 9.2). RL techniques select suitable actions based on the feedback received from</text>
<text top="496" left="79" width="454" height="9" font="font3" id="p2_t34" reading_order_no="33" segment_no="3" tag_type="text">the surrounding environment in order to maximize a reward function over time. An important example</text>
<text top="508" left="79" width="453" height="9" font="font3" id="p2_t35" reading_order_no="34" segment_no="3" tag_type="text">of RL applied to physical layer communication is the flexible and opportunistic use of the underutilized</text>
<text top="520" left="79" width="73" height="9" font="font3" id="p2_t36" reading_order_no="35" segment_no="3" tag_type="text">frequency bands.</text>
<text top="533" left="94" width="438" height="9" font="font3" id="p2_t37" reading_order_no="36" segment_no="4" tag_type="text">RL techniques involve a set of possible actions, a set of states and a reward signal. These notions will</text>
<text top="545" left="79" width="453" height="9" font="font3" id="p2_t38" reading_order_no="37" segment_no="4" tag_type="text">be rigorously defined in the next section. Some examples of typical actions are transmit on a given band</text>
<text top="557" left="79" width="453" height="9" font="font3" id="p2_t39" reading_order_no="38" segment_no="4" tag_type="text">or not, or select among the pre-designed beamformers or pre-coders. The states of the environment the</text>
<text top="569" left="79" width="453" height="9" font="font3" id="p2_t40" reading_order_no="39" segment_no="4" tag_type="text">agent observes are, for instance, whether the selected band is idle or not, or feedback from the receiver</text>
<text top="581" left="79" width="453" height="9" font="font3" id="p2_t41" reading_order_no="40" segment_no="4" tag_type="text">that the observed SINR value is below a threshold on certain channels or the transmission has been</text>
<text top="593" left="79" width="195" height="9" font="font3" id="p2_t42" reading_order_no="41" segment_no="4" tag_type="text">successful (+1) or not (0 or -1), respectively.</text>
<text top="607" left="94" width="438" height="9" font="font3" id="p2_t43" reading_order_no="42" segment_no="5" tag_type="text">In the particular example of the opportunistic spectrum access, the cardinality of action and state</text>
<text top="619" left="79" width="453" height="9" font="font3" id="p2_t44" reading_order_no="43" segment_no="5" tag_type="text">sets is small. Hence the learning or convergence phase is faster than in a scenario where the action space</text>
<text top="631" left="79" width="453" height="9" font="font3" id="p2_t45" reading_order_no="44" segment_no="5" tag_type="text">would be much larger. If we imagine a smart radio device that is able to do link adaptation by choosing</text>
<text top="643" left="79" width="453" height="9" font="font3" id="p2_t46" reading_order_no="45" segment_no="5" tag_type="text">the modulation technique, transmit power and to select a channel to transmit its signal, the search space</text>
<text top="654" left="79" width="453" height="9" font="font3" id="p2_t47" reading_order_no="46" segment_no="5" tag_type="text">would become much larger. Hence, the convergence towards the optimal result that maximizes a given</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p2_t48" reading_order_no="47" segment_no="5" tag_type="text">reward function, would take much longer time and may become complex for RL approaches. However,</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p2_t49" reading_order_no="48" segment_no="5" tag_type="text">recent advances in Deep RL methods have alleviated this problem. An example on this scenario, i.e. link</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p2_t50" reading_order_no="49" segment_no="5" tag_type="text">adaptation, will be given in this chapter. Another practical example considered later in this chapter is</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p2_t51" reading_order_no="50" segment_no="5" tag_type="text">smart radio access network with the goal to optimize the energy consumption of a cluster of BSs. This</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p2_t52" reading_order_no="51" segment_no="5" tag_type="text">is a demanding problem with a high dimensional state space which makes finding the optimal solution<a href="deeplearning_paper3.html#4">9.2). </a>RL techniques select suitable actions based on the feedback received from</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p2_t53" reading_order_no="52" segment_no="5" tag_type="text">using classical optimization technique difficult. RL may help to find a good solution without completely</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p2_t54" reading_order_no="53" segment_no="6" tag_type="text">2</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font6" size="10" family="CMBX10" color="#000000"/>
	<fontspec id="font7" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font8" size="7" family="CMMI7" color="#000000"/>
	<fontspec id="font9" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font10" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font11" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font12" size="10" family="dsrom10" color="#000000"/>
	<fontspec id="font13" size="10" family="CMEX10" color="#000000"/>
	<fontspec id="font14" size="10" family="MSBM10" color="#000000"/>
<text top="72" left="79" width="105" height="9" font="font3" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="title">9.1. INTRODUCTION</text>
<text top="110" left="79" width="181" height="9" font="font3" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="text">avoiding the problem complexity, though.</text>
<text top="122" left="94" width="438" height="9" font="font3" id="p3_t3" reading_order_no="2" segment_no="2" tag_type="text">Deep learning (DL) strategies involve multiple layers of artificial neural networks (ANN) that are able</text>
<text top="134" left="79" width="454" height="9" font="font3" id="p3_t4" reading_order_no="3" segment_no="2" tag_type="text">to extract hidden structures of labeled ( supervised learning ) or unlabeled data ( unsupervised learning )</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p3_t5" reading_order_no="4" segment_no="2" tag_type="text">[1]. These techniques prove to be very efficient in many applications such as image classification, speech</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p3_t6" reading_order_no="5" segment_no="2" tag_type="text">recognition and synthesis, and all applications involving a large amount of data to be proceed. Recently,</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p3_t7" reading_order_no="6" segment_no="2" tag_type="text">researchers in wireless communications have shown interest for using ML and DL in wireless networks</text>
<text top="182" left="79" width="453" height="9" font="font3" id="p3_t8" reading_order_no="7" segment_no="2" tag_type="text">design (see [2, 3] and references therein for a complete state of the art on that topic), mainly for networking</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p3_t9" reading_order_no="8" segment_no="2" tag_type="text">issues, but more recently also for PHY layer, such as user-channel matching or performance optimization<a href="deeplearning_paper3.html#43">[1]. </a>These techniques prove to be very efficient in many applications such as image classification, speech</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p3_t10" reading_order_no="9" segment_no="2" tag_type="text">in large scale randomly deployed networks. This is particularly useful when a complete mathematical</text>
<text top="218" left="79" width="453" height="9" font="font3" id="p3_t11" reading_order_no="10" segment_no="2" tag_type="text">model of the behavior of a network is intractable due for example to the large dimensionality of the state</text>
<text top="230" left="79" width="453" height="9" font="font3" id="p3_t12" reading_order_no="11" segment_no="2" tag_type="text">or action spaces. RL and DL can be combined into deep reinforcement learning (DRL) approach when<a href="deeplearning_paper3.html#43">[2, 3] </a>and references therein for a complete state of the art on that topic), mainly for networking</text>
<text top="242" left="79" width="453" height="9" font="font3" id="p3_t13" reading_order_no="12" segment_no="2" tag_type="text">the number of observable states or possible actions are too large for conventional RL. This technique</text>
<text top="253" left="79" width="453" height="9" font="font3" id="p3_t14" reading_order_no="13" segment_no="2" tag_type="text">relies on the reinforcement learning principle, i.e. an agent interacting with its environment, but the</text>
<text top="265" left="79" width="429" height="9" font="font3" id="p3_t15" reading_order_no="14" segment_no="2" tag_type="text">action to be taken is obtained through a non-linear processing involving neural networks (NN) [4].</text>
<text top="277" left="94" width="438" height="9" font="font3" id="p3_t16" reading_order_no="15" segment_no="3" tag_type="text">In this chapter, we will give comprehensive examples of applying RL in optimizing the physical layer</text>
<text top="289" left="79" width="453" height="9" font="font3" id="p3_t17" reading_order_no="16" segment_no="3" tag_type="text">of wireless communications by defining different class of problems and the possible solutions to handle</text>
<text top="301" left="79" width="453" height="9" font="font3" id="p3_t18" reading_order_no="17" segment_no="3" tag_type="text">them. In Section 9.2, we present all the basic theory needed to address a RL problem, i.e. Markov decision</text>
<text top="313" left="79" width="453" height="9" font="font3" id="p3_t19" reading_order_no="18" segment_no="3" tag_type="text">process (MDP), Partially observable Markov decision process (POMDP), but also two very important and</text>
<text top="325" left="79" width="453" height="9" font="font3" id="p3_t20" reading_order_no="19" segment_no="3" tag_type="text">widely used algorithms for RL, i.e. the Q-learning and SARSA algorithms. We also introduce the deep</text>
<text top="337" left="79" width="453" height="9" font="font3" id="p3_t21" reading_order_no="20" segment_no="3" tag_type="text">reinforcement learning (DRL) paradigm and the section ends with an introduction to the multi-armed<a href="deeplearning_paper3.html#43">[4].</a></text>
<text top="349" left="79" width="453" height="9" font="font3" id="p3_t22" reading_order_no="21" segment_no="3" tag_type="text">bandits (MAB) framework. Section 9.3 focuses on some toy examples to illustrate how the basic concepts</text>
<text top="361" left="79" width="453" height="9" font="font3" id="p3_t23" reading_order_no="22" segment_no="3" tag_type="text">of RL are employed in communication systems. We present applications extracted from literature with</text>
<text top="373" left="79" width="453" height="9" font="font3" id="p3_t24" reading_order_no="23" segment_no="3" tag_type="text">simplified system models using similar notation as in Section 9.2 of this Chapter. In Section 9.3, we also<a href="deeplearning_paper3.html#4">9.2, </a>we present all the basic theory needed to address a RL problem, i.e. Markov decision</text>
<text top="385" left="79" width="453" height="9" font="font3" id="p3_t25" reading_order_no="24" segment_no="3" tag_type="text">focus on modeling RL problems, i.e. how action and state spaces and rewards are chosen. The Chapter is</text>
<text top="397" left="79" width="453" height="9" font="font3" id="p3_t26" reading_order_no="25" segment_no="3" tag_type="text">concluded in Section 9.4 with a prospective thought on RL trends and it ends with a review of a broader</text>
<text top="409" left="79" width="131" height="9" font="font3" id="p3_t27" reading_order_no="26" segment_no="3" tag_type="text">state of the art in Section 9.5.</text>
<text top="435" left="79" width="52" height="9" font="font6" id="p3_t28" reading_order_no="27" segment_no="4" tag_type="text">Notations:<a href="deeplearning_paper3.html#20">9.3 </a>focuses on some toy examples to illustrate how the basic concepts</text>
<text top="435" left="141" width="391" height="9" font="font3" id="p3_t29" reading_order_no="28" segment_no="4" tag_type="text">The table at beginning of the Chapter summarizes the notation used in the following. We</text>
<text top="447" left="79" width="453" height="9" font="font3" id="p3_t30" reading_order_no="29" segment_no="4" tag_type="text">review the main ones here. Random variables, and stochastic processes when depending on time, are<a href="deeplearning_paper3.html#4">9.2 </a>of this Chapter. In Section <a href="deeplearning_paper3.html#20">9.3, </a>we also</text>
<text top="459" left="79" width="454" height="9" font="font3" id="p3_t31" reading_order_no="30" segment_no="4" tag_type="text">denoted in capital font, e.g. X ( t ), while their realizations in normal font, i.e. x ( t ). Random vectors</text>
<text top="471" left="79" width="166" height="9" font="font3" id="p3_t32" reading_order_no="31" segment_no="4" tag_type="text">are denoted in capital bold font, e.g.<a href="deeplearning_paper3.html#37">9.4 </a>with a prospective thought on RL trends and it ends with a review of a broader</text>
<text top="471" left="253" width="227" height="9" font="font6" id="p3_t33" reading_order_no="32" segment_no="4" tag_type="text">X ( t ), and their realizations in small bold font, i.e.<a href="deeplearning_paper3.html#38">9.5.</a></text>
<text top="471" left="488" width="44" height="9" font="font6" id="p3_t34" reading_order_no="33" segment_no="4" tag_type="text">x ( t ). The</text>
<text top="483" left="79" width="454" height="9" font="font3" id="p3_t35" reading_order_no="34" segment_no="4" tag_type="text">conditional probability distribution of a random variable X at time t + 1 given another random variable</text>
<text top="494" left="79" width="454" height="11" font="font7" id="p3_t36" reading_order_no="35" segment_no="4" tag_type="text">Y at t , is denoted as P X ( t +1) | Y ( t ) ( x | y ) and when no ambiguity is possible on the underlying distribution</text>
<text top="506" left="79" width="454" height="10" font="font3" id="p3_t37" reading_order_no="36" segment_no="4" tag_type="text">function, simply by p ( x | y ), being understood that the first and the second arguments in p ( · | · ) rely to</text>
<text top="518" left="79" width="454" height="9" font="font3" id="p3_t38" reading_order_no="37" segment_no="4" tag_type="text">the values of the random variables at time t + 1 and t respectively, except otherwise mentioned. Vectors</text>
<text top="530" left="79" width="454" height="10" font="font3" id="p3_t39" reading_order_no="38" segment_no="4" tag_type="text">and matrices are denoted with bold font. Moreover, 1 { a } is the indicator function, which is equal to 1 if</text>
<text top="542" left="79" width="454" height="9" font="font7" id="p3_t40" reading_order_no="39" segment_no="4" tag_type="text">a is true and 0 otherwise. D ( P || Q ) is the Kullback-Leibler divergence between the distributions P and</text>
<text top="554" left="79" width="85" height="9" font="font7" id="p3_t41" reading_order_no="40" segment_no="4" tag_type="text">Q and is defined as</text>
<text top="571" left="214" width="52" height="10" font="font7" id="p3_t42" reading_order_no="41" segment_no="5" tag_type="formula">D ( P || Q ) =</text>
<text top="565" left="269" width="6" height="4" font="font13" id="p3_t43" reading_order_no="42" segment_no="5" tag_type="formula">Z</text>
<text top="582" left="275" width="5" height="7" font="font10" id="p3_t44" reading_order_no="43" segment_no="5" tag_type="formula">I</text>
<text top="565" left="282" width="28" height="16" font="font3" id="p3_t45" reading_order_no="44" segment_no="5" tag_type="formula">log dP</text>
<text top="567" left="298" width="37" height="21" font="font7" id="p3_t46" reading_order_no="45" segment_no="5" tag_type="formula">dQ dP 4</text>
<text top="572" left="328" width="22" height="10" font="font3" id="p3_t47" reading_order_no="46" segment_no="5" tag_type="formula">= E P</text>
<text top="565" left="359" width="27" height="16" font="font3" id="p3_t48" reading_order_no="47" segment_no="5" tag_type="formula">log dP</text>
<text top="579" left="374" width="13" height="9" font="font7" id="p3_t49" reading_order_no="48" segment_no="5" tag_type="formula">dQ</text>
<text top="572" left="395" width="3" height="9" font="font7" id="p3_t50" reading_order_no="49" segment_no="5" tag_type="formula">,</text>
<text top="596" left="79" width="325" height="9" font="font3" id="p3_t51" reading_order_no="50" segment_no="6" tag_type="text">where dP/dQ is called the Radon-Nikodym derivative and is defined if P</text>
<text top="596" left="423" width="110" height="9" font="font7" id="p3_t52" reading_order_no="51" segment_no="6" tag_type="text">Q , i.e. the measure P is</text>
<text top="607" left="79" width="454" height="10" font="font3" id="p3_t53" reading_order_no="52" segment_no="6" tag_type="text">absolutely continuous w.r.t. the measure Q . This means that for all x ∈ I , if Q ( x ) = 0 then P ( x ) = 0.</text>
<text top="620" left="79" width="453" height="9" font="font3" id="p3_t54" reading_order_no="53" segment_no="6" tag_type="text">This definition holds if the probability measures are continuous but also if they are discrete. In the former</text>
<text top="631" left="79" width="453" height="9" font="font3" id="p3_t55" reading_order_no="54" segment_no="6" tag_type="text">case, the Radon-Nikodym derivative is simply the ratio between the density probability functions and in</text>
<text top="643" left="79" width="415" height="9" font="font3" id="p3_t56" reading_order_no="55" segment_no="6" tag_type="text">the latter case, it is the ratio of the probability mass functions that can be denoted as p and q .</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p3_t57" reading_order_no="56" segment_no="7" tag_type="text">3</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font15" size="12" family="CMBX12" color="#000000"/>
	<fontspec id="font16" size="7" family="CMMIB7" color="#000000"/>
	<fontspec id="font17" size="10" family="CMMIB10" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="21" height="13" font="font0" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="title">9.2</text>
<text top="110" left="116" width="263" height="13" font="font0" id="p4_t3" reading_order_no="2" segment_no="1" tag_type="title">Reinforcement Learning: background</text>
<text top="137" left="79" width="28" height="11" font="font15" id="p4_t4" reading_order_no="3" segment_no="2" tag_type="title">9.2.1</text>
<text top="137" left="120" width="55" height="11" font="font15" id="p4_t5" reading_order_no="4" segment_no="2" tag_type="title">Overview</text>
<text top="157" left="79" width="453" height="9" font="font3" id="p4_t6" reading_order_no="5" segment_no="3" tag_type="text">RL may be considered as a sequential decision making process. Unlike supervised learning it does not</text>
<text top="169" left="79" width="453" height="9" font="font3" id="p4_t7" reading_order_no="6" segment_no="3" tag_type="text">need annotation or labeling of input-output pairs. The decision-maker in RL is called an agent. An agent</text>
<text top="181" left="79" width="453" height="9" font="font3" id="p4_t8" reading_order_no="7" segment_no="3" tag_type="text">has to sequentially make decisions that affect future decisions. The agent interacts with its environment</text>
<text top="193" left="79" width="453" height="9" font="font3" id="p4_t9" reading_order_no="8" segment_no="3" tag_type="text">by taking different actions. There are a number of different alternative actions the agent has to choose</text>
<text top="205" left="79" width="453" height="9" font="font3" id="p4_t10" reading_order_no="9" segment_no="3" tag_type="text">from. Selecting the best action requires considering not only immediate but also long-term effects of</text>
<text top="217" left="79" width="453" height="9" font="font3" id="p4_t11" reading_order_no="10" segment_no="3" tag_type="text">its actions. After taking an action, the agent observes the environment state and receives a reward.</text>
<text top="229" left="79" width="453" height="9" font="font3" id="p4_t12" reading_order_no="11" segment_no="3" tag_type="text">Given the new state of the environment, the agent takes the next action. A trajectory or history of</text>
<text top="241" left="79" width="453" height="9" font="font3" id="p4_t13" reading_order_no="12" segment_no="3" tag_type="text">subsequent states, actions and rewards is created over time. The reward quantifies the quality of the</text>
<text top="253" left="79" width="453" height="9" font="font3" id="p4_t14" reading_order_no="13" segment_no="3" tag_type="text">action and consequently captures what is important for the learning system. The objective of the learning</text>
<text top="265" left="79" width="453" height="9" font="font3" id="p4_t15" reading_order_no="14" segment_no="3" tag_type="text">is typically to maximize the sum of future rewards but also some other objective that is a function of</text>
<text top="277" left="79" width="453" height="9" font="font3" id="p4_t16" reading_order_no="15" segment_no="3" tag_type="text">the rewards may be employed. The learning takes place by trial and error so that the rewards reinforce</text>
<text top="288" left="79" width="453" height="9" font="font3" id="p4_t17" reading_order_no="16" segment_no="3" tag_type="text">decisions that move the objective towards its optimum. As a consequence the system learns to make good</text>
<text top="300" left="79" width="453" height="9" font="font3" id="p4_t18" reading_order_no="17" segment_no="3" tag_type="text">decisions and can independently improve its performance by probing different actions for different states.</text>
<text top="312" left="79" width="453" height="9" font="font3" id="p4_t19" reading_order_no="18" segment_no="3" tag_type="text">Hence, consequences to future actions are learned. Commonly the immediate rewards are emphasized</text>
<text top="324" left="79" width="453" height="9" font="font3" id="p4_t20" reading_order_no="19" segment_no="3" tag_type="text">and the future rewards are discounted over time. Sometimes actions with small immediate reward can</text>
<text top="336" left="79" width="453" height="9" font="font3" id="p4_t21" reading_order_no="20" segment_no="3" tag_type="text">lead to a higher payoff in a long term. One would like to choose the action that trades off between the</text>
<text top="348" left="79" width="280" height="9" font="font3" id="p4_t22" reading_order_no="21" segment_no="3" tag_type="text">immediate rewards and the future payoffs the best possible way.</text>
<text top="360" left="94" width="438" height="9" font="font3" id="p4_t23" reading_order_no="22" segment_no="4" tag_type="text">RL are typically modeled using an MDP framework. The MDP provides a formal model to design and</text>
<text top="372" left="79" width="453" height="9" font="font3" id="p4_t24" reading_order_no="23" segment_no="4" tag_type="text">analyze RL problems as well as a rigorous way to design algorithms that can perform optimal decision</text>
<text top="384" left="79" width="453" height="9" font="font3" id="p4_t25" reading_order_no="24" segment_no="4" tag_type="text">making in sequential scenarios. If one models the problem as an MDP, then there exists a number of</text>
<text top="396" left="79" width="453" height="9" font="font3" id="p4_t26" reading_order_no="25" segment_no="4" tag_type="text">algorithms that will be able to automatically solve the decision problem. However, real practical problems</text>
<text top="408" left="79" width="453" height="9" font="font3" id="p4_t27" reading_order_no="26" segment_no="4" tag_type="text">cannot be strictly modeled with an MDP in general, but this framework can be a good approximation of</text>
<text top="420" left="79" width="445" height="9" font="font3" id="p4_t28" reading_order_no="27" segment_no="4" tag_type="text">the physical phenomena that occur in the problem such that this model may perform well in practice.</text>
<text top="446" left="79" width="28" height="11" font="font15" id="p4_t29" reading_order_no="28" segment_no="5" tag_type="title">9.2.2</text>
<text top="446" left="120" width="148" height="11" font="font15" id="p4_t30" reading_order_no="29" segment_no="5" tag_type="title">Markov Decision Process</text>
<text top="466" left="79" width="453" height="9" font="font3" id="p4_t31" reading_order_no="30" segment_no="6" tag_type="text">An MDP model is comprised of four components: a set of states, a set of actions, the transitions, i.e. how</text>
<text top="478" left="79" width="453" height="9" font="font3" id="p4_t32" reading_order_no="31" segment_no="6" tag_type="text">actions change the state and the immediate reward signal due to the actions. We consider a sequence of</text>
<text top="489" left="79" width="453" height="10" font="font3" id="p4_t33" reading_order_no="32" segment_no="6" tag_type="text">discrete time steps t = { 0 , 1 , 2 , ... } and we briefly describe each component. The state describes the set</text>
<text top="502" left="79" width="453" height="9" font="font3" id="p4_t34" reading_order_no="33" segment_no="6" tag_type="text">of all possible values of dynamic information relevant to the decision process. In a broad sense, one could</text>
<text top="514" left="79" width="453" height="9" font="font3" id="p4_t35" reading_order_no="34" segment_no="6" tag_type="text">think that a state describes the way the world currently exists and how an action may change the state of</text>
<text top="526" left="79" width="453" height="9" font="font3" id="p4_t36" reading_order_no="35" segment_no="6" tag_type="text">the world. Obviously we are considering only an abstract and narrow view of the world that is relevant</text>
<text top="538" left="79" width="453" height="9" font="font3" id="p4_t37" reading_order_no="36" segment_no="6" tag_type="text">to the learning task at hand and our operational environment. The state at time instance t is represented</text>
<text top="549" left="79" width="453" height="9" font="font3" id="p4_t38" reading_order_no="37" segment_no="6" tag_type="text">by a random variable S ( t ) ∈ S where S is the state space, i.e. the set of all possible values of dynamic</text>
<text top="561" left="79" width="453" height="9" font="font3" id="p4_t39" reading_order_no="38" segment_no="6" tag_type="text">information relevant to the learning process. In the context of physical layer of wireless communications,</text>
<text top="573" left="79" width="453" height="9" font="font3" id="p4_t40" reading_order_no="39" segment_no="6" tag_type="text">the state could describe, for example, the occupancy of the radio spectrum, or the battery charging level</text>
<text top="585" left="79" width="98" height="9" font="font3" id="p4_t41" reading_order_no="40" segment_no="6" tag_type="text">of the user equipment.</text>
<text top="597" left="94" width="438" height="9" font="font3" id="p4_t42" reading_order_no="41" segment_no="7" tag_type="text">The environment in which the agent acts is modeled as a Markov chain. A Markov chain is a stochastic</text>
<text top="609" left="79" width="453" height="9" font="font3" id="p4_t43" reading_order_no="42" segment_no="7" tag_type="text">model for describing the sequential state transitions in memoryless systems. A memoryless transition</text>
<text top="621" left="79" width="49" height="9" font="font3" id="p4_t44" reading_order_no="43" segment_no="7" tag_type="text">means that</text>
<text top="631" left="218" width="176" height="13" font="font7" id="p4_t45" reading_order_no="44" segment_no="8" tag_type="formula">P S ( t +1) | S ( t ) ( s 0 | s ) = P S ( t +1) | S ( t ) ( s 0 | s ) ,</text>
<text top="633" left="512" width="20" height="9" font="font3" id="p4_t46" reading_order_no="45" segment_no="8" tag_type="text">(9.1)</text>
<text top="649" left="79" width="19" height="9" font="font3" id="p4_t47" reading_order_no="46" segment_no="9" tag_type="text">with</text>
<text top="658" left="201" width="89" height="15" font="font7" id="p4_t48" reading_order_no="47" segment_no="10" tag_type="formula">P S ( t +1) | S ( t ) ( s 0 | s ) 4</text>
<text top="660" left="283" width="128" height="13" font="font3" id="p4_t49" reading_order_no="48" segment_no="10" tag_type="formula">= P [ S ( t + 1) = s 0 | S ( t ) = s ] ,</text>
<text top="662" left="512" width="20" height="9" font="font3" id="p4_t50" reading_order_no="49" segment_no="10" tag_type="text">(9.2)</text>
<text top="678" left="79" width="454" height="9" font="font3" id="p4_t51" reading_order_no="50" segment_no="11" tag_type="text">if S is a discrete set. Moreover S ( t ) = [ S (0) , · · · , S ( t )] and s = [ s (0) , · · · , s ( t )], S ( t ) is the state at time</text>
<text top="690" left="79" width="454" height="11" font="font3" id="p4_t52" reading_order_no="51" segment_no="11" tag_type="text">instance t and P S ( t +1) | S ( t ) is the conditional distribution of S ( t + 1) given S ( t ). In other words, the</text>
<text top="702" left="79" width="454" height="9" font="font3" id="p4_t53" reading_order_no="52" segment_no="11" tag_type="text">probability of the state transition to state S ( t + 1) is only dependent on the current state S ( t ). Hence,</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p4_t54" reading_order_no="53" segment_no="11" tag_type="text">there is no need to remember the history of the past states to determine the current state transition</text>
<text top="726" left="79" width="50" height="9" font="font3" id="p4_t55" reading_order_no="54" segment_no="11" tag_type="text">probability.</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p4_t56" reading_order_no="55" segment_no="12" tag_type="text">4</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font18" size="12" family="CMR12" color="#000000"/>
<text top="72" left="79" width="252" height="9" font="font3" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="94" width="438" height="9" font="font3" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="text">The actions are the set of possible alternatives we can take. The problem is to know which of these</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p5_t3" reading_order_no="2" segment_no="1" tag_type="text">actions to choose in a particular state of the environment. The possible actions the agent can choose at</text>
<text top="133" left="79" width="453" height="10" font="font3" id="p5_t4" reading_order_no="3" segment_no="1" tag_type="text">state S ( t ) is represented with the random variable A ( t ) ∈ A where A is the action space. The possible</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p5_t5" reading_order_no="4" segment_no="1" tag_type="text">actions may depend on the current state. Typical actions in a wireless communication context can be</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p5_t6" reading_order_no="5" segment_no="1" tag_type="text">giving access to the channel, sensing the channel, selecting a beam pattern or adjusting the power, for</text>
<text top="170" left="79" width="38" height="9" font="font3" id="p5_t7" reading_order_no="6" segment_no="1" tag_type="text">instance.</text>
<text top="182" left="94" width="438" height="9" font="font3" id="p5_t8" reading_order_no="7" segment_no="2" tag_type="text">The action the agent takes activates a state transition of the Markov chain. The state transition</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p5_t9" reading_order_no="8" segment_no="2" tag_type="text">function specifies in a probabilistic sense the next state of the environment as a function of its current</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p5_t10" reading_order_no="9" segment_no="2" tag_type="text">state and the action the agent selected. It defines how the available actions may change the state. Since</text>
<text top="218" left="79" width="453" height="9" font="font3" id="p5_t11" reading_order_no="10" segment_no="2" tag_type="text">an action could have different effects, depending upon the state, we need to specify the action’s effect for</text>
<text top="230" left="79" width="379" height="9" font="font3" id="p5_t12" reading_order_no="11" segment_no="2" tag_type="text">each state in the MDP. The state transition probabilities depend on the action such as</text>
<text top="252" left="226" width="113" height="15" font="font7" id="p5_t13" reading_order_no="12" segment_no="3" tag_type="formula">P S ( t +1) | S ( t ) ,A ( t ) ( s 0 | s, a ) 4</text>
<text top="254" left="331" width="56" height="12" font="font3" id="p5_t14" reading_order_no="13" segment_no="3" tag_type="formula">= p ( s 0 | s, a ) .</text>
<text top="257" left="512" width="20" height="9" font="font3" id="p5_t15" reading_order_no="14" segment_no="3" tag_type="text">(9.3)</text>
<text top="280" left="79" width="453" height="9" font="font3" id="p5_t16" reading_order_no="15" segment_no="4" tag_type="text">If we want to make the decision making process automatic and run it in a computer, then we must be</text>
<text top="292" left="79" width="453" height="9" font="font3" id="p5_t17" reading_order_no="16" segment_no="4" tag_type="text">able to quantify the value associated to the actions taken. The value is a scalar and called a reward. The</text>
<text top="303" left="79" width="453" height="10" font="font3" id="p5_t18" reading_order_no="17" segment_no="4" tag_type="text">reward from action A ( t ) is represented by a random variable R ( t ) ∈ R , where R is the set of rewards,</text>
<text top="316" left="79" width="336" height="9" font="font3" id="p5_t19" reading_order_no="18" segment_no="4" tag_type="text">and it specifies the immediate value for performing each action in each state.</text>
<text top="329" left="94" width="438" height="9" font="font3" id="p5_t20" reading_order_no="19" segment_no="5" tag_type="text">The reward provides the means for comparing different actions and choosing the right one for achieving</text>
<text top="341" left="79" width="453" height="9" font="font3" id="p5_t21" reading_order_no="20" segment_no="5" tag_type="text">the objective of the learning process. The reward distribution depends on which action was selected and</text>
<text top="353" left="79" width="453" height="9" font="font3" id="p5_t22" reading_order_no="21" segment_no="5" tag_type="text">on the state transition. The conditional probability distribution defining the dynamics of an MDP process</text>
<text top="364" left="79" width="7" height="9" font="font3" id="p5_t23" reading_order_no="22" segment_no="5" tag_type="text">is</text>
<text top="375" left="204" width="147" height="15" font="font7" id="p5_t24" reading_order_no="23" segment_no="6" tag_type="formula">P R ( t +1) S ( t +1) | S ( t ) A ( t ) ( r, s 0 | s, a ) 4</text>
<text top="377" left="344" width="64" height="12" font="font3" id="p5_t25" reading_order_no="24" segment_no="6" tag_type="formula">= p ( r, s 0 | s, a ) .</text>
<text top="380" left="512" width="20" height="9" font="font3" id="p5_t26" reading_order_no="25" segment_no="6" tag_type="text">(9.4)</text>
<text top="400" left="79" width="453" height="9" font="font3" id="p5_t27" reading_order_no="26" segment_no="7" tag_type="text">An MDP can last for a finite or infinite number of time steps, distinguishing between finite and infinite</text>
<text top="412" left="79" width="453" height="9" font="font3" id="p5_t28" reading_order_no="27" segment_no="7" tag_type="text">horizon models and methods. The interconnection between the different elements of an MDP can be</text>
<text top="424" left="79" width="225" height="9" font="font3" id="p5_t29" reading_order_no="28" segment_no="7" tag_type="text">illustrated with the well-known diagram of Fig. 9.1.</text>
<text top="464" left="273" width="66" height="11" font="font18" id="p5_t30" reading_order_no="29" segment_no="8" tag_type="figure">Environment</text>
<text top="548" left="291" width="31" height="11" font="font18" id="p5_t31" reading_order_no="35" segment_no="8" tag_type="figure">Agent</text>
<text top="508" left="122" width="29" height="9" font="font3" id="p5_t32" reading_order_no="31" segment_no="8" tag_type="figure">Action</text>
<text top="518" left="127" width="19" height="9" font="font7" id="p5_t33" reading_order_no="32" segment_no="8" tag_type="figure">A ( t )</text>
<text top="508" left="468" width="23" height="9" font="font3" id="p5_t34" reading_order_no="33" segment_no="8" tag_type="figure">State</text>
<text top="518" left="470" width="18" height="9" font="font7" id="p5_t35" reading_order_no="34" segment_no="8" tag_type="figure">S ( t )</text>
<text top="450" left="371" width="35" height="9" font="font7" id="p5_t36" reading_order_no="30" segment_no="8" tag_type="figure">S ( t + 1)</text>
<text top="550" left="372" width="33" height="9" font="font3" id="p5_t37" reading_order_no="36" segment_no="8" tag_type="figure">Reward</text>
<text top="563" left="370" width="37" height="9" font="font7" id="p5_t38" reading_order_no="37" segment_no="8" tag_type="figure">R ( t + 1)</text>
<text top="601" left="190" width="232" height="9" font="font3" id="p5_t39" reading_order_no="38" segment_no="9" tag_type="text">Figure 9.1: Reinforcement learning principle in MDP</text>
<text top="626" left="94" width="439" height="9" font="font3" id="p5_t40" reading_order_no="39" segment_no="10" tag_type="text">A reward function R () gives a payoff R ( t + 1) for choosing action A ( t ) in state S ( t ) resulting in</text>
<text top="638" left="79" width="454" height="9" font="font3" id="p5_t41" reading_order_no="40" segment_no="10" tag_type="text">new state S ( t + 1). After selecting an action the agent obtains the reward and the next state, but</text>
<text top="650" left="79" width="453" height="9" font="font3" id="p5_t42" reading_order_no="41" segment_no="10" tag_type="text">no information on which action would have been the best choice towards its objective in a long term.</text>
<text top="662" left="79" width="453" height="9" font="font3" id="p5_t43" reading_order_no="42" segment_no="10" tag_type="text">Hence, the agent will perform active probing and obtain experience about the possible system states,</text>
<text top="674" left="79" width="453" height="9" font="font3" id="p5_t44" reading_order_no="43" segment_no="10" tag_type="text">available actions, transitions and rewards to learn how to act optimally. A very widely used objective is</text>
<text top="686" left="79" width="363" height="9" font="font3" id="p5_t45" reading_order_no="44" segment_no="10" tag_type="text">to maximize the expected sum of discounted rewards over an infinite horizon [5, 6]:</text>
<text top="719" left="248" width="36" height="10" font="font7" id="p5_t46" reading_order_no="45" segment_no="11" tag_type="formula">J π = E π</text>
<text top="708" left="286" width="17" height="7" font="font13" id="p5_t47" reading_order_no="46" segment_no="11" tag_type="formula">" ∞</text>
<text top="716" left="292" width="14" height="4" font="font13" id="p5_t48" reading_order_no="47" segment_no="11" tag_type="formula">X</text>
<text top="732" left="293" width="13" height="6" font="font8" id="p5_t49" reading_order_no="48" segment_no="11" tag_type="formula">t =0</text>
<text top="717" left="308" width="46" height="11" font="font7" id="p5_t50" reading_order_no="49" segment_no="11" tag_type="formula">γ t R ( t + 1)</text>
<text top="708" left="353" width="6" height="4" font="font13" id="p5_t51" reading_order_no="50" segment_no="11" tag_type="formula">#</text>
<text top="719" left="361" width="3" height="9" font="font7" id="p5_t52" reading_order_no="51" segment_no="11" tag_type="formula">,</text>
<text top="719" left="512" width="20" height="9" font="font3" id="p5_t53" reading_order_no="52" segment_no="11" tag_type="text">(9.5)</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p5_t54" reading_order_no="53" segment_no="12" tag_type="text">5</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font19" size="5" family="CMSY5" color="#000000"/>
	<fontspec id="font20" size="6" family="CMR6" color="#000000"/>
	<fontspec id="font21" size="8" family="CMR8" color="#000000"/>
	<fontspec id="font22" size="8" family="CMMI8" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="454" height="9" font="font3" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">where γ &lt; 1 is the discount factor emphasizing more immediate rewards and the expectation is taken</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p6_t3" reading_order_no="2" segment_no="1" tag_type="text">over the distribution of the rewards, following a certain policy π whose the meaning will be detailed</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p6_t4" reading_order_no="3" segment_no="1" tag_type="text">hereafter. Discounting is the most analytically tractable and hence the most commonly used approach.</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p6_t5" reading_order_no="4" segment_no="1" tag_type="text">Other objective functions may be employed including expected reward over a finite time horizon and</text>
<text top="158" left="79" width="383" height="9" font="font3" id="p6_t6" reading_order_no="5" segment_no="1" tag_type="text">regret, which measures the expected loss in the learning compared to an optimal policy.</text>
<text top="170" left="94" width="438" height="9" font="font3" id="p6_t7" reading_order_no="6" segment_no="2" tag_type="text">The solution to an MDP is called a policy and it simply specifies the best sequence of actions to take</text>
<text top="181" left="79" width="454" height="10" font="font3" id="p6_t8" reading_order_no="7" segment_no="2" tag_type="text">during the learning process. Policy maps states to actions, i.e. π : S → A and can be deterministic , i.e.</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p6_t9" reading_order_no="8" segment_no="2" tag_type="text">a single or a set of deterministic actions is performed when the agent encounters the state S ( t ), or it can</text>
<text top="205" left="79" width="454" height="11" font="font3" id="p6_t10" reading_order_no="9" segment_no="2" tag_type="text">be stochastic and is defined as the conditional probability measure of A ( t ) given S ( t ) i.e. P A ( t ) | S ( t ) ( a | s ),</text>
<text top="216" left="79" width="453" height="11" font="font3" id="p6_t11" reading_order_no="10" segment_no="2" tag_type="text">often simply denoted as π ( a | s ) 1 . It is basically a sequence of the decision rules to be used at each</text>
<text top="230" left="79" width="453" height="9" font="font3" id="p6_t12" reading_order_no="11" segment_no="2" tag_type="text">decision time instance. For the infinite-horizon discounted model, there exists an optimal deterministic</text>
<text top="242" left="79" width="453" height="9" font="font3" id="p6_t13" reading_order_no="12" segment_no="2" tag_type="text">stationary policy. The finite-horizon model is appropriate when the system has a hard deadline or the</text>
<text top="253" left="79" width="111" height="9" font="font3" id="p6_t14" reading_order_no="13" segment_no="2" tag_type="text">agent’s lifespan is known.</text>
<text top="265" left="94" width="438" height="9" font="font3" id="p6_t15" reading_order_no="14" segment_no="3" tag_type="text">An optimal policy would need to consider all the future actions. Such policies are called non-myopic</text>
<text top="277" left="79" width="453" height="9" font="font3" id="p6_t16" reading_order_no="15" segment_no="3" tag_type="text">policies. In comparison, a policy where the agent maximizes the immediate reward is called a myopic</text>
<text top="289" left="79" width="454" height="9" font="font3" id="p6_t17" reading_order_no="16" segment_no="3" tag_type="text">policy. Typically, the myopic policies are suboptimal for γ &gt; 0. The goal is to derive a policy which gives</text>
<text top="301" left="79" width="287" height="9" font="font3" id="p6_t18" reading_order_no="17" segment_no="3" tag_type="text">the best actions to take for each state, for the considered horizon.</text>
<text top="327" left="79" width="77" height="9" font="font6" id="p6_t19" reading_order_no="18" segment_no="4" tag_type="title">Value functions</text>
<text top="345" left="79" width="454" height="9" font="font3" id="p6_t20" reading_order_no="19" segment_no="5" tag_type="text">In order to be able to derive the optimal policy that maximizes the function J in (9.5), one needs to</text>
<text top="357" left="79" width="454" height="11" font="font3" id="p6_t21" reading_order_no="20" segment_no="5" tag_type="text">evaluate the value of the states under a certain policy π , i.e. a function v π : S → R . This is defined as</text>
<text top="369" left="79" width="454" height="9" font="font3" id="p6_t22" reading_order_no="21" segment_no="5" tag_type="text">the expectation, over the policy π , of the discounted reward obtained when starting from the state s ∈ S ,</text>
<text top="381" left="79" width="72" height="9" font="font3" id="p6_t23" reading_order_no="22" segment_no="5" tag_type="text">that is to say [5]</text>
<text top="402" left="203" width="48" height="10" font="font7" id="p6_t24" reading_order_no="23" segment_no="6" tag_type="formula">v π ( s ) = E π</text>
<text top="391" left="253" width="17" height="7" font="font13" id="p6_t25" reading_order_no="24" segment_no="6" tag_type="formula">" ∞</text>
<text top="399" left="259" width="14" height="4" font="font13" id="p6_t26" reading_order_no="25" segment_no="6" tag_type="formula">X</text>
<text top="416" left="259" width="13" height="6" font="font8" id="p6_t27" reading_order_no="26" segment_no="6" tag_type="formula">t =0</text>
<text top="400" left="275" width="90" height="11" font="font7" id="p6_t28" reading_order_no="27" segment_no="6" tag_type="formula">γ t R ( t + 1) | S (0) = s</text>
<text top="392" left="365" width="6" height="4" font="font13" id="p6_t29" reading_order_no="28" segment_no="6" tag_type="formula">#</text>
<text top="402" left="372" width="37" height="9" font="font7" id="p6_t30" reading_order_no="29" segment_no="6" tag_type="formula">, ∀ s ∈ S .</text>
<text top="402" left="512" width="20" height="9" font="font3" id="p6_t31" reading_order_no="30" segment_no="6" tag_type="text">(9.6)</text>
<text top="428" left="79" width="454" height="9" font="font3" id="p6_t32" reading_order_no="31" segment_no="7" tag_type="text">Since the state value function depends on the state s taken for the computation, this quantity may be</text>
<text top="440" left="79" width="453" height="9" font="font3" id="p6_t33" reading_order_no="32" segment_no="7" tag_type="text">averaged over the states in order to obtained the average reward in (9.5). The state value function</text>
<text top="452" left="79" width="454" height="9" font="font3" id="p6_t34" reading_order_no="33" segment_no="7" tag_type="text">represents the average of the discounted reward that would be obtained starting from an initial state s</text>
<text top="464" left="79" width="117" height="9" font="font3" id="p6_t35" reading_order_no="34" segment_no="7" tag_type="text">and following the policy π .</text>
<text top="475" left="94" width="439" height="11" font="font3" id="p6_t36" reading_order_no="35" segment_no="8" tag_type="text">Similarly, one can defined the action-state function, q π : S × A → R , by averaging the discounted</text>
<text top="487" left="79" width="453" height="10" font="font3" id="p6_t37" reading_order_no="36" segment_no="8" tag_type="text">reward when starting at t = 0 from state s ∈ S and taken the action a ∈ A , and then following the policy</text>
<text top="500" left="79" width="447" height="9" font="font7" id="p6_t38" reading_order_no="37" segment_no="8" tag_type="text">π thereafter. We hence have now an expectation conditioned on the state and the action, that is [5, 6]</text>
<text top="530" left="158" width="57" height="10" font="font7" id="p6_t39" reading_order_no="38" segment_no="9" tag_type="formula">q π ( s, a ) = E π</text>
<text top="519" left="217" width="17" height="7" font="font13" id="p6_t40" reading_order_no="39" segment_no="9" tag_type="formula">" ∞</text>
<text top="527" left="223" width="14" height="4" font="font13" id="p6_t41" reading_order_no="40" segment_no="9" tag_type="formula">X</text>
<text top="543" left="224" width="13" height="6" font="font8" id="p6_t42" reading_order_no="41" segment_no="9" tag_type="formula">t =0</text>
<text top="528" left="239" width="133" height="11" font="font7" id="p6_t43" reading_order_no="42" segment_no="9" tag_type="formula">γ t R ( t + 1) | S (0) = s, A (0) = a</text>
<text top="519" left="372" width="6" height="4" font="font13" id="p6_t44" reading_order_no="43" segment_no="9" tag_type="formula">#</text>
<text top="529" left="380" width="74" height="10" font="font7" id="p6_t45" reading_order_no="44" segment_no="9" tag_type="formula">, ∀ ( s, a ) ∈ S × A .</text>
<text top="530" left="512" width="20" height="9" font="font3" id="p6_t46" reading_order_no="45" segment_no="9" tag_type="text">(9.7)</text>
<text top="565" left="79" width="93" height="9" font="font6" id="p6_t47" reading_order_no="46" segment_no="10" tag_type="title">Bellman equations</text>
<text top="584" left="79" width="453" height="9" font="font3" id="p6_t48" reading_order_no="47" segment_no="11" tag_type="text">A remarkable property of the state value function in (9.6) is that it follows a recursive relation that is</text>
<text top="595" left="79" width="446" height="10" font="font3" id="p6_t49" reading_order_no="48" segment_no="11" tag_type="text">widely known as the Bellman equation [7] of the state value function. For all s ∈ S , one can prove [5]:</text>
<text top="616" left="185" width="51" height="12" font="font7" id="p6_t50" reading_order_no="49" segment_no="12" tag_type="formula">v π ( s ) = X</text>
<text top="632" left="221" width="16" height="7" font="font8" id="p6_t51" reading_order_no="50" segment_no="12" tag_type="formula">a ∈A</text>
<text top="618" left="239" width="31" height="10" font="font7" id="p6_t52" reading_order_no="51" segment_no="12" tag_type="formula">π ( a | s )</text>
<text top="616" left="286" width="14" height="4" font="font13" id="p6_t53" reading_order_no="52" segment_no="12" tag_type="formula">X</text>
<text top="632" left="272" width="43" height="8" font="font9" id="p6_t54" reading_order_no="53" segment_no="12" tag_type="formula">( s 0 ,r ) ∈S×R</text>
<text top="616" left="316" width="111" height="12" font="font7" id="p6_t55" reading_order_no="54" segment_no="12" tag_type="formula">p ( s 0 , r | s, a ) [ r + γv π ( s 0 )] .</text>
<text top="619" left="512" width="20" height="9" font="font3" id="p6_t56" reading_order_no="55" segment_no="12" tag_type="text">(9.8)</text>
<text top="651" left="79" width="453" height="9" font="font3" id="p6_t57" reading_order_no="56" segment_no="13" tag_type="text">The expression in (9.8) is known as the Bellman’s equation for the state value function and has to<a href="deeplearning_paper3.html#6">)</a></text>
<text top="663" left="79" width="453" height="10" font="font3" id="p6_t58" reading_order_no="57" segment_no="13" tag_type="text">be seen as the expectation of the random variable R ( t + 1) + γv π ( S ( t + 1)) over the joint distribution<a href="deeplearning_paper3.html#6">1</a></text>
<text top="675" left="79" width="454" height="11" font="font7" id="p6_t59" reading_order_no="58" segment_no="13" tag_type="text">P A ( t ) | S ( t ) P S ( t +1) R ( t +1) | S ( t ) A ( t ) . The Bellman’s equation links the state value function at the current state<a href="deeplearning_paper3.html#6">. </a>It is basically a sequence of the decision rules to be used at each</text>
<text top="687" left="79" width="453" height="9" font="font3" id="p6_t60" reading_order_no="59" segment_no="13" tag_type="text">with the next state value function averaged over all possible states and rewards knowing the current state</text>
<text top="699" left="79" width="453" height="9" font="font3" id="p6_t61" reading_order_no="60" segment_no="13" tag_type="text">and the policy π . The value of a state S ( t ) is the expected instantaneous reward added to the expected</text>
<text top="717" left="90" width="443" height="8" font="font21" id="p6_t62" reading_order_no="61" segment_no="14" tag_type="footnote">1 Note in that case, both a and s refers to the value of the action and the state respectively at time t , because action is</text>
<text top="728" left="79" width="326" height="7" font="font21" id="p6_t63" reading_order_no="62" segment_no="14" tag_type="footnote">immediately chosen while observing a given state and not delayed to the next time slot.</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p6_t64" reading_order_no="63" segment_no="15" tag_type="text">6</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font23" size="8" family="CMTI8" color="#000000"/>
<text top="72" left="79" width="252" height="9" font="font3" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="79" width="454" height="9" font="font3" id="p7_t2" reading_order_no="1" segment_no="1" tag_type="text">discounted value of the next states, when the policy π is followed. The proof of this relation relies on</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p7_t3" reading_order_no="2" segment_no="1" tag_type="text">separating (9.6) in two terms, i.e. t = 0 and t &gt; 0, and using essentially the Markovian property and the</text>
<text top="132" left="79" width="236" height="11" font="font3" id="p7_t4" reading_order_no="3" segment_no="1" tag_type="text">Bayes’ rule in the second term to make appear v π ( s 0 ).</text>
<text top="146" left="94" width="439" height="10" font="font3" id="p7_t5" reading_order_no="4" segment_no="2" tag_type="text">The optimal state value and action-state value functions are obtained by maximizing v π ( s ) and q π ( s, a )<a href="deeplearning_paper3.html#6">(9.6) </a>in two terms, i.e.</text>
<text top="158" left="79" width="119" height="9" font="font3" id="p7_t6" reading_order_no="5" segment_no="2" tag_type="text">over the policies, that is [6]</text>
<text top="171" left="267" width="51" height="10" font="font7" id="p7_t7" reading_order_no="6" segment_no="3" tag_type="formula">v ∗ ( s ) = sup</text>
<text top="181" left="302" width="16" height="7" font="font8" id="p7_t8" reading_order_no="7" segment_no="3" tag_type="formula">π ∈ Φ</text>
<text top="171" left="320" width="25" height="9" font="font7" id="p7_t9" reading_order_no="8" segment_no="3" tag_type="formula">v π ( s ) ,</text>
<text top="171" left="512" width="20" height="9" font="font3" id="p7_t10" reading_order_no="9" segment_no="3" tag_type="text">(9.9)</text>
<text top="195" left="79" width="16" height="9" font="font3" id="p7_t11" reading_order_no="10" segment_no="4" tag_type="text">and</text>
<text top="208" left="258" width="60" height="10" font="font7" id="p7_t12" reading_order_no="11" segment_no="5" tag_type="formula">q ∗ ( s, a ) = sup</text>
<text top="218" left="302" width="16" height="7" font="font8" id="p7_t13" reading_order_no="12" segment_no="5" tag_type="formula">π ∈ Φ</text>
<text top="208" left="320" width="35" height="9" font="font7" id="p7_t14" reading_order_no="13" segment_no="5" tag_type="formula">q π ( s, a ) ,</text>
<text top="208" left="507" width="25" height="9" font="font3" id="p7_t15" reading_order_no="14" segment_no="5" tag_type="text">(9.10)</text>
<text top="232" left="79" width="454" height="11" font="font3" id="p7_t16" reading_order_no="15" segment_no="6" tag_type="text">where Φ is the set of all stationary policies, i.e. policies that do not evolve with time 2 . Moreover, (9.9)</text>
<text top="246" left="79" width="454" height="11" font="font3" id="p7_t17" reading_order_no="16" segment_no="6" tag_type="text">can be written w.r.t. (9.10) as v ∗ ( s ) = sup a ∈A q ∗ ( s, a ). The optimal state value function also obeys to</text>
<text top="257" left="79" width="223" height="9" font="font3" id="p7_t18" reading_order_no="17" segment_no="6" tag_type="text">the Bellman recursion and one can show that [5, 6]</text>
<text top="292" left="178" width="51" height="10" font="font7" id="p7_t19" reading_order_no="18" segment_no="7" tag_type="formula">v ∗ ( s ) = sup</text>
<text top="302" left="213" width="16" height="7" font="font8" id="p7_t20" reading_order_no="19" segment_no="7" tag_type="formula">a ∈ A</text>
<text top="278" left="230" width="9" height="4" font="font13" id="p7_t21" reading_order_no="20" segment_no="7" tag_type="formula"></text>
<text top="287" left="230" width="9" height="4" font="font13" id="p7_t22" reading_order_no="21" segment_no="7" tag_type="formula"></text>
<text top="305" left="230" width="9" height="4" font="font13" id="p7_t23" reading_order_no="22" segment_no="7" tag_type="formula"></text>
<text top="289" left="253" width="14" height="4" font="font13" id="p7_t24" reading_order_no="23" segment_no="7" tag_type="formula">X</text>
<text top="305" left="239" width="43" height="8" font="font9" id="p7_t25" reading_order_no="24" segment_no="7" tag_type="formula">( r,s 0 ) ∈S×R</text>
<text top="289" left="283" width="106" height="13" font="font7" id="p7_t26" reading_order_no="25" segment_no="7" tag_type="formula">p ( r, s 0 | s, a ) [ r + γv ∗ ( s 0 )]</text>
<text top="278" left="388" width="9" height="4" font="font13" id="p7_t27" reading_order_no="26" segment_no="7" tag_type="formula"></text>
<text top="287" left="388" width="9" height="4" font="font13" id="p7_t28" reading_order_no="27" segment_no="7" tag_type="formula"><a href="deeplearning_paper3.html#43">[6]</a></text>
<text top="305" left="388" width="9" height="4" font="font13" id="p7_t29" reading_order_no="28" segment_no="7" tag_type="formula"></text>
<text top="291" left="402" width="32" height="10" font="font11" id="p7_t30" reading_order_no="29" segment_no="7" tag_type="formula">∀ s ∈ S .</text>
<text top="292" left="507" width="25" height="9" font="font3" id="p7_t31" reading_order_no="30" segment_no="7" tag_type="text">(9.11)</text>
<text top="326" left="79" width="454" height="9" font="font3" id="p7_t32" reading_order_no="31" segment_no="8" tag_type="text">This last equation means that the expected return from a given state s and following the optimal policy</text>
<text top="336" left="79" width="453" height="11" font="font7" id="p7_t33" reading_order_no="32" segment_no="8" tag_type="text">π ∗ is equal to the expected return following the best action from that state. Moreover, substituting</text>
<text top="348" left="79" width="453" height="12" font="font7" id="p7_t34" reading_order_no="33" segment_no="8" tag_type="text">v ∗ ( s 0 ) in (9.11) with the supremum over the actions of q ∗ ( s 0 , a ), we obtain the iterative property on the</text>
<text top="361" left="79" width="119" height="9" font="font3" id="p7_t35" reading_order_no="34" segment_no="8" tag_type="text">action-state value function:</text>
<text top="387" left="187" width="41" height="11" font="font7" id="p7_t36" reading_order_no="35" segment_no="9" tag_type="formula">q ∗ ( s, a ) =</text>
<text top="384" left="246" width="14" height="4" font="font13" id="p7_t37" reading_order_no="36" segment_no="9" tag_type="formula">X</text>
<text top="401" left="231" width="43" height="7" font="font9" id="p7_t38" reading_order_no="37" segment_no="9" tag_type="formula">( s 0 ,r ) ∈S×R</text>
<text top="385" left="276" width="99" height="11" font="font7" id="p7_t39" reading_order_no="38" segment_no="9" tag_type="formula">p ( s 0 , r | s, a ) r + γ sup</text>
<text top="397" left="358" width="19" height="7" font="font8" id="p7_t40" reading_order_no="39" segment_no="9" tag_type="formula">a 0 ∈A</text>
<text top="385" left="379" width="46" height="13" font="font7" id="p7_t41" reading_order_no="40" segment_no="9" tag_type="formula">q ∗ ( s 0 , a 0 ) .</text>
<text top="387" left="507" width="25" height="9" font="font3" id="p7_t42" reading_order_no="41" segment_no="9" tag_type="text">(9.12)</text>
<text top="421" left="79" width="422" height="11" font="font3" id="p7_t43" reading_order_no="42" segment_no="10" tag_type="text">Expressions in (9.11) and (9.12) are the Bellman optimality equations for v ∗ and q ∗ respectively.</text>
<text top="433" left="94" width="439" height="11" font="font3" id="p7_t44" reading_order_no="43" segment_no="11" tag_type="text">The policy should mostly select the actions that maximize q ∗ ( s, · ). A policy that chooses only actions</text>
<text top="445" left="79" width="454" height="9" font="font3" id="p7_t45" reading_order_no="44" segment_no="11" tag_type="text">that maximize a given action-state function q ( s, · ) for all s is called greedy w.r.t. q . A greedy policy</text>
<text top="457" left="79" width="453" height="9" font="font3" id="p7_t46" reading_order_no="45" segment_no="11" tag_type="text">is a decision procedure based on the immediate reward without considering the long term payoff. In</text>
<text top="469" left="79" width="453" height="9" font="font3" id="p7_t47" reading_order_no="46" segment_no="11" tag_type="text">general, only considering local or immediate reward may prevent from finding the highest payoff on the</text>
<text top="481" left="79" width="454" height="11" font="font3" id="p7_t48" reading_order_no="47" segment_no="11" tag_type="text">long term. However, since v ∗ or q ∗ already contain the reward consequences of all possible states, then</text>
<text top="493" left="79" width="453" height="10" font="font3" id="p7_t49" reading_order_no="48" segment_no="11" tag_type="text">a greedy policy w.r.t. q ∗ is hence optimal on the long run [6]. Hence, if one is able to compute q ∗ , the</text>
<text top="505" left="79" width="98" height="9" font="font3" id="p7_t50" reading_order_no="49" segment_no="11" tag_type="text">optimal policy follows.</text>
<text top="532" left="79" width="86" height="9" font="font6" id="p7_t51" reading_order_no="50" segment_no="12" tag_type="title">Policy evaluation</text>
<text top="551" left="79" width="453" height="9" font="font3" id="p7_t52" reading_order_no="51" segment_no="13" tag_type="text">RL algorithms aim at finding the best sequences of actions in order to get close to (9.11). As we will</text>
<text top="563" left="79" width="453" height="9" font="font3" id="p7_t53" reading_order_no="52" segment_no="13" tag_type="text">see later with the Q-learning, the idea is to keep an estimate of the optimal state or action state value</text>
<text top="575" left="79" width="453" height="9" font="font3" id="p7_t54" reading_order_no="53" segment_no="13" tag_type="text">functions at each time step and find a way to make them converge to the optimal value functions. The</text>
<text top="587" left="79" width="453" height="9" font="font3" id="p7_t55" reading_order_no="54" segment_no="13" tag_type="text">expressions in (9.11) and (9.12) are fixed point equations and can be solved using dynamic programming.</text>
<text top="599" left="79" width="454" height="9" font="font3" id="p7_t56" reading_order_no="55" segment_no="13" tag_type="text">Given a policy π , the policy evaluation algorithm computes firstly finds the value function for immediate</text>
<text top="611" left="79" width="453" height="9" font="font3" id="p7_t57" reading_order_no="56" segment_no="13" tag_type="text">rewards and then extends the time horizon one by one. Basically one just adds the immediate reward of</text>
<text top="623" left="79" width="453" height="9" font="font3" id="p7_t58" reading_order_no="57" segment_no="13" tag_type="text">each of the available actions to the value function computed in previous step. This way one builds the</text>
<text top="635" left="79" width="454" height="9" font="font3" id="p7_t59" reading_order_no="58" segment_no="13" tag_type="text">value function in each iteration based on the previous one. Let consider a deterministic policy a = π ( s ).</text>
<text top="647" left="79" width="302" height="9" font="font3" id="p7_t60" reading_order_no="59" segment_no="13" tag_type="text">A sequence of value functions can be obtained for all states s such as<a href="deeplearning_paper3.html#7">time</a></text>
<text top="671" left="175" width="41" height="9" font="font7" id="p7_t61" reading_order_no="60" segment_no="14" tag_type="formula">v t +1 ( s ) =<a href="deeplearning_paper3.html#7">2</a></text>
<text top="671" left="251" width="27" height="9" font="font7" id="p7_t62" reading_order_no="61" segment_no="14" tag_type="formula">r ( s, a )<a href="deeplearning_paper3.html#7">. </a>Moreover, <a href="deeplearning_paper3.html#7">(9.9)</a></text>
<text top="684" left="251" width="27" height="4" font="font13" id="p7_t63" reading_order_no="62" segment_no="14" tag_type="formula">| {z }<a href="deeplearning_paper3.html#7">(9.10) </a>as</text>
<text top="689" left="219" width="91" height="6" font="font9" id="p7_t64" reading_order_no="63" segment_no="14" tag_type="formula">immediate average reward</text>
<text top="671" left="312" width="13" height="9" font="font3" id="p7_t65" reading_order_no="64" segment_no="14" tag_type="formula">+ γ</text>
<text top="668" left="340" width="14" height="4" font="font13" id="p7_t66" reading_order_no="65" segment_no="14" tag_type="formula">X</text>
<text top="684" left="338" width="17" height="7" font="font8" id="p7_t67" reading_order_no="66" segment_no="14" tag_type="formula">s 0 ∈S</text>
<text top="668" left="357" width="69" height="12" font="font7" id="p7_t68" reading_order_no="67" segment_no="14" tag_type="formula">p ( s 0 | s, a ) v t ( s 0 ) ,</text>
<text top="695" left="338" width="4" height="4" font="font13" id="p7_t69" reading_order_no="68" segment_no="14" tag_type="formula">|</text>
<text top="695" left="377" width="9" height="4" font="font13" id="p7_t70" reading_order_no="69" segment_no="14" tag_type="formula">{z</text>
<text top="695" left="421" width="4" height="4" font="font13" id="p7_t71" reading_order_no="70" segment_no="14" tag_type="formula">}</text>
<text top="700" left="327" width="110" height="6" font="font9" id="p7_t72" reading_order_no="71" segment_no="14" tag_type="formula">value function of previous steps</text>
<text top="671" left="507" width="25" height="9" font="font3" id="p7_t73" reading_order_no="72" segment_no="14" tag_type="text">(9.13)</text>
<text top="717" left="90" width="442" height="8" font="font21" id="p7_t74" reading_order_no="73" segment_no="15" tag_type="footnote">2 Either a fixed over time deterministic policy or a stationary stochastic policy, in the strict sense, i.e. the conditional</text>
<text top="728" left="79" width="165" height="7" font="font21" id="p7_t75" reading_order_no="74" segment_no="15" tag_type="footnote">law of A ( t ) given S ( t ) does not depend on t .</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p7_t76" reading_order_no="75" segment_no="16" tag_type="text">7<a href="deeplearning_paper3.html#43">[5, 6]</a></text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font24" size="5" family="CMMI5" color="#000000"/>
	<fontspec id="font25" size="5" family="CMR5" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="text">where the immediate average reward, r ( s, a ), is the expectation over the distribution of the rewards</text>
<text top="121" left="79" width="454" height="12" font="font3" id="p8_t3" reading_order_no="2" segment_no="1" tag_type="text">knowing the action π ( s ) taken in the state s at time t , i.e. r ( s, a ) = E P R | SA [ R ( t + 1) | S ( t ) = s, A ( t ) = a ].</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p8_t4" reading_order_no="3" segment_no="1" tag_type="text">This iterative equation can be computed for each action taken in each state. This equation converges</text>
<text top="146" left="79" width="454" height="9" font="font3" id="p8_t5" reading_order_no="4" segment_no="1" tag_type="text">because the linear operator linking the value function of state S ( t ) to the value functions of states S ( t + 1)</text>
<text top="158" left="79" width="156" height="9" font="font3" id="p8_t6" reading_order_no="5" segment_no="1" tag_type="text">is a maximum norm contraction [6].</text>
<text top="170" left="94" width="438" height="9" font="font3" id="p8_t7" reading_order_no="6" segment_no="2" tag_type="text">Similarly, the action-state value iteration algorithm aims at iteratively solving the following equation</text>
<text top="182" left="79" width="109" height="9" font="font3" id="p8_t8" reading_order_no="7" segment_no="2" tag_type="text">for a deterministic policy</text>
<text top="206" left="149" width="50" height="9" font="font7" id="p8_t9" reading_order_no="8" segment_no="3" tag_type="formula">q t +1 ( s, a ) =</text>
<text top="206" left="234" width="27" height="9" font="font7" id="p8_t10" reading_order_no="9" segment_no="3" tag_type="formula">r ( s, a )</text>
<text top="219" left="234" width="27" height="4" font="font13" id="p8_t11" reading_order_no="10" segment_no="3" tag_type="formula">| {z }</text>
<text top="224" left="202" width="91" height="6" font="font9" id="p8_t12" reading_order_no="11" segment_no="3" tag_type="formula">immediate average reward</text>
<text top="206" left="295" width="12" height="9" font="font3" id="p8_t13" reading_order_no="12" segment_no="3" tag_type="formula">+ γ</text>
<text top="203" left="340" width="14" height="4" font="font13" id="p8_t14" reading_order_no="13" segment_no="3" tag_type="formula">X</text>
<text top="219" left="338" width="17" height="7" font="font8" id="p8_t15" reading_order_no="14" segment_no="3" tag_type="formula">s 0 ∈S</text>
<text top="203" left="357" width="78" height="12" font="font7" id="p8_t16" reading_order_no="15" segment_no="3" tag_type="formula">p ( s 0 | s, a ) q t ( s 0 , a ) .</text>
<text top="230" left="338" width="4" height="4" font="font13" id="p8_t17" reading_order_no="16" segment_no="3" tag_type="formula">|</text>
<text top="230" left="382" width="9" height="4" font="font13" id="p8_t18" reading_order_no="17" segment_no="3" tag_type="formula">{z</text>
<text top="230" left="430" width="4" height="4" font="font13" id="p8_t19" reading_order_no="18" segment_no="3" tag_type="formula">}</text>
<text top="235" left="310" width="154" height="6" font="font9" id="p8_t20" reading_order_no="19" segment_no="3" tag_type="formula">action-state value function of previous steps</text>
<text top="206" left="507" width="25" height="9" font="font3" id="p8_t21" reading_order_no="20" segment_no="3" tag_type="text">(9.14)</text>
<text top="253" left="79" width="453" height="9" font="font3" id="p8_t22" reading_order_no="21" segment_no="4" tag_type="text">The problem of this procedure is that the agent needs to know the complete dynamic of the MDP, i.e.</text>
<text top="263" left="79" width="454" height="11" font="font7" id="p8_t23" reading_order_no="22" segment_no="4" tag_type="text">p ( r, s 0 | s, a ), that is rarely the case in practice. Fortunately, there are algorithms like Q-learning that</text>
<text top="277" left="79" width="370" height="9" font="font3" id="p8_t24" reading_order_no="23" segment_no="4" tag_type="text">converge to the optimal policy without knowing the dynamic of the physical process.</text>
<text top="304" left="79" width="100" height="9" font="font6" id="p8_t25" reading_order_no="24" segment_no="5" tag_type="title">Policy improvement</text>
<text top="322" left="79" width="453" height="9" font="font3" id="p8_t26" reading_order_no="25" segment_no="6" tag_type="text">The iterative procedures described above allow us to evaluate the performance of a policy. Indeed, by</text>
<text top="334" left="79" width="454" height="10" font="font3" id="p8_t27" reading_order_no="26" segment_no="6" tag_type="text">choosing a given policy π 0 one can compute (9.13) or (9.14) until convergence, e.g. | v t +1 ( s ) − v t ( s ) | ≤ ,</text>
<text top="346" left="79" width="25" height="9" font="font3" id="p8_t28" reading_order_no="27" segment_no="6" tag_type="text">where</text>
<text top="346" left="116" width="417" height="9" font="font3" id="p8_t29" reading_order_no="28" segment_no="6" tag_type="text">is an arbitrary small number. When the procedure has converged, the state value obtained, i.e.</text>
<text top="358" left="79" width="454" height="10" font="font7" id="p8_t30" reading_order_no="29" segment_no="6" tag_type="text">v π 0 ( s ) is the state value function achieved by following the policy π 0 for state s . Hence, one has the</text>
<text top="370" left="79" width="454" height="10" font="font5" id="p8_t31" reading_order_no="30" segment_no="6" tag_type="text">evaluation of the performance of the policy π 0 . But we still do not know if it is the best policy, i.e.</text>
<text top="382" left="79" width="247" height="10" font="font3" id="p8_t32" reading_order_no="31" segment_no="6" tag_type="text">leading to the maximal value of the state function v ∗ ( s ).</text>
<text top="394" left="94" width="438" height="9" font="font3" id="p8_t33" reading_order_no="32" segment_no="7" tag_type="text">In order to progress to the optimal policy, one needs a policy improvement step. The idea is to create</text>
<text top="404" left="79" width="454" height="11" font="font3" id="p8_t34" reading_order_no="33" segment_no="7" tag_type="text">a new policy π 0 that differs from π by a different action taken when being in state s , e.g. π 0 ( s ) 6 = π ( s ). If</text>
<text top="416" left="79" width="454" height="12" font="font3" id="p8_t35" reading_order_no="34" segment_no="7" tag_type="text">we are able to choose an action when being in state s such that q π ( s, π 0 ( s )) ≥ v π ( s ), then v π 0 ( s ) ≥ v π ( s ),</text>
<text top="428" left="79" width="454" height="11" font="font11" id="p8_t36" reading_order_no="35" segment_no="7" tag_type="text">∀ s ∈ S [5]. Basically, it means that by taking an action a 0 such that a 0 6 = π ( s ) and following the policy</text>
<text top="440" left="79" width="454" height="12" font="font7" id="p8_t37" reading_order_no="36" segment_no="7" tag_type="text">π hereafter such that q π ( s, a 0 ) ≥ v π ( s ), the policy π 0 should not be worse than π on the performance of</text>
<text top="454" left="79" width="454" height="10" font="font3" id="p8_t38" reading_order_no="37" segment_no="7" tag_type="text">the learning task. Let π t denote the policy obtained at iteration t . Once (9.14) has converged under π t ,</text>
<text top="466" left="79" width="454" height="10" font="font3" id="p8_t39" reading_order_no="38" segment_no="7" tag_type="text">we create π t +1 by choosing the greedy policy w.r.t. q π t which is obtained by choosing the action that</text>
<text top="478" left="79" width="454" height="9" font="font3" id="p8_t40" reading_order_no="39" segment_no="7" tag_type="text">maximizes the right-hand side of (9.14) for all states s , and we repeat the process until having no policy</text>
<text top="490" left="79" width="63" height="9" font="font3" id="p8_t41" reading_order_no="40" segment_no="7" tag_type="text">improvements.</text>
<text top="517" left="79" width="258" height="9" font="font6" id="p8_t42" reading_order_no="41" segment_no="8" tag_type="title">Combination of policy evaluation and improvement</text>
<text top="535" left="79" width="453" height="9" font="font3" id="p8_t43" reading_order_no="42" segment_no="9" tag_type="text">In practice, the two procedures introduced above, i.e. the policy evaluation and improvement, can be</text>
<text top="547" left="79" width="454" height="9" font="font3" id="p8_t44" reading_order_no="43" segment_no="9" tag_type="text">combined in a one algorithm called: value iteration . This algorithm consists in taking the action that</text>
<text top="559" left="79" width="454" height="9" font="font3" id="p8_t45" reading_order_no="44" segment_no="9" tag_type="text">maximizes the (action-)state value function at each time step, i.e. taking the supremum over A of the</text>
<text top="569" left="79" width="454" height="13" font="font3" id="p8_t46" reading_order_no="45" segment_no="9" tag_type="text">right hand side of (9.13) for the state value function and replacing q t ( s 0 , a ) by sup a 0 ∈A q t ( s 0 , a 0 ) in (9.14)</text>
<text top="583" left="79" width="454" height="10" font="font3" id="p8_t47" reading_order_no="46" segment_no="9" tag_type="text">for the action-state value function. The new equations obtained also converge to v ∗ and q ∗ thanks to the</text>
<text top="595" left="79" width="453" height="10" font="font3" id="p8_t48" reading_order_no="47" segment_no="9" tag_type="text">fixed-point theorem and the property of the Bellman equation. It means that a greedy policy w.r.t. v t</text>
<text top="607" left="79" width="261" height="10" font="font3" id="p8_t49" reading_order_no="48" segment_no="9" tag_type="text">( q t ) allows to converge to the optimum (action-)state value.</text>
<text top="634" left="79" width="28" height="11" font="font15" id="p8_t50" reading_order_no="49" segment_no="10" tag_type="title">9.2.3</text>
<text top="634" left="120" width="265" height="11" font="font15" id="p8_t51" reading_order_no="50" segment_no="10" tag_type="title">Partially observable Markov decision process<a href="deeplearning_paper3.html#43">[6].</a></text>
<text top="654" left="79" width="453" height="9" font="font3" id="p8_t52" reading_order_no="51" segment_no="11" tag_type="text">Complete observability is required for MDP based learning. When the states of the underlying Markovian</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p8_t53" reading_order_no="52" segment_no="11" tag_type="text">process are not directly observable, the situation is referred as a POMDP. This model adds sensor</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p8_t54" reading_order_no="53" segment_no="11" tag_type="text">measurements that contain information about the state of the MDP model as well as observation function</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p8_t55" reading_order_no="54" segment_no="11" tag_type="text">describing the conditional observation probabilities. Instead of observing the current state of the process</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p8_t56" reading_order_no="55" segment_no="11" tag_type="text">directly, we may just have access to a noisy version or estimate of the state. Typically, we use physical</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p8_t57" reading_order_no="56" segment_no="11" tag_type="text">sensors to acquire a set of noisy observations containing relevant information about the state. The</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p8_t58" reading_order_no="57" segment_no="11" tag_type="text">principle of reinforcement learning under POMDP is illustrated on Fig. 9.2. For example, in physical</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p8_t59" reading_order_no="58" segment_no="12" tag_type="text">8</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="252" height="9" font="font3" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="text">layer wireless communication we need to estimate key system parameters such as the channel impulse or</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p9_t3" reading_order_no="2" segment_no="1" tag_type="text">frequency response, channel matrix and its covariance, SINR, perform time and frequency synchronization</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p9_t4" reading_order_no="3" segment_no="1" tag_type="text">as well as design transmit or receive beamformers, precoders or decoders from data acquired by the</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p9_t5" reading_order_no="4" segment_no="1" tag_type="text">receivers. The received signals contain unobservable random noise and harmful interferences. Moreover,</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p9_t6" reading_order_no="5" segment_no="1" tag_type="text">in order to adapt our transmitters and optimize the resources usage, we may need to use feedback</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p9_t7" reading_order_no="6" segment_no="1" tag_type="text">from receivers or exploit channel reciprocity that are both subject to error. Most of the parameters</text>
<text top="182" left="79" width="453" height="9" font="font3" id="p9_t8" reading_order_no="7" segment_no="1" tag_type="text">characterizing the state of a wireless system at the physical layer are defined in a continuous space.</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p9_t9" reading_order_no="8" segment_no="1" tag_type="text">Hence, the conventional MDP model is not necessarily applicable. The errors caused by noisy sensors</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p9_t10" reading_order_no="9" segment_no="1" tag_type="text">are unobservable and random and need to be described using probability distributions. Consequently,</text>
<text top="218" left="79" width="286" height="9" font="font3" id="p9_t11" reading_order_no="10" segment_no="1" tag_type="text">the states are also described in terms of probability distributions.</text>
<text top="255" left="273" width="66" height="11" font="font18" id="p9_t12" reading_order_no="11" segment_no="2" tag_type="figure">Environment</text>
<text top="339" left="291" width="31" height="11" font="font18" id="p9_t13" reading_order_no="20" segment_no="2" tag_type="figure">Agent</text>
<text top="298" left="445" width="33" height="11" font="font18" id="p9_t14" reading_order_no="17" segment_no="2" tag_type="figure">Sensor</text>
<text top="299" left="122" width="29" height="9" font="font3" id="p9_t15" reading_order_no="13" segment_no="2" tag_type="figure">Action</text>
<text top="309" left="127" width="19" height="9" font="font7" id="p9_t16" reading_order_no="14" segment_no="2" tag_type="figure">A ( t )</text>
<text top="262" left="468" width="23" height="9" font="font3" id="p9_t17" reading_order_no="15" segment_no="2" tag_type="figure">State</text>
<text top="272" left="470" width="18" height="9" font="font7" id="p9_t18" reading_order_no="16" segment_no="2" tag_type="figure">S ( t )</text>
<text top="241" left="371" width="35" height="9" font="font7" id="p9_t19" reading_order_no="12" segment_no="2" tag_type="figure">S ( t + 1)</text>
<text top="341" left="372" width="33" height="9" font="font3" id="p9_t20" reading_order_no="21" segment_no="2" tag_type="figure">Reward</text>
<text top="354" left="370" width="37" height="9" font="font7" id="p9_t21" reading_order_no="22" segment_no="2" tag_type="figure">R ( t + 1)</text>
<text top="321" left="425" width="34" height="9" font="font3" id="p9_t22" reading_order_no="18" segment_no="2" tag_type="figure">Observ.</text>
<text top="323" left="468" width="20" height="9" font="font7" id="p9_t23" reading_order_no="19" segment_no="2" tag_type="figure">O ( t )</text>
<text top="392" left="79" width="453" height="9" font="font3" id="p9_t24" reading_order_no="23" segment_no="3" tag_type="text">Figure 9.2: Reinforcement learning principle in POMDP. Contrarily to MDP in Fig. 9.1 the state of the</text>
<text top="404" left="79" width="453" height="9" font="font3" id="p9_t25" reading_order_no="24" segment_no="3" tag_type="text">environment is observed through a sensor that gives a partial state of the environment. For instance, the</text>
<text top="416" left="79" width="453" height="9" font="font3" id="p9_t26" reading_order_no="25" segment_no="3" tag_type="text">sensor may be the receiver in PHY layer communication and actions would be adapting the operational</text>
<text top="428" left="79" width="191" height="9" font="font3" id="p9_t27" reading_order_no="26" segment_no="3" tag_type="text">parameters of the transmitter/receiver pair.</text>
<text top="451" left="94" width="438" height="9" font="font3" id="p9_t28" reading_order_no="27" segment_no="4" tag_type="text">In MDP-based learning our goal is to find a mapping from states to actions. In case of POMDPs,</text>
<text top="463" left="79" width="453" height="9" font="font3" id="p9_t29" reading_order_no="28" segment_no="4" tag_type="text">we are looking for a mapping from probability distributions associated with the states to actions. The</text>
<text top="475" left="79" width="453" height="9" font="font3" id="p9_t30" reading_order_no="29" segment_no="4" tag_type="text">probability distribution over all possible model states is called belief state in POMDP jargon and the</text>
<text top="487" left="79" width="454" height="9" font="font3" id="p9_t31" reading_order_no="30" segment_no="4" tag_type="text">entire probability space (i.e. the set of all possible probability distributions) is called the belief space .</text>
<text top="499" left="79" width="453" height="9" font="font3" id="p9_t32" reading_order_no="31" segment_no="4" tag_type="text">The observations acquired by physical sensors contain unobservable and random noise. Hence, we need to</text>
<text top="511" left="79" width="453" height="9" font="font3" id="p9_t33" reading_order_no="32" segment_no="4" tag_type="text">specify a probabilistic observation model called observation function in POMDP jargon. This observation</text>
<text top="523" left="79" width="453" height="9" font="font3" id="p9_t34" reading_order_no="33" segment_no="4" tag_type="text">model simply tells us the probability of each observation for each state in the model. It is formally</text>
<text top="535" left="79" width="453" height="9" font="font3" id="p9_t35" reading_order_no="34" segment_no="4" tag_type="text">given as a conditional probability of an observation given a state-action couple. As a result, we will</text>
<text top="547" left="79" width="453" height="9" font="font3" id="p9_t36" reading_order_no="35" segment_no="4" tag_type="text">have uncertainty about the state due to incomplete or uncertain information. That means we have a</text>
<text top="559" left="79" width="453" height="9" font="font3" id="p9_t37" reading_order_no="36" segment_no="4" tag_type="text">set of states, but we can never be certain in which state we are. The uncertainties are handled by</text>
<text top="571" left="79" width="453" height="9" font="font3" id="p9_t38" reading_order_no="37" segment_no="4" tag_type="text">associating a probability distribution over the set of possible states. The distribution is defined by the</text>
<text top="583" left="79" width="453" height="9" font="font3" id="p9_t39" reading_order_no="38" segment_no="4" tag_type="text">set of observations, observation probabilities, and the underlying MDP. The POMPD model considers<a href="deeplearning_paper3.html#5">9.1 </a>the state of the</text>
<text top="595" left="79" width="453" height="9" font="font5" id="p9_t40" reading_order_no="39" segment_no="4" tag_type="text">beliefs of the states instead of the actual states in MDP. The belief is a measure in interval [0, 1] that</text>
<text top="607" left="79" width="453" height="9" font="font3" id="p9_t41" reading_order_no="40" segment_no="4" tag_type="text">describes the probability of being in a specific state. The agent interacts with the environment and</text>
<text top="619" left="79" width="453" height="9" font="font3" id="p9_t42" reading_order_no="41" segment_no="4" tag_type="text">receives observations, and then updates its belief state by updating the probability distribution of the</text>
<text top="631" left="79" width="453" height="9" font="font3" id="p9_t43" reading_order_no="42" segment_no="4" tag_type="text">current state. In practice, the agent updates its belief state by using a state estimator based on the last</text>
<text top="643" left="79" width="453" height="9" font="font3" id="p9_t44" reading_order_no="43" segment_no="4" tag_type="text">action, current acquired observation and the previous belief state. By solving a POMDP problem we find</text>
<text top="654" left="79" width="273" height="9" font="font3" id="p9_t45" reading_order_no="44" segment_no="4" tag_type="text">a policy that tells which action is optimal for each belief state.</text>
<text top="666" left="94" width="439" height="9" font="font3" id="p9_t46" reading_order_no="45" segment_no="5" tag_type="text">A POMDP is a tuple &lt; S , A , R , Ω , O &gt; . The environment is represented by a set of states S =</text>
<text top="678" left="79" width="454" height="10" font="font11" id="p9_t47" reading_order_no="46" segment_no="5" tag_type="text">{ s 1 , s 2 , ..., s N } with |S| = N , a set of possible actions denoted by A = { a 1 , a 2 , ..., a K } with |A| = K .</text>
<text top="688" left="79" width="454" height="11" font="font3" id="p9_t48" reading_order_no="47" segment_no="5" tag_type="text">The transition from state s to new state s 0 given the action a is governed by a probabilistic transition</text>
<text top="702" left="79" width="454" height="11" font="font3" id="p9_t49" reading_order_no="48" segment_no="5" tag_type="text">function as in (9.3) that is the conditional distribution P S ( t +1) | S ( t ) ,A ( t ) . As an immediate result, the</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p9_t50" reading_order_no="49" segment_no="5" tag_type="text">agent receives a reward R ( t + 1) that depends on the state s the agent was in and the action a it took.</text>
<text top="724" left="79" width="454" height="11" font="font3" id="p9_t51" reading_order_no="50" segment_no="5" tag_type="text">These components of the POMDP-tuple are as in a conventional MDP. Since state s 0 is not directly</text>
<text top="756" left="304" width="5" height="9" font="font3" id="p9_t52" reading_order_no="51" segment_no="6" tag_type="text">9</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p10_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p10_t2" reading_order_no="1" segment_no="1" tag_type="text">observable, it can be estimated by using observations that contain information about the state. A finite</text>
<text top="121" left="79" width="453" height="10" font="font3" id="p10_t3" reading_order_no="2" segment_no="1" tag_type="text">set of observations emitted by the state is denoted by z ∈ Ω with | Ω | = M . An observation function</text>
<text top="134" left="79" width="454" height="9" font="font3" id="p10_t4" reading_order_no="3" segment_no="1" tag_type="text">defines a probability distribution for each action A ( t ) and the resulting state S ( t + 1). The observation</text>
<text top="146" left="79" width="454" height="9" font="font7" id="p10_t5" reading_order_no="4" segment_no="1" tag_type="text">Z ( t + 1) is then a random variable and the probability distribution of the observations conditioned on</text>
<text top="157" left="79" width="306" height="15" font="font3" id="p10_t6" reading_order_no="5" segment_no="1" tag_type="text">the new state S ( t + 1) and action A ( t ) is P Z ( t +1) | S ( t +1) ,A ( t ) ( z | s 0 , a ) 4</text>
<text top="159" left="378" width="56" height="11" font="font3" id="p10_t7" reading_order_no="6" segment_no="1" tag_type="text">= O ( z, s 0 , a ).</text>
<text top="173" left="94" width="255" height="9" font="font3" id="p10_t8" reading_order_no="7" segment_no="2" tag_type="text">In a POMDP, the following cycle of events takes place.</text>
<text top="173" left="358" width="175" height="9" font="font3" id="p10_t9" reading_order_no="8" segment_no="2" tag_type="text">An environment is in state S and the</text>
<text top="185" left="79" width="160" height="9" font="font3" id="p10_t10" reading_order_no="9" segment_no="2" tag_type="text">agent computed the belief state B .</text>
<text top="185" left="248" width="284" height="9" font="font3" id="p10_t11" reading_order_no="10" segment_no="2" tag_type="text">The agent takes an action A using the policy π ( · | B ) and the</text>
<text top="195" left="79" width="454" height="11" font="font3" id="p10_t12" reading_order_no="11" segment_no="2" tag_type="text">environment transitions to a new state S 0 according to the state transition distribution. The new state</text>
<text top="209" left="79" width="453" height="9" font="font3" id="p10_t13" reading_order_no="12" segment_no="2" tag_type="text">is not directly observable, but the environment emits an observation Z according to the conditional</text>
<text top="219" left="79" width="454" height="11" font="font3" id="p10_t14" reading_order_no="13" segment_no="2" tag_type="text">distribution O ( z, s 0 , a ). Then the agent receives a reward R for action taken from belief state B . Finally,</text>
<text top="233" left="79" width="453" height="9" font="font3" id="p10_t15" reading_order_no="14" segment_no="2" tag_type="text">the agent updates the belief state and runs the cycle again. As a result, a trajectory or history of</text>
<text top="245" left="79" width="453" height="9" font="font3" id="p10_t16" reading_order_no="15" segment_no="2" tag_type="text">subsequent belief states, actions, observation and rewards is created over time. The goal of an agent is</text>
<text top="257" left="79" width="453" height="9" font="font3" id="p10_t17" reading_order_no="16" segment_no="2" tag_type="text">to learn a policy π that finds actions that maximize the policy’s value. There are many ways to measure</text>
<text top="269" left="79" width="413" height="9" font="font3" id="p10_t18" reading_order_no="17" segment_no="2" tag_type="text">the quality. The most common choice is the discounted sum of rewards as in the case of MDP.</text>
<text top="281" left="94" width="438" height="9" font="font3" id="p10_t19" reading_order_no="18" segment_no="3" tag_type="text">The belief state is a sufficient statistic that contains all the relevant information about the past</text>
<text top="293" left="79" width="453" height="9" font="font3" id="p10_t20" reading_order_no="19" segment_no="3" tag_type="text">observations and enjoys the Markov property. The next belief state depends only on the current belief</text>
<text top="305" left="79" width="453" height="9" font="font3" id="p10_t21" reading_order_no="20" segment_no="3" tag_type="text">state and the current action and observation. Hence, there is no need to remember the whole history</text>
<text top="317" left="79" width="453" height="9" font="font3" id="p10_t22" reading_order_no="21" segment_no="3" tag_type="text">of actions and observations. Updating the distribution requires using the transition and observation</text>
<text top="332" left="79" width="268" height="9" font="font3" id="p10_t23" reading_order_no="22" segment_no="3" tag_type="text">probabilities and a formula stemming from the Bayes rule.</text>
<text top="328" left="357" width="106" height="14" font="font3" id="p10_t24" reading_order_no="23" segment_no="3" tag_type="text">Let us denote P B ( s ) 4</text>
<text top="332" left="456" width="77" height="10" font="font3" id="p10_t25" reading_order_no="24" segment_no="3" tag_type="text">= P B [ S = s ] the</text>
<text top="344" left="79" width="454" height="9" font="font3" id="p10_t26" reading_order_no="25" segment_no="3" tag_type="text">distribution of the belief state, which is the probability that the environment is in state s under the</text>
<text top="356" left="79" width="453" height="9" font="font3" id="p10_t27" reading_order_no="26" segment_no="3" tag_type="text">distribution of the belief. The belief state update that can be considered as a state estimation step in</text>
<text top="368" left="79" width="137" height="9" font="font3" id="p10_t28" reading_order_no="27" segment_no="3" tag_type="text">POMDP framework is given by</text>
<text top="387" left="219" width="85" height="18" font="font7" id="p10_t29" reading_order_no="28" segment_no="4" tag_type="formula">P B ( s 0 ) = O ( z, s 0 , a )</text>
<text top="402" left="266" width="34" height="9" font="font7" id="p10_t30" reading_order_no="29" segment_no="4" tag_type="formula">p ( z | b, a )</text>
<text top="392" left="307" width="14" height="4" font="font13" id="p10_t31" reading_order_no="30" segment_no="4" tag_type="formula">X</text>
<text top="409" left="307" width="14" height="7" font="font8" id="p10_t32" reading_order_no="31" segment_no="4" tag_type="formula">s ∈S</text>
<text top="393" left="323" width="71" height="12" font="font7" id="p10_t33" reading_order_no="32" segment_no="4" tag_type="formula">p ( s 0 | s, a ) P B ( s ) .</text>
<text top="395" left="507" width="25" height="9" font="font3" id="p10_t34" reading_order_no="33" segment_no="4" tag_type="text">(9.15)</text>
<text top="426" left="79" width="398" height="11" font="font3" id="p10_t35" reading_order_no="34" segment_no="5" tag_type="text">The denominator is a normalizing constant that makes the sum on s 0 equal to one, i.e.</text>
<text top="427" left="485" width="48" height="10" font="font7" id="p10_t36" reading_order_no="35" segment_no="5" tag_type="text">p ( z | b, a ) =</text>
<text top="438" left="79" width="454" height="13" font="font13" id="p10_t37" reading_order_no="36" segment_no="5" tag_type="text">P s 0 ∈S O ( z, s 0 , a ) P s ∈S p ( s 0 | s, a ) P B ( s ). The distributions are updated simply by applying the Bayes’</text>
<text top="451" left="79" width="453" height="9" font="font3" id="p10_t38" reading_order_no="37" segment_no="5" tag_type="text">Rule and using the model parameters. Similarly to MDP, the agent wishes to choose its actions such</text>
<text top="461" left="79" width="166" height="11" font="font3" id="p10_t39" reading_order_no="38" segment_no="5" tag_type="text">that it learns an optimal policy π ∗ ( b ).</text>
<text top="475" left="94" width="438" height="9" font="font3" id="p10_t40" reading_order_no="39" segment_no="6" tag_type="text">The main practical difficulty in POMDP models is finding a solution that is a policy that chooses</text>
<text top="487" left="79" width="453" height="9" font="font3" id="p10_t41" reading_order_no="40" segment_no="6" tag_type="text">optimal action for each belief state. One may use value iteration or policy iteration approaches as in the</text>
<text top="499" left="79" width="453" height="9" font="font3" id="p10_t42" reading_order_no="41" segment_no="6" tag_type="text">case of MDP. Now the state space is just defined in terms of probability distributions. Algorithms used</text>
<text top="511" left="79" width="453" height="9" font="font3" id="p10_t43" reading_order_no="42" segment_no="6" tag_type="text">for solving POMDP typically stem from dynamic programming. It can be solved using value iteration,</text>
<text top="523" left="79" width="454" height="10" font="font3" id="p10_t44" reading_order_no="43" segment_no="6" tag_type="text">policy iteration or a variety of other techniques developed for solving MDP. Optimal value function v ∗ is</text>
<text top="533" left="79" width="454" height="11" font="font3" id="p10_t45" reading_order_no="44" segment_no="6" tag_type="text">then the value function associated with an optimal policy π ∗ . The early approach for solving POMDPs</text>
<text top="547" left="79" width="453" height="9" font="font3" id="p10_t46" reading_order_no="45" segment_no="6" tag_type="text">was using belief-MDP formulation. The value iteration follows the Bellman’s equation already introduced</text>
<text top="559" left="79" width="57" height="9" font="font3" id="p10_t47" reading_order_no="46" segment_no="6" tag_type="text">for MDP, i.e.</text>
<text top="581" left="211" width="22" height="10" font="font7" id="p10_t48" reading_order_no="47" segment_no="7" tag_type="formula">v 0 ( b )</text>
<text top="581" left="243" width="8" height="9" font="font3" id="p10_t49" reading_order_no="48" segment_no="7" tag_type="formula">=</text>
<text top="581" left="260" width="8" height="9" font="font3" id="p10_t50" reading_order_no="49" segment_no="7" tag_type="formula">0 .</text>
<text top="581" left="507" width="25" height="9" font="font3" id="p10_t51" reading_order_no="50" segment_no="7" tag_type="text">(9.16)</text>
<text top="598" left="202" width="31" height="10" font="font7" id="p10_t52" reading_order_no="51" segment_no="8" tag_type="formula">v t +1 ( b )</text>
<text top="598" left="243" width="8" height="9" font="font3" id="p10_t53" reading_order_no="52" segment_no="8" tag_type="formula">=</text>
<text top="598" left="261" width="15" height="9" font="font3" id="p10_t54" reading_order_no="53" segment_no="8" tag_type="formula">sup</text>
<text top="608" left="260" width="17" height="7" font="font8" id="p10_t55" reading_order_no="54" segment_no="8" tag_type="formula">a ∈A</text>
<text top="595" left="276" width="64" height="12" font="font3" id="p10_t56" reading_order_no="55" segment_no="8" tag_type="formula">[ r ( b, a ) + γ X</text>
<text top="612" left="325" width="16" height="7" font="font8" id="p10_t57" reading_order_no="56" segment_no="8" tag_type="formula">z ∈ Ω</text>
<text top="596" left="342" width="68" height="12" font="font7" id="p10_t58" reading_order_no="57" segment_no="8" tag_type="formula">p ( z | b, a ) v t ( b 0 )] ,</text>
<text top="598" left="507" width="25" height="9" font="font3" id="p10_t59" reading_order_no="58" segment_no="8" tag_type="text">(9.17)</text>
<text top="630" left="79" width="454" height="12" font="font3" id="p10_t60" reading_order_no="59" segment_no="9" tag_type="text">where r ( b, a ) = P s ∈S P B ( s ) r ( s, a ) is the average reward for action a in the belief state b and where</text>
<text top="643" left="79" width="454" height="9" font="font7" id="p10_t61" reading_order_no="60" segment_no="9" tag_type="text">r ( s, a ) is the expected reward obtained in state s and taking action a that has been defined in (9.13) and</text>
<text top="654" left="79" width="454" height="9" font="font3" id="p10_t62" reading_order_no="61" segment_no="9" tag_type="text">(9.14). Moreover, p ( z | b, a ) has been defined previously and denotes the conditional distribution of the</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p10_t63" reading_order_no="62" segment_no="9" tag_type="text">observation z at time t + 1, given the belief state b and taking action a at t . A value function may be</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p10_t64" reading_order_no="63" segment_no="9" tag_type="text">modeled with a structure that can be exploited in making the problem solving easier or even feasible.</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p10_t65" reading_order_no="64" segment_no="9" tag_type="text">For example a piecewise linear and convex approximation may be used in finite horizon scenarios. Value</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p10_t66" reading_order_no="65" segment_no="9" tag_type="text">iteration provides an exact way of determining the value function for POMDP. Hence, the optimal action</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p10_t67" reading_order_no="66" segment_no="9" tag_type="text">can be found from the value function for a belief state. Unfortunately, the complexity of solving POMDP</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p10_t68" reading_order_no="67" segment_no="9" tag_type="text">problem via value iteration is exponential in the number of observations and actions. Moreover, the</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p10_t69" reading_order_no="68" segment_no="10" tag_type="text">10</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="252" height="9" font="font3" id="p11_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p11_t2" reading_order_no="1" segment_no="1" tag_type="text">dimensionality of the belief space grows proportionally to the number of states. The dimensionality of</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p11_t3" reading_order_no="2" segment_no="1" tag_type="text">belief space can be reduced using a parametric model such as a Gaussian mixture model. Also point-based</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p11_t4" reading_order_no="3" segment_no="1" tag_type="text">value iteration has been proposed for solving POMDP. In such methods, a small set of reachable belief</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p11_t5" reading_order_no="4" segment_no="1" tag_type="text">points are selected and the Bellman updates are done at these points while storing values and gradients.</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p11_t6" reading_order_no="5" segment_no="1" tag_type="text">Heuristic methods employing search trees have been developed, too. The methods build an AND/OR</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p11_t7" reading_order_no="6" segment_no="1" tag_type="text">tree of reachable belief states from the current belief and perform a search over the tree using well-known</text>
<text top="182" left="79" width="157" height="9" font="font3" id="p11_t8" reading_order_no="7" segment_no="1" tag_type="text">methods such as branch-and-bound.</text>
<text top="194" left="94" width="438" height="9" font="font3" id="p11_t9" reading_order_no="8" segment_no="2" tag_type="text">Value iteration is used very commonly since the classical policy iteration algorithm for POMDP</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p11_t10" reading_order_no="9" segment_no="2" tag_type="text">proposed in [8] has a high complexity and is thus less popular. There are, however, algorithms of lower<a href="deeplearning_paper3.html#43">[8] </a>has a high complexity and is thus less popular. There are, however, algorithms of lower</text>
<text top="218" left="79" width="453" height="9" font="font3" id="p11_t11" reading_order_no="10" segment_no="2" tag_type="text">complexity. One can also use the policy iteration methods as it has been described in Section 9.2.2 for<a href="deeplearning_paper3.html#8">9.2.2 </a>for</text>
<text top="230" left="79" width="117" height="9" font="font3" id="p11_t12" reading_order_no="11" segment_no="2" tag_type="text">solving POMDP problems.</text>
<text top="256" left="79" width="28" height="11" font="font15" id="p11_t13" reading_order_no="12" segment_no="3" tag_type="title">9.2.4</text>
<text top="256" left="120" width="199" height="11" font="font15" id="p11_t14" reading_order_no="13" segment_no="3" tag_type="title">Q-learning and SARSA algorithm</text>
<text top="276" left="79" width="453" height="9" font="font3" id="p11_t15" reading_order_no="14" segment_no="4" tag_type="text">Q-learning and the current State, current Action, next Reward, next State and next Action (SARSA)</text>
<text top="288" left="79" width="453" height="9" font="font3" id="p11_t16" reading_order_no="15" segment_no="4" tag_type="text">algorithms are iterative procedures to learn the optimal policy that maximize the expected reward from</text>
<text top="300" left="79" width="453" height="9" font="font3" id="p11_t17" reading_order_no="16" segment_no="4" tag_type="text">any starting state, and without the knowledge of the MPD dynamics. Both algorithms exhibit some</text>
<text top="312" left="79" width="453" height="9" font="font3" id="p11_t18" reading_order_no="17" segment_no="4" tag_type="text">similarities but differ in some key points that we detail hereafter. Q-learning and SARSA are called</text>
<text top="324" left="79" width="453" height="9" font="font3" id="p11_t19" reading_order_no="18" segment_no="4" tag_type="text">tabulated algorithms, i.e. they are based on the construction of a look-up table, a.k.a Q-table, that</text>
<text top="335" left="79" width="453" height="9" font="font3" id="p11_t20" reading_order_no="19" segment_no="4" tag_type="text">allows the algorithm to trace and update the expected action-value function for all action-state pairs</text>
<text top="347" left="79" width="453" height="9" font="font3" id="p11_t21" reading_order_no="20" segment_no="4" tag_type="text">( s, a ) ∈ S × A . Once the Q-table has been built, the optimal policy is to choose the action with the</text>
<text top="359" left="79" width="453" height="9" font="font3" id="p11_t22" reading_order_no="21" segment_no="4" tag_type="text">highest score. The basic idea of both algorithms is to build a new estimate from an old estimate, which</text>
<text top="371" left="79" width="453" height="9" font="font3" id="p11_t23" reading_order_no="22" segment_no="4" tag_type="text">is updated by an incremental difference between a target and the old estimate. This can be formalized</text>
<text top="383" left="79" width="44" height="9" font="font3" id="p11_t24" reading_order_no="23" segment_no="4" tag_type="text">as follows:</text>
<text top="408" left="176" width="59" height="9" font="font7" id="p11_t25" reading_order_no="24" segment_no="5" tag_type="formula">q t ( S ( t ) , A ( t ))</text>
<text top="421" left="176" width="4" height="4" font="font13" id="p11_t26" reading_order_no="25" segment_no="5" tag_type="formula">|</text>
<text top="421" left="201" width="9" height="4" font="font13" id="p11_t27" reading_order_no="26" segment_no="5" tag_type="formula">{z</text>
<text top="421" left="182" width="52" height="11" font="font13" id="p11_t28" reading_order_no="27" segment_no="5" tag_type="formula">} new estimate</text>
<text top="407" left="237" width="72" height="10" font="font11" id="p11_t29" reading_order_no="28" segment_no="5" tag_type="formula">← q t ( S ( t ) , A ( t ))</text>
<text top="421" left="250" width="4" height="4" font="font13" id="p11_t30" reading_order_no="29" segment_no="5" tag_type="formula">|</text>
<text top="421" left="275" width="9" height="4" font="font13" id="p11_t31" reading_order_no="30" segment_no="5" tag_type="formula">{z</text>
<text top="421" left="258" width="50" height="11" font="font13" id="p11_t32" reading_order_no="31" segment_no="5" tag_type="formula">} old estimate</text>
<text top="408" left="310" width="17" height="9" font="font3" id="p11_t33" reading_order_no="32" segment_no="5" tag_type="formula">+ α t</text>
<text top="391" left="330" width="7" height="4" font="font13" id="p11_t34" reading_order_no="33" segment_no="5" tag_type="formula"></text>
<text top="408" left="330" width="26" height="11" font="font13" id="p11_t35" reading_order_no="34" segment_no="5" tag_type="formula">  T t +1</text>
<text top="421" left="337" width="19" height="4" font="font13" id="p11_t36" reading_order_no="35" segment_no="5" tag_type="formula">| {z }</text>
<text top="425" left="336" width="21" height="6" font="font9" id="p11_t37" reading_order_no="36" segment_no="5" tag_type="formula">target</text>
<text top="407" left="359" width="68" height="10" font="font11" id="p11_t38" reading_order_no="37" segment_no="5" tag_type="formula">− q t ( S ( t ) , A ( t ))</text>
<text top="421" left="368" width="4" height="4" font="font13" id="p11_t39" reading_order_no="38" segment_no="5" tag_type="formula">|</text>
<text top="421" left="393" width="9" height="4" font="font13" id="p11_t40" reading_order_no="39" segment_no="5" tag_type="formula">{z</text>
<text top="421" left="376" width="50" height="11" font="font13" id="p11_t41" reading_order_no="40" segment_no="5" tag_type="formula">} old estimate</text>
<text top="391" left="427" width="7" height="4" font="font13" id="p11_t42" reading_order_no="41" segment_no="5" tag_type="formula"></text>
<text top="408" left="427" width="11" height="11" font="font13" id="p11_t43" reading_order_no="42" segment_no="5" tag_type="formula">  ,</text>
<text top="408" left="507" width="25" height="9" font="font3" id="p11_t44" reading_order_no="43" segment_no="5" tag_type="text">(9.18)</text>
<text top="439" left="79" width="454" height="10" font="font3" id="p11_t45" reading_order_no="44" segment_no="6" tag_type="text">where α t is the learning rate at time t which is a scalar between 0 and 1. The learning rate tells us how</text>
<text top="451" left="79" width="453" height="9" font="font3" id="p11_t46" reading_order_no="45" segment_no="6" tag_type="text">much we want to explore something new and how much we want to exploit the current choice. Indeed, in</text>
<text top="463" left="79" width="453" height="9" font="font3" id="p11_t47" reading_order_no="46" segment_no="6" tag_type="text">each of the reinforcement learning algorithms, exploitation and exploration have to be balanced in order</text>
<text top="475" left="79" width="453" height="9" font="font3" id="p11_t48" reading_order_no="47" segment_no="6" tag_type="text">to trade-off between the desire for the agent to increase its immediate reward, i.e. exploiting actions that</text>
<text top="487" left="79" width="453" height="9" font="font3" id="p11_t49" reading_order_no="48" segment_no="6" tag_type="text">gave good results so far, and the need to explore new combinations to discover strategies that may lead to</text>
<text top="499" left="79" width="453" height="9" font="font3" id="p11_t50" reading_order_no="49" segment_no="6" tag_type="text">larger rewards in the future, this is the exploration. At the beginning of the procedure, one may expect</text>
<text top="511" left="79" width="453" height="9" font="font3" id="p11_t51" reading_order_no="50" segment_no="6" tag_type="text">to spend more time to explore while the Q-table fills up then the agent exploits more than it explores in</text>
<text top="520" left="79" width="394" height="12" font="font3" id="p11_t52" reading_order_no="51" segment_no="6" tag_type="text">order to increase its reward. The learning rate should satisfy the following conditions P ∞</text>
<text top="523" left="465" width="68" height="11" font="font7" id="p11_t53" reading_order_no="52" segment_no="6" tag_type="text">t =0 α t = ∞ and</text>
<text top="532" left="79" width="19" height="7" font="font13" id="p11_t54" reading_order_no="53" segment_no="6" tag_type="text">P ∞</text>
<text top="534" left="90" width="442" height="12" font="font7" id="p11_t55" reading_order_no="54" segment_no="6" tag_type="text">t =0 α 2 t &lt; ∞ , e.g. α t = 1 / ( t + 1). Finally, note that α can also be taken as a constant less than one,</text>
<text top="547" left="79" width="453" height="9" font="font3" id="p11_t56" reading_order_no="55" segment_no="6" tag_type="text">and hence not satisfying the conditions above. However in practice, learning tasks occur over a finite</text>
<text top="559" left="79" width="205" height="9" font="font3" id="p11_t57" reading_order_no="56" segment_no="6" tag_type="text">time horizon hence the conditions are satisfied.</text>
<text top="585" left="79" width="56" height="9" font="font6" id="p11_t58" reading_order_no="57" segment_no="7" tag_type="text">Q-learning.</text>
<text top="585" left="145" width="387" height="9" font="font3" id="p11_t59" reading_order_no="58" segment_no="7" tag_type="text">Q-learning has first been introduced by Watkins in 1989 [9]. In this algorithm, the target</text>
<text top="597" left="79" width="45" height="9" font="font3" id="p11_t60" reading_order_no="59" segment_no="7" tag_type="text">is equal to</text>
<text top="609" left="217" width="107" height="9" font="font7" id="p11_t61" reading_order_no="60" segment_no="8" tag_type="formula">T t +1 = R ( t + 1) + γ max</text>
<text top="616" left="305" width="19" height="8" font="font8" id="p11_t62" reading_order_no="61" segment_no="8" tag_type="formula">a 0 ∈A</text>
<text top="606" left="326" width="69" height="12" font="font7" id="p11_t63" reading_order_no="62" segment_no="8" tag_type="formula">q t ( S ( t + 1) , a 0 ) ,</text>
<text top="609" left="507" width="25" height="9" font="font3" id="p11_t64" reading_order_no="63" segment_no="8" tag_type="text">(9.19)</text>
<text top="631" left="79" width="453" height="9" font="font3" id="p11_t65" reading_order_no="64" segment_no="9" tag_type="text">which is nothing but the algorithmic form of the Bellman equation in (9.12). When the algorithm has</text>
<text top="643" left="79" width="454" height="9" font="font3" id="p11_t66" reading_order_no="65" segment_no="9" tag_type="text">converged, T t +1 should be equal to q t ( S ( t ) , A ( t )), nullifying the difference term in (9.18). Let us more</text>
<text top="654" left="79" width="231" height="9" font="font3" id="p11_t67" reading_order_no="66" segment_no="9" tag_type="text">closely examine how the Q-learning algorithm works.</text>
<text top="666" left="94" width="438" height="9" font="font3" id="p11_t68" reading_order_no="67" segment_no="10" tag_type="text">The Q-table is a table with the states along the rows and columns representing the different actions</text>
<text top="678" left="79" width="454" height="10" font="font3" id="p11_t69" reading_order_no="68" segment_no="10" tag_type="text">we can take. At time step t = 0, the Q-table is initialized to 0, i.e. q 0 ( s (0) , a (0)) = 0, ∀ ( s, a ) ∈ S × A .</text>
<text top="690" left="79" width="453" height="11" font="font3" id="p11_t70" reading_order_no="69" segment_no="10" tag_type="text">A starting state is chosen randomly with a certain probability, i.e. P [ S (0) = s ]. At the beginning, the</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p11_t71" reading_order_no="70" segment_no="10" tag_type="text">agent does not know the reward that each action will provide, hence it chooses one action randomly.</text>
<text top="712" left="79" width="454" height="11" font="font3" id="p11_t72" reading_order_no="71" segment_no="10" tag_type="text">This action leads to a new state s 0 and a reward r in a stochastic manner according to p ( r, s 0 | s, a ) if</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p11_t73" reading_order_no="72" segment_no="10" tag_type="text">the problem is stochastic or according to deterministic rules otherwise. The immediate reward the agent</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p11_t74" reading_order_no="73" segment_no="11" tag_type="formula">11</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p12_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="454" height="9" font="font3" id="p12_t2" reading_order_no="1" segment_no="1" tag_type="text">receives by taking the action a ( t ) at time t is the variable R ( t + 1) in (9.19). Then the algorithm looks</text>
<text top="120" left="79" width="453" height="11" font="font3" id="p12_t3" reading_order_no="2" segment_no="1" tag_type="text">at the line represented by the state S ( t + 1) = s 0 in the Q-table and chooses the value that is maximum</text>
<text top="132" left="79" width="454" height="12" font="font3" id="p12_t4" reading_order_no="3" segment_no="1" tag_type="text">in the line, i.e. this corresponds to the term max a 0 ∈A q t ( S ( t + 1) , a 0 ) in (9.19); among all possible actions</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p12_t5" reading_order_no="4" segment_no="1" tag_type="text">from the next state we end up at t + 1, i.e. S ( t + 1), one chooses the one that leads to the maximum</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p12_t6" reading_order_no="5" segment_no="1" tag_type="text">expected return. The difference in (9.18) acts as a gradient that allows to reinforce some actions in a</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p12_t7" reading_order_no="6" segment_no="1" tag_type="text">given state or on the contrary, to dissuade the agent to take some other actions being in a given state. By</text>
<text top="182" left="79" width="454" height="9" font="font3" id="p12_t8" reading_order_no="7" segment_no="1" tag_type="text">choosing the learning rate, α t = 1 / ( t + 1) for instance, the agent will pay less and less attention over time</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p12_t9" reading_order_no="8" segment_no="1" tag_type="text">to future expected returns in the update of the current estimation of the Q-table. If α = 0, the agent</text>
<text top="206" left="79" width="454" height="9" font="font3" id="p12_t10" reading_order_no="9" segment_no="1" tag_type="text">does not learn anymore while α = 1 means that the agent keeps an active learning behavior. Moreover,</text>
<text top="218" left="79" width="454" height="9" font="font7" id="p12_t11" reading_order_no="10" segment_no="1" tag_type="text">γ in (9.18) is reminded to be the discounting factor and it defines how much the agent cares about future</text>
<text top="230" left="79" width="453" height="9" font="font3" id="p12_t12" reading_order_no="11" segment_no="1" tag_type="text">rewards, i.e. the ones that will be obtained starting from S ( t + 1), compared to the immediate reward,<a href="deeplearning_paper3.html#11">(9.19). </a>Then the algorithm looks</text>
<text top="242" left="79" width="57" height="9" font="font3" id="p12_t13" reading_order_no="12" segment_no="1" tag_type="text">i.e. R ( t + 1).</text>
<text top="253" left="94" width="438" height="9" font="font3" id="p12_t14" reading_order_no="13" segment_no="2" tag_type="text">Q-learning is an algorithm that explores and exploits at the same time. But which policy should be</text>
<text top="265" left="79" width="453" height="9" font="font3" id="p12_t15" reading_order_no="14" segment_no="2" tag_type="text">followed when updating the Q-table, i.e. how are the actions chosen at each time step? Actually, and this</text>
<text top="277" left="79" width="454" height="9" font="font3" id="p12_t16" reading_order_no="15" segment_no="2" tag_type="text">is the strength of Q-learning, it does not (so much) matter for the algorithm convergence. The − greedy</text>
<text top="289" left="79" width="446" height="9" font="font3" id="p12_t17" reading_order_no="16" segment_no="2" tag_type="text">policy is, however, a widely used technique. It consists in randomly choosing an action with probability</text>
<text top="301" left="79" width="454" height="9" font="font3" id="p12_t18" reading_order_no="17" segment_no="2" tag_type="text">and the action that maximizes the current action-state value at time t with probability 1 − . Of course,</text>
<text top="313" left="87" width="445" height="9" font="font3" id="p12_t19" reading_order_no="18" segment_no="2" tag_type="text">can be kept constant or may vary during the learning in order to explore more at the beginning, i.e.</text>
<text top="325" left="87" width="178" height="9" font="font11" id="p12_t20" reading_order_no="19" segment_no="2" tag_type="text">≈ 1, and exploit more after a while, i.e.</text>
<text top="325" left="292" width="240" height="9" font="font3" id="p12_t21" reading_order_no="20" segment_no="2" tag_type="text">1. However, it has been shown that Q-learning makes</text>
<text top="337" left="79" width="454" height="10" font="font3" id="p12_t22" reading_order_no="21" segment_no="2" tag_type="text">the Q-table converge to q ∗ , and hence to the optimal policy, as soon as every action-state pair has been</text>
<text top="349" left="79" width="453" height="9" font="font3" id="p12_t23" reading_order_no="22" segment_no="2" tag_type="text">visited an infinite number of times, irrespective to the policy followed during the training [10, 11]. This</text>
<text top="361" left="79" width="224" height="9" font="font3" id="p12_t24" reading_order_no="23" segment_no="2" tag_type="text">property makes Q-learning an off-policy procedure.</text>
<text top="373" left="94" width="438" height="9" font="font3" id="p12_t25" reading_order_no="24" segment_no="3" tag_type="text">In some practical problems, MDP may present some terminal states, i.e. absorbing states, and the</text>
<text top="385" left="79" width="453" height="9" font="font3" id="p12_t26" reading_order_no="25" segment_no="3" tag_type="text">learning is done over several episodes. Once the agent reaches the terminal state the episode ends and</text>
<text top="397" left="79" width="353" height="9" font="font3" id="p12_t27" reading_order_no="26" segment_no="3" tag_type="text">the algorithm restarts at a random state and keep going to estimate the Q-table.</text>
<text top="423" left="79" width="93" height="9" font="font6" id="p12_t28" reading_order_no="27" segment_no="4" tag_type="text">SARSA algorithm.</text>
<text top="423" left="183" width="350" height="9" font="font3" id="p12_t29" reading_order_no="28" segment_no="4" tag_type="text">SARSA is an algorithm that has been proposed in [12] and differs from Q-learning</text>
<text top="435" left="79" width="261" height="9" font="font3" id="p12_t30" reading_order_no="29" segment_no="4" tag_type="text">simply by the target definition in (9.18). In SARSA, we use</text>
<text top="456" left="214" width="184" height="9" font="font7" id="p12_t31" reading_order_no="30" segment_no="5" tag_type="formula">T t +1 = R ( t + 1) + γq t ( S ( t + 1) , A ( t + 1)) .</text>
<text top="456" left="507" width="25" height="9" font="font3" id="p12_t32" reading_order_no="31" segment_no="5" tag_type="text">(9.20)</text>
<text top="477" left="79" width="453" height="9" font="font3" id="p12_t33" reading_order_no="32" segment_no="6" tag_type="text">The difference with the Q-learning approach is that the next action A ( t + 1) to be taken when the agent</text>
<text top="489" left="79" width="454" height="9" font="font3" id="p12_t34" reading_order_no="33" segment_no="6" tag_type="text">observes the next state S ( t + 1) starting from the state S ( t ) and having taken the action A ( t ), is no<a href="deeplearning_paper3.html#11">(9.19); </a>among all possible actions</text>
<text top="501" left="79" width="454" height="9" font="font3" id="p12_t35" reading_order_no="34" segment_no="6" tag_type="text">longer the one that maximizes the next expected return from the next state S ( t + 1). The action A ( t + 1)</text>
<text top="513" left="79" width="454" height="9" font="font3" id="p12_t36" reading_order_no="35" segment_no="6" tag_type="text">has to be taken according to a policy, this is why SARSA is called an on-policy method. The policy the</text>
<text top="525" left="79" width="454" height="9" font="font3" id="p12_t37" reading_order_no="36" segment_no="6" tag_type="text">agent follows when being in state S ( t ), hence the choice of the action at time t , i.e. A ( t ), is the behavior</text>
<text top="537" left="79" width="454" height="9" font="font5" id="p12_t38" reading_order_no="37" segment_no="6" tag_type="text">policy while the choice of the action to be taken when being in state S ( t + 1), i.e. A ( t + 1), characterizes</text>
<text top="549" left="79" width="454" height="9" font="font3" id="p12_t39" reading_order_no="38" segment_no="6" tag_type="text">the target policy . In Q-learning, the target policy was greedy w.r.t. q t , i.e. the action that maximizes</text>
<text top="561" left="79" width="453" height="9" font="font3" id="p12_t40" reading_order_no="39" segment_no="6" tag_type="text">the next expected action-state value is chosen. In SARSA on the other hand, the agent updates the Q-</text>
<text top="572" left="79" width="454" height="9" font="font3" id="p12_t41" reading_order_no="40" segment_no="6" tag_type="text">table from the quintuple ( S ( t ) , A ( t ) , R ( t + 1) , S ( t + 1) , A ( t + 1)) where both behavior and target policies</text>
<text top="584" left="79" width="13" height="9" font="font3" id="p12_t42" reading_order_no="41" segment_no="6" tag_type="text">are<a href="deeplearning_paper3.html#11">(9.18) </a>acts as a gradient that allows to reinforce some actions in a</text>
<text top="584" left="101" width="432" height="9" font="font11" id="p12_t43" reading_order_no="42" segment_no="6" tag_type="text">− greedy, i.e. the next action to be taken while observing state S ( t + 1) is chosen randomly with</text>
<text top="596" left="79" width="65" height="9" font="font3" id="p12_t44" reading_order_no="43" segment_no="6" tag_type="text">the probability</text>
<text top="594" left="156" width="377" height="12" font="font3" id="p12_t45" reading_order_no="44" segment_no="6" tag_type="text">and the action maximizing q t ( S ( t + 1) , a 0 ) is taken with probability 1 − . Under the</text>
<text top="608" left="79" width="453" height="9" font="font3" id="p12_t46" reading_order_no="45" segment_no="6" tag_type="text">assumption that the actions taken under the target policy are, at least occasionally, also taken under the</text>
<text top="620" left="79" width="386" height="10" font="font3" id="p12_t47" reading_order_no="46" segment_no="6" tag_type="text">behavior policy, SARSA converges to the optimal policy, i.e. the greedy policy w.r.t. q ∗ .</text>
<text top="647" left="79" width="28" height="11" font="font15" id="p12_t48" reading_order_no="47" segment_no="7" tag_type="title">9.2.5</text>
<text top="647" left="120" width="53" height="11" font="font15" id="p12_t49" reading_order_no="48" segment_no="7" tag_type="title">Deep RL</text>
<text top="666" left="79" width="454" height="9" font="font3" id="p12_t50" reading_order_no="49" segment_no="8" tag_type="text">The previous Q -learning or SARSA approaches are suitable when the dimensions of the state and action</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p12_t51" reading_order_no="50" segment_no="8" tag_type="text">spaces of the problem are small or moderate. In that case, a look-up table can be used to update the</text>
<text top="690" left="79" width="453" height="9" font="font7" id="p12_t52" reading_order_no="51" segment_no="8" tag_type="text">Q values. However, when the number of states or possible actions becomes large, the complexity and</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p12_t53" reading_order_no="52" segment_no="8" tag_type="text">the storage needs of Q -learning become prohibitive. Instead of using a table for updating the action-</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p12_t54" reading_order_no="53" segment_no="8" tag_type="text">state value function, one may search for approximating the action-state values with a suitable function</text>
<text top="726" left="79" width="453" height="11" font="font7" id="p12_t55" reading_order_no="54" segment_no="8" tag_type="text">q θ : S × A → R with the vector parameter θ . The simplest approximation function is the linear one. This</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p12_t56" reading_order_no="55" segment_no="9" tag_type="text">12</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="252" height="9" font="font3" id="p13_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="108" left="79" width="454" height="12" font="font3" id="p13_t2" reading_order_no="1" segment_no="1" tag_type="text">consists of finding a suitable mapping, ψ : S × A → R d , used to represent the features of the action-state</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p13_t3" reading_order_no="2" segment_no="1" tag_type="text">couple, where d is the dimension of this feature representation [6]. The entries of the vector ψ ( s, a ) are the</text>
<text top="134" left="79" width="454" height="9" font="font5" id="p13_t4" reading_order_no="3" segment_no="1" tag_type="text">basis functions and they span the space in which the Q function is approached. The linear approximation</text>
<text top="143" left="79" width="453" height="12" font="font3" id="p13_t5" reading_order_no="4" segment_no="1" tag_type="text">of the action-value function is hence q θ ( s, a ) = θ T ψ ( s, a ) and will be used to approximate the optimal</text>
<text top="158" left="79" width="79" height="9" font="font3" id="p13_t6" reading_order_no="5" segment_no="1" tag_type="text">action-state value.</text>
<text top="170" left="94" width="439" height="9" font="font3" id="p13_t7" reading_order_no="6" segment_no="2" tag_type="text">If an oracle would give us the action-state function under the policy π , one could compute an error</text>
<text top="182" left="79" width="234" height="9" font="font3" id="p13_t8" reading_order_no="7" segment_no="2" tag_type="text">function between a target and a prediction as follows:</text>
<text top="209" left="220" width="45" height="11" font="font7" id="p13_t9" reading_order_no="8" segment_no="3" tag_type="formula">L ( θ ) = E π</text>
<text top="207" left="278" width="94" height="12" font="font7" id="p13_t10" reading_order_no="9" segment_no="3" tag_type="formula">q π ( S, A ) − θ T ψ ( S, A )</text>
<text top="202" left="378" width="4" height="6" font="font9" id="p13_t11" reading_order_no="10" segment_no="3" tag_type="formula">2</text>
<text top="209" left="389" width="3" height="9" font="font7" id="p13_t12" reading_order_no="11" segment_no="3" tag_type="formula">,</text>
<text top="209" left="507" width="25" height="9" font="font3" id="p13_t13" reading_order_no="12" segment_no="3" tag_type="text">(9.21)<a href="deeplearning_paper3.html#43">[6]. </a>The entries of the vector</text>
<text top="237" left="79" width="454" height="9" font="font3" id="p13_t14" reading_order_no="13" segment_no="4" tag_type="text">where the expectation is taken over the joint distribution of the state, reward and action. The weights θ</text>
<text top="249" left="79" width="271" height="9" font="font3" id="p13_t15" reading_order_no="14" segment_no="4" tag_type="text">can be updated using the gradient of the loss function such as</text>
<text top="273" left="190" width="76" height="11" font="font17" id="p13_t16" reading_order_no="15" segment_no="5" tag_type="formula">θ t +1 = θ t + α t E π</text>
<text top="269" left="268" width="104" height="14" font="font13" id="p13_t17" reading_order_no="16" segment_no="5" tag_type="formula">h q π ( S, A ) − θ T ψ ( S, A )</text>
<text top="269" left="380" width="43" height="13" font="font17" id="p13_t18" reading_order_no="17" segment_no="5" tag_type="formula">ψ ( S, A ) i .</text>
<text top="273" left="507" width="25" height="9" font="font3" id="p13_t19" reading_order_no="18" segment_no="5" tag_type="text">(9.22)</text>
<text top="298" left="79" width="453" height="9" font="font3" id="p13_t20" reading_order_no="19" segment_no="6" tag_type="text">However, the agent never knows in general the true value of the objective, i.e. the true action-state value</text>
<text top="310" left="79" width="454" height="9" font="font3" id="p13_t21" reading_order_no="20" segment_no="6" tag_type="text">function under the policy π . It can only estimate this value. For the Q-learning algorithm, q π ( S, A ) in</text>
<text top="322" left="79" width="453" height="9" font="font3" id="p13_t22" reading_order_no="21" segment_no="6" tag_type="text">(9.22) is substituted by expression in (9.19), where the estimation of the action-state function is replaced</text>
<text top="334" left="79" width="163" height="9" font="font3" id="p13_t23" reading_order_no="22" segment_no="6" tag_type="text">by its linear approximation such that</text>
<text top="361" left="142" width="64" height="10" font="font17" id="p13_t24" reading_order_no="23" segment_no="7" tag_type="formula">θ t +1 = θ t + α t</text>
<text top="361" left="215" width="75" height="9" font="font7" id="p13_t25" reading_order_no="24" segment_no="7" tag_type="formula">R ( t + 1) + γ max</text>
<text top="369" left="271" width="19" height="7" font="font8" id="p13_t26" reading_order_no="25" segment_no="7" tag_type="formula">a 0 ∈A</text>
<text top="359" left="291" width="134" height="11" font="font17" id="p13_t27" reading_order_no="26" segment_no="7" tag_type="formula">θ T ψ ( S ( t + 1) , a 0 ) − θ T ψ ( S, A )</text>
<text top="361" left="434" width="36" height="9" font="font17" id="p13_t28" reading_order_no="27" segment_no="7" tag_type="formula">ψ ( S, A ) .</text>
<text top="361" left="507" width="25" height="9" font="font3" id="p13_t29" reading_order_no="28" segment_no="7" tag_type="text">(9.23)</text>
<text top="389" left="79" width="454" height="9" font="font3" id="p13_t30" reading_order_no="29" segment_no="8" tag_type="text">However, the features extraction phase, i.e. constructing the function ψ , can be very complex if the</text>
<text top="401" left="79" width="155" height="9" font="font3" id="p13_t31" reading_order_no="30" segment_no="8" tag_type="text">problem dimension is very large [6].</text>
<text top="413" left="94" width="438" height="9" font="font3" id="p13_t32" reading_order_no="31" segment_no="9" tag_type="text">One may observe that when passing from (9.22) to (9.23), we did not limit ourselves to substitute</text>
<text top="422" left="79" width="454" height="13" font="font7" id="p13_t33" reading_order_no="32" segment_no="9" tag_type="text">q π ( S, A ) by R ( t + 1) + γ max a 0 ∈A θ T ψ ( S ( t + 1) , a 0 ) but the expectation operator also vanished. There</text>
<text top="436" left="79" width="453" height="12" font="font3" id="p13_t34" reading_order_no="33" segment_no="9" tag_type="text">are only random samples from a given database (a batch) to compute the sequence { θ } t . The latter is</text>
<text top="449" left="79" width="453" height="9" font="font3" id="p13_t35" reading_order_no="34" segment_no="9" tag_type="text">a random sequence and hence we are not sure that this will decrease the value of the loss at each step.</text>
<text top="461" left="79" width="454" height="9" font="font3" id="p13_t36" reading_order_no="35" segment_no="9" tag_type="text">However, one can show that this will decrease the loss in average . This is the principle of the stochastic</text>
<text top="473" left="79" width="93" height="9" font="font3" id="p13_t37" reading_order_no="36" segment_no="9" tag_type="text">gradient descent [13].</text>
<text top="499" left="79" width="86" height="9" font="font6" id="p13_t38" reading_order_no="37" segment_no="10" tag_type="text">Deep Q-learning.</text>
<text top="499" left="175" width="357" height="9" font="font3" id="p13_t39" reading_order_no="38" segment_no="10" tag_type="text">The approximation function can be non-linear, but has to be differentiable w.r.t.</text>
<text top="511" left="79" width="453" height="9" font="font3" id="p13_t40" reading_order_no="39" segment_no="10" tag_type="text">the parameters of the approximation function. The principle of the deep Q-learning is simple and consists</text>
<text top="523" left="79" width="453" height="9" font="font3" id="p13_t41" reading_order_no="40" segment_no="10" tag_type="text">of designing a neural network that outputs all the action-state values for all possible actions in a given</text>
<text top="534" left="79" width="454" height="10" font="font3" id="p13_t42" reading_order_no="41" segment_no="10" tag_type="text">state s , q θ ( s, · ), ∀ s ∈ S . In other words, if the state space is of dimension n , i.e. each state is represented</text>
<text top="547" left="79" width="453" height="9" font="font3" id="p13_t43" reading_order_no="42" segment_no="10" tag_type="text">by a vector of n components, and if there are m possible actions for each state, the neural network is</text>
<text top="557" left="79" width="453" height="12" font="font3" id="p13_t44" reading_order_no="43" segment_no="10" tag_type="text">a mapping from R n to R m . The idea to use a deep neural network to approximate the Q-function in a</text>
<text top="571" left="79" width="453" height="9" font="font3" id="p13_t45" reading_order_no="44" segment_no="10" tag_type="text">reinforcement learning setting has been first introduced by Mnih et al. in [14], where authors proposed</text>
<text top="583" left="79" width="309" height="9" font="font3" id="p13_t46" reading_order_no="45" segment_no="10" tag_type="text">to use a deep convolution neural network to learn to play Atari games.</text>
<text top="595" left="94" width="438" height="9" font="font3" id="p13_t47" reading_order_no="46" segment_no="11" tag_type="text">We will not discuss here in details the different neural networks and deep learning in general and</text>
<text top="607" left="79" width="453" height="9" font="font3" id="p13_t48" reading_order_no="47" segment_no="11" tag_type="text">the interested reader may refer to some reference books dealing with neural networks and the basics of</text>
<text top="619" left="79" width="453" height="9" font="font3" id="p13_t49" reading_order_no="48" segment_no="11" tag_type="text">deep learning, such as the book of Courville, Goodfellow and Bengio [1]. An ANN is made of (artificial)</text>
<text top="631" left="79" width="453" height="9" font="font3" id="p13_t50" reading_order_no="49" segment_no="11" tag_type="text">neurons that are linked through several layers. There exists several kind of neural networks, such as</text>
<text top="643" left="79" width="453" height="9" font="font3" id="p13_t51" reading_order_no="50" segment_no="11" tag_type="text">the feedforward neural network (FNN), the recurrent neural network, convolutional neural network as</text>
<text top="654" left="79" width="453" height="9" font="font3" id="p13_t52" reading_order_no="51" segment_no="11" tag_type="text">mentioned above and many others (cf. for instance [3] for a classification of ANN). In the following, we</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p13_t53" reading_order_no="52" segment_no="11" tag_type="text">will just refer to FNN. The main ingredients of an FNN in our case are an input layer that accepts the</text>
<text top="678" left="79" width="454" height="9" font="font3" id="p13_t54" reading_order_no="53" segment_no="11" tag_type="text">states of the decision process, or more specifically, a representation of the states, an output layer that</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p13_t55" reading_order_no="54" segment_no="11" tag_type="text">returns the estimated action-state values for all actions for a given state at the input and several hidden</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p13_t56" reading_order_no="55" segment_no="11" tag_type="text">layers between both as illustrated on Fig. 9.3 where two hidden layers has been considered. The input</text>
<text top="713" left="79" width="453" height="12" font="font3" id="p13_t57" reading_order_no="56" segment_no="11" tag_type="text">layer is made of n 0 neurons, and takes the samples s 0 ∈ R n as an entry. The layer ` = 1 , · · · , L has</text>
<text top="726" left="79" width="453" height="10" font="font7" id="p13_t58" reading_order_no="57" segment_no="11" tag_type="text">n ` neurons. The L + 1 layer is the output layer, and has n L +1 neurons. It outputs the approximation</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p13_t59" reading_order_no="58" segment_no="12" tag_type="text">13</text>
</page>
<page number="14" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font26" size="7" family="CMBX7" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p14_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="230" left="155" width="48" height="9" font="font3" id="p14_t2" reading_order_no="4" segment_no="1" tag_type="figure">Input layer</text>
<text top="136" left="134" width="8" height="9" font="font7" id="p14_t3" reading_order_no="1" segment_no="1" tag_type="figure">s 1</text>
<text top="170" left="134" width="8" height="9" font="font7" id="p14_t4" reading_order_no="2" segment_no="1" tag_type="figure">s 2</text>
<text top="204" left="134" width="8" height="9" font="font7" id="p14_t5" reading_order_no="3" segment_no="1" tag_type="figure">s 3</text>
<text top="230" left="234" width="33" height="9" font="font3" id="p14_t6" reading_order_no="5" segment_no="1" tag_type="figure">Layer 1</text>
<text top="230" left="305" width="33" height="9" font="font3" id="p14_t7" reading_order_no="6" segment_no="1" tag_type="figure">Layer 2</text>
<text top="230" left="364" width="56" height="9" font="font3" id="p14_t8" reading_order_no="7" segment_no="1" tag_type="figure">Output layer</text>
<text top="142" left="433" width="45" height="10" font="font7" id="p14_t9" reading_order_no="8" segment_no="1" tag_type="figure">q Θ ( s 0 , a 1 )</text>
<text top="176" left="433" width="45" height="10" font="font7" id="p14_t10" reading_order_no="9" segment_no="1" tag_type="figure">q Θ ( s 0 , a 2 )</text>
<text top="267" left="219" width="173" height="9" font="font3" id="p14_t11" reading_order_no="10" segment_no="2" tag_type="text">Figure 9.3: Feedforward neural network</text>
<text top="299" left="79" width="454" height="10" font="font3" id="p14_t12" reading_order_no="11" segment_no="3" tag_type="text">of the Q-function, for the entry s 0 , i.e. q Θ ( s 0 , a i ) of each action a i , i = 1 , · · · , n L +1 . Θ represents the</text>
<text top="311" left="79" width="288" height="9" font="font3" id="p14_t13" reading_order_no="12" segment_no="3" tag_type="text">parameters of the neural network that will be explained hereafter.</text>
<text top="322" left="94" width="302" height="12" font="font3" id="p14_t14" reading_order_no="13" segment_no="4" tag_type="text">For all ` = 1 , · · · , L + 1, the vector output at the layer ` , s ` ∈ R n ` , is</text>
<text top="345" left="261" width="91" height="11" font="font17" id="p14_t15" reading_order_no="14" segment_no="5" tag_type="formula">s ` = f ` ( θ ` s ` − 1 + b ` )</text>
<text top="345" left="507" width="25" height="9" font="font3" id="p14_t16" reading_order_no="15" segment_no="5" tag_type="text">(9.24)</text>
<text top="365" left="79" width="452" height="13" font="font3" id="p14_t17" reading_order_no="16" segment_no="6" tag_type="text">where θ ` ∈ R n ` − 1 × n ` is the matrix of weights between the neurons of layer ` − 1 and the layer ` , i.e. θ ` i,j</text>
<text top="379" left="79" width="454" height="11" font="font3" id="p14_t18" reading_order_no="17" segment_no="6" tag_type="text">is the weight between the i − th neuron of layer ` − 1 and the j − th neuron of layer ` , s ` − 1 is the signal</text>
<text top="391" left="79" width="454" height="10" font="font3" id="p14_t19" reading_order_no="18" segment_no="6" tag_type="text">output of the layer ` − 1. Moreover, f ` is the activation function at the layer ` whose objective is to</text>
<text top="403" left="79" width="453" height="9" font="font3" id="p14_t20" reading_order_no="19" segment_no="6" tag_type="text">perform a non-linear operation on the linear combination of the signal output of the previous layer. The</text>
<text top="415" left="79" width="453" height="9" font="font3" id="p14_t21" reading_order_no="20" segment_no="6" tag_type="text">function applies at each neuron of layer ` , i.e. the function applies to each entry of the vector in the</text>
<text top="427" left="79" width="454" height="10" font="font3" id="p14_t22" reading_order_no="21" segment_no="6" tag_type="text">argument of f ` in (9.24). The activation function aims at enabling the signal output. It exists several</text>
<text top="439" left="79" width="453" height="9" font="font3" id="p14_t23" reading_order_no="22" segment_no="6" tag_type="text">kind of activation functions, e.g. sigmoidal, hyperbolic tangent, ReLU (Rectified Linear Unit) and ELU</text>
<text top="451" left="79" width="453" height="10" font="font3" id="p14_t24" reading_order_no="23" segment_no="6" tag_type="text">(exponential linear unit), the reader may refer to [2] for a more detailed description. Finally, b ` is the</text>
<text top="463" left="79" width="454" height="9" font="font3" id="p14_t25" reading_order_no="24" segment_no="6" tag_type="text">bias term of the neurons of the layer ` that can change the threshold of the input signal value at which</text>
<text top="475" left="79" width="453" height="9" font="font3" id="p14_t26" reading_order_no="25" segment_no="6" tag_type="text">the neurons enable the output signal. The parameters of the FNN is made of the succession of matrix</text>
<text top="484" left="79" width="324" height="13" font="font3" id="p14_t27" reading_order_no="26" segment_no="6" tag_type="text">weights and bias terms between each layers and is denoted: Θ = { θ ` , b ` } L</text>
<text top="487" left="398" width="17" height="11" font="font3" id="p14_t28" reading_order_no="27" segment_no="6" tag_type="text">` =1 .</text>
<text top="499" left="94" width="438" height="9" font="font3" id="p14_t29" reading_order_no="28" segment_no="7" tag_type="text">In order to make a neural network learn, a loss function should be defined between a target, i.e. a</text>
<text top="511" left="79" width="454" height="9" font="font3" id="p14_t30" reading_order_no="29" segment_no="7" tag_type="text">desired output, and the actual output obtained at iteration t . It can be defined as in (9.21) by using the</text>
<text top="523" left="79" width="138" height="9" font="font3" id="p14_t31" reading_order_no="30" segment_no="7" tag_type="text">target in Q-learning, i.e. (9.19):</text>
<text top="551" left="134" width="81" height="10" font="font7" id="p14_t32" reading_order_no="31" segment_no="8" tag_type="formula">L t ( Θ t ) = E S,A,R,S 0</text>
<text top="551" left="229" width="75" height="9" font="font7" id="p14_t33" reading_order_no="32" segment_no="8" tag_type="formula">R ( t + 1) + γ max</text>
<text top="559" left="291" width="6" height="6" font="font8" id="p14_t34" reading_order_no="33" segment_no="8" tag_type="formula">a 0</text>
<text top="551" left="305" width="17" height="12" font="font7" id="p14_t35" reading_order_no="34" segment_no="8" tag_type="formula">q Θ − t</text>
<text top="548" left="325" width="134" height="13" font="font3" id="p14_t36" reading_order_no="35" segment_no="8" tag_type="formula">( S ( t + 1) , a 0 ) − q Θ t ( S ( t ) , A ( t ))</text>
<text top="544" left="464" width="4" height="6" font="font9" id="p14_t37" reading_order_no="36" segment_no="8" tag_type="formula">2</text>
<text top="551" left="476" width="3" height="9" font="font7" id="p14_t38" reading_order_no="37" segment_no="8" tag_type="formula">,</text>
<text top="551" left="507" width="25" height="9" font="font3" id="p14_t39" reading_order_no="38" segment_no="8" tag_type="text">(9.25)</text>
<text top="577" left="79" width="244" height="13" font="font3" id="p14_t40" reading_order_no="39" segment_no="9" tag_type="text">where Θ t is the set of parameters at iteration t and Θ −</text>
<text top="585" left="317" width="3" height="6" font="font8" id="p14_t41" reading_order_no="40" segment_no="9" tag_type="text">t</text>
<text top="580" left="327" width="206" height="9" font="font3" id="p14_t42" reading_order_no="41" segment_no="9" tag_type="text">is the network parameters used to compute the</text>
<text top="592" left="79" width="454" height="9" font="font3" id="p14_t43" reading_order_no="42" segment_no="9" tag_type="text">target at iteration t . Deriving the loss function w.r.t. the network parameters we obtained a generalized</text>
<text top="604" left="79" width="308" height="9" font="font3" id="p14_t44" reading_order_no="43" segment_no="9" tag_type="text">stochastic gradient descent for non linear Q-function approximation as</text>
<text top="634" left="89" width="70" height="10" font="font6" id="p14_t45" reading_order_no="44" segment_no="10" tag_type="formula">Θ t +1 = Θ t + α t</text>
<text top="634" left="169" width="74" height="9" font="font7" id="p14_t46" reading_order_no="45" segment_no="10" tag_type="formula">R ( t + 1) + γ max</text>
<text top="642" left="224" width="19" height="8" font="font8" id="p14_t47" reading_order_no="46" segment_no="10" tag_type="formula">a 0 ∈A</text>
<text top="634" left="245" width="16" height="13" font="font7" id="p14_t48" reading_order_no="47" segment_no="10" tag_type="formula">q Θ − t</text>
<text top="632" left="263" width="132" height="12" font="font3" id="p14_t49" reading_order_no="48" segment_no="10" tag_type="formula">( S ( t + 1) , a 0 ) − q Θ t ( S ( t ) , A ( t ))</text>
<text top="634" left="403" width="8" height="9" font="font11" id="p14_t50" reading_order_no="49" segment_no="10" tag_type="formula">×</text>
<text top="656" left="411" width="86" height="10" font="font11" id="p14_t51" reading_order_no="50" segment_no="10" tag_type="formula">∇ Θ t q Θ t ( S ( t ) , A ( t )) ,</text>
<text top="656" left="507" width="25" height="9" font="font3" id="p14_t52" reading_order_no="51" segment_no="10" tag_type="text">(9.26)</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p14_t53" reading_order_no="52" segment_no="11" tag_type="text">which decreases the expected loss under certain conditions. The gradient in (9.26) should be understood</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p14_t54" reading_order_no="53" segment_no="11" tag_type="text">as the gradient w.r.t. the weights keeping the bias constant and also the gradient w.r.t. the bias keeping</text>
<text top="702" left="79" width="92" height="9" font="font3" id="p14_t55" reading_order_no="54" segment_no="11" tag_type="text">the weights constant.</text>
<text top="712" left="94" width="439" height="11" font="font3" id="p14_t56" reading_order_no="55" segment_no="12" tag_type="text">However, this method may diverge if naively implemented. Indeed, if the samples ( s, a, r, s 0 ) are</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p14_t57" reading_order_no="56" segment_no="12" tag_type="text">obtained by sampling the Markov chain at each successive transition, the data given to the neural network</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p14_t58" reading_order_no="57" segment_no="13" tag_type="text">14</text>
</page>
<page number="15" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="252" height="9" font="font3" id="p15_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p15_t2" reading_order_no="1" segment_no="1" tag_type="text">will be correlated and non i.i.d and will be not ideal for training the neural network. A solution is to</text>
<text top="122" left="79" width="454" height="9" font="font3" id="p15_t3" reading_order_no="2" segment_no="1" tag_type="text">store the experience tuples ( s ( t ) , a ( t ) , r ( t + 1) , s ( t + 1)) into a memory pooled over many episodes (an</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p15_t4" reading_order_no="3" segment_no="1" tag_type="text">episode ends when a terminal state is reached) and to uniformly choose at each time instant a tuple from</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p15_t5" reading_order_no="4" segment_no="1" tag_type="text">the memory in order to update (9.26). This is what is called experience replay and it breaks correlation</text>
<text top="155" left="79" width="453" height="12" font="font3" id="p15_t6" reading_order_no="5" segment_no="1" tag_type="text">among the samples [14]. Moreover, if the target network Θ − , used to retrieve the Q-values, is updated</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p15_t7" reading_order_no="6" segment_no="1" tag_type="text">after each iteration with the new network computed at time t , then the policy may oscillate and data</text>
<text top="182" left="79" width="453" height="9" font="font3" id="p15_t8" reading_order_no="7" segment_no="1" tag_type="text">values may switch from an extreme to another and the parameters could diverge. The idea is to freeze</text>
<text top="191" left="79" width="454" height="12" font="font3" id="p15_t9" reading_order_no="8" segment_no="1" tag_type="text">the target network Θ − during a certain number of time steps T and to update the target network with</text>
<text top="206" left="79" width="400" height="9" font="font6" id="p15_t10" reading_order_no="9" segment_no="1" tag_type="text">Θ in (9.26) after T steps. This allows for reducing the oscillations or the risk of divergence.</text>
<text top="218" left="94" width="438" height="9" font="font3" id="p15_t11" reading_order_no="10" segment_no="2" tag_type="text">There is another unwanted effect of the Q-learning that is the overestimation of the action-state values.</text>
<text top="230" left="79" width="453" height="9" font="font3" id="p15_t12" reading_order_no="11" segment_no="2" tag_type="text">This bias leads the algorithm to perform poorly in some stochastic environments and comes from the</text>
<text top="242" left="79" width="453" height="9" font="font3" id="p15_t13" reading_order_no="12" segment_no="2" tag_type="text">fact that the max operator is used to estimate the maximum expected value. Indeed, the max operator</text>
<text top="254" left="79" width="454" height="9" font="font3" id="p15_t14" reading_order_no="13" segment_no="2" tag_type="text">in (9.19) aims at estimating the value in (9.7) for the next state S ( t + 1) which is an expectation. This</text>
<text top="266" left="79" width="453" height="9" font="font3" id="p15_t15" reading_order_no="14" segment_no="2" tag_type="text">method, often called the single estimator , has a positive bias that can be shown to follow from Jensen’s</text>
<text top="278" left="79" width="454" height="9" font="font3" id="p15_t16" reading_order_no="15" segment_no="2" tag_type="text">inequality. Van Hasselt proposed in [15] to use a double estimator technique to unbiase the estimation</text>
<text top="290" left="79" width="453" height="9" font="font3" id="p15_t17" reading_order_no="16" segment_no="2" tag_type="text">of the maximum expected value that occurs in the Bellman’s equation of the action-state value function.</text>
<text top="300" left="79" width="454" height="11" font="font3" id="p15_t18" reading_order_no="17" segment_no="2" tag_type="text">The idea is to create two sets of unbiased estimators w.r.t. the expectation, i.e. q A and q B , that will</text>
<text top="314" left="79" width="175" height="9" font="font3" id="p15_t19" reading_order_no="18" segment_no="2" tag_type="text">be applied on two sets of samples, i.e.</text>
<text top="313" left="263" width="269" height="10" font="font11" id="p15_t20" reading_order_no="19" segment_no="2" tag_type="text">A and B , such that the estimators are unbiased w.r.t. the</text>
<text top="324" left="79" width="454" height="11" font="font3" id="p15_t21" reading_order_no="20" segment_no="2" tag_type="text">mean on these samples. A and B contain the samples associated to the random variables q A ( S ( t ) , · ) and<a href="deeplearning_paper3.html#14">(9.26). </a>This is what is called</text>
<text top="336" left="79" width="454" height="11" font="font7" id="p15_t22" reading_order_no="21" segment_no="2" tag_type="text">q B ( S ( t ) , · ) respectively. The maximum expected value of the first estimator, i.e. q A is estimated with the</text>
<text top="348" left="79" width="453" height="11" font="font3" id="p15_t23" reading_order_no="22" segment_no="2" tag_type="text">max operator on the set of the experiences A , i.e. max a q A ( s 0 , a ) = q A ( s 0 , a ∗ ) as in the regular Q-learning</text>
<text top="360" left="79" width="453" height="11" font="font3" id="p15_t24" reading_order_no="23" segment_no="2" tag_type="text">algorithm. Then, we use the action a ∗ on the estimator of the Q-function on the set B , as an estimation<a href="deeplearning_paper3.html#44">[14]. </a>Moreover, if the target network</text>
<text top="372" left="79" width="454" height="12" font="font3" id="p15_t25" reading_order_no="24" segment_no="2" tag_type="text">of the maximum expected value of max a E q A ( S 0 , a ) . A similar update is performed with b ∗ on q B and</text>
<text top="384" left="79" width="430" height="11" font="font3" id="p15_t26" reading_order_no="25" segment_no="2" tag_type="text">using q A . The iterative system of equations in the double Q-learning are such as [15, Algorithm 1]</text>
<text top="407" left="114" width="11" height="11" font="font7" id="p15_t27" reading_order_no="26" segment_no="3" tag_type="formula">q A</text>
<text top="409" left="119" width="56" height="11" font="font3" id="p15_t28" reading_order_no="27" segment_no="3" tag_type="formula">t ( S ( t ) , A ( t ))</text>
<text top="408" left="184" width="10" height="9" font="font11" id="p15_t29" reading_order_no="28" segment_no="3" tag_type="formula">←</text>
<text top="407" left="204" width="11" height="11" font="font7" id="p15_t30" reading_order_no="29" segment_no="3" tag_type="formula">q A</text>
<text top="409" left="209" width="77" height="11" font="font3" id="p15_t31" reading_order_no="30" segment_no="3" tag_type="formula">t ( S ( t ) , A ( t )) + α t</text>
<text top="407" left="293" width="64" height="11" font="font7" id="p15_t32" reading_order_no="31" segment_no="3" tag_type="formula">R ( t + 1) + γq B</text>
<text top="407" left="351" width="87" height="13" font="font3" id="p15_t33" reading_order_no="32" segment_no="3" tag_type="formula">t ( S ( t + 1) , a ∗ ) − q A</text>
<text top="409" left="432" width="65" height="11" font="font3" id="p15_t34" reading_order_no="33" segment_no="3" tag_type="formula">t ( S ( t ) , A ( t )) ,</text>
<text top="423" left="114" width="11" height="11" font="font7" id="p15_t35" reading_order_no="34" segment_no="3" tag_type="formula">q B</text>
<text top="425" left="118" width="57" height="11" font="font3" id="p15_t36" reading_order_no="35" segment_no="3" tag_type="formula">t ( S ( t ) , A ( t ))</text>
<text top="425" left="184" width="10" height="9" font="font11" id="p15_t37" reading_order_no="36" segment_no="3" tag_type="formula">←</text>
<text top="423" left="204" width="11" height="11" font="font7" id="p15_t38" reading_order_no="37" segment_no="3" tag_type="formula">q B</text>
<text top="423" left="209" width="149" height="13" font="font3" id="p15_t39" reading_order_no="38" segment_no="3" tag_type="formula">t ( S ( t ) , A ( t )) + α t R ( t + 1) + γq A<a href="deeplearning_paper3.html#14">(9.26) </a>after</text>
<text top="423" left="351" width="86" height="13" font="font3" id="p15_t40" reading_order_no="39" segment_no="3" tag_type="formula">t ( S ( t + 1) , b ∗ ) − q B</text>
<text top="425" left="431" width="65" height="11" font="font3" id="p15_t41" reading_order_no="40" segment_no="3" tag_type="formula">t ( S ( t ) , A ( t )) .</text>
<text top="449" left="79" width="453" height="9" font="font3" id="p15_t42" reading_order_no="41" segment_no="4" tag_type="text">The principle of the double Q-learning can be applied to any approximation techniques of the action-state</text>
<text top="461" left="79" width="317" height="9" font="font3" id="p15_t43" reading_order_no="42" segment_no="4" tag_type="text">function, and in particular when a deep neural network is employed [16].</text>
<text top="491" left="79" width="28" height="11" font="font15" id="p15_t44" reading_order_no="43" segment_no="5" tag_type="title">9.2.6</text>
<text top="491" left="120" width="122" height="11" font="font15" id="p15_t45" reading_order_no="44" segment_no="5" tag_type="title">Multi-armed bandits<a href="deeplearning_paper3.html#11">(9.19) </a>aims at estimating the value in <a href="deeplearning_paper3.html#6">(9.7) </a>for the next state</text>
<text top="513" left="79" width="69" height="9" font="font6" id="p15_t46" reading_order_no="45" segment_no="6" tag_type="title">Fundamentals</text>
<text top="532" left="79" width="454" height="10" font="font3" id="p15_t47" reading_order_no="46" segment_no="7" tag_type="text">A multi-armed bandit (MAB) model holds its name from a slot machine with several levers 3 , that the</text>
<text top="545" left="79" width="453" height="9" font="font3" id="p15_t48" reading_order_no="47" segment_no="7" tag_type="text">player/user activates in the hope to hit the prize. Each machine has a certain probability to deliver</text>
<text top="557" left="79" width="453" height="9" font="font3" id="p15_t49" reading_order_no="48" segment_no="7" tag_type="text">the money to the player. The goal of the agent, to keep the terminology used in the RL framework, is</text>
<text top="569" left="79" width="453" height="9" font="font3" id="p15_t50" reading_order_no="49" segment_no="7" tag_type="text">to play the machine that gives the maximum expected gain in the long run. Mathematically, a MAB</text>
<text top="580" left="79" width="454" height="10" font="font3" id="p15_t51" reading_order_no="50" segment_no="7" tag_type="text">model is a collection of K random variables R i , i = 1 , · · · , K , where i denotes the ”arm” of the bandit,</text>
<text top="593" left="79" width="454" height="10" font="font3" id="p15_t52" reading_order_no="51" segment_no="7" tag_type="text">each distributed as P R i , unknown to the agent. The player sequentially chooses an arm, i.e. the action</text>
<text top="604" left="79" width="454" height="12" font="font3" id="p15_t53" reading_order_no="52" segment_no="7" tag_type="text">of the agent { A ( t ) } t ≥ 0 and collects rewards over time { R ( t + 1) } t ≥ 0 . When the agent pulls arm a as<a href="deeplearning_paper3.html#44">[15] </a>to use a</text>
<text top="617" left="79" width="107" height="9" font="font3" id="p15_t54" reading_order_no="53" segment_no="7" tag_type="text">its action at time t , i.e.</text>
<text top="617" left="194" width="338" height="10" font="font7" id="p15_t55" reading_order_no="54" segment_no="7" tag_type="text">A ( t ) = a , it gets a reward randomly drawn from the distribution P R a , i.e.</text>
<text top="628" left="79" width="454" height="11" font="font7" id="p15_t56" reading_order_no="55" segment_no="7" tag_type="text">R ( t ) ∼ P R a . The goal of the agent is to maximize the expected rewards obtained up the time horizon T ,</text>
<text top="644" left="79" width="35" height="10" font="font3" id="p15_t57" reading_order_no="56" segment_no="7" tag_type="text">i.e. E P R</text>
<text top="639" left="117" width="20" height="8" font="font13" id="p15_t58" reading_order_no="57" segment_no="7" tag_type="text">h P T</text>
<text top="639" left="132" width="400" height="16" font="font7" id="p15_t59" reading_order_no="58" segment_no="7" tag_type="text">t =1 R ( t ) i . By denoting µ i the expectation of arm i , i.e. µ i = E [ R i ], the agent should play</text>
<text top="656" left="79" width="454" height="12" font="font3" id="p15_t60" reading_order_no="59" segment_no="7" tag_type="text">as much as possible the arm with the maximum mean reward, µ ∗ = arg max i µ i , and less as possible the</text>
<text top="670" left="79" width="453" height="9" font="font3" id="p15_t61" reading_order_no="60" segment_no="7" tag_type="text">suboptimal arms. However, the arm with the maximum mean reward is of course unknown to the agent,</text>
<text top="682" left="79" width="453" height="9" font="font3" id="p15_t62" reading_order_no="61" segment_no="7" tag_type="text">in general, and the agent has to make decisions, defining its policy, based only on the past observations.</text>
<text top="694" left="79" width="453" height="9" font="font3" id="p15_t63" reading_order_no="62" segment_no="7" tag_type="text">To do so, the agent has to explore sufficiently in order to accumulate information on the rewards given by</text>
<text top="706" left="79" width="453" height="9" font="font3" id="p15_t64" reading_order_no="63" segment_no="7" tag_type="text">the arms, and also has to exploit the best arms, i.e. those that have given the highest cumulated rewards</text>
<text top="726" left="90" width="204" height="9" font="font21" id="p15_t65" reading_order_no="64" segment_no="8" tag_type="footnote">3 Or equivalently several one-armed gambling machines</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p15_t66" reading_order_no="65" segment_no="9" tag_type="text">15</text>
</page>
<page number="16" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font27" size="5" family="CMMIB5" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p16_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p16_t2" reading_order_no="1" segment_no="1" tag_type="text">so far. This is the famous exploration-exploitation tradeoff we already mentioned above that each RL</text>
<text top="122" left="79" width="119" height="9" font="font3" id="p16_t3" reading_order_no="2" segment_no="1" tag_type="text">algorithm has to deal with.</text>
<text top="134" left="94" width="438" height="9" font="font3" id="p16_t4" reading_order_no="3" segment_no="2" tag_type="text">One could consider the MAB as a special case of MDP described in Subsection 9.2.2 where only a<a href="deeplearning_paper3.html#4">9.2.2 </a>where only a</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p16_t5" reading_order_no="4" segment_no="2" tag_type="text">single state is considered. The conditional probability distribution defining the dynamic of this particular</text>
<text top="158" left="79" width="107" height="9" font="font3" id="p16_t6" reading_order_no="5" segment_no="2" tag_type="text">MDP in (9.4) reduces to<a href="deeplearning_paper3.html#5">(9.4) </a>reduces to</text>
<text top="179" left="179" width="230" height="15" font="font7" id="p16_t7" reading_order_no="6" segment_no="3" tag_type="formula">P R ( t +1) ,S ( t +1) | S ( t ) ,A ( t ) ( r, s 0 | s, a ) = P R ( t ) | A ( t ) ( r | a ) 4</text>
<text top="184" left="402" width="31" height="10" font="font3" id="p16_t8" reading_order_no="7" segment_no="3" tag_type="formula">= P R a .</text>
<text top="184" left="507" width="25" height="9" font="font3" id="p16_t9" reading_order_no="8" segment_no="3" tag_type="text">(9.27)</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p16_t10" reading_order_no="9" segment_no="4" tag_type="text">In the definition above, the state transition vanishes because there is only one state in the MDP and the</text>
<text top="218" left="79" width="247" height="9" font="font3" id="p16_t11" reading_order_no="10" segment_no="4" tag_type="text">reward is immediately obtained after pulling the lever a .</text>
<text top="230" left="94" width="438" height="9" font="font3" id="p16_t12" reading_order_no="11" segment_no="5" tag_type="text">There are two schools of thought in MAB, or two approaches: the Bayesian, proposed by Thompson</text>
<text top="242" left="79" width="454" height="9" font="font3" id="p16_t13" reading_order_no="12" segment_no="5" tag_type="text">[17], and the frequentist approach e.g. [18, 19]. When the distribution of arms depends on a parameter θ ,</text>
<text top="257" left="79" width="371" height="10" font="font3" id="p16_t14" reading_order_no="13" segment_no="5" tag_type="text">the MAB framework is said to be parametric , i.e. the distribution of the MAB is P R θ =</text>
<text top="257" left="459" width="19" height="11" font="font7" id="p16_t15" reading_order_no="14" segment_no="5" tag_type="text">P R θ 1</text>
<text top="256" left="480" width="45" height="13" font="font7" id="p16_t16" reading_order_no="15" segment_no="5" tag_type="text">, · · · , P R θK</text>
<text top="271" left="79" width="453" height="10" font="font3" id="p16_t17" reading_order_no="16" segment_no="5" tag_type="text">and θ = ( θ 1 , · · · , θ K ) is the vector parameter of the MAB. In the Bayesian setting, θ is a random</text>
<text top="284" left="79" width="453" height="9" font="font3" id="p16_t18" reading_order_no="17" segment_no="5" tag_type="text">variable drawn from a prior distribution P θ . In i.i.d. scenario, e.g. the rewards are drawn from a</text>
<text top="294" left="79" width="454" height="11" font="font3" id="p16_t19" reading_order_no="18" segment_no="5" tag_type="text">Bernouilli distribution 4 , and conditionally to θ a , R a ( t ) ∼ P R a with mean µ a and the elements of the</text>
<text top="307" left="79" width="454" height="11" font="font3" id="p16_t20" reading_order_no="19" segment_no="5" tag_type="text">series { R a ( t ) } t,a are independent. In the frequentist approach, R a ( t ) has the same properties as in the</text>
<text top="319" left="79" width="454" height="9" font="font3" id="p16_t21" reading_order_no="20" segment_no="5" tag_type="text">Bayesian setting, but where θ is no longer random. Instead it is an unknown deterministic parameter. We</text>
<text top="331" left="79" width="453" height="9" font="font3" id="p16_t22" reading_order_no="21" segment_no="5" tag_type="text">will not discuss the difference between Bayesian and frequentist approaches further and the interested</text>
<text top="343" left="79" width="453" height="9" font="font3" id="p16_t23" reading_order_no="22" segment_no="5" tag_type="text">reader may consult the excellent treatise in [20] for more details about the both approaches. In the</text>
<text top="355" left="79" width="422" height="9" font="font3" id="p16_t24" reading_order_no="23" segment_no="5" tag_type="text">following, we will focus on the frequentist approach for which the notion of regret is defined [21].</text>
<text top="382" left="79" width="37" height="9" font="font6" id="p16_t25" reading_order_no="24" segment_no="6" tag_type="text">Regret.</text>
<text top="382" left="127" width="406" height="9" font="font3" id="p16_t26" reading_order_no="25" segment_no="6" tag_type="text">The regret under the time horizon T can be understood rather intuitively: this is the dif-</text>
<text top="394" left="79" width="453" height="9" font="font3" id="p16_t27" reading_order_no="26" segment_no="6" tag_type="text">ference between the average expected reward one would obtain if one always plays the optimal arm</text>
<text top="406" left="79" width="453" height="9" font="font3" id="p16_t28" reading_order_no="27" segment_no="6" tag_type="text">and the average expected reward obtained following a policy π , which is the sequential series of actions</text>
<text top="417" left="79" width="453" height="10" font="font7" id="p16_t29" reading_order_no="28" segment_no="6" tag_type="text">A (0) , A (1) , · · · A ( T − 1), different from the optimal one. By denoting the index of the optimal arm as</text>
<text top="431" left="79" width="234" height="12" font="font7" id="p16_t30" reading_order_no="29" segment_no="6" tag_type="text">o = arg max i µ i and its expected reward µ ∗ = max i µ i</text>
<text top="429" left="317" width="7" height="7" font="font10" id="p16_t31" reading_order_no="30" segment_no="6" tag_type="text">4</text>
<text top="433" left="317" width="179" height="10" font="font3" id="p16_t32" reading_order_no="31" segment_no="6" tag_type="text">= E [ R o ( t )], the regret may be written as</text>
<text top="462" left="235" width="88" height="13" font="font11" id="p16_t33" reading_order_no="32" segment_no="7" tag_type="formula">D µ ( T ) = T µ ∗ − E P R</text>
<text top="454" left="326" width="15" height="7" font="font13" id="p16_t34" reading_order_no="33" segment_no="7" tag_type="formula">" T</text>
<text top="462" left="332" width="14" height="4" font="font13" id="p16_t35" reading_order_no="34" segment_no="7" tag_type="formula">X</text>
<text top="479" left="333" width="13" height="6" font="font8" id="p16_t36" reading_order_no="35" segment_no="7" tag_type="formula">t =1</text>
<text top="465" left="348" width="19" height="9" font="font7" id="p16_t37" reading_order_no="36" segment_no="7" tag_type="formula">R ( t )</text>
<text top="454" left="367" width="6" height="4" font="font13" id="p16_t38" reading_order_no="37" segment_no="7" tag_type="formula">#</text>
<text top="465" left="375" width="3" height="9" font="font7" id="p16_t39" reading_order_no="38" segment_no="7" tag_type="formula">.</text>
<text top="465" left="507" width="25" height="9" font="font3" id="p16_t40" reading_order_no="39" segment_no="7" tag_type="text">(9.28)</text>
<text top="496" left="79" width="454" height="10" font="font3" id="p16_t41" reading_order_no="40" segment_no="8" tag_type="text">Note that the regret depends on the parameter µ = ( µ 1 , · · · , µ K ). The regret can also be seen in a</text>
<text top="508" left="79" width="454" height="9" font="font3" id="p16_t42" reading_order_no="41" segment_no="8" tag_type="text">different, but totally equivalent, way. Let us consider several experiments of duration T , each arm i =</text>
<text top="520" left="79" width="453" height="10" font="font3" id="p16_t43" reading_order_no="42" segment_no="8" tag_type="text">1 , · · · , K will be played N i ( T ), which is a random variable for all i , with the expected value E [ N i ( T )]. For</text>
<text top="532" left="79" width="454" height="9" font="font3" id="p16_t44" reading_order_no="43" segment_no="8" tag_type="text">instance let us assume that K = 3 and T = 9. During the first experiment, arm 1 has been played 4 times,</text>
<text top="542" left="79" width="454" height="11" font="font3" id="p16_t45" reading_order_no="44" segment_no="8" tag_type="text">arm 2 has been played 3 times and arm 3, 2 times 5 . Assuming a stationary setting, i.e. the distribution</text>
<text top="554" left="79" width="454" height="12" font="font3" id="p16_t46" reading_order_no="45" segment_no="8" tag_type="text">of the regret does not vary in time, the regret in (9.28) becomes T µ ∗ − 4 E [ R 1 ] − 3 E [ R 2 ] − 2 E [ R 3 ] or</text>
<text top="566" left="79" width="453" height="11" font="font7" id="p16_t47" reading_order_no="46" segment_no="8" tag_type="text">T µ ∗ − 4 µ 1 − 3 µ 2 − 2 µ 3 . The second experiment gives (5 , 2 , 2) for arms 1, 2 and 3 respectively and so</text>
<text top="580" left="79" width="453" height="9" font="font3" id="p16_t48" reading_order_no="47" segment_no="8" tag_type="text">on. If n experiments are run, the empirical average of the regret over the experiments, D µ ( T ), in our</text>
<text top="592" left="79" width="60" height="9" font="font3" id="p16_t49" reading_order_no="48" segment_no="8" tag_type="text">example gives</text>
<text top="604" left="193" width="70" height="12" font="font7" id="p16_t50" reading_order_no="49" segment_no="9" tag_type="formula">D µ ( T ) = T µ ∗ −</text>
<text top="607" left="271" width="137" height="9" font="font7" id="p16_t51" reading_order_no="50" segment_no="9" tag_type="formula">N 1 ( T ) µ 1 + N 2 ( T ) µ 2 + N 3 ( T ) µ 3</text>
<text top="607" left="416" width="3" height="9" font="font7" id="p16_t52" reading_order_no="51" segment_no="9" tag_type="formula">,</text>
<text top="607" left="507" width="25" height="9" font="font3" id="p16_t53" reading_order_no="52" segment_no="9" tag_type="text">(9.29)</text>
<text top="628" left="79" width="74" height="11" font="font3" id="p16_t54" reading_order_no="53" segment_no="10" tag_type="text">where N i ( T ) = 1</text>
<text top="627" left="149" width="23" height="14" font="font13" id="p16_t55" reading_order_no="54" segment_no="10" tag_type="text">n P n</text>
<text top="630" left="167" width="366" height="11" font="font7" id="p16_t56" reading_order_no="55" segment_no="10" tag_type="text">j =1 N i ( j ; T ) is the empirical average of the number of times arm i has been pulled at</text>
<text top="644" left="79" width="454" height="11" font="font3" id="p16_t57" reading_order_no="56" segment_no="10" tag_type="text">time T . When n → ∞ , N i ( T ) → E [ N i ( T )], and hence the regret in (9.28) can be expressed as a function</text>
<text top="657" left="79" width="253" height="9" font="font3" id="p16_t58" reading_order_no="57" segment_no="10" tag_type="text">of the average number of times each arm has been played:<a href="deeplearning_paper3.html#44">[17], </a>and the frequentist approach e.g. <a href="deeplearning_paper3.html#44">[18, 19]. </a>When the distribution of arms depends on a parameter</text>
<text top="687" left="234" width="39" height="10" font="font11" id="p16_t59" reading_order_no="58" segment_no="11" tag_type="formula">D µ ( T ) =</text>
<text top="678" left="279" width="7" height="6" font="font8" id="p16_t60" reading_order_no="59" segment_no="11" tag_type="formula">K</text>
<text top="685" left="276" width="14" height="4" font="font13" id="p16_t61" reading_order_no="60" segment_no="11" tag_type="formula">X</text>
<text top="702" left="276" width="13" height="6" font="font8" id="p16_t62" reading_order_no="61" segment_no="11" tag_type="formula">i =1</text>
<text top="685" left="292" width="87" height="13" font="font3" id="p16_t63" reading_order_no="62" segment_no="11" tag_type="formula">( µ ∗ − µ i ) E [ N i ( T )] ,</text>
<text top="688" left="507" width="25" height="9" font="font3" id="p16_t64" reading_order_no="63" segment_no="11" tag_type="text">(9.30)</text>
<text top="717" left="90" width="151" height="8" font="font21" id="p16_t65" reading_order_no="64" segment_no="12" tag_type="footnote">4 We will see later the Markovian setting</text>
<text top="726" left="90" width="282" height="9" font="font21" id="p16_t66" reading_order_no="65" segment_no="13" tag_type="footnote">5 The sequence of the arm selection depends on the policy the agent follows.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p16_t67" reading_order_no="66" segment_no="14" tag_type="text">16</text>
</page>
<page number="17" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="252" height="9" font="font3" id="p17_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p17_t2" reading_order_no="1" segment_no="1" tag_type="text">where the expectation depends on µ . From (9.30), it seems clear that a policy should play as much as</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p17_t3" reading_order_no="2" segment_no="1" tag_type="text">possible the best arm, and as less as possible the suboptimal ones. Lai and Robbins have shown, [19, Th.</text>
<text top="133" left="79" width="454" height="10" font="font3" id="p17_t4" reading_order_no="3" segment_no="1" tag_type="text">2], that any policy cannot have a better regret than a logarithmic one asymptotically, i.e. when T → ∞ .<a href="deeplearning_paper3.html#16">(9.30), </a>it seems clear that a policy should play as much as</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p17_t5" reading_order_no="4" segment_no="1" tag_type="text">It is equivalent to say that the average number of plays of suboptimal arms is logarithmically bounded<a href="deeplearning_paper3.html#44">[19, </a>Th.</text>
<text top="155" left="79" width="302" height="12" font="font3" id="p17_t6" reading_order_no="5" segment_no="1" tag_type="text">when T → ∞ , i.e. for all i such as µ i &lt; µ ∗ and if µ ∈ [0 , 1] K we have</text>
<text top="187" left="228" width="32" height="9" font="font3" id="p17_t7" reading_order_no="6" segment_no="2" tag_type="formula">lim inf</text>
<text top="195" left="244" width="21" height="7" font="font8" id="p17_t8" reading_order_no="7" segment_no="2" tag_type="formula">T →∞</text>
<text top="180" left="268" width="41" height="10" font="font14" id="p17_t9" reading_order_no="8" segment_no="2" tag_type="formula">E [ N i ( T )]</text>
<text top="194" left="278" width="20" height="9" font="font3" id="p17_t10" reading_order_no="9" segment_no="2" tag_type="formula">log T</text>
<text top="186" left="313" width="8" height="9" font="font11" id="p17_t11" reading_order_no="10" segment_no="2" tag_type="formula">≥</text>
<text top="180" left="352" width="5" height="9" font="font3" id="p17_t12" reading_order_no="11" segment_no="2" tag_type="formula">1</text>
<text top="194" left="325" width="29" height="10" font="font7" id="p17_t13" reading_order_no="12" segment_no="2" tag_type="formula">D ( P R i</text>
<text top="193" left="356" width="28" height="11" font="font11" id="p17_t14" reading_order_no="13" segment_no="2" tag_type="formula">|| P R o )</text>
<text top="187" left="507" width="25" height="9" font="font3" id="p17_t15" reading_order_no="14" segment_no="2" tag_type="text">(9.31)</text>
<text top="216" left="79" width="59" height="10" font="font3" id="p17_t16" reading_order_no="15" segment_no="3" tag_type="text">where D ( P R i</text>
<text top="215" left="141" width="392" height="11" font="font11" id="p17_t17" reading_order_no="16" segment_no="3" tag_type="text">|| P R o ) is the Kullback-Leibler divergence between the reward distributions of arm i and</text>
<text top="228" left="79" width="453" height="9" font="font3" id="p17_t18" reading_order_no="17" segment_no="3" tag_type="text">the optimal arm respectively. This result gives the bound of fundamental performance of any sequential</text>
<text top="240" left="79" width="453" height="9" font="font3" id="p17_t19" reading_order_no="18" segment_no="3" tag_type="text">policy in MAB settings, but it does not give any insight on how to explicitly design a policy that achieves</text>
<text top="252" left="79" width="453" height="9" font="font3" id="p17_t20" reading_order_no="19" segment_no="3" tag_type="text">this bound. Several algorithms have been proposed that are known to be order optimal, i.e. whose the</text>
<text top="264" left="79" width="453" height="9" font="font3" id="p17_t21" reading_order_no="20" segment_no="3" tag_type="text">regret behaves logarithmically with time. We will not detail all of them in this chapter but we will focus</text>
<text top="276" left="79" width="453" height="9" font="font3" id="p17_t22" reading_order_no="21" segment_no="3" tag_type="text">on a particular class that is applicable to physical layer communications: the upper confidence bound</text>
<text top="288" left="79" width="78" height="9" font="font3" id="p17_t23" reading_order_no="22" segment_no="3" tag_type="text">(UCB) algorithm.</text>
<text top="317" left="79" width="182" height="9" font="font6" id="p17_t24" reading_order_no="23" segment_no="4" tag_type="text">Upper confidence bound algorithm.</text>
<text top="317" left="272" width="261" height="9" font="font3" id="p17_t25" reading_order_no="24" segment_no="4" tag_type="text">The UCB algorithms are based on the computation of an</text>
<text top="329" left="79" width="453" height="9" font="font3" id="p17_t26" reading_order_no="25" segment_no="4" tag_type="text">index for each arm and selecting the arm with the highest index. The index is composed of two terms.</text>
<text top="342" left="79" width="333" height="10" font="font3" id="p17_t27" reading_order_no="26" segment_no="4" tag_type="text">The first is an estimate of the average reward of arm i at time t , i.e. ˆ µ i ( t ) =</text>
<text top="340" left="423" width="4" height="6" font="font9" id="p17_t28" reading_order_no="27" segment_no="4" tag_type="text">1</text>
<text top="348" left="416" width="18" height="6" font="font8" id="p17_t29" reading_order_no="28" segment_no="4" tag_type="text">N i ( t )</text>
<text top="339" left="437" width="30" height="7" font="font13" id="p17_t30" reading_order_no="29" segment_no="4" tag_type="text">P N i ( t )</text>
<text top="342" left="448" width="85" height="11" font="font7" id="p17_t31" reading_order_no="30" segment_no="4" tag_type="text">n =1 R i ( n ), and the</text>
<text top="355" left="79" width="453" height="9" font="font3" id="p17_t32" reading_order_no="31" segment_no="4" tag_type="text">second term is a measure of the uncertainty of this estimation. The UCB algorithm consists of choosing</text>
<text top="367" left="79" width="453" height="9" font="font3" id="p17_t33" reading_order_no="32" segment_no="4" tag_type="text">the maximum of this uncertainty to compute the index. Auer et al. proposed several algorithms in [21]</text>
<text top="379" left="79" width="454" height="9" font="font3" id="p17_t34" reading_order_no="33" segment_no="4" tag_type="text">for bounded expected rewards in [0 , 1]. The most famous algorithm is UCB1 that allows one to choose</text>
<text top="391" left="79" width="173" height="9" font="font3" id="p17_t35" reading_order_no="34" segment_no="4" tag_type="text">arm a ( t + 1) at the next time such that</text>
<text top="425" left="222" width="80" height="9" font="font7" id="p17_t36" reading_order_no="35" segment_no="5" tag_type="formula">a ( t + 1) = arg max</text>
<text top="433" left="292" width="3" height="6" font="font8" id="p17_t37" reading_order_no="36" segment_no="5" tag_type="formula">i</text>
<text top="414" left="304" width="6" height="4" font="font13" id="p17_t38" reading_order_no="37" segment_no="5" tag_type="formula">"</text>
<text top="425" left="310" width="31" height="9" font="font3" id="p17_t39" reading_order_no="38" segment_no="5" tag_type="formula">ˆ µ i ( t ) +</text>
<text top="413" left="343" width="36" height="14" font="font13" id="p17_t40" reading_order_no="39" segment_no="5" tag_type="formula">s 2 log t</text>
<text top="431" left="355" width="23" height="10" font="font7" id="p17_t41" reading_order_no="40" segment_no="5" tag_type="formula">N i ( t )</text>
<text top="414" left="380" width="6" height="4" font="font13" id="p17_t42" reading_order_no="41" segment_no="5" tag_type="formula">#</text>
<text top="425" left="388" width="3" height="9" font="font7" id="p17_t43" reading_order_no="42" segment_no="5" tag_type="formula">,</text>
<text top="425" left="507" width="25" height="9" font="font3" id="p17_t44" reading_order_no="43" segment_no="5" tag_type="text">(9.32)</text>
<text top="456" left="79" width="453" height="9" font="font3" id="p17_t45" reading_order_no="44" segment_no="6" tag_type="text">where the second term in the square root can be obtained using Chernoff-Hoeffding inequality, which</text>
<text top="468" left="79" width="454" height="10" font="font3" id="p17_t46" reading_order_no="45" segment_no="6" tag_type="text">represents the upper bound of the confidence interval in the estimation of ˆ µ i . The more arm i is played,</text>
<text top="480" left="79" width="454" height="10" font="font3" id="p17_t47" reading_order_no="46" segment_no="6" tag_type="text">i.e. as N i ( t ) increases, the smaller the confidence interval, which means the index value relies on the</text>
<text top="492" left="79" width="454" height="9" font="font3" id="p17_t48" reading_order_no="47" segment_no="6" tag_type="text">average value of the cumulated reward obtained so far. When arm i is played less the second term is</text>
<text top="504" left="79" width="453" height="9" font="font3" id="p17_t49" reading_order_no="48" segment_no="6" tag_type="text">larger, which encourages the agent to play another arm. The second term allows for exploration while</text>
<text top="516" left="79" width="453" height="9" font="font3" id="p17_t50" reading_order_no="49" segment_no="6" tag_type="text">the first term encourages the exploitation of the arm that has given the largest rewards so far. Moreover,</text>
<text top="528" left="79" width="453" height="9" font="font3" id="p17_t51" reading_order_no="50" segment_no="6" tag_type="text">thanks to the log term that is unbounded with time, all arms will be played asymptotically. Note that</text>
<text top="540" left="79" width="453" height="9" font="font3" id="p17_t52" reading_order_no="51" segment_no="6" tag_type="text">other UCB algorithms can perform very well in practice such as the Kullback-Leibler UCB (KL-UCB)</text>
<text top="552" left="79" width="453" height="9" font="font3" id="p17_t53" reading_order_no="52" segment_no="6" tag_type="text">for which a non-asymptotic regret bound can be proved [22]. Other algorithms have been proposed in</text>
<text top="564" left="79" width="453" height="9" font="font3" id="p17_t54" reading_order_no="53" segment_no="6" tag_type="text">order to deal with non-stationary environments in which the parameters of the distribution may change</text>
<text top="576" left="79" width="64" height="9" font="font3" id="p17_t55" reading_order_no="54" segment_no="6" tag_type="text">with time [23].</text>
<text top="606" left="79" width="28" height="11" font="font15" id="p17_t56" reading_order_no="55" segment_no="7" tag_type="title">9.2.7</text>
<text top="606" left="120" width="100" height="11" font="font15" id="p17_t57" reading_order_no="56" segment_no="7" tag_type="title">Markovian MAB</text>
<text top="627" left="79" width="453" height="9" font="font3" id="p17_t58" reading_order_no="57" segment_no="8" tag_type="text">Historically, the first bandits studied were binary, i.e. the rewards are drawn from a Bernouilli distribu-</text>
<text top="636" left="79" width="453" height="12" font="font3" id="p17_t59" reading_order_no="58" segment_no="8" tag_type="text">tion. Actually the result of Lai and Robbins in [19] is quite general and holds for µ ∈ [0 , 1] K for any</text>
<text top="651" left="79" width="454" height="10" font="font3" id="p17_t60" reading_order_no="59" segment_no="8" tag_type="text">distribution of the arms P R i . However, the independence of the rewards is an important assumption that</text>
<text top="663" left="79" width="234" height="9" font="font3" id="p17_t61" reading_order_no="60" segment_no="8" tag_type="text">does not necessarily hold in many practical problems.</text>
<text top="675" left="94" width="438" height="9" font="font3" id="p17_t62" reading_order_no="61" segment_no="9" tag_type="text">In particular, the case of Markovian rewards is of practical interest in wireless communication. Each</text>
<text top="687" left="79" width="454" height="10" font="font3" id="p17_t63" reading_order_no="62" segment_no="9" tag_type="text">arm in Markovian MAB is characterized by a non-periodic Markov chain with finite state space S i .</text>
<text top="699" left="79" width="454" height="10" font="font3" id="p17_t64" reading_order_no="63" segment_no="9" tag_type="text">For all arm i , the agent receives a positive reward R i ( s ) that depends on the state observed for arm</text>
<text top="711" left="79" width="454" height="9" font="font7" id="p17_t65" reading_order_no="64" segment_no="9" tag_type="text">i . The change from a state to another follows a Markovian process, under the conditional probability</text>
<text top="724" left="79" width="66" height="13" font="font7" id="p17_t66" reading_order_no="65" segment_no="9" tag_type="text">P S i ( t +1) | S i ( t ) ( s 0</text>
<text top="722" left="143" width="33" height="15" font="font11" id="p17_t67" reading_order_no="66" segment_no="9" tag_type="text">i | s i ) 4</text>
<text top="724" left="168" width="27" height="11" font="font3" id="p17_t68" reading_order_no="67" segment_no="9" tag_type="text">= p ( s 0</text>
<text top="726" left="193" width="340" height="11" font="font11" id="p17_t69" reading_order_no="68" segment_no="9" tag_type="text">i | s i ). The stationary distribution of the Markov chain of arm i is denoted as</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p17_t70" reading_order_no="69" segment_no="10" tag_type="text">17</text>
</page>
<page number="18" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p18_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="107" left="79" width="70" height="15" font="font17" id="p18_t2" reading_order_no="1" segment_no="1" tag_type="text">P i = n P S i ( s ) 4</text>
<text top="111" left="142" width="66" height="11" font="font3" id="p18_t3" reading_order_no="2" segment_no="1" tag_type="text">= p i ( s ) , S i ∈ S i</text>
<text top="107" left="208" width="200" height="14" font="font13" id="p18_t4" reading_order_no="3" segment_no="1" tag_type="text">o . Each arm i has an expected reward that is</text>
<text top="137" left="239" width="38" height="13" font="font7" id="p18_t5" reading_order_no="4" segment_no="2" tag_type="formula">µ i = X</text>
<text top="154" left="262" width="17" height="7" font="font8" id="p18_t6" reading_order_no="5" segment_no="2" tag_type="formula">s ∈S i</text>
<text top="136" left="280" width="52" height="14" font="font7" id="p18_t7" reading_order_no="6" segment_no="2" tag_type="formula">r i ( s ) p i ( s ) 4</text>
<text top="140" left="324" width="49" height="12" font="font3" id="p18_t8" reading_order_no="7" segment_no="2" tag_type="formula">= E P Si [ R i ]</text>
<text top="140" left="507" width="25" height="9" font="font3" id="p18_t9" reading_order_no="8" segment_no="2" tag_type="text">(9.33)</text>
<text top="171" left="79" width="454" height="9" font="font3" id="p18_t10" reading_order_no="9" segment_no="3" tag_type="text">The goal of the agent is still to find a policy π , i.e. the sequential observation of arms, that minimizes</text>
<text top="183" left="79" width="454" height="9" font="font3" id="p18_t11" reading_order_no="10" segment_no="3" tag_type="text">the regret over the time, which is defined in (9.28) or (9.30). When the agent observes an arm i at t ,</text>
<text top="195" left="79" width="453" height="9" font="font3" id="p18_t12" reading_order_no="11" segment_no="3" tag_type="text">it samples a Markovian process which evolves with time. A particular attention has to be paid on the</text>
<text top="207" left="79" width="454" height="9" font="font3" id="p18_t13" reading_order_no="12" segment_no="3" tag_type="text">status of Markov chains that are not observed that leads to the distinction between the rested and restless</text>
<text top="219" left="79" width="24" height="9" font="font3" id="p18_t14" reading_order_no="13" segment_no="3" tag_type="text">cases.</text>
<text top="244" left="79" width="69" height="9" font="font6" id="p18_t15" reading_order_no="14" segment_no="4" tag_type="text">Rested MAB.</text>
<text top="244" left="158" width="374" height="9" font="font3" id="p18_t16" reading_order_no="15" segment_no="4" tag_type="text">A Markovian MAB is qualified as rested, when only the Markov chain of the arm that</text>
<text top="256" left="79" width="453" height="9" font="font3" id="p18_t17" reading_order_no="16" segment_no="4" tag_type="text">is played evolves, the others remaining frozen. This assumption is strong since the Markov chains of</text>
<text top="268" left="79" width="453" height="9" font="font3" id="p18_t18" reading_order_no="17" segment_no="4" tag_type="text">the arms that are not observed do not evolve with time and it does not matter how much time elapsed</text>
<text top="280" left="79" width="453" height="9" font="font3" id="p18_t19" reading_order_no="18" segment_no="4" tag_type="text">between two consecutive visits to a given arm. The authors in [24] were the first to be interested in the</text>
<text top="291" left="79" width="454" height="10" font="font3" id="p18_t20" reading_order_no="19" segment_no="4" tag_type="text">fundamental performance in terms of regret of Markovian MAB with multiple plays 6 and they proposed</text>
<text top="304" left="79" width="454" height="9" font="font3" id="p18_t21" reading_order_no="20" segment_no="4" tag_type="text">a policy that is asymptotically efficient, i.e. that achieves the regret lower bound asymptotically. The</text>
<text top="316" left="79" width="453" height="9" font="font3" id="p18_t22" reading_order_no="21" segment_no="4" tag_type="text">authors in [25] showed that a slightly modified UCB1 achieves a logarithmic regret uniformly over time</text>
<text top="328" left="79" width="63" height="9" font="font3" id="p18_t23" reading_order_no="22" segment_no="4" tag_type="text">in this setting.</text>
<text top="354" left="79" width="75" height="9" font="font6" id="p18_t24" reading_order_no="23" segment_no="5" tag_type="text">Restless MAB.</text>
<text top="354" left="165" width="368" height="9" font="font3" id="p18_t25" reading_order_no="24" segment_no="5" tag_type="text">A Markovian MAB is considered as restless, if the Markov chains of all arms evolve</text>
<text top="366" left="79" width="453" height="9" font="font3" id="p18_t26" reading_order_no="25" segment_no="5" tag_type="text">with time, irrespective of which arm is played. This assumption implies radical changes in the regret</text>
<text top="378" left="79" width="453" height="9" font="font3" id="p18_t27" reading_order_no="26" segment_no="5" tag_type="text">analysis because the state we will observe when pulling an arm i at t + 1 directly depends on the time</text>
<text top="390" left="79" width="453" height="9" font="font3" id="p18_t28" reading_order_no="27" segment_no="5" tag_type="text">elapsed since the last visit to this arm. This is because the reward distribution we get by playing an arm</text>
<text top="401" left="79" width="453" height="9" font="font3" id="p18_t29" reading_order_no="28" segment_no="5" tag_type="text">depends on the time elapsed between two consecutive plays of this arm and since arms are not played</text>
<text top="413" left="79" width="453" height="9" font="font3" id="p18_t30" reading_order_no="29" segment_no="5" tag_type="text">continuously, the sample path experienced by the agent does not correspond to a sample path followed</text>
<text top="425" left="79" width="453" height="9" font="font3" id="p18_t31" reading_order_no="30" segment_no="5" tag_type="text">when observing a discrete time homogeneous Markov chain. The solution came from [26] where the</text>
<text top="437" left="79" width="454" height="9" font="font3" id="p18_t32" reading_order_no="31" segment_no="5" tag_type="text">authors proposed a regenerative cycle algorithm to deal with the discontinuous observation of evolving</text>
<text top="449" left="79" width="453" height="9" font="font3" id="p18_t33" reading_order_no="32" segment_no="5" tag_type="text">Markov chains. In practice, the agent still keeps going to apply the UCB1 algorithm, introduced earlier,</text>
<text top="461" left="79" width="454" height="9" font="font3" id="p18_t34" reading_order_no="33" segment_no="5" tag_type="text">but computes the index only on the samples the agent has collected when observing a given arm i .</text>
<text top="473" left="79" width="453" height="9" font="font3" id="p18_t35" reading_order_no="34" segment_no="5" tag_type="text">This structure requires one to observe an arm during a certain amount of time before computing the</text>
<text top="485" left="79" width="453" height="9" font="font3" id="p18_t36" reading_order_no="35" segment_no="5" tag_type="text">UCB1 index. The interested reader may refer to [26] for further details and [25] for the extension to</text>
<text top="497" left="79" width="453" height="9" font="font3" id="p18_t37" reading_order_no="36" segment_no="5" tag_type="text">multiple plays. This setting is particularly interesting because it finds a natural application in wireless</text>
<text top="509" left="79" width="453" height="9" font="font3" id="p18_t38" reading_order_no="37" segment_no="5" tag_type="text">communications with the opportunistic spectrum access scenario when the state of the bands the user has</text>
<text top="521" left="79" width="453" height="9" font="font3" id="p18_t39" reading_order_no="38" segment_no="5" tag_type="text">to access evolves independently of the action the user takes. For example, the band may be sporadically</text>
<text top="533" left="79" width="349" height="9" font="font3" id="p18_t40" reading_order_no="39" segment_no="5" tag_type="text">occupied by another system or the propagation condition may evolve with time.</text>
<text top="559" left="79" width="87" height="9" font="font6" id="p18_t41" reading_order_no="40" segment_no="6" tag_type="title">Contextual MAB</text>
<text top="577" left="79" width="453" height="9" font="font3" id="p18_t42" reading_order_no="41" segment_no="7" tag_type="text">Contextual MAB generalizes the classical concept introduced above towards more general reinforcement</text>
<text top="589" left="79" width="453" height="9" font="font3" id="p18_t43" reading_order_no="42" segment_no="7" tag_type="text">learning. Conventional MAB does not take advantage of any knowledge about the environment. The</text>
<text top="601" left="79" width="453" height="9" font="font3" id="p18_t44" reading_order_no="43" segment_no="7" tag_type="text">basic idea of contextual MAB is to condition the decision making on the state of the environment.</text>
<text top="613" left="79" width="453" height="9" font="font3" id="p18_t45" reading_order_no="44" segment_no="7" tag_type="text">This allows for making the decisions based both on the particular scenario we are in and the previous</text>
<text top="625" left="79" width="453" height="9" font="font3" id="p18_t46" reading_order_no="45" segment_no="7" tag_type="text">observations we have acquired. A contextual MAB algorithm observes a context in the form of useful</text>
<text top="637" left="79" width="453" height="9" font="font3" id="p18_t47" reading_order_no="46" segment_no="7" tag_type="text">side information, followed by a decision by choosing one action from the set of alternative ones. It then</text>
<text top="649" left="79" width="453" height="9" font="font3" id="p18_t48" reading_order_no="47" segment_no="7" tag_type="text">observes an outcome of that decision which defines the obtained reward. In order to benefit from the</text>
<text top="661" left="79" width="453" height="9" font="font3" id="p18_t49" reading_order_no="48" segment_no="7" tag_type="text">context information, there needs to exist dependency between the expected reward of an action and its</text>
<text top="673" left="79" width="453" height="9" font="font3" id="p18_t50" reading_order_no="49" segment_no="7" tag_type="text">context. The goal of learning is to maximize a cumulative reward function over the time span of interest.</text>
<text top="685" left="79" width="453" height="9" font="font3" id="p18_t51" reading_order_no="50" segment_no="7" tag_type="text">The side information in physical layer communications may be a feature vector containing for example</text>
<text top="697" left="79" width="453" height="9" font="font3" id="p18_t52" reading_order_no="51" segment_no="7" tag_type="text">location and device information, experienced interference, received signal strength fingerprint, channel</text>
<text top="709" left="79" width="453" height="9" font="font3" id="p18_t53" reading_order_no="52" segment_no="7" tag_type="text">state information (CSI) of a particular user, or potential priority information. Such side information</text>
<text top="726" left="90" width="387" height="9" font="font21" id="p18_t54" reading_order_no="53" segment_no="8" tag_type="footnote">6 When the player may pull more than one arm or if multiple players are considered (without collisions).</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p18_t55" reading_order_no="54" segment_no="9" tag_type="text">18</text>
</page>
<page number="19" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font28" size="8" family="CMSY8" color="#000000"/>
	<fontspec id="font29" size="6" family="CMMI6" color="#000000"/>
	<fontspec id="font30" size="6" family="CMSY6" color="#000000"/>
<text top="72" left="79" width="252" height="9" font="font3" id="p19_t1" reading_order_no="0" segment_no="0" tag_type="title">9.2. REINFORCEMENT LEARNING: BACKGROUND</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p19_t2" reading_order_no="1" segment_no="1" tag_type="text">would allow for selection of a base station, service set or antennas such that higher rewards are achieved.</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p19_t3" reading_order_no="2" segment_no="1" tag_type="text">Extensions of well-known UCB algorithms have been developed for contextual MAB problems. Since</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p19_t4" reading_order_no="3" segment_no="1" tag_type="text">the context plays an important role in the dynamic of contextual MAB, it may be crucial to detect the</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p19_t5" reading_order_no="4" segment_no="1" tag_type="text">change of a context in order to adapt the policy to this change. Methods based on statistical multiple</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p19_t6" reading_order_no="5" segment_no="1" tag_type="text">change-point detection and sequential multiple hypothesis testing may be used for that purpose [27, 28].<a href="deeplearning_paper3.html#44">[27, 28].</a></text>
<text top="184" left="79" width="89" height="9" font="font6" id="p19_t7" reading_order_no="6" segment_no="2" tag_type="title">Adversarial MAB</text>
<text top="202" left="79" width="453" height="9" font="font3" id="p19_t8" reading_order_no="7" segment_no="3" tag_type="text">Adversarial bandit problems are defined using sequential game formulation. The adversarial model means</text>
<text top="214" left="79" width="453" height="9" font="font3" id="p19_t9" reading_order_no="8" segment_no="3" tag_type="text">that the decisions may lead to the worst possible payoff instead of optimistic view of always making choices</text>
<text top="226" left="79" width="453" height="9" font="font3" id="p19_t10" reading_order_no="9" segment_no="3" tag_type="text">that lead to optimal payoff. The problems are modeled assuming deterministic and uninformed adversary</text>
<text top="238" left="79" width="453" height="9" font="font3" id="p19_t11" reading_order_no="10" segment_no="3" tag_type="text">and that the payoffs or costs for all arms and all time steps of playing the arms are chosen in advance.</text>
<text top="250" left="79" width="453" height="9" font="font3" id="p19_t12" reading_order_no="11" segment_no="3" tag_type="text">The adversary is often assumed to be uninformed so that it makes its choices independent of the previous</text>
<text top="262" left="79" width="453" height="9" font="font3" id="p19_t13" reading_order_no="12" segment_no="3" tag_type="text">outcomes of the strategy. At each iteration, a MAB agent chooses an arm it plays while an adversary</text>
<text top="274" left="79" width="453" height="9" font="font3" id="p19_t14" reading_order_no="13" segment_no="3" tag_type="text">chooses the payoff structure for each arm. In other words, the reward of each arm is no longer chosen</text>
<text top="285" left="79" width="453" height="10" font="font3" id="p19_t15" reading_order_no="14" segment_no="3" tag_type="text">to be stochastic but they are deterministically assigned to an unknown sequence, r (1) , r (2) , · · · where</text>
<text top="296" left="79" width="453" height="13" font="font6" id="p19_t16" reading_order_no="15" segment_no="3" tag_type="text">r ( t ) = ( r 1 ( t ) , · · · , r K ( t )) T and r i ( t ) ∈ R ⊂ R is the reward of the i − th arm at time t . By denoting</text>
<text top="309" left="79" width="454" height="11" font="font3" id="p19_t17" reading_order_no="16" segment_no="3" tag_type="text">the policy π that maps a time slot to an arm index to play at the next time slot 7 , π (1) , π (2) , · · · is the</text>
<text top="323" left="79" width="454" height="9" font="font3" id="p19_t18" reading_order_no="17" segment_no="3" tag_type="text">sequence of plays of the agent. Considering a time-horizon h , the goal of the agent is to minimize the</text>
<text top="335" left="79" width="26" height="9" font="font3" id="p19_t19" reading_order_no="18" segment_no="3" tag_type="text">regret</text>
<text top="356" left="208" width="23" height="10" font="font11" id="p19_t20" reading_order_no="19" segment_no="4" tag_type="formula">D h =</text>
<text top="356" left="245" width="19" height="9" font="font3" id="p19_t21" reading_order_no="20" segment_no="4" tag_type="formula">max</text>
<text top="365" left="234" width="41" height="7" font="font8" id="p19_t22" reading_order_no="21" segment_no="4" tag_type="formula">j ∈{ 1 , ··· ,K }</text>
<text top="346" left="282" width="5" height="6" font="font8" id="p19_t23" reading_order_no="22" segment_no="4" tag_type="formula">h</text>
<text top="353" left="277" width="14" height="4" font="font13" id="p19_t24" reading_order_no="23" segment_no="4" tag_type="formula">X</text>
<text top="370" left="278" width="13" height="6" font="font8" id="p19_t25" reading_order_no="24" segment_no="4" tag_type="formula">t =1</text>
<text top="356" left="293" width="44" height="10" font="font7" id="p19_t26" reading_order_no="25" segment_no="4" tag_type="formula">r j ( t ) − E π</text>
<text top="346" left="339" width="16" height="6" font="font13" id="p19_t27" reading_order_no="26" segment_no="4" tag_type="formula">" h</text>
<text top="353" left="345" width="14" height="4" font="font13" id="p19_t28" reading_order_no="27" segment_no="4" tag_type="formula">X</text>
<text top="370" left="345" width="13" height="6" font="font8" id="p19_t29" reading_order_no="28" segment_no="4" tag_type="formula">t =1</text>
<text top="356" left="361" width="33" height="10" font="font7" id="p19_t30" reading_order_no="29" segment_no="4" tag_type="formula">R π ( t ) ( t )</text>
<text top="346" left="394" width="6" height="4" font="font13" id="p19_t31" reading_order_no="30" segment_no="4" tag_type="formula">#</text>
<text top="356" left="402" width="3" height="9" font="font7" id="p19_t32" reading_order_no="31" segment_no="4" tag_type="formula">,</text>
<text top="356" left="507" width="25" height="9" font="font3" id="p19_t33" reading_order_no="32" segment_no="4" tag_type="text">(9.34)</text>
<text top="383" left="79" width="454" height="10" font="font3" id="p19_t34" reading_order_no="33" segment_no="5" tag_type="text">where the adversary chooses r 1 (1) , r 2 (1) ..., r k ( t ) and the player strategy chooses actions π ( t ). Since the</text>
<text top="395" left="79" width="454" height="10" font="font3" id="p19_t35" reading_order_no="34" segment_no="5" tag_type="text">policy is commonly assumed to be random, the regret is also random (so the notation R π ( t ) ( t )), and the</text>
<text top="407" left="79" width="453" height="9" font="font3" id="p19_t36" reading_order_no="35" segment_no="5" tag_type="text">expectation is taken over the distribution of the policy. Adversarial bandits is an important generalization</text>
<text top="419" left="79" width="453" height="9" font="font3" id="p19_t37" reading_order_no="36" segment_no="5" tag_type="text">of the bandit problem since no assumptions on the underlying distributions are made, hence the name</text>
<text top="431" left="79" width="453" height="9" font="font3" id="p19_t38" reading_order_no="37" segment_no="5" tag_type="text">adversarial. The player has access to the trace of rewards for the actions that the algorithm chose in</text>
<text top="443" left="79" width="453" height="9" font="font3" id="p19_t39" reading_order_no="38" segment_no="5" tag_type="text">previous rounds, but does not know the rewards of actions that were not selected. Widely used methods</text>
<text top="455" left="79" width="453" height="9" font="font3" id="p19_t40" reading_order_no="39" segment_no="5" tag_type="text">such as UCB are not suitable for this problem formulation. The exponential-weight algorithm called</text>
<text top="467" left="79" width="445" height="9" font="font5" id="p19_t41" reading_order_no="40" segment_no="5" tag_type="text">Exp3 for exploration and exploitation is widely used for solving Adversarial bandits problems [29, 30].</text>
<text top="717" left="90" width="424" height="8" font="font21" id="p19_t42" reading_order_no="41" segment_no="6" tag_type="footnote">7 Actually the policy is a mapping from the set of indices and rewards obtained so far to the next index, i.e.</text>
<text top="718" left="521" width="11" height="7" font="font22" id="p19_t43" reading_order_no="42" segment_no="6" tag_type="footnote">π :</text>
<text top="725" left="79" width="132" height="10" font="font21" id="p19_t44" reading_order_no="43" segment_no="6" tag_type="footnote">( { 1 , · · · , K } × R ) t − 1 → { 1 , · · · , K }</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p19_t45" reading_order_no="44" segment_no="7" tag_type="text">19</text>
</page>
<page number="20" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font31" size="7" family="MSBM7" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p20_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="21" height="13" font="font0" id="p20_t2" reading_order_no="1" segment_no="1" tag_type="title">9.3</text>
<text top="110" left="116" width="122" height="13" font="font0" id="p20_t3" reading_order_no="2" segment_no="1" tag_type="title">RL at PHY layer</text>
<text top="136" left="79" width="453" height="9" font="font3" id="p20_t4" reading_order_no="3" segment_no="2" tag_type="text">In this section, the concepts of Q-learning, deep-learning and MAB are illustrated through some examples</text>
<text top="148" left="79" width="453" height="9" font="font3" id="p20_t5" reading_order_no="4" segment_no="2" tag_type="text">deriving from communication problems addressed in literature. The first example in Section 9.3.1 deals<a href="deeplearning_paper3.html#20">9.3.1 </a>deals</text>
<text top="160" left="79" width="453" height="9" font="font3" id="p20_t6" reading_order_no="5" segment_no="2" tag_type="text">with power management under queuing delay constraint in a point-to-point wireless fading communication</text>
<text top="172" left="79" width="453" height="9" font="font3" id="p20_t7" reading_order_no="6" segment_no="2" tag_type="text">channel and derives from [31, 32]. This problem has been widely studied in literature under various<a href="deeplearning_paper3.html#45">[31, 32]. </a>This problem has been widely studied in literature under various</text>
<text top="184" left="79" width="453" height="9" font="font3" id="p20_t8" reading_order_no="7" segment_no="2" tag_type="text">formalization including with MDP (cf. the discussion in Section 9.5) and a rather simple toy example<a href="deeplearning_paper3.html#38">9.5) </a>and a rather simple toy example</text>
<text top="196" left="79" width="453" height="9" font="font3" id="p20_t9" reading_order_no="8" segment_no="2" tag_type="text">can be extracted from it. In Section 9.3.2, we present two examples to illustrate the reinforcement learning<a href="deeplearning_paper3.html#22">9.3.2, </a>we present two examples to illustrate the reinforcement learning</text>
<text top="207" left="79" width="453" height="9" font="font3" id="p20_t10" reading_order_no="9" segment_no="2" tag_type="text">in large dimension, i.e. optimal caching in a single cell network in Section 9.3.2 and the extension of<a href="deeplearning_paper3.html#22">9.3.2 </a>and the extension of</text>
<text top="219" left="79" width="453" height="9" font="font3" id="p20_t11" reading_order_no="10" segment_no="2" tag_type="text">the single user power-delay management problem dealt with in Section 9.3.1 to the multi-user case with<a href="deeplearning_paper3.html#20">9.3.1 </a>to the multi-user case with</text>
<text top="231" left="79" width="453" height="9" font="font3" id="p20_t12" reading_order_no="11" segment_no="2" tag_type="text">large state and action spaces in Section 9.3.2. Finally Section 9.3.3 illustrates the use of MAB with the<a href="deeplearning_paper3.html#24">9.3.2. </a>Finally Section <a href="deeplearning_paper3.html#25">9.3.3 </a>illustrates the use of MAB with the</text>
<text top="243" left="79" width="453" height="9" font="font3" id="p20_t13" reading_order_no="12" segment_no="2" tag_type="text">opportunistic spectrum access (OSA) problem and green networking issue in Sections 9.3.3 and 9.3.3,<a href="deeplearning_paper3.html#25">9.3.3 </a>and <a href="deeplearning_paper3.html#27">9.3.3,</a></text>
<text top="255" left="79" width="453" height="9" font="font3" id="p20_t14" reading_order_no="13" segment_no="2" tag_type="text">respectively. This section ends with experimental results and proof-of-concept to validate the MAB</text>
<text top="267" left="79" width="453" height="9" font="font3" id="p20_t15" reading_order_no="14" segment_no="2" tag_type="text">principle applied to the OSA issue. These examples do not aim at providing the full solution of the</text>
<text top="279" left="79" width="453" height="9" font="font3" id="p20_t16" reading_order_no="15" segment_no="2" tag_type="text">problem raised, which can be found in the related literature, but rather a simple problem statement with</text>
<text top="291" left="79" width="453" height="9" font="font3" id="p20_t17" reading_order_no="16" segment_no="2" tag_type="text">explicit action and state spaces and the reward function that can be chosen to solve the problem with a</text>
<text top="303" left="79" width="138" height="9" font="font3" id="p20_t18" reading_order_no="17" segment_no="2" tag_type="text">reinforcement learning strategy.</text>
<text top="329" left="79" width="28" height="11" font="font15" id="p20_t19" reading_order_no="18" segment_no="3" tag_type="title">9.3.1</text>
<text top="329" left="120" width="148" height="11" font="font15" id="p20_t20" reading_order_no="19" segment_no="3" tag_type="title">Example with Q-learning</text>
<text top="349" left="79" width="453" height="9" font="font3" id="p20_t21" reading_order_no="20" segment_no="4" tag_type="text">Let us consider a point to point communication system over a block fading channel, i.e. the channel gain</text>
<text top="361" left="79" width="453" height="9" font="font3" id="p20_t22" reading_order_no="21" segment_no="4" tag_type="text">is assumed to be constant on a slot of duration ∆ t and changes from one slot to another according to the</text>
<text top="371" left="79" width="454" height="12" font="font3" id="p20_t23" reading_order_no="22" segment_no="4" tag_type="text">distribution P H ( t +1) | H ( t ) ( h 0 | h ) with H ∈ H where H is assumed to be a finite countable set. At each</text>
<text top="385" left="79" width="453" height="9" font="font3" id="p20_t24" reading_order_no="23" segment_no="4" tag_type="text">time slot, the transmitter can send a packet or remain silent and it can also choose its transmit power as</text>
<text top="397" left="79" width="453" height="9" font="font3" id="p20_t25" reading_order_no="24" segment_no="4" tag_type="text">well as its modulation and coding scheme, e.g. a 16-QAM with a convolutional code of rate 1/2. At each</text>
<text top="409" left="79" width="453" height="9" font="font3" id="p20_t26" reading_order_no="25" segment_no="4" tag_type="text">time slot, a certain number of bits is generated by the application layer and stored in a buffer waiting for</text>
<text top="421" left="79" width="453" height="9" font="font3" id="p20_t27" reading_order_no="26" segment_no="4" tag_type="text">their transmission. The transmitter aims at sending the highest number of bits as possible with minimal</text>
<text top="433" left="79" width="283" height="9" font="font3" id="p20_t28" reading_order_no="27" segment_no="4" tag_type="text">power consumption while limiting the waiting time in the buffer.</text>
<text top="445" left="94" width="438" height="9" font="font3" id="p20_t29" reading_order_no="28" segment_no="5" tag_type="text">At each time slot t , N ( t ) new bits are generated and stored in the buffer before being transmit-</text>
<text top="456" left="79" width="67" height="13" font="font3" id="p20_t30" reading_order_no="29" segment_no="5" tag_type="text">ted. { N ( t ) } t ∈ N</text>
<text top="456" left="151" width="382" height="10" font="font3" id="p20_t31" reading_order_no="30" segment_no="5" tag_type="text">are i.i.d. random variables with the distribution P N ( n ). The buffer state B ( t ) ∈ B =</text>
<text top="468" left="79" width="453" height="10" font="font11" id="p20_t32" reading_order_no="31" segment_no="5" tag_type="text">{ 0 , 1 , · · · , B max } represents the number of bits stored in the queue at time t and B max is the maxi-</text>
<text top="479" left="79" width="454" height="10" font="font3" id="p20_t33" reading_order_no="32" segment_no="5" tag_type="text">mal buffer size. At each time slot, transmitter chooses β ( t ) ∈ { 1 , · · · , B ( t ) } bits 8 to be transmitted</text>
<text top="492" left="79" width="453" height="10" font="font3" id="p20_t34" reading_order_no="33" segment_no="5" tag_type="text">and encodes them into a codeword of length n c channel uses, assumed to be fixed. The rough spectral</text>
<text top="504" left="79" width="454" height="10" font="font3" id="p20_t35" reading_order_no="34" segment_no="5" tag_type="text">efficiency of the transmission is hence ρ ( t ) = β ( t ) /n c . Moreover, transmitter chooses its power level</text>
<text top="516" left="79" width="303" height="10" font="font7" id="p20_t36" reading_order_no="35" segment_no="5" tag_type="text">P tx ( t ) ∈ { 0 , p 1 , · · · , p max } and hence the total power consumed at t is</text>
<text top="533" left="249" width="114" height="12" font="font7" id="p20_t37" reading_order_no="36" segment_no="6" tag_type="formula">P tot ( t ) = p on + α − 1 P tx ( t ) ,</text>
<text top="535" left="507" width="25" height="9" font="font3" id="p20_t38" reading_order_no="37" segment_no="6" tag_type="text">(9.35)</text>
<text top="553" left="79" width="454" height="10" font="font3" id="p20_t39" reading_order_no="38" segment_no="7" tag_type="text">where p on is the static power consumed by the electronic circuits and α ∈ ]0 , 1] the efficiency of the power</text>
<text top="565" left="79" width="329" height="10" font="font3" id="p20_t40" reading_order_no="39" segment_no="7" tag_type="text">amplifier. One can define the state space of the power consumption as P =</text>
<text top="564" left="416" width="110" height="11" font="font7" id="p20_t41" reading_order_no="40" segment_no="7" tag_type="text">p on , · · · , p on + α − 1 p max .</text>
<text top="578" left="94" width="282" height="9" font="font3" id="p20_t42" reading_order_no="41" segment_no="7" tag_type="text">The codeword error probability, a.k.a. pairwise error probability ,</text>
<text top="578" left="386" width="53" height="9" font="font3" id="p20_t43" reading_order_no="42" segment_no="7" tag_type="text">is defined as</text>
<text top="596" left="268" width="83" height="11" font="font3" id="p20_t44" reading_order_no="43" segment_no="8" tag_type="formula">= P [ ˆ m ( t ) 6 = m ( t )] ,</text>
<text top="597" left="507" width="25" height="9" font="font3" id="p20_t45" reading_order_no="44" segment_no="8" tag_type="text">(9.36)</text>
<text top="615" left="79" width="454" height="9" font="font3" id="p20_t46" reading_order_no="45" segment_no="9" tag_type="text">where m ( t ) and ˆ m ( t ) are the message sent and estimated at the receiver at time t , respectively. This</text>
<text top="627" left="79" width="453" height="9" font="font3" id="p20_t47" reading_order_no="46" segment_no="9" tag_type="text">probability has a complex expression in general (not always available in closed-form) that depends on the</text>
<text top="643" left="79" width="367" height="9" font="font3" id="p20_t48" reading_order_no="47" segment_no="9" tag_type="text">channel and modulation coding scheme, the transmit power and the channel gain, i.e.</text>
<text top="638" left="458" width="7" height="7" font="font10" id="p20_t49" reading_order_no="48" segment_no="9" tag_type="text">4</text>
<text top="643" left="458" width="75" height="9" font="font3" id="p20_t50" reading_order_no="49" segment_no="9" tag_type="text">= f ( β, n c , p tx , h ).</text>
<text top="655" left="79" width="453" height="9" font="font3" id="p20_t51" reading_order_no="50" segment_no="9" tag_type="text">Bounds and approximations in finite block length exist, i.e. when n c is finite, but remain complex to</text>
<text top="667" left="79" width="73" height="9" font="font3" id="p20_t52" reading_order_no="51" segment_no="9" tag_type="text">evaluate [33, 34].</text>
<text top="679" left="94" width="438" height="9" font="font3" id="p20_t53" reading_order_no="52" segment_no="10" tag_type="text">The buffer state evolution, i.e. the number of bits stored in the queue, can be described by a Markov</text>
<text top="691" left="79" width="107" height="9" font="font3" id="p20_t54" reading_order_no="53" segment_no="10" tag_type="text">chain with the dynamics</text>
<text top="706" left="194" width="225" height="13" font="font7" id="p20_t55" reading_order_no="54" segment_no="11" tag_type="formula">B ( t + 1) = [ B ( t ) − β ( t ) · 1 { ˆ m ( t ) = m ( t ) } ] + + N ( t ) .</text>
<text top="709" left="507" width="25" height="9" font="font3" id="p20_t56" reading_order_no="55" segment_no="11" tag_type="text">(9.37)</text>
<text top="726" left="90" width="246" height="9" font="font21" id="p20_t57" reading_order_no="56" segment_no="12" tag_type="footnote">8 Only a maximum of B ( t ) bits can be encoded and sent at time t .</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p20_t58" reading_order_no="57" segment_no="13" tag_type="text">20</text>
</page>
<page number="21" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="117" height="9" font="font3" id="p21_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="110" left="79" width="454" height="9" font="font3" id="p21_t2" reading_order_no="1" segment_no="1" tag_type="text">This equation states that the number of bits in the buffer in the next time slot, B ( t + 1), is the number of</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p21_t3" reading_order_no="2" segment_no="1" tag_type="text">bits that is stored at the current time slot, B ( t ), plus the new bits arriving in the buffer, N ( t ), minus the</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p21_t4" reading_order_no="3" segment_no="1" tag_type="text">number of bits that has been sent through the channel if the transmission is successful, i.e. the indicator</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p21_t5" reading_order_no="4" segment_no="1" tag_type="text">function is equal to 1 if so and 0 otherwise. Otherwise the packet remains in the queue and another</text>
<text top="158" left="79" width="176" height="9" font="font3" id="p21_t6" reading_order_no="5" segment_no="1" tag_type="text">attempt will occur in the next time slot.</text>
<text top="185" left="79" width="59" height="9" font="font6" id="p21_t7" reading_order_no="6" segment_no="2" tag_type="text">State space.</text>
<text top="185" left="149" width="384" height="9" font="font3" id="p21_t8" reading_order_no="7" segment_no="2" tag_type="text">The state space can be defined as the space containing the channel state, the buffer state</text>
<text top="196" left="79" width="240" height="10" font="font3" id="p21_t9" reading_order_no="8" segment_no="2" tag_type="text">and the power consumption state, i.e. S = H × B × P .</text>
<text top="224" left="79" width="67" height="9" font="font6" id="p21_t10" reading_order_no="9" segment_no="3" tag_type="text">Action space.</text>
<text top="224" left="156" width="377" height="9" font="font3" id="p21_t11" reading_order_no="10" segment_no="3" tag_type="text">At each time slot, transmitter chooses a power, including the choice not to transmit, i.e.</text>
<text top="234" left="79" width="454" height="12" font="font7" id="p21_t12" reading_order_no="11" segment_no="3" tag_type="text">P tx ( t ) = 0, and a number of bits β ( t ) that is mapped into a codeword 9 with a fixed block length n c but with</text>
<text top="247" left="79" width="454" height="11" font="font3" id="p21_t13" reading_order_no="12" segment_no="3" tag_type="text">a variable rate ρ ( t ) = β ( t ) /n c . The action space is then described as A = { 0 , p 1 , · · · , p max }×{ 1 , · · · , B ( t ) } .</text>
<text top="275" left="79" width="129" height="9" font="font6" id="p21_t14" reading_order_no="13" segment_no="4" tag_type="text">Reward / cost functions.</text>
<text top="275" left="218" width="314" height="9" font="font3" id="p21_t15" reading_order_no="14" segment_no="4" tag_type="text">In this type of problem, one may be interested to transmit bits with</text>
<text top="287" left="79" width="453" height="9" font="font3" id="p21_t16" reading_order_no="15" segment_no="4" tag_type="text">the minimal power while limiting the awaiting time in the buffer. In that case, the global reward can</text>
<text top="299" left="79" width="453" height="9" font="font3" id="p21_t17" reading_order_no="16" segment_no="4" tag_type="text">be expressed w.r.t. two cost functions, i.e. the power and the waiting time cost functions [32, 31],</text>
<text top="309" left="79" width="454" height="12" font="font7" id="p21_t18" reading_order_no="17" segment_no="4" tag_type="text">c : A × S → R + and w : A × S → R + 10 . The power consumption depends on the transmit power at time</text>
<text top="323" left="79" width="453" height="10" font="font7" id="p21_t19" reading_order_no="18" segment_no="4" tag_type="text">t that depends on the target error rate , the channel state h and the code rate ρ , i.e. β since n c is fixed.</text>
<text top="335" left="79" width="145" height="9" font="font3" id="p21_t20" reading_order_no="19" segment_no="4" tag_type="text">The power cost can be defined as</text>
<text top="363" left="237" width="10" height="9" font="font7" id="p21_t21" reading_order_no="20" segment_no="5" tag_type="formula">c :</text>
<text top="356" left="262" width="26" height="9" font="font11" id="p21_t22" reading_order_no="21" segment_no="5" tag_type="formula">A × S</text>
<text top="356" left="299" width="32" height="11" font="font11" id="p21_t23" reading_order_no="22" segment_no="5" tag_type="formula">−→ R +</text>
<text top="369" left="264" width="23" height="9" font="font3" id="p21_t24" reading_order_no="23" segment_no="5" tag_type="formula">( a, s )</text>
<text top="363" left="299" width="75" height="15" font="font11" id="p21_t25" reading_order_no="24" segment_no="5" tag_type="formula">7−→ p tot ( , h, β ) .</text>
<text top="363" left="507" width="25" height="9" font="font3" id="p21_t26" reading_order_no="25" segment_no="5" tag_type="text">(9.38)</text>
<text top="391" left="79" width="181" height="9" font="font3" id="p21_t27" reading_order_no="26" segment_no="6" tag_type="text">The buffer waiting time cost is defined as</text>
<text top="418" left="145" width="13" height="9" font="font7" id="p21_t28" reading_order_no="27" segment_no="7" tag_type="formula">w :</text>
<text top="412" left="173" width="26" height="9" font="font11" id="p21_t29" reading_order_no="28" segment_no="7" tag_type="formula">A × S</text>
<text top="412" left="210" width="32" height="11" font="font11" id="p21_t30" reading_order_no="29" segment_no="7" tag_type="formula">−→ R +</text>
<text top="424" left="175" width="22" height="9" font="font3" id="p21_t31" reading_order_no="30" segment_no="7" tag_type="formula">( a, s )</text>
<text top="418" left="210" width="255" height="16" font="font11" id="p21_t32" reading_order_no="31" segment_no="7" tag_type="formula">7−→ η 1 { b ( t + 1) &gt; B max } + ( b ( t ) − β ( t ) 1 { ˆ m ( t ) = m ( t ) } ) ,</text>
<text top="418" left="507" width="25" height="9" font="font3" id="p21_t33" reading_order_no="32" segment_no="7" tag_type="text">(9.39)</text>
<text top="446" left="79" width="454" height="9" font="font3" id="p21_t34" reading_order_no="33" segment_no="8" tag_type="text">The first term represents the cost to be in overflow with η a constant, for the sake of simplicity. It means</text>
<text top="457" left="79" width="454" height="10" font="font3" id="p21_t35" reading_order_no="34" segment_no="8" tag_type="text">that the cost to pay when the buffer is in overflow is independent of the amount of the overflow 11 . The</text>
<text top="470" left="79" width="453" height="9" font="font3" id="p21_t36" reading_order_no="35" segment_no="8" tag_type="text">second term is the holding cost, i.e. the cost for keeping b − β bits in the buffer if the transmission is<a href="deeplearning_paper3.html#21">ord</a></text>
<text top="482" left="79" width="45" height="9" font="font3" id="p21_t37" reading_order_no="36" segment_no="8" tag_type="text">successful.<a href="deeplearning_paper3.html#21">9</a></text>
<text top="509" left="79" width="33" height="9" font="font6" id="p21_t38" reading_order_no="37" segment_no="9" tag_type="text">Policy.</text>
<text top="509" left="122" width="410" height="9" font="font3" id="p21_t39" reading_order_no="38" segment_no="9" tag_type="text">The transmission scheduling policy consists in mapping the system state to an action at each</text>
<text top="520" left="79" width="453" height="10" font="font3" id="p21_t40" reading_order_no="39" segment_no="9" tag_type="text">time slot t , i.e. according to the buffer state, the channel state observed at the receiver 12 and the target</text>
<text top="533" left="79" width="453" height="9" font="font3" id="p21_t41" reading_order_no="40" segment_no="9" tag_type="text">error rate desired, the policy tells us how many information bits stored in the queue we should encode</text>
<text top="545" left="79" width="217" height="9" font="font3" id="p21_t42" reading_order_no="41" segment_no="9" tag_type="text">at the next transmission slot and at which power.</text>
<text top="557" left="94" width="438" height="9" font="font3" id="p21_t43" reading_order_no="42" segment_no="10" tag_type="text">Hence a desirable policy should solve an optimization problem. From the cost functions defined</text>
<text top="572" left="79" width="399" height="10" font="font3" id="p21_t44" reading_order_no="43" segment_no="10" tag_type="text">previously, the expected discounted power and waiting time costs, given an initial state s 0</text>
<text top="568" left="482" width="7" height="7" font="font10" id="p21_t45" reading_order_no="44" segment_no="10" tag_type="text">4</text>
<text top="572" left="482" width="51" height="9" font="font3" id="p21_t46" reading_order_no="45" segment_no="10" tag_type="text">= S (0), are</text>
<text top="584" left="79" width="46" height="9" font="font3" id="p21_t47" reading_order_no="46" segment_no="10" tag_type="text">defined as:</text>
<text top="603" left="205" width="54" height="10" font="font7" id="p21_t48" reading_order_no="47" segment_no="11" tag_type="formula">C π ( s 0 ) = E π</text>
<text top="592" left="261" width="18" height="7" font="font13" id="p21_t49" reading_order_no="48" segment_no="11" tag_type="formula">" ∞</text>
<text top="600" left="267" width="14" height="4" font="font13" id="p21_t50" reading_order_no="49" segment_no="11" tag_type="formula">X</text>
<text top="617" left="268" width="13" height="6" font="font8" id="p21_t51" reading_order_no="50" segment_no="11" tag_type="formula">t =0</text>
<text top="601" left="283" width="113" height="12" font="font7" id="p21_t52" reading_order_no="51" segment_no="11" tag_type="formula">γ t c ( A ( t ) , S ( t )) | S (0) = s 0</text>
<text top="593" left="397" width="6" height="4" font="font13" id="p21_t53" reading_order_no="52" segment_no="11" tag_type="formula">#</text>
<text top="603" left="404" width="3" height="9" font="font7" id="p21_t54" reading_order_no="53" segment_no="11" tag_type="formula">.</text>
<text top="603" left="507" width="25" height="9" font="font3" id="p21_t55" reading_order_no="54" segment_no="11" tag_type="text">(9.40)</text>
<text top="630" left="79" width="16" height="9" font="font3" id="p21_t56" reading_order_no="55" segment_no="12" tag_type="text">and</text>
<text top="650" left="202" width="57" height="10" font="font7" id="p21_t57" reading_order_no="56" segment_no="13" tag_type="formula">W π ( s 0 ) = E π</text>
<text top="639" left="261" width="17" height="7" font="font13" id="p21_t58" reading_order_no="57" segment_no="13" tag_type="formula">" ∞</text>
<text top="647" left="267" width="14" height="4" font="font13" id="p21_t59" reading_order_no="58" segment_no="13" tag_type="formula">X</text>
<text top="663" left="268" width="13" height="6" font="font8" id="p21_t60" reading_order_no="59" segment_no="13" tag_type="formula">t =0</text>
<text top="648" left="283" width="116" height="11" font="font7" id="p21_t61" reading_order_no="60" segment_no="13" tag_type="formula">γ t w ( A ( t ) , S ( t )) | S (0) = s 0</text>
<text top="639" left="399" width="6" height="4" font="font13" id="p21_t62" reading_order_no="61" segment_no="13" tag_type="formula">#</text>
<text top="650" left="407" width="3" height="9" font="font7" id="p21_t63" reading_order_no="62" segment_no="13" tag_type="formula">.</text>
<text top="650" left="507" width="25" height="9" font="font3" id="p21_t64" reading_order_no="63" segment_no="13" tag_type="text">(9.41)</text>
<text top="679" left="90" width="443" height="8" font="font21" id="p21_t65" reading_order_no="64" segment_no="14" tag_type="footnote">9 This transformation is the usual way to consider the encoding phase in information theory. In practice, a transmitter</text>
<text top="690" left="79" width="392" height="7" font="font21" id="p21_t66" reading_order_no="65" segment_no="14" tag_type="footnote">selects a channel coding rate and the resulting bit-train is grouped into symbols in a chosen constellation.</text>
<text top="698" left="87" width="446" height="9" font="font21" id="p21_t67" reading_order_no="66" segment_no="15" tag_type="footnote">10 Beside the action space, the waiting time cost only depends on the buffer state B and hence w is incentive to the other</text>
<text top="708" left="79" width="42" height="8" font="font21" id="p21_t68" reading_order_no="67" segment_no="15" tag_type="footnote">states in S .</text>
<text top="717" left="87" width="270" height="8" font="font21" id="p21_t69" reading_order_no="68" segment_no="16" tag_type="footnote">11 One can make this cost dependent on the amount of overflow, see [32].</text>
<text top="726" left="87" width="290" height="9" font="font21" id="p21_t70" reading_order_no="69" segment_no="17" tag_type="footnote">12 The channel state information is assumed to be fed back to the transmitter.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p21_t71" reading_order_no="70" segment_no="18" tag_type="text">21</text>
</page>
<page number="22" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p22_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p22_t2" reading_order_no="1" segment_no="1" tag_type="text">The expectation is taken over the distribution of the policy and the dynamic of the underlying MDP. The</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p22_t3" reading_order_no="2" segment_no="1" tag_type="text">problem of finding the minimal power consumption while limiting the waiting time cost can be formally</text>
<text top="132" left="79" width="96" height="11" font="font3" id="p22_t4" reading_order_no="3" segment_no="1" tag_type="text">described as [32, 31] 13<a href="deeplearning_paper3.html#45">[32, 31]</a></text>
<text top="147" left="226" width="17" height="9" font="font3" id="p22_t5" reading_order_no="4" segment_no="2" tag_type="formula">min<a href="deeplearning_paper3.html#22">13</a></text>
<text top="155" left="226" width="17" height="7" font="font8" id="p22_t6" reading_order_no="5" segment_no="2" tag_type="formula">π ∈ Φ</text>
<text top="146" left="246" width="140" height="10" font="font7" id="p22_t7" reading_order_no="6" segment_no="2" tag_type="formula">C π ( s 0 ) s.t. W π ( s 0 ) ≤ δ ∀ s 0 ∈ S .</text>
<text top="147" left="507" width="25" height="9" font="font3" id="p22_t8" reading_order_no="7" segment_no="2" tag_type="text">(9.42)</text>
<text top="169" left="79" width="453" height="9" font="font3" id="p22_t9" reading_order_no="8" segment_no="3" tag_type="text">The problem relies to a constrained optimisation problem with unknown dynamics. One can combine the</text>
<text top="181" left="79" width="454" height="9" font="font3" id="p22_t10" reading_order_no="9" segment_no="3" tag_type="text">power and waiting time cost functions c and w in (9.38) and (9.39), respectively, in a dual Lagrangian</text>
<text top="192" left="79" width="454" height="11" font="font3" id="p22_t11" reading_order_no="10" segment_no="3" tag_type="text">expression such that ` ( a, s ; λ ) = c ( a, s ) + λw ( a, s ), λ ∈ R + as proposed in [32, 31]. One can hence write</text>
<text top="205" left="79" width="453" height="9" font="font3" id="p22_t12" reading_order_no="11" segment_no="3" tag_type="text">an expected discounted Lagrangian cost on the same model than in (9.40), for instance, but substituting</text>
<text top="217" left="79" width="29" height="9" font="font7" id="p22_t13" reading_order_no="12" segment_no="3" tag_type="text">c by ` .</text>
<text top="229" left="94" width="439" height="9" font="font3" id="p22_t14" reading_order_no="13" segment_no="4" tag_type="text">One can apply the Q-learning algorithm detailed in Section 9.2.4 by replacing the reward R in (9.19)</text>
<text top="241" left="79" width="453" height="9" font="font3" id="p22_t15" reading_order_no="14" segment_no="4" tag_type="text">by the average discounted Lagrangian cost to obtain the optimal policy that minimizes the average power</text>
<text top="253" left="79" width="183" height="9" font="font3" id="p22_t16" reading_order_no="15" segment_no="4" tag_type="text">consumed under a buffer delay constraint.</text>
<text top="274" left="79" width="453" height="9" font="font6" id="p22_t17" reading_order_no="16" segment_no="5" tag_type="text">Remark 1. The naive implementation of the Q-learning algorithm may be inefficient in terms of the</text>
<text top="286" left="79" width="453" height="9" font="font5" id="p22_t18" reading_order_no="17" segment_no="5" tag_type="text">algorithm’s convergence time as reported in [32]. Indeed, Q-learning does not assume any knowledge about</text>
<text top="298" left="79" width="453" height="9" font="font5" id="p22_t19" reading_order_no="18" segment_no="5" tag_type="text">the dynamic of the underlying MDP. Hence, the exploration part, which is fundamental in Q-learning,</text>
<text top="309" left="79" width="453" height="9" font="font5" id="p22_t20" reading_order_no="19" segment_no="5" tag_type="text">slows down the convergence time due to the large number of combination of states and actions. However,</text>
<text top="321" left="79" width="453" height="9" font="font5" id="p22_t21" reading_order_no="20" segment_no="5" tag_type="text">in wireless communication some dynamics may not be completely unknown. The authors in [32] proposed</text>
<text top="333" left="79" width="453" height="9" font="font5" id="p22_t22" reading_order_no="21" segment_no="5" tag_type="text">to use the concept of the post-decision states, presented in wireless communication literature in [36]. The</text>
<text top="345" left="79" width="453" height="9" font="font5" id="p22_t23" reading_order_no="22" segment_no="5" tag_type="text">concept consists of reducing the amount of state to explore to take good decisions on the long run by</text>
<text top="357" left="79" width="422" height="9" font="font5" id="p22_t24" reading_order_no="23" segment_no="5" tag_type="text">basing the actions to take on states that would be observed considering only the known dynamics.</text>
<text top="385" left="79" width="28" height="11" font="font15" id="p22_t25" reading_order_no="24" segment_no="6" tag_type="title">9.3.2</text>
<text top="385" left="120" width="139" height="11" font="font15" id="p22_t26" reading_order_no="25" segment_no="6" tag_type="title">Example with Deep-RL</text>
<text top="405" left="79" width="453" height="9" font="font3" id="p22_t27" reading_order_no="26" segment_no="7" tag_type="text">When the state and action spaces become large, the tabulated methods for SARSA or Q-learning are no</text>
<text top="417" left="79" width="453" height="9" font="font3" id="p22_t28" reading_order_no="27" segment_no="7" tag_type="text">longer practical. In that case, methods relying on the approximation of the Q function are meaningful</text>
<text top="429" left="79" width="312" height="9" font="font3" id="p22_t29" reading_order_no="28" segment_no="7" tag_type="text">like the linear approximation and those based on deep neural networks.</text>
<text top="456" left="79" width="157" height="9" font="font6" id="p22_t30" reading_order_no="29" segment_no="8" tag_type="title">Cache enabled communications</text>
<text top="475" left="79" width="453" height="9" font="font3" id="p22_t31" reading_order_no="30" segment_no="9" tag_type="text">Mobile Internet allows anyone to access heterogeneous data in mobility. However, all the contents are</text>
<text top="487" left="79" width="453" height="9" font="font3" id="p22_t32" reading_order_no="31" segment_no="9" tag_type="text">not requested the same by the users, i.e. the data do not have the same popularity and some videos,</text>
<text top="499" left="79" width="453" height="9" font="font3" id="p22_t33" reading_order_no="32" segment_no="9" tag_type="text">for instance, may be more requested than other files by a user. In order to reduce the data traffic on</text>
<text top="511" left="79" width="453" height="9" font="font3" id="p22_t34" reading_order_no="33" segment_no="9" tag_type="text">the backhaul link, and hence the network operating costs, the most requested files can be kept into the</text>
<text top="523" left="79" width="453" height="9" font="font3" id="p22_t35" reading_order_no="34" segment_no="9" tag_type="text">storage unit of the base station; this is what is called caching. Hence, the most ”popular” files are stored<a href="deeplearning_paper3.html#21">(9.38) </a>and <a href="deeplearning_paper3.html#21">(9.39), </a>respectively, in a dual Lagrangian</text>
<text top="535" left="79" width="453" height="9" font="font3" id="p22_t36" reading_order_no="35" segment_no="9" tag_type="text">at the base station and can be delivered quickly to the user when requested and hence reducing the cost</text>
<text top="547" left="79" width="184" height="9" font="font3" id="p22_t37" reading_order_no="36" segment_no="9" tag_type="text">to download the file from a distant server.</text>
<text top="559" left="94" width="438" height="9" font="font3" id="p22_t38" reading_order_no="37" segment_no="10" tag_type="text">The problem of learning the optimal caching strategy for satisfying users demand or data offloading</text>
<text top="571" left="79" width="453" height="9" font="font3" id="p22_t39" reading_order_no="38" segment_no="10" tag_type="text">in various environments has been addressed in lot of works, e.g. [4, Table V]. In this section, we will</text>
<text top="583" left="79" width="453" height="9" font="font3" id="p22_t40" reading_order_no="39" segment_no="10" tag_type="text">focus on a simple example to illustrate how the caching problem may be addressed with deep Q-learning.</text>
<text top="595" left="79" width="453" height="9" font="font3" id="p22_t41" reading_order_no="40" segment_no="10" tag_type="text">The example is an adaptation of [37] where this problem is studied in a more complex setting. We briefly</text>
<text top="607" left="79" width="453" height="9" font="font3" id="p22_t42" reading_order_no="41" segment_no="10" tag_type="text">summarize how to properly choose the action and state spaces and rewards in order to find the optimal</text>
<text top="619" left="79" width="73" height="9" font="font3" id="p22_t43" reading_order_no="42" segment_no="10" tag_type="text">caching strategy.</text>
<text top="631" left="94" width="438" height="9" font="font3" id="p22_t44" reading_order_no="43" segment_no="11" tag_type="text">Let us consider a network with a single cell serving many users. Each user may ask for a file f in the</text>
<text top="642" left="79" width="213" height="10" font="font3" id="p22_t45" reading_order_no="44" segment_no="11" tag_type="text">set F = { 1 , · · · , F } and we assume that only M</text>
<text top="643" left="309" width="224" height="9" font="font7" id="p22_t46" reading_order_no="45" segment_no="11" tag_type="text">F files can be stored at the base station. The files</text>
<text top="655" left="79" width="453" height="9" font="font3" id="p22_t47" reading_order_no="46" segment_no="11" tag_type="text">are requested randomly according to a certain distribution characterizing their popularity at time t . The</text>
<text top="664" left="79" width="453" height="12" font="font3" id="p22_t48" reading_order_no="47" segment_no="11" tag_type="text">popularity of the files is modeled as a random vector P ( t ) = [P 1 ( t ) , · · · , P F ( t )] T where the distribution</text>
<text top="677" left="79" width="453" height="11" font="font3" id="p22_t49" reading_order_no="48" segment_no="11" tag_type="text">of each popularity can be modeled with Zipf’s law 14 . This law gives the average number of occurrences</text>
<text top="691" left="79" width="172" height="9" font="font3" id="p22_t50" reading_order_no="49" segment_no="11" tag_type="text">of each file and can be estimated online</text>
<text top="707" left="87" width="446" height="9" font="font21" id="p22_t51" reading_order_no="50" segment_no="12" tag_type="footnote">13 Note that one could have searched for minimizing the waiting time cost under a total power budget as studied in [35],</text>
<text top="718" left="79" width="122" height="7" font="font21" id="p22_t52" reading_order_no="51" segment_no="12" tag_type="footnote">that leads to equivalent strategy.</text>
<text top="726" left="87" width="398" height="9" font="font21" id="p22_t53" reading_order_no="52" segment_no="13" tag_type="footnote">14 Zipf’s law has been first used to characterize the frequency of occurrence of a word according to its rank.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p22_t54" reading_order_no="53" segment_no="14" tag_type="text">22</text>
</page>
<page number="23" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font32" size="10" family="CMBSY10" color="#000000"/>
	<fontspec id="font33" size="7" family="CMBSY7" color="#000000"/>
	<fontspec id="font34" size="5" family="CMBSY5" color="#000000"/>
<text top="72" left="79" width="117" height="9" font="font3" id="p23_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="110" left="94" width="438" height="9" font="font3" id="p23_t2" reading_order_no="1" segment_no="1" tag_type="text">The goal of the network is to decide the files to cache and those to remove from the cache storing unit</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p23_t3" reading_order_no="2" segment_no="1" tag_type="text">at each time slot. Because of the large number of possible requested files the number of possible choices</text>
<text top="132" left="79" width="108" height="11" font="font3" id="p23_t4" reading_order_no="3" segment_no="1" tag_type="text">is huge, i.e. 2 M with M</text>
<text top="134" left="205" width="327" height="9" font="font3" id="p23_t5" reading_order_no="4" segment_no="1" tag_type="text">1. Classical tabulated Q-learning approach, like the example presented in</text>
<text top="146" left="79" width="126" height="9" font="font3" id="p23_t6" reading_order_no="5" segment_no="1" tag_type="text">Section 9.3.1, is not suitable.</text>
<text top="175" left="79" width="67" height="9" font="font6" id="p23_t7" reading_order_no="6" segment_no="2" tag_type="text">Action space.</text>
<text top="174" left="156" width="377" height="10" font="font3" id="p23_t8" reading_order_no="7" segment_no="2" tag_type="text">Let A be the set of caching action vectors , that contains the binary action vector A ( t )</text>
<text top="185" left="79" width="453" height="14" font="font3" id="p23_t9" reading_order_no="8" segment_no="2" tag_type="text">at time t such that A = n A | A ∈ { 0 , 1 } F , A T 1 = M o . A f ( t ) ∈ { 0 , 1 } , f ∈ { 1 , · · · , F } is a random<a href="deeplearning_paper3.html#20">9.3.1, </a>is not suitable.</text>
<text top="204" left="79" width="368" height="9" font="font3" id="p23_t10" reading_order_no="9" segment_no="2" tag_type="text">variable that is equal to 1 if the file f is cached in the BS and 0 otherwise at time t .</text>
<text top="233" left="79" width="59" height="9" font="font6" id="p23_t11" reading_order_no="10" segment_no="3" tag_type="text">State space.</text>
<text top="233" left="149" width="384" height="9" font="font3" id="p23_t12" reading_order_no="11" segment_no="3" tag_type="text">The state space is made of the popularity profile and the caching action vector, the latter</text>
<text top="245" left="79" width="453" height="9" font="font3" id="p23_t13" reading_order_no="12" segment_no="3" tag_type="text">being also an indicator of the cache status at time t . The popularity profile vector P is assumed to evolve</text>
<text top="257" left="79" width="291" height="9" font="font3" id="p23_t14" reading_order_no="13" segment_no="3" tag_type="text">according to a Markov chain with |P| states taken in the set P =</text>
<text top="255" left="379" width="153" height="11" font="font6" id="p23_t15" reading_order_no="14" segment_no="3" tag_type="text">P 1 , · · · , P |P| . The state space is</text>
<text top="269" left="79" width="79" height="9" font="font3" id="p23_t16" reading_order_no="15" segment_no="3" tag_type="text">hence S = P × A .</text>
<text top="298" left="79" width="112" height="9" font="font6" id="p23_t17" reading_order_no="16" segment_no="4" tag_type="text">Reward/cost function.</text>
<text top="298" left="201" width="332" height="9" font="font3" id="p23_t18" reading_order_no="17" segment_no="4" tag_type="text">Similar to the previous example, the reward takes the form of a cost function</text>
<text top="310" left="79" width="9" height="9" font="font3" id="p23_t19" reading_order_no="18" segment_no="4" tag_type="text">as</text>
<text top="328" left="167" width="10" height="9" font="font7" id="p23_t20" reading_order_no="19" segment_no="5" tag_type="formula">c :</text>
<text top="321" left="192" width="26" height="9" font="font11" id="p23_t21" reading_order_no="20" segment_no="5" tag_type="formula">A × S</text>
<text top="321" left="229" width="32" height="11" font="font11" id="p23_t22" reading_order_no="21" segment_no="5" tag_type="formula">−→ R +</text>
<text top="335" left="193" width="25" height="9" font="font3" id="p23_t23" reading_order_no="22" segment_no="5" tag_type="formula">( a , p )</text>
<text top="328" left="229" width="215" height="16" font="font11" id="p23_t24" reading_order_no="23" segment_no="5" tag_type="formula">7−→ λ 1 a ( t ) T ( 1 − a ( t − 1)) + λ 2 ( 1 − a ( t )) T p ( t ) .</text>
<text top="328" left="507" width="25" height="9" font="font3" id="p23_t25" reading_order_no="24" segment_no="5" tag_type="text">(9.43)</text>
<text top="354" left="79" width="453" height="9" font="font3" id="p23_t26" reading_order_no="25" segment_no="6" tag_type="text">The cost function is made of two parts: i) a term related to the cost of not having a file cached in the</text>
<text top="366" left="79" width="453" height="9" font="font3" id="p23_t27" reading_order_no="26" segment_no="6" tag_type="text">previous time slot, i.e. term 1 − a ( t − 1), which needs to be cached in the current time slot and ii) an</text>
<text top="378" left="79" width="453" height="10" font="font3" id="p23_t28" reading_order_no="27" segment_no="6" tag_type="text">immediate cost for non caching the file with high popularity profile at time t . The constants λ 1 and λ 2</text>
<text top="390" left="79" width="223" height="9" font="font3" id="p23_t29" reading_order_no="28" segment_no="6" tag_type="text">allow to balance the importance of these two costs.</text>
<text top="419" left="79" width="33" height="9" font="font6" id="p23_t30" reading_order_no="29" segment_no="7" tag_type="text">Policy.</text>
<text top="417" left="122" width="410" height="11" font="font3" id="p23_t31" reading_order_no="30" segment_no="7" tag_type="text">The goal of the reinforcement learning is to learn the optimal policy π ∗ : S → A that minimizes</text>
<text top="431" left="79" width="196" height="9" font="font3" id="p23_t32" reading_order_no="31" segment_no="7" tag_type="text">the long term weighted average cost function</text>
<text top="460" left="186" width="57" height="12" font="font7" id="p23_t33" reading_order_no="32" segment_no="8" tag_type="formula">π ∗ = arg min</text>
<text top="473" left="218" width="17" height="7" font="font8" id="p23_t34" reading_order_no="33" segment_no="8" tag_type="formula">π ∈ Φ</text>
<text top="465" left="246" width="11" height="8" font="font14" id="p23_t35" reading_order_no="34" segment_no="8" tag_type="formula">E π</text>
<text top="452" left="260" width="17" height="7" font="font13" id="p23_t36" reading_order_no="35" segment_no="8" tag_type="formula">" ∞</text>
<text top="460" left="265" width="14" height="4" font="font13" id="p23_t37" reading_order_no="36" segment_no="8" tag_type="formula">X</text>
<text top="477" left="266" width="13" height="6" font="font8" id="p23_t38" reading_order_no="37" segment_no="8" tag_type="formula">t =0</text>
<text top="461" left="281" width="135" height="11" font="font7" id="p23_t39" reading_order_no="38" segment_no="8" tag_type="formula">γ t c (( A ( t ) , P ( t ))) | S (0) = s (0)</text>
<text top="452" left="415" width="6" height="4" font="font13" id="p23_t40" reading_order_no="39" segment_no="8" tag_type="formula">#</text>
<text top="463" left="423" width="3" height="9" font="font7" id="p23_t41" reading_order_no="40" segment_no="8" tag_type="formula">,</text>
<text top="463" left="507" width="25" height="9" font="font3" id="p23_t42" reading_order_no="41" segment_no="8" tag_type="text">(9.44)</text>
<text top="495" left="79" width="453" height="9" font="font3" id="p23_t43" reading_order_no="42" segment_no="10" tag_type="text">where the expectation is carried out through the distribution of the policy (if stochastic policy is used)</text>
<text top="507" left="79" width="453" height="9" font="font3" id="p23_t44" reading_order_no="43" segment_no="10" tag_type="text">and the distribution of the random variables A ( t ) and P ( t ). Given a state s ( t ) at time t , the policy looks</text>
<text top="519" left="79" width="454" height="9" font="font3" id="p23_t45" reading_order_no="44" segment_no="10" tag_type="text">for the set of files to be stored at time t + 1, i.e. a ( t + 1) according to the popularity profile observed so</text>
<text top="531" left="79" width="15" height="9" font="font3" id="p23_t46" reading_order_no="45" segment_no="10" tag_type="text">far.</text>
<text top="544" left="94" width="292" height="11" font="font3" id="p23_t47" reading_order_no="46" segment_no="11" tag_type="text">By denoting the state transition probabilities as P S ( t ) | S ( t − 1) , A ( t − 1)</text>
<text top="540" left="390" width="7" height="7" font="font10" id="p23_t48" reading_order_no="47" segment_no="11" tag_type="text">4</text>
<text top="543" left="390" width="142" height="10" font="font3" id="p23_t49" reading_order_no="48" segment_no="11" tag_type="text">= p ( s 0 | s , a ), the Q-function can</text>
<text top="556" left="79" width="193" height="9" font="font3" id="p23_t50" reading_order_no="49" segment_no="11" tag_type="text">be obtained using the Bellman’s equation as</text>
<text top="578" left="207" width="110" height="13" font="font7" id="p23_t51" reading_order_no="50" segment_no="12" tag_type="formula">q π ( s , a ) = c ( s , a ) + γ X</text>
<text top="594" left="301" width="17" height="9" font="font16" id="p23_t52" reading_order_no="51" segment_no="12" tag_type="formula">s 0 ∈S</text>
<text top="579" left="321" width="85" height="12" font="font7" id="p23_t53" reading_order_no="52" segment_no="12" tag_type="formula">p ( s 0 | s , a ) q π ( s 0 , a )</text>
<text top="581" left="507" width="25" height="9" font="font3" id="p23_t54" reading_order_no="53" segment_no="12" tag_type="text">(9.45)</text>
<text top="617" left="79" width="113" height="12" font="font3" id="p23_t55" reading_order_no="54" segment_no="13" tag_type="text">where c ( s , a ) = E πP S 0 | SA</text>
<text top="617" left="195" width="338" height="9" font="font3" id="p23_t56" reading_order_no="55" segment_no="13" tag_type="text">[ c ( A ( t ) , P ( t ))]. Finding the optimal policy in (9.44) that is the solution of</text>
<text top="631" left="79" width="453" height="9" font="font3" id="p23_t57" reading_order_no="56" segment_no="13" tag_type="text">(9.45), which is obtained after policy evaluation and improvement steps, requires one to know the dy-</text>
<text top="643" left="79" width="453" height="9" font="font3" id="p23_t58" reading_order_no="57" segment_no="13" tag_type="text">namics of the underlying Markov chain. The authors in [37] proposed a Q-learning algorithm with linear</text>
<text top="654" left="79" width="453" height="9" font="font3" id="p23_t59" reading_order_no="58" segment_no="13" tag_type="text">function approximation as introduced in Section 9.2.5 in order to cope with the high dimensionality of</text>
<text top="664" left="79" width="453" height="11" font="font3" id="p23_t60" reading_order_no="59" segment_no="13" tag_type="text">the problem, i.e. q ( s , a ) ≈ ψ ( s ) T ( 1 − a ) where ψ ( s ) is a state dependent feature vector that can be</text>
<text top="676" left="79" width="454" height="11" font="font3" id="p23_t61" reading_order_no="60" segment_no="13" tag_type="text">expressed as θ p + θ R a in which θ p represents the average cost of non caching files when the popularity is</text>
<text top="689" left="79" width="453" height="10" font="font3" id="p23_t62" reading_order_no="61" segment_no="13" tag_type="text">in state p , and θ R is the average cache refreshing cost per file. By adapting the recursion in (9.23) to the</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p23_t63" reading_order_no="62" segment_no="13" tag_type="text">problem above, i.e. the reward is replaced by the cost function in (9.43), the maximization is replaced by</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p23_t64" reading_order_no="63" segment_no="13" tag_type="text">a minimization operation. One can show that this technique is able to converge to the optimal caching</text>
<text top="726" left="79" width="247" height="9" font="font3" id="p23_t65" reading_order_no="64" segment_no="13" tag_type="text">strategy for large number of files and popularity profiles.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p23_t66" reading_order_no="65" segment_no="9" tag_type="text">23</text>
</page>
<page number="24" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p24_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="217" height="9" font="font6" id="p24_t2" reading_order_no="1" segment_no="1" tag_type="title">Multi-user scheduling and power allocation</text>
<text top="128" left="79" width="453" height="9" font="font3" id="p24_t3" reading_order_no="2" segment_no="2" tag_type="text">Let us extend the problem presented in Section 9.3.1 by adding multiple users in the system. An orthog-<a href="deeplearning_paper3.html#20">9.3.1 </a>by adding multiple users in the system. An orthog-</text>
<text top="140" left="79" width="454" height="9" font="font3" id="p24_t4" reading_order_no="3" segment_no="2" tag_type="text">onal frequency division multiple access downlink network, with a single cell and K users is considered.</text>
<text top="152" left="79" width="453" height="10" font="font3" id="p24_t5" reading_order_no="4" segment_no="2" tag_type="text">The whole time-frequency resource is divided in N rb resource blocks (RBs) and one RB is made of N s</text>
<text top="164" left="79" width="453" height="9" font="font3" id="p24_t6" reading_order_no="5" segment_no="2" tag_type="text">subcarriers. The base station handles K queues, one for each user, and has to serve the users by allocating</text>
<text top="176" left="79" width="453" height="9" font="font3" id="p24_t7" reading_order_no="6" segment_no="2" tag_type="text">the suitable transmission powers and the number of RBs in order to transmit the maximum number of</text>
<text top="188" left="79" width="394" height="9" font="font3" id="p24_t8" reading_order_no="7" segment_no="2" tag_type="text">bits with the minimal total power consumption and under buffer waiting time constraints.</text>
<text top="200" left="94" width="439" height="10" font="font3" id="p24_t9" reading_order_no="8" segment_no="3" tag_type="text">The channel gain is considered to be constant on one RB, i.e. over N s subcarriers and during ∆ T RB ,</text>
<text top="212" left="79" width="281" height="9" font="font3" id="p24_t10" reading_order_no="9" segment_no="3" tag_type="text">and varies from one RB to another according to the distribution</text>
<text top="241" left="184" width="97" height="13" font="font7" id="p24_t11" reading_order_no="10" segment_no="4" tag_type="formula">P H ( t +1) | H ( t ) ( h 0 | h ) =</text>
<text top="233" left="288" width="7" height="6" font="font8" id="p24_t12" reading_order_no="11" segment_no="4" tag_type="formula">K</text>
<text top="240" left="285" width="13" height="4" font="font13" id="p24_t13" reading_order_no="12" segment_no="4" tag_type="formula">Y</text>
<text top="257" left="284" width="15" height="6" font="font8" id="p24_t14" reading_order_no="13" segment_no="4" tag_type="formula">k =1</text>
<text top="233" left="301" width="12" height="6" font="font8" id="p24_t15" reading_order_no="14" segment_no="4" tag_type="formula">N rb</text>
<text top="240" left="301" width="13" height="4" font="font13" id="p24_t16" reading_order_no="15" segment_no="4" tag_type="formula">Y</text>
<text top="257" left="300" width="14" height="6" font="font8" id="p24_t17" reading_order_no="16" segment_no="4" tag_type="formula">r =1</text>
<text top="241" left="316" width="79" height="13" font="font7" id="p24_t18" reading_order_no="17" segment_no="4" tag_type="formula">P H kr ( t +1) | H kr ( t ) ( h 0</text>
<text top="243" left="393" width="35" height="11" font="font11" id="p24_t19" reading_order_no="18" segment_no="4" tag_type="formula">kr | h kr )</text>
<text top="274" left="79" width="453" height="10" font="font3" id="p24_t20" reading_order_no="19" segment_no="5" tag_type="text">with H kr ( t ) ∈ H the random variable representing the channel gain of user k ∈ K = { 1 , · · · , K } on RB</text>
<text top="286" left="79" width="454" height="10" font="font3" id="p24_t21" reading_order_no="20" segment_no="5" tag_type="text">number r ∈ N RB = { 1 , · · · , N RB } at time t . This relation means that the channel gains are independent</text>
<text top="298" left="79" width="453" height="9" font="font3" id="p24_t22" reading_order_no="21" segment_no="5" tag_type="text">from one RB to another in frequency and from a user to another. Similarly than in Section 9.3.1, the</text>
<text top="310" left="79" width="454" height="10" font="font3" id="p24_t23" reading_order_no="22" segment_no="5" tag_type="text">application layer of each user generates N k ( t ) bits/packets at time slot t according to the distribution</text>
<text top="322" left="79" width="454" height="10" font="font7" id="p24_t24" reading_order_no="23" segment_no="5" tag_type="text">P N ( n ). The generated bits are stored in a buffer for each user characterized by its size B k ( t ) ∈ B k ,</text>
<text top="333" left="79" width="454" height="11" font="font3" id="p24_t25" reading_order_no="24" segment_no="5" tag_type="text">where B k is defined similarly than in Section 9.3.1 for all users k ∈ K . We assume that only a packet</text>
<text top="346" left="79" width="454" height="9" font="font3" id="p24_t26" reading_order_no="25" segment_no="5" tag_type="text">of L information bits can be sent per user and per time slot. BS can choose the modulation and coding</text>
<text top="357" left="79" width="453" height="11" font="font3" id="p24_t27" reading_order_no="26" segment_no="5" tag_type="text">scheme (MCS) for each user, mcs k ∈ MC = { mcs 1 , · · · , mcs C } , i.e. a couple ( χ k , ρ k ) where χ k and ρ k</text>
<text top="370" left="79" width="454" height="9" font="font3" id="p24_t28" reading_order_no="27" segment_no="5" tag_type="text">are the modulation order in a QAM constellation and the rate of channel encoder for user k , respectively.</text>
<text top="382" left="79" width="453" height="9" font="font3" id="p24_t29" reading_order_no="28" segment_no="5" tag_type="text">The MCSs are ordered from the lowest to the highest spectral efficiency. A set of MCS used for the LTE</text>
<text top="394" left="79" width="160" height="9" font="font3" id="p24_t30" reading_order_no="29" segment_no="5" tag_type="text">system can be found in [38, Table I].</text>
<text top="406" left="94" width="438" height="9" font="font3" id="p24_t31" reading_order_no="30" segment_no="6" tag_type="text">The power and RBs allocation can be done at once by the BS by choosing the transmission power</text>
<text top="417" left="79" width="454" height="10" font="font3" id="p24_t32" reading_order_no="31" segment_no="6" tag_type="text">vector to allocate to user k over all the RBs at time t , P k ( t ), in the power state space P such that</text>
<text top="428" left="79" width="163" height="14" font="font11" id="p24_t33" reading_order_no="32" segment_no="6" tag_type="formula">P = n P | P ∈ { 0 , p 1 , · · · , p max } N RB ,</text>
<text top="431" left="251" width="4" height="6" font="font9" id="p24_t34" reading_order_no="33" segment_no="6" tag_type="formula">1</text>
<text top="438" left="245" width="15" height="7" font="font8" id="p24_t35" reading_order_no="34" segment_no="6" tag_type="formula">N RB</text>
<text top="430" left="262" width="36" height="13" font="font17" id="p24_t36" reading_order_no="35" segment_no="6" tag_type="formula">P T 1 N RB</text>
<text top="432" left="303" width="25" height="10" font="font11" id="p24_t37" reading_order_no="36" segment_no="6" tag_type="formula">≤ p tot</text>
<text top="428" left="329" width="204" height="14" font="font13" id="p24_t38" reading_order_no="37" segment_no="6" tag_type="formula">o , where p tot is the maximum average power</text>
<text top="447" left="79" width="454" height="9" font="font3" id="p24_t39" reading_order_no="38" segment_no="6" tag_type="text">budget that can be allocated to a user over all the subcarriers. The power of user k on RB r at time t ,</text>
<text top="459" left="79" width="454" height="10" font="font3" id="p24_t40" reading_order_no="39" segment_no="6" tag_type="text">i.e. p kr ( t ), is null if the RB is not used by the user. In papers where this kind of problem is handled with</text>
<text top="471" left="79" width="453" height="9" font="font3" id="p24_t41" reading_order_no="40" segment_no="6" tag_type="text">classical convex optimization tools, RB allocation is dealt with an auxiliary variable that is equal to one</text>
<text top="483" left="79" width="209" height="9" font="font3" id="p24_t42" reading_order_no="41" segment_no="6" tag_type="text">when user k uses RB r and 0 otherwise [39, 38].</text>
<text top="498" left="94" width="253" height="9" font="font3" id="p24_t43" reading_order_no="42" segment_no="7" tag_type="text">The error probability of user k is defined as in (9.36), i.e.</text>
<text top="502" left="356" width="4" height="6" font="font8" id="p24_t44" reading_order_no="43" segment_no="7" tag_type="text">k</text>
<text top="494" left="364" width="7" height="7" font="font10" id="p24_t45" reading_order_no="44" segment_no="7" tag_type="text">4</text>
<text top="498" left="364" width="169" height="11" font="font3" id="p24_t46" reading_order_no="45" segment_no="7" tag_type="text">= P [ˆ ω k ( t ) 6 = ω k ( t )], where ω k ( t ) is the</text>
<text top="510" left="79" width="453" height="9" font="font3" id="p24_t47" reading_order_no="46" segment_no="7" tag_type="text">message sent by user k at time t . It depends on the chosen MCS, the transmission power and the channel</text>
<text top="522" left="79" width="405" height="9" font="font3" id="p24_t48" reading_order_no="47" segment_no="7" tag_type="text">state experienced over each RB allocated to the user k . The queue dynamic of user k is then</text>
<text top="541" left="190" width="232" height="13" font="font7" id="p24_t49" reading_order_no="48" segment_no="8" tag_type="formula">B k ( t + 1) = [ B k ( t ) − L · 1 { ˆ ω k ( t ) } = ω k ( t )] + + N k ( t ) .</text>
<text top="544" left="507" width="25" height="9" font="font3" id="p24_t50" reading_order_no="49" segment_no="8" tag_type="text">(9.46)</text>
<text top="570" left="79" width="60" height="9" font="font6" id="p24_t51" reading_order_no="50" segment_no="9" tag_type="text">State space.</text>
<text top="570" left="150" width="383" height="9" font="font3" id="p24_t52" reading_order_no="51" segment_no="9" tag_type="text">The state space is made of the buffer state of each user, the channel gain state of each</text>
<text top="580" left="79" width="439" height="11" font="font3" id="p24_t53" reading_order_no="52" segment_no="9" tag_type="text">user on each RB allocated to it and the power consumed by each user, i.e. S = H KN RB × B K × P K .</text>
<text top="608" left="79" width="67" height="9" font="font6" id="p24_t54" reading_order_no="53" segment_no="10" tag_type="text">Action space.</text>
<text top="608" left="156" width="377" height="9" font="font3" id="p24_t55" reading_order_no="54" segment_no="10" tag_type="text">BS chooses the power and the MCS couple mcs k = ( χ k , ρ k ) to allocate to all users. The</text>
<text top="617" left="79" width="172" height="12" font="font3" id="p24_t56" reading_order_no="55" segment_no="10" tag_type="text">action space is hence A = P K × MC K .</text>
<text top="645" left="79" width="117" height="9" font="font6" id="p24_t57" reading_order_no="56" segment_no="11" tag_type="text">Reward/cost functions.</text>
<text top="645" left="206" width="326" height="9" font="font3" id="p24_t58" reading_order_no="57" segment_no="11" tag_type="text">The objective of the network operator may be to minimize the power con-</text>
<text top="657" left="79" width="453" height="9" font="font3" id="p24_t59" reading_order_no="58" segment_no="11" tag_type="text">sumed to serve all the K users in the cell while guaranteeing a limited buffer waiting time. We assume</text>
<text top="669" left="79" width="453" height="9" font="font3" id="p24_t60" reading_order_no="59" segment_no="11" tag_type="text">that the power consumption is only made of the transmit power, i.e. static power consumption is ne-</text>
<text top="681" left="79" width="454" height="9" font="font3" id="p24_t61" reading_order_no="60" segment_no="11" tag_type="text">glected. The power consumed by user k depends on the target error rate required, the observed channel</text>
<text top="693" left="79" width="366" height="9" font="font3" id="p24_t62" reading_order_no="61" segment_no="11" tag_type="text">state and the used MCS. The total power consumption of the cell can be written as</text>
<text top="721" left="198" width="10" height="9" font="font7" id="p24_t63" reading_order_no="62" segment_no="12" tag_type="formula">c :</text>
<text top="714" left="223" width="26" height="9" font="font11" id="p24_t64" reading_order_no="63" segment_no="12" tag_type="formula">A × S</text>
<text top="714" left="260" width="32" height="11" font="font11" id="p24_t65" reading_order_no="64" segment_no="12" tag_type="formula">−→ R +</text>
<text top="728" left="225" width="24" height="9" font="font3" id="p24_t66" reading_order_no="65" segment_no="12" tag_type="formula">( a , s )</text>
<text top="725" left="260" width="37" height="11" font="font11" id="p24_t67" reading_order_no="66" segment_no="12" tag_type="formula">7−→ P K</text>
<text top="726" left="290" width="113" height="13" font="font17" id="p24_t68" reading_order_no="67" segment_no="12" tag_type="formula">k =1 p k ( k , h k , mcs k ) T 1 N RB</text>
<text top="721" left="410" width="3" height="9" font="font7" id="p24_t69" reading_order_no="68" segment_no="12" tag_type="formula">.</text>
<text top="721" left="507" width="25" height="9" font="font3" id="p24_t70" reading_order_no="69" segment_no="12" tag_type="text">(9.47)</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p24_t71" reading_order_no="70" segment_no="13" tag_type="text">24</text>
</page>
<page number="25" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="117" height="9" font="font3" id="p25_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p25_t2" reading_order_no="1" segment_no="1" tag_type="text">The buffer waiting time cost is defined similarly than in (9.39), hence the total cost of the waiting time<a href="deeplearning_paper3.html#21">(9.39), </a>hence the total cost of the waiting time</text>
<text top="122" left="79" width="63" height="9" font="font3" id="p25_t3" reading_order_no="2" segment_no="1" tag_type="text">over the cell is</text>
<text top="146" left="116" width="14" height="9" font="font7" id="p25_t4" reading_order_no="3" segment_no="2" tag_type="formula">w :</text>
<text top="138" left="145" width="26" height="9" font="font11" id="p25_t5" reading_order_no="4" segment_no="2" tag_type="formula">A × S</text>
<text top="138" left="181" width="32" height="11" font="font11" id="p25_t6" reading_order_no="5" segment_no="2" tag_type="formula">−→ R +</text>
<text top="152" left="146" width="24" height="9" font="font3" id="p25_t7" reading_order_no="6" segment_no="2" tag_type="formula">( a , s )</text>
<text top="149" left="181" width="37" height="12" font="font11" id="p25_t8" reading_order_no="7" segment_no="2" tag_type="formula">7−→ P K</text>
<text top="146" left="211" width="256" height="17" font="font7" id="p25_t9" reading_order_no="8" segment_no="2" tag_type="formula">k =1 η 1 { b k ( t + 1) &gt; B max } + ( b k ( t ) − L 1 { ˆ ω k ( t ) = ω k ( t ) } ) ,</text>
<text top="146" left="507" width="25" height="9" font="font3" id="p25_t10" reading_order_no="9" segment_no="2" tag_type="text">(9.48)</text>
<text top="171" left="79" width="453" height="9" font="font3" id="p25_t11" reading_order_no="10" segment_no="3" tag_type="text">The average Lagrangian discounted cost, which is identified to our Q-function, can be obtained similarly</text>
<text top="183" left="79" width="93" height="9" font="font3" id="p25_t12" reading_order_no="11" segment_no="3" tag_type="text">than in Section 9.3.1.</text>
<text top="209" left="162" width="79" height="10" font="font7" id="p25_t13" reading_order_no="12" segment_no="4" tag_type="formula">q π ( s 0 , a 0 ; λ ) = E π</text>
<text top="198" left="243" width="17" height="7" font="font13" id="p25_t14" reading_order_no="13" segment_no="4" tag_type="formula">" ∞</text>
<text top="206" left="249" width="14" height="4" font="font13" id="p25_t15" reading_order_no="14" segment_no="4" tag_type="formula">X</text>
<text top="223" left="250" width="13" height="6" font="font8" id="p25_t16" reading_order_no="15" segment_no="4" tag_type="formula">t =0</text>
<text top="207" left="265" width="175" height="11" font="font7" id="p25_t17" reading_order_no="16" segment_no="4" tag_type="formula">γ t ` ( A ( t ) , S ( t ); λ ) | S (0) = s 0 , A ( t ) = a 0</text>
<text top="198" left="440" width="6" height="4" font="font13" id="p25_t18" reading_order_no="17" segment_no="4" tag_type="formula">#</text>
<text top="209" left="448" width="3" height="9" font="font7" id="p25_t19" reading_order_no="18" segment_no="4" tag_type="formula">,</text>
<text top="209" left="507" width="25" height="9" font="font3" id="p25_t20" reading_order_no="19" segment_no="4" tag_type="text">(9.49)</text>
<text top="237" left="79" width="453" height="10" font="font3" id="p25_t21" reading_order_no="20" segment_no="5" tag_type="text">where s 0 , a 0 are the initial state and action vectors and ` ( a , s ; λ ) is defined similarly than in Section</text>
<text top="249" left="79" width="358" height="9" font="font3" id="p25_t22" reading_order_no="21" segment_no="5" tag_type="text">9.3.1 but with the functions in (9.47) and (9.48) and λ is the Lagrange multiplier.</text>
<text top="275" left="79" width="33" height="9" font="font6" id="p25_t23" reading_order_no="22" segment_no="6" tag_type="text">Policy.</text>
<text top="273" left="122" width="410" height="11" font="font3" id="p25_t24" reading_order_no="23" segment_no="6" tag_type="text">The problem is to find the policy π ∗ that minimizes (9.49) for all ( s 0 , a 0 ) for a given λ . This</text>
<text top="287" left="79" width="453" height="9" font="font3" id="p25_t25" reading_order_no="24" segment_no="6" tag_type="text">optimization problem deals with a huge number of variables and the classical tabulated Q-learning requires</text>
<text top="299" left="79" width="453" height="9" font="font3" id="p25_t26" reading_order_no="25" segment_no="6" tag_type="text">to much time to converge and too much capacity storage. In this situation, DRL can be a suitable solution</text>
<text top="311" left="79" width="453" height="9" font="font3" id="p25_t27" reading_order_no="26" segment_no="6" tag_type="text">to approach the function in (9.49) by implementing, for instance, an FNN as illustrated in Fig. 9.3. For</text>
<text top="323" left="79" width="453" height="9" font="font3" id="p25_t28" reading_order_no="27" segment_no="6" tag_type="text">a given set of parameters of the deep network at time t , i.e. Θ t , the loss function is defined as in (9.25)</text>
<text top="335" left="79" width="454" height="10" font="font3" id="p25_t29" reading_order_no="28" segment_no="6" tag_type="text">where R ( t ) = ` ( A ( t ) , S ( t ); λ ) and the optimal Q-function is approached at time t by q Θ t ( S ( t ) , A ( t ); λ ).</text>
<text top="347" left="79" width="437" height="9" font="font3" id="p25_t30" reading_order_no="29" segment_no="6" tag_type="text">The optimal set of weights of the neural network can be updated using (9.26) with proper variables.</text>
<text top="373" left="79" width="28" height="11" font="font15" id="p25_t31" reading_order_no="30" segment_no="7" tag_type="title">9.3.3</text>
<text top="373" left="120" width="124" height="11" font="font15" id="p25_t32" reading_order_no="31" segment_no="7" tag_type="title">Examples with MAB</text>
<text top="394" left="79" width="149" height="9" font="font6" id="p25_t33" reading_order_no="32" segment_no="8" tag_type="title">Multi-channel access problem</text>
<text top="412" left="79" width="454" height="9" font="font3" id="p25_t34" reading_order_no="33" segment_no="9" tag_type="text">Let us consider a set of K independent channels, i.e. K = { 1 , · · · , K } that can be used opportunistically</text>
<text top="424" left="79" width="454" height="9" font="font3" id="p25_t35" reading_order_no="34" segment_no="9" tag_type="text">by U users, with K ≥ U to communicate with a BS. The channels may be also used by other users that</text>
<text top="436" left="79" width="453" height="9" font="font3" id="p25_t36" reading_order_no="35" segment_no="9" tag_type="text">belong to a primary network. The users can sense one or more channels to estimate if they are used</text>
<text top="448" left="79" width="453" height="9" font="font3" id="p25_t37" reading_order_no="36" segment_no="9" tag_type="text">or not. To perform this task, the users can rely on multiple signal processing techniques ranging from</text>
<text top="460" left="79" width="453" height="9" font="font3" id="p25_t38" reading_order_no="37" segment_no="9" tag_type="text">the simple energy detector to sophisticated signal classifiers [40]. If the channel is detected to be free,</text>
<text top="472" left="79" width="453" height="9" font="font3" id="p25_t39" reading_order_no="38" segment_no="9" tag_type="text">the user transmits in that band, otherwise the transmitter remains silent. The action space is hence</text>
<text top="484" left="79" width="454" height="9" font="font11" id="p25_t40" reading_order_no="39" segment_no="9" tag_type="text">A = { transmit , silent } . When the channel is free however, it may be rated with a low or a high quality</text>
<text top="496" left="79" width="453" height="9" font="font3" id="p25_t41" reading_order_no="40" segment_no="9" tag_type="text">depending on the level of the received SINR for instance or the actual data rate a user experienced on</text>
<text top="507" left="79" width="453" height="10" font="font3" id="p25_t42" reading_order_no="41" segment_no="9" tag_type="text">it. The state space may be limited to S = { busy , free } but the quality of the band can be included in</text>
<text top="520" left="79" width="453" height="9" font="font3" id="p25_t43" reading_order_no="42" segment_no="9" tag_type="text">the reward function as it has been proposed in [41, 23]. In the case where a single user is considered i.e.</text>
<text top="532" left="79" width="29" height="9" font="font7" id="p25_t44" reading_order_no="43" segment_no="9" tag_type="text">U = 1:</text>
<text top="543" left="244" width="124" height="10" font="font7" id="p25_t45" reading_order_no="44" segment_no="10" tag_type="formula">R k ( t ) = (1 − S k ( t )) f ( S k ( t )) ,</text>
<text top="544" left="507" width="25" height="9" font="font3" id="p25_t46" reading_order_no="45" segment_no="10" tag_type="text">(9.50)</text>
<text top="560" left="79" width="454" height="10" font="font3" id="p25_t47" reading_order_no="46" segment_no="11" tag_type="text">where S k ( t ) ∈ { 0 , 1 } is the state of band k at time t where 1 means that the band is detected as occupied</text>
<text top="572" left="79" width="454" height="10" font="font3" id="p25_t48" reading_order_no="47" segment_no="11" tag_type="text">and 0 that is free. Moreover, f ( S k ( t )) is the observed data rate on band k when it is in the state S k ( t ),</text>
<text top="583" left="79" width="245" height="11" font="font3" id="p25_t49" reading_order_no="48" segment_no="11" tag_type="text">and it can be considered that f ( S k ( t )) ∈ [0 , 1] , ∀ S k ( t ) 15 .</text>
<text top="596" left="94" width="439" height="9" font="font3" id="p25_t50" reading_order_no="49" segment_no="12" tag_type="text">The goal for an agent is to select the band k that maximizes the data rate on the long run. If the</text>
<text top="608" left="79" width="453" height="9" font="font3" id="p25_t51" reading_order_no="50" segment_no="12" tag_type="text">expected rewards of each band were known, the optimal strategy would be to sense and to transmit (when</text>
<text top="620" left="79" width="453" height="9" font="font3" id="p25_t52" reading_order_no="51" segment_no="12" tag_type="text">possible) always on the band presenting the maximal expected reward. In the absence of this knowledge,</text>
<text top="632" left="79" width="454" height="9" font="font3" id="p25_t53" reading_order_no="52" segment_no="12" tag_type="text">the agent has to learn what are the best bands and concentrate on them. An index-based policy can be</text>
<text top="644" left="79" width="453" height="9" font="font3" id="p25_t54" reading_order_no="53" segment_no="12" tag_type="text">proposed to solve this problem, like UCB algorithm which is order-optimal as explained in Section 9.2.6.</text>
<text top="656" left="79" width="453" height="9" font="font3" id="p25_t55" reading_order_no="54" segment_no="12" tag_type="text">For instance, in [23], authors proposed to compute the index I k ( t ) for each band k and choose the one</text>
<text top="668" left="79" width="297" height="9" font="font3" id="p25_t56" reading_order_no="55" segment_no="12" tag_type="text">with the highest value at the next round. The index is computed as</text>
<text top="693" left="247" width="72" height="9" font="font7" id="p25_t57" reading_order_no="56" segment_no="13" tag_type="formula">I k ( t ) = r k ( t ) + g<a href="deeplearning_paper3.html#20">9.3.1.</a></text>
<text top="686" left="339" width="4" height="9" font="font7" id="p25_t58" reading_order_no="57" segment_no="13" tag_type="formula">t</text>
<text top="700" left="329" width="23" height="9" font="font7" id="p25_t59" reading_order_no="58" segment_no="13" tag_type="formula">n k ( t )</text>
<text top="693" left="362" width="3" height="9" font="font7" id="p25_t60" reading_order_no="59" segment_no="13" tag_type="formula">,</text>
<text top="693" left="507" width="25" height="9" font="font3" id="p25_t61" reading_order_no="60" segment_no="13" tag_type="text">(9.51)</text>
<text top="717" left="87" width="446" height="8" font="font21" id="p25_t62" reading_order_no="61" segment_no="14" tag_type="footnote">15 By convention, f (1) = 0. Moreover, the experienced data rate can be normalized w.r.t. the channel capacity achievable</text>
<text top="728" left="79" width="59" height="7" font="font21" id="p25_t63" reading_order_no="62" segment_no="14" tag_type="footnote">in that channel.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p25_t64" reading_order_no="63" segment_no="15" tag_type="text">25</text>
</page>
<page number="26" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p26_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="355" height="10" font="font3" id="p26_t2" reading_order_no="1" segment_no="1" tag_type="text">where n k ( t ) is the number of times band k has been sensed up to time t , r k ( t ) =</text>
<text top="109" left="444" width="4" height="6" font="font9" id="p26_t3" reading_order_no="2" segment_no="1" tag_type="text">1</text>
<text top="116" left="437" width="19" height="6" font="font8" id="p26_t4" reading_order_no="3" segment_no="1" tag_type="text">n k ( t )</text>
<text top="107" left="459" width="28" height="7" font="font13" id="p26_t5" reading_order_no="4" segment_no="1" tag_type="text">P n k ( t )</text>
<text top="109" left="469" width="64" height="12" font="font7" id="p26_t6" reading_order_no="5" segment_no="1" tag_type="text">t 0 =1 r k ( p k ( t 0 ))</text>
<text top="122" left="79" width="454" height="12" font="font3" id="p26_t7" reading_order_no="6" segment_no="1" tag_type="text">where p k ( t 0 ) is a sensing time instant corresponding to the t 0 -th visit of the band k . Finally, g is a concave</text>
<text top="134" left="79" width="453" height="11" font="font3" id="p26_t8" reading_order_no="7" segment_no="1" tag_type="text">function with g (1) = 0, e.g. g ( x ) = p log( x ). One may notice that this function is different from the</text>
<text top="147" left="79" width="338" height="14" font="font3" id="p26_t9" reading_order_no="8" segment_no="1" tag_type="text">classical bias of UCB introduces in Section 9.2.6, which is in the form q log( t )</text>
<text top="152" left="398" width="134" height="12" font="font3" id="p26_t10" reading_order_no="9" segment_no="1" tag_type="text">n k ( t ) . The authors proved that</text>
<text top="166" left="79" width="453" height="9" font="font3" id="p26_t11" reading_order_no="10" segment_no="1" tag_type="text">the proposed index policy has an aggressive exploration characteristic compared to the UCB where the</text>
<text top="178" left="79" width="453" height="9" font="font3" id="p26_t12" reading_order_no="11" segment_no="1" tag_type="text">bias increases slowly. An aggressive exploration statistic means that the optimal data rate will be reached</text>
<text top="190" left="79" width="453" height="9" font="font3" id="p26_t13" reading_order_no="12" segment_no="1" tag_type="text">faster than in a non aggressive exploration characteristic (classical UCB) but it will also be lower than</text>
<text top="202" left="79" width="55" height="9" font="font3" id="p26_t14" reading_order_no="13" segment_no="1" tag_type="text">classic UCB.</text>
<text top="214" left="94" width="438" height="9" font="font3" id="p26_t15" reading_order_no="14" segment_no="2" tag_type="text">According to the statistical model of the rewards, the problem described may fall into the classes of</text>
<text top="226" left="79" width="453" height="9" font="font3" id="p26_t16" reading_order_no="15" segment_no="2" tag_type="text">rested MAB or restless MAB. If the rewards, i.e. the experienced data rate of the users, are i.i.d. on each</text>
<text top="238" left="79" width="453" height="10" font="font3" id="p26_t17" reading_order_no="16" segment_no="2" tag_type="text">band, the problem is a rested-MAB. If however, the state S k ( t ) is described by a 2-state Markov chain,</text>
<text top="250" left="79" width="453" height="9" font="font3" id="p26_t18" reading_order_no="17" segment_no="2" tag_type="text">i.e. Gilbert-Elliot model, so the data rate, and hence the problem can be classified as a restless MAB. In</text>
<text top="262" left="79" width="407" height="9" font="font3" id="p26_t19" reading_order_no="18" segment_no="2" tag_type="text">[23] authors showed that the policy is order-optimal for i.i.d. and Markovian rewards as well.</text>
<text top="274" left="94" width="438" height="9" font="font3" id="p26_t20" reading_order_no="19" segment_no="3" tag_type="text">In the previous example, the reward signal was a function of the state of the channel sensed and</text>
<text top="286" left="79" width="453" height="9" font="font3" id="p26_t21" reading_order_no="20" segment_no="3" tag_type="text">the data rate experienced in this channel by the user. In some applications, one may be interested in</text>
<text top="298" left="79" width="453" height="9" font="font3" id="p26_t22" reading_order_no="21" segment_no="3" tag_type="text">acquiring knowledge both on the channel availability and on channel quality. The channel quality has</text>
<text top="310" left="79" width="453" height="9" font="font3" id="p26_t23" reading_order_no="22" segment_no="3" tag_type="text">to be taken in a broad sense. It may be the noise level, including the average power density of the</text>
<text top="322" left="79" width="453" height="9" font="font3" id="p26_t24" reading_order_no="23" segment_no="3" tag_type="text">interference, the average data rate experienced in this band, etc. The dynamic spectrum access with</text>
<text top="334" left="79" width="453" height="9" font="font3" id="p26_t25" reading_order_no="24" segment_no="3" tag_type="text">different channel quality can be represented as in Fig. 9.4. In this example, the state space is extended</text>
<text top="345" left="79" width="454" height="10" font="font3" id="p26_t26" reading_order_no="25" segment_no="3" tag_type="text">to S = { busy , low quality , high quality } . The action space is still the same, i.e. A = { transmit , silent } ,</text>
<text top="358" left="79" width="453" height="9" font="font3" id="p26_t27" reading_order_no="26" segment_no="3" tag_type="text">but the transmission now occurs on a state, i.e. channel is free, that can be explicitly rated with a high</text>
<text top="370" left="79" width="453" height="9" font="font3" id="p26_t28" reading_order_no="27" segment_no="3" tag_type="text">or low quality. On may solve this problem by considering a full RL approach, using value function as</text>
<text top="382" left="79" width="453" height="9" font="font3" id="p26_t29" reading_order_no="28" segment_no="3" tag_type="text">described in Section 9.2. However, a simple index-based policy can perform very well in this context.</text>
<text top="393" left="79" width="454" height="9" font="font3" id="p26_t30" reading_order_no="29" segment_no="3" tag_type="text">Authors in [42] proposed that each user u ∈ { 1 , · · · , U } can compute the following index for each band</text>
<text top="405" left="79" width="244" height="9" font="font3" id="p26_t31" reading_order_no="30" segment_no="3" tag_type="text">and choose the maximum one as the next band to sense</text>
<text top="433" left="199" width="32" height="12" font="font7" id="p26_t32" reading_order_no="31" segment_no="4" tag_type="formula">I u k ( t ) =</text>
<text top="428" left="244" width="5" height="9" font="font3" id="p26_t33" reading_order_no="32" segment_no="4" tag_type="formula">1</text>
<text top="440" left="235" width="11" height="10" font="font7" id="p26_t34" reading_order_no="33" segment_no="4" tag_type="formula">n u</text>
<text top="441" left="241" width="17" height="12" font="font3" id="p26_t35" reading_order_no="34" segment_no="4" tag_type="formula">k ( t )</text>
<text top="422" left="260" width="19" height="9" font="font8" id="p26_t36" reading_order_no="35" segment_no="4" tag_type="formula">n u k ( t )</text>
<text top="432" left="263" width="14" height="4" font="font13" id="p26_t37" reading_order_no="36" segment_no="4" tag_type="formula">X</text>
<text top="448" left="262" width="16" height="7" font="font8" id="p26_t38" reading_order_no="37" segment_no="4" tag_type="formula">t 0 =1</text>
<text top="432" left="281" width="86" height="13" font="font7" id="p26_t39" reading_order_no="38" segment_no="4" tag_type="formula">s u k ( p u k ( t 0 )) − q u k ( t ) +</text>
<text top="424" left="369" width="43" height="13" font="font13" id="p26_t40" reading_order_no="39" segment_no="4" tag_type="formula">s α log( t )</text>
<text top="440" left="385" width="11" height="10" font="font7" id="p26_t41" reading_order_no="40" segment_no="4" tag_type="formula">n u</text>
<text top="441" left="391" width="16" height="12" font="font3" id="p26_t42" reading_order_no="41" segment_no="4" tag_type="formula">k ( t )</text>
<text top="435" left="507" width="25" height="9" font="font3" id="p26_t43" reading_order_no="42" segment_no="4" tag_type="text">(9.52)</text>
<text top="464" left="79" width="454" height="9" font="font3" id="p26_t44" reading_order_no="43" segment_no="5" tag_type="text">where the first term is the empirical average of the reward obtained by user u when sensing band k , i.e.</text>
<text top="476" left="79" width="453" height="9" font="font3" id="p26_t45" reading_order_no="44" segment_no="5" tag_type="text">0 if the channel is occupied and 1 if the channel is free. The second term is a function of the empirical</text>
<text top="488" left="79" width="272" height="9" font="font3" id="p26_t46" reading_order_no="45" segment_no="5" tag_type="text">average of the quality of band k for user u and is expressed as</text>
<text top="511" left="257" width="10" height="11" font="font7" id="p26_t47" reading_order_no="46" segment_no="6" tag_type="formula">q u</text>
<text top="505" left="262" width="51" height="19" font="font3" id="p26_t48" reading_order_no="47" segment_no="6" tag_type="formula">k ( t ) = βm u</text>
<text top="507" left="308" width="43" height="11" font="font3" id="p26_t49" reading_order_no="48" segment_no="6" tag_type="formula">k ( t ) log( t )</text>
<text top="519" left="311" width="22" height="12" font="font7" id="p26_t50" reading_order_no="49" segment_no="6" tag_type="formula">n u k ( t )</text>
<text top="513" left="352" width="3" height="9" font="font7" id="p26_t51" reading_order_no="50" segment_no="6" tag_type="formula">,</text>
<text top="513" left="507" width="25" height="9" font="font3" id="p26_t52" reading_order_no="51" segment_no="6" tag_type="text">(9.53)</text>
<text top="540" left="79" width="119" height="13" font="font3" id="p26_t53" reading_order_no="52" segment_no="7" tag_type="text">where m u k ( t ) = g u ∗ ( t ) − g u,k</text>
<text top="547" left="187" width="4" height="6" font="font9" id="p26_t54" reading_order_no="53" segment_no="7" tag_type="text">1</text>
<text top="540" left="199" width="51" height="11" font="font3" id="p26_t55" reading_order_no="54" segment_no="7" tag_type="text">( t ) and g u,k</text>
<text top="547" left="238" width="4" height="6" font="font9" id="p26_t56" reading_order_no="55" segment_no="7" tag_type="text">1</text>
<text top="542" left="251" width="282" height="9" font="font3" id="p26_t57" reading_order_no="56" segment_no="7" tag_type="text">( t ) denotes the empirical average of the quality of band k , when</text>
<text top="554" left="79" width="244" height="9" font="font3" id="p26_t58" reading_order_no="57" segment_no="7" tag_type="text">available, sensed by user u at time t and is expressed as</text>
<text top="583" left="236" width="16" height="11" font="font7" id="p26_t59" reading_order_no="58" segment_no="8" tag_type="formula">g u,k</text>
<text top="590" left="241" width="4" height="6" font="font9" id="p26_t60" reading_order_no="59" segment_no="8" tag_type="formula">1</text>
<text top="585" left="253" width="21" height="9" font="font3" id="p26_t61" reading_order_no="60" segment_no="8" tag_type="formula">( t ) =</text>
<text top="579" left="287" width="5" height="9" font="font3" id="p26_t62" reading_order_no="61" segment_no="8" tag_type="formula">1</text>
<text top="591" left="279" width="22" height="12" font="font7" id="p26_t63" reading_order_no="62" segment_no="8" tag_type="formula">n u k ( t )</text>
<text top="572" left="304" width="19" height="9" font="font8" id="p26_t64" reading_order_no="63" segment_no="8" tag_type="formula">n u k ( t )</text>
<text top="582" left="306" width="14" height="4" font="font13" id="p26_t65" reading_order_no="64" segment_no="8" tag_type="formula">X</text>
<text top="598" left="306" width="15" height="7" font="font8" id="p26_t66" reading_order_no="65" segment_no="8" tag_type="formula">t 0 =1<a href="deeplearning_paper3.html#15">9.2.6, </a>which is in the form</text>
<text top="583" left="325" width="15" height="11" font="font7" id="p26_t67" reading_order_no="66" segment_no="8" tag_type="formula">r u,k</text>
<text top="590" left="329" width="4" height="6" font="font9" id="p26_t68" reading_order_no="67" segment_no="8" tag_type="formula">1</text>
<text top="583" left="341" width="14" height="11" font="font3" id="p26_t69" reading_order_no="68" segment_no="8" tag_type="formula">( p u</text>
<text top="583" left="350" width="26" height="13" font="font3" id="p26_t70" reading_order_no="69" segment_no="8" tag_type="formula">k ( t 0 )) ,</text>
<text top="585" left="507" width="25" height="9" font="font3" id="p26_t71" reading_order_no="70" segment_no="8" tag_type="text">(9.54)</text>
<text top="615" left="79" width="46" height="11" font="font3" id="p26_t72" reading_order_no="71" segment_no="9" tag_type="text">where r u,k</text>
<text top="622" left="113" width="4" height="6" font="font9" id="p26_t73" reading_order_no="72" segment_no="9" tag_type="text">1</text>
<text top="615" left="126" width="407" height="13" font="font3" id="p26_t74" reading_order_no="73" segment_no="9" tag_type="text">( p k u ( t 0 )) is the reward obtained by user u rating the quality of band k , between 0 and 1, at</text>
<text top="628" left="79" width="179" height="14" font="font3" id="p26_t75" reading_order_no="74" segment_no="9" tag_type="text">time p u k ( t 0 ). Finally, g u ∗ ( t ) = max k ∈K g u,k</text>
<text top="635" left="247" width="4" height="6" font="font9" id="p26_t76" reading_order_no="75" segment_no="9" tag_type="text">1</text>
<text top="631" left="259" width="274" height="9" font="font3" id="p26_t77" reading_order_no="76" segment_no="9" tag_type="text">( t ). The last term in (9.52) is the classical exploration term of</text>
<text top="643" left="79" width="453" height="9" font="font3" id="p26_t78" reading_order_no="77" segment_no="9" tag_type="text">UCB algorithm where the parameter α forces the exploration of other bands to find channels that are</text>
<text top="654" left="79" width="107" height="9" font="font3" id="p26_t79" reading_order_no="78" segment_no="9" tag_type="text">the most often available.</text>
<text top="666" left="94" width="438" height="9" font="font3" id="p26_t80" reading_order_no="79" segment_no="10" tag_type="text">The parameter β in (9.53) gives weight to the channel quality. At each of iteration, the agent computes</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p26_t81" reading_order_no="80" segment_no="10" tag_type="text">the empirical mean, up to the current time instant, on the quality observed if the band is free. In the</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p26_t82" reading_order_no="81" segment_no="10" tag_type="text">same time, the best channel among those already tried is updated and the score is computed by weighting</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p26_t83" reading_order_no="82" segment_no="10" tag_type="text">the difference between the estimated best channel and the current channel. If α and β increase, the agent</text>
<text top="714" left="79" width="454" height="9" font="font3" id="p26_t84" reading_order_no="83" segment_no="10" tag_type="text">explores more than it exploits. When α and β decrease, the empirical mean of the states dominates the</text>
<text top="726" left="79" width="443" height="9" font="font3" id="p26_t85" reading_order_no="84" segment_no="10" tag_type="text">index calculation and the exploitation of the best band computed in the previous iteration is favored.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p26_t86" reading_order_no="85" segment_no="11" tag_type="text">26</text>
</page>
<page number="27" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="117" height="9" font="font3" id="p27_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="110" left="296" width="60" height="9" font="font3" id="p27_t2" reading_order_no="1" segment_no="1" tag_type="figure">Time instants</text>
<text top="139" left="148" width="44" height="9" font="font3" id="p27_t3" reading_order_no="2" segment_no="1" tag_type="figure">Channel 1</text>
<text top="139" left="219" width="16" height="9" font="font3" id="p27_t4" reading_order_no="3" segment_no="1" tag_type="figure">free</text>
<text top="139" left="245" width="77" height="9" font="font3" id="p27_t5" reading_order_no="4" segment_no="1" tag_type="figure">busy busy busy</text>
<text top="139" left="332" width="16" height="9" font="font3" id="p27_t6" reading_order_no="5" segment_no="1" tag_type="figure">free</text>
<text top="139" left="359" width="20" height="9" font="font3" id="p27_t7" reading_order_no="6" segment_no="1" tag_type="figure">busy</text>
<text top="139" left="389" width="16" height="9" font="font3" id="p27_t8" reading_order_no="7" segment_no="1" tag_type="figure">free</text>
<text top="139" left="415" width="49" height="9" font="font3" id="p27_t9" reading_order_no="8" segment_no="1" tag_type="figure">busy busy</text>
<text top="167" left="148" width="44" height="9" font="font3" id="p27_t10" reading_order_no="9" segment_no="1" tag_type="figure">Channel 2</text>
<text top="166" left="217" width="20" height="9" font="font3" id="p27_t11" reading_order_no="10" segment_no="1" tag_type="figure">busy</text>
<text top="167" left="247" width="16" height="9" font="font3" id="p27_t12" reading_order_no="11" segment_no="1" tag_type="figure">free</text>
<text top="167" left="276" width="16" height="9" font="font3" id="p27_t13" reading_order_no="12" segment_no="1" tag_type="figure">free</text>
<text top="166" left="302" width="20" height="9" font="font3" id="p27_t14" reading_order_no="13" segment_no="1" tag_type="figure">busy</text>
<text top="167" left="332" width="16" height="9" font="font3" id="p27_t15" reading_order_no="14" segment_no="1" tag_type="figure">free</text>
<text top="167" left="361" width="16" height="9" font="font3" id="p27_t16" reading_order_no="15" segment_no="1" tag_type="figure">free</text>
<text top="166" left="387" width="77" height="9" font="font3" id="p27_t17" reading_order_no="16" segment_no="1" tag_type="figure">busy busy busy</text>
<text top="224" left="147" width="47" height="9" font="font3" id="p27_t18" reading_order_no="17" segment_no="1" tag_type="figure">Channel K</text>
<text top="224" left="219" width="16" height="9" font="font3" id="p27_t19" reading_order_no="18" segment_no="1" tag_type="figure">free</text>
<text top="223" left="245" width="20" height="9" font="font3" id="p27_t20" reading_order_no="19" segment_no="1" tag_type="figure">busy</text>
<text top="224" left="276" width="16" height="9" font="font3" id="p27_t21" reading_order_no="20" segment_no="1" tag_type="figure">free</text>
<text top="223" left="302" width="20" height="9" font="font3" id="p27_t22" reading_order_no="21" segment_no="1" tag_type="figure">busy</text>
<text top="224" left="332" width="16" height="9" font="font3" id="p27_t23" reading_order_no="22" segment_no="1" tag_type="figure">free</text>
<text top="224" left="361" width="16" height="9" font="font3" id="p27_t24" reading_order_no="23" segment_no="1" tag_type="figure">free</text>
<text top="224" left="389" width="16" height="9" font="font3" id="p27_t25" reading_order_no="24" segment_no="1" tag_type="figure">free</text>
<text top="224" left="417" width="16" height="9" font="font3" id="p27_t26" reading_order_no="25" segment_no="1" tag_type="figure">free</text>
<text top="223" left="444" width="20" height="9" font="font3" id="p27_t27" reading_order_no="26" segment_no="1" tag_type="figure">busy</text>
<text top="266" left="247" width="16" height="9" font="font3" id="p27_t28" reading_order_no="27" segment_no="1" tag_type="figure">free</text>
<text top="268" left="279" width="48" height="9" font="font3" id="p27_t29" reading_order_no="28" segment_no="1" tag_type="figure">low quality</text>
<text top="266" left="361" width="16" height="9" font="font3" id="p27_t30" reading_order_no="29" segment_no="1" tag_type="figure">free</text>
<text top="268" left="391" width="52" height="9" font="font3" id="p27_t31" reading_order_no="30" segment_no="1" tag_type="figure">high quality</text>
<text top="309" left="155" width="301" height="9" font="font3" id="p27_t32" reading_order_no="31" segment_no="2" tag_type="text">Figure 9.4: Dynamic spectrum access with different channel qualities</text>
<text top="342" left="94" width="438" height="9" font="font3" id="p27_t33" reading_order_no="32" segment_no="3" tag_type="text">Due to the restless nature of the problem, the index computation cannot be done when one just starts</text>
<text top="354" left="79" width="453" height="9" font="font3" id="p27_t34" reading_order_no="33" segment_no="3" tag_type="text">observing a Markov chain after being selected. Indeed, arms evolve independently irrespective of which</text>
<text top="366" left="79" width="453" height="9" font="font3" id="p27_t35" reading_order_no="34" segment_no="3" tag_type="text">band is selected or not (i.e. restless MAB) and the distribution of rewards that user u gets from a band k</text>
<text top="378" left="79" width="453" height="9" font="font3" id="p27_t36" reading_order_no="35" segment_no="3" tag_type="text">is a function of the time elapsed since the last time user u sensed this band. The sequence of observations</text>
<text top="390" left="79" width="453" height="9" font="font3" id="p27_t37" reading_order_no="36" segment_no="3" tag_type="text">of a band that is not continuously sensed does not correspond to a Markov chain. To overcome this</text>
<text top="402" left="79" width="453" height="9" font="font3" id="p27_t38" reading_order_no="37" segment_no="3" tag_type="text">issue, when user u observes a given band, algorithm waits for encountering a predefined state, named</text>
<text top="414" left="79" width="453" height="9" font="font5" id="p27_t39" reading_order_no="38" segment_no="3" tag_type="text">regenerative state e.g. ξ k , [26]. Once ξ k is encountered, rewards are started to be recorded until ξ k be</text>
<text top="424" left="79" width="454" height="12" font="font3" id="p27_t40" reading_order_no="39" segment_no="3" tag_type="text">observed a second time and the policy index I u k is computed and another band selected according to the</text>
<text top="438" left="79" width="453" height="9" font="font3" id="p27_t41" reading_order_no="40" segment_no="3" tag_type="text">result. This structure is necessary to deal with the restless nature of the problem in order to re-create the</text>
<text top="449" left="79" width="453" height="9" font="font3" id="p27_t42" reading_order_no="41" segment_no="3" tag_type="text">condition of continuous observation of Markov chain. It is worth mentioning, however, that exploitation</text>
<text top="461" left="79" width="243" height="9" font="font3" id="p27_t43" reading_order_no="42" segment_no="3" tag_type="text">in this context occurs whenever a free band is detected.</text>
<text top="474" left="94" width="438" height="9" font="font3" id="p27_t44" reading_order_no="43" segment_no="4" tag_type="text">Moreover, the multi-player setting makes the problem rather involved since collisions between agents</text>
<text top="486" left="79" width="453" height="9" font="font3" id="p27_t45" reading_order_no="44" segment_no="4" tag_type="text">need to be handled. The random rank idea from [43] has been adapted to this problem. Each user</text>
<text top="497" left="79" width="454" height="10" font="font3" id="p27_t46" reading_order_no="45" segment_no="4" tag_type="text">maintains an ordered set of channel indexes (arms indexes), i.e. K u = σ u ( K ), where σ u is a permutation</text>
<text top="509" left="79" width="454" height="10" font="font3" id="p27_t47" reading_order_no="46" segment_no="4" tag_type="text">of { 1 , · · · , K } for user u , with σ u (1) &gt; · · · &gt; σ u ( K ) from the best to the worst rated. The rank r for user</text>
<text top="520" left="79" width="454" height="11" font="font7" id="p27_t48" reading_order_no="47" segment_no="4" tag_type="text">u corresponds to the r − th entry in the set K u . If users u and u 0 choose the same channel to sense the</text>
<text top="533" left="79" width="454" height="10" font="font3" id="p27_t49" reading_order_no="48" segment_no="4" tag_type="text">next time slot, they collide. In that case, they draw a random number from their respective sets K u and</text>
<text top="545" left="79" width="325" height="10" font="font11" id="p27_t50" reading_order_no="49" segment_no="4" tag_type="text">K u 0 as their new rank and go for these new channels in the next time slot.</text>
<text top="575" left="79" width="90" height="9" font="font6" id="p27_t51" reading_order_no="50" segment_no="5" tag_type="title">Green networking</text>
<text top="594" left="79" width="453" height="9" font="font3" id="p27_t52" reading_order_no="51" segment_no="6" tag_type="text">The radio access networks are not used at their full capacity all the time. Experimental results in voice<a href="deeplearning_paper3.html#44">[26]. </a>Once</text>
<text top="606" left="79" width="453" height="9" font="font3" id="p27_t53" reading_order_no="52" segment_no="6" tag_type="text">call information recorded by operators over one week exhibit periods with high traffic load and others</text>
<text top="618" left="79" width="453" height="9" font="font3" id="p27_t54" reading_order_no="53" segment_no="6" tag_type="text">with moderate to low traffic [44]. Hence, it may be beneficial for a network operator to dynamically</text>
<text top="630" left="79" width="453" height="9" font="font3" id="p27_t55" reading_order_no="54" segment_no="6" tag_type="text">switch off BS that does not handle high traffic in its cell at a given time in order to maximize the energy</text>
<text top="642" left="79" width="453" height="9" font="font3" id="p27_t56" reading_order_no="55" segment_no="6" tag_type="text">efficiency of the network. However, the set of BS to be switched OFF should be chosen with care while</text>
<text top="654" left="79" width="222" height="9" font="font3" id="p27_t57" reading_order_no="56" segment_no="6" tag_type="text">maintaining a sufficient quality of service for users.</text>
<text top="666" left="94" width="438" height="9" font="font3" id="p27_t58" reading_order_no="57" segment_no="7" tag_type="text">Let us consider an heterogeneous wireless cellular network made of macro and small cells where the</text>
<text top="677" left="79" width="453" height="12" font="font3" id="p27_t59" reading_order_no="58" segment_no="7" tag_type="text">set of BS Y = { 1 , 2 , · · · , Y } lies in a two dimensional area in R 2 , each serving a cell k . The decision</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p27_t60" reading_order_no="59" segment_no="7" tag_type="text">to switch ON or OFF a BS is taken by a central controller and depends on the traffic load of each cell</text>
<text top="702" left="79" width="453" height="10" font="font3" id="p27_t61" reading_order_no="60" segment_no="7" tag_type="text">and its power consumption. The traffic load ρ k ( t ) of a cell k at time t depends on the statistic of the</text>
<text top="714" left="79" width="453" height="10" font="font3" id="p27_t62" reading_order_no="61" segment_no="7" tag_type="text">arrival and departure processes and on the data rate Θ k ( x, t ) that can be provided by cell k to the user</text>
<text top="726" left="79" width="111" height="9" font="font3" id="p27_t63" reading_order_no="62" segment_no="7" tag_type="text">positioned at x at time t .</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p27_t64" reading_order_no="63" segment_no="8" tag_type="text">27</text>
</page>
<page number="28" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font35" size="7" family="CMEX7" color="#000000"/>
	<fontspec id="font36" size="9" family="CMR9" color="#000000"/>
<text top="72" left="262" width="271" height="9" font="font3" id="p28_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="94" width="438" height="9" font="font3" id="p28_t2" reading_order_no="1" segment_no="1" tag_type="text">The maximization of the energy efficiency of the network by selecting the set of transmitting BS is</text>
<text top="122" left="79" width="262" height="9" font="font3" id="p28_t3" reading_order_no="2" segment_no="1" tag_type="text">an NP-hard problem and can be expressed as following [45]:<a href="deeplearning_paper3.html#46">[45]:</a></text>
<text top="153" left="186" width="42" height="10" font="font11" id="p28_t4" reading_order_no="3" segment_no="2" tag_type="formula">Y on ∗ ( t ) =</text>
<text top="153" left="239" width="36" height="9" font="font3" id="p28_t5" reading_order_no="4" segment_no="2" tag_type="formula">arg max</text>
<text top="162" left="254" width="23" height="7" font="font10" id="p28_t6" reading_order_no="5" segment_no="2" tag_type="formula">Y on ( t )</text>
<text top="143" left="278" width="6" height="4" font="font13" id="p28_t7" reading_order_no="6" segment_no="2" tag_type="formula">"</text>
<text top="152" left="295" width="11" height="4" font="font13" id="p28_t8" reading_order_no="7" segment_no="2" tag_type="formula">P</text>
<text top="164" left="284" width="32" height="7" font="font8" id="p28_t9" reading_order_no="8" segment_no="2" tag_type="formula">k ∈Y on ( t )</text>
<text top="142" left="327" width="8" height="3" font="font35" id="p28_t10" reading_order_no="9" segment_no="2" tag_type="formula">P</text>
<text top="150" left="319" width="25" height="7" font="font24" id="p28_t11" reading_order_no="10" segment_no="2" tag_type="formula">x ∈C k ( t )</text>
<text top="143" left="345" width="27" height="6" font="font9" id="p28_t12" reading_order_no="11" segment_no="2" tag_type="formula">Θ k ( x,t )</text>
<text top="159" left="336" width="19" height="6" font="font8" id="p28_t13" reading_order_no="12" segment_no="2" tag_type="formula">P k ( t )</text>
<text top="143" left="373" width="6" height="4" font="font13" id="p28_t14" reading_order_no="13" segment_no="2" tag_type="formula">#</text>
<text top="153" left="384" width="13" height="9" font="font3" id="p28_t15" reading_order_no="14" segment_no="2" tag_type="formula">s.t.</text>
<text top="174" left="186" width="16" height="10" font="font3" id="p28_t16" reading_order_no="15" segment_no="2" tag_type="formula">( c 1 )</text>
<text top="173" left="239" width="122" height="11" font="font3" id="p28_t17" reading_order_no="16" segment_no="2" tag_type="formula">0 ≤ ρ k ( t ) ≤ ρ th , ∀ k ∈ Y on ( t )</text>
<text top="186" left="186" width="16" height="9" font="font3" id="p28_t18" reading_order_no="17" segment_no="2" tag_type="formula">( c 2 )</text>
<text top="185" left="239" width="178" height="10" font="font3" id="p28_t19" reading_order_no="18" segment_no="2" tag_type="formula">Θ k ( x, t ) ≥ Θ min , ∀ x ∈ C k ( t ) , ∀ k ∈ Y on ( t )</text>
<text top="198" left="186" width="16" height="9" font="font3" id="p28_t20" reading_order_no="19" segment_no="2" tag_type="formula">( c 3 )</text>
<text top="197" left="239" width="47" height="10" font="font11" id="p28_t21" reading_order_no="20" segment_no="2" tag_type="formula">Y on ( t ) 6 = ∅</text>
<text top="171" left="507" width="25" height="9" font="font3" id="p28_t22" reading_order_no="21" segment_no="2" tag_type="text">(9.55)</text>
<text top="219" left="79" width="453" height="10" font="font3" id="p28_t23" reading_order_no="22" segment_no="3" tag_type="text">where Y on ( t ) is the set of active BS at time t , P k ( t ), C k ( t ) are the power consumed and the coverage of</text>
<text top="232" left="79" width="454" height="9" font="font3" id="p28_t24" reading_order_no="23" segment_no="3" tag_type="text">cell k at time t , respectively. Moreover, ρ th and Θ min are the traffic load upper limit and the minimum</text>
<text top="242" left="79" width="454" height="11" font="font3" id="p28_t25" reading_order_no="24" segment_no="3" tag_type="text">required data rate per user, respectively. Constraint ( c 1 ) is stated for stability reason 16 , ( c 2 ) states that</text>
<text top="256" left="79" width="454" height="9" font="font3" id="p28_t26" reading_order_no="25" segment_no="3" tag_type="text">each user has to be served with a minimum data rate and ( c 3 ) ensures that at least one BS is active at</text>
<text top="268" left="79" width="453" height="9" font="font3" id="p28_t27" reading_order_no="26" segment_no="3" tag_type="text">each time slot. Finding the optimal configuration by an exhaustive search would be prohibitive in large</text>
<text top="278" left="79" width="356" height="11" font="font3" id="p28_t28" reading_order_no="27" segment_no="3" tag_type="text">networks since the optimal BS active set belongs to a set of 2 Y − 1 combinations.</text>
<text top="292" left="94" width="438" height="9" font="font3" id="p28_t29" reading_order_no="28" segment_no="4" tag_type="text">Authors in [45] have shown that this problem can be solved with MAB formulation. The problem</text>
<text top="304" left="79" width="454" height="9" font="font3" id="p28_t30" reading_order_no="29" segment_no="4" tag_type="text">can be illustrated with Fig. 9.5 where at each iteration, the central controller chooses an action a among</text>
<text top="313" left="79" width="454" height="12" font="font11" id="p28_t31" reading_order_no="30" segment_no="4" tag_type="text">|A| = 2 Y − 1 possible actions, i.e. a ( t ) = [ a 1 ( t ) , · · · , a Y ( t )] T with a k ( t ) = 1 if BS k is switched ON</text>
<text top="327" left="79" width="453" height="10" font="font3" id="p28_t32" reading_order_no="31" segment_no="4" tag_type="text">at t and 0 otherwise, and where A is the action space. The state is represented by a random variable</text>
<text top="339" left="79" width="453" height="9" font="font7" id="p28_t33" reading_order_no="32" segment_no="4" tag_type="text">S ( t ) ∈ { 0 , 1 } where s ( t ) = 1 if all constraints in (9.55) are satisfied and 0 otherwise. In other words,</text>
<text top="351" left="79" width="453" height="9" font="font3" id="p28_t34" reading_order_no="33" segment_no="4" tag_type="text">the value of the state of the Markov chain relies on the fact that the selected action leads to a feasible</text>
<text top="363" left="79" width="454" height="9" font="font3" id="p28_t35" reading_order_no="34" segment_no="4" tag_type="text">solution of (9.55). Observing the state s ( t ) and taking the action a ( t ) at time t lead to the network in</text>
<text top="375" left="79" width="454" height="9" font="font3" id="p28_t36" reading_order_no="35" segment_no="4" tag_type="text">the state S ( t + 1) and give a reward R ( t + 1) in the next time slot according to the conditional transition</text>
<text top="387" left="79" width="453" height="9" font="font3" id="p28_t37" reading_order_no="36" segment_no="4" tag_type="text">probability distribution introduced in (9.4). The reward is the energy efficiency computed as in the cost</text>
<text top="399" left="79" width="79" height="9" font="font3" id="p28_t38" reading_order_no="37" segment_no="4" tag_type="text">function in (9.55).</text>
<text top="448" left="247" width="44" height="9" font="font3" id="p28_t39" reading_order_no="39" segment_no="5" tag_type="figure">Controller</text>
<text top="424" left="243" width="80" height="9" font="font3" id="p28_t40" reading_order_no="38" segment_no="5" tag_type="figure">RQoS-UCB policy</text>
<text top="438" left="342" width="138" height="10" font="font3" id="p28_t41" reading_order_no="40" segment_no="5" tag_type="figure">Action a ( t ) = [ a 1 ( t ) , · · · , a Y ( t )]</text>
<text top="464" left="349" width="30" height="9" font="font7" id="p28_t42" reading_order_no="41" segment_no="5" tag_type="figure">a i ( t ) =</text>
<text top="458" left="395" width="5" height="9" font="font3" id="p28_t43" reading_order_no="42" segment_no="5" tag_type="figure">1</text>
<text top="458" left="410" width="47" height="9" font="font3" id="p28_t44" reading_order_no="43" segment_no="5" tag_type="figure">BS i active</text>
<text top="470" left="395" width="5" height="9" font="font3" id="p28_t45" reading_order_no="44" segment_no="5" tag_type="figure">0</text>
<text top="470" left="410" width="56" height="9" font="font3" id="p28_t46" reading_order_no="45" segment_no="5" tag_type="figure">BS i inactive</text>
<text top="511" left="224" width="33" height="9" font="font3" id="p28_t47" reading_order_no="48" segment_no="5" tag_type="figure">Reward</text>
<text top="523" left="222" width="37" height="9" font="font7" id="p28_t48" reading_order_no="49" segment_no="5" tag_type="figure">R ( t + 1)</text>
<text top="511" left="144" width="23" height="9" font="font3" id="p28_t49" reading_order_no="46" segment_no="5" tag_type="figure">State</text>
<text top="523" left="138" width="35" height="9" font="font7" id="p28_t50" reading_order_no="47" segment_no="5" tag_type="figure">S ( t + 1)</text>
<text top="638" left="250" width="94" height="9" font="font3" id="p28_t51" reading_order_no="50" segment_no="5" tag_type="figure">Network environment</text>
<text top="604" left="382" width="40" height="8" font="font36" id="p28_t52" reading_order_no="51" segment_no="5" tag_type="figure">Macro BS</text>
<text top="621" left="383" width="38" height="8" font="font36" id="p28_t53" reading_order_no="52" segment_no="5" tag_type="figure">Micro BS</text>
<text top="673" left="189" width="234" height="9" font="font3" id="p28_t54" reading_order_no="53" segment_no="6" tag_type="text">Figure 9.5: RL framework for BS switching operation</text>
<text top="696" left="94" width="438" height="9" font="font3" id="p28_t55" reading_order_no="54" segment_no="7" tag_type="text">In [45] the authors proposed to apply the same policy as in (9.52), where the index of the user can</text>
<text top="708" left="79" width="454" height="10" font="font3" id="p28_t56" reading_order_no="55" segment_no="7" tag_type="text">be dropped out, k represents the number of the actions and where the middle term q k ( t ) is expressed as</text>
<text top="726" left="87" width="290" height="9" font="font21" id="p28_t57" reading_order_no="56" segment_no="8" tag_type="footnote">16 A traffic load that is too high results in diverging queue size in the network.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p28_t58" reading_order_no="57" segment_no="9" tag_type="text">28</text>
</page>
<page number="29" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="117" height="9" font="font3" id="p29_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="110" left="79" width="454" height="10" font="font3" id="p29_t2" reading_order_no="1" segment_no="1" tag_type="text">in (9.53) and where r 1 ( t ) in (9.54) is the energy efficiency of the network at time t when the set of active<a href="deeplearning_paper3.html#26">(9.53) </a>and where</text>
<text top="122" left="79" width="274" height="9" font="font3" id="p29_t3" reading_order_no="2" segment_no="1" tag_type="text">BS is such that constraints in (9.55) are satisfied, i.e. s ( t ) = 1.</text>
<text top="140" left="79" width="453" height="9" font="font6" id="p29_t4" reading_order_no="3" segment_no="2" tag_type="text">Remark 2. Of course, modeling (9.55) with MAB framework does not change that the problem dimen-</text>
<text top="152" left="79" width="453" height="9" font="font5" id="p29_t5" reading_order_no="4" segment_no="2" tag_type="text">sionality is exponential in the number of BS. However, MAB explores only once all network configurations</text>
<text top="164" left="79" width="453" height="9" font="font5" id="p29_t6" reading_order_no="5" segment_no="2" tag_type="text">to assign an index to each of them and then chooses the next configuration according to the highest index</text>
<text top="176" left="79" width="447" height="9" font="font5" id="p29_t7" reading_order_no="6" segment_no="2" tag_type="text">computed at the previous iteration with (9.52) , instead of doing an exhaustive search at each time slot.<a href="deeplearning_paper3.html#26">(9.54) </a>is the energy efficiency of the network at time</text>
<text top="193" left="79" width="454" height="9" font="font6" id="p29_t8" reading_order_no="7" segment_no="3" tag_type="text">Remark 3. This problem could also be addressed with DRL technique. Indeed, if the number of BS is</text>
<text top="205" left="79" width="453" height="9" font="font5" id="p29_t9" reading_order_no="8" segment_no="3" tag_type="text">too large, the convergence time of MAB algorithms is too large which makes the problem unsolvable for</text>
<text top="217" left="79" width="453" height="9" font="font5" id="p29_t10" reading_order_no="9" segment_no="3" tag_type="text">large scale networks. DRL can be used instead and the set of BS to switch on may be obtained by a DNN<a href="deeplearning_paper3.html#28">(9.55) </a>are satisfied, i.e.</text>
<text top="229" left="79" width="147" height="9" font="font5" id="p29_t11" reading_order_no="10" segment_no="3" tag_type="text">for each state of the environment.</text>
<text top="255" left="79" width="28" height="11" font="font15" id="p29_t12" reading_order_no="11" segment_no="4" tag_type="title">9.3.4</text>
<text top="255" left="120" width="122" height="11" font="font15" id="p29_t13" reading_order_no="12" segment_no="4" tag_type="title">Real world examples</text>
<text top="275" left="79" width="453" height="9" font="font3" id="p29_t14" reading_order_no="13" segment_no="5" tag_type="text">We propose to give details on a few concrete implementations of RL for communication, which rely on</text>
<text top="287" left="79" width="453" height="9" font="font3" id="p29_t15" reading_order_no="14" segment_no="5" tag_type="text">theoretical applications of RL using the UCB algorithms for single user cognitive radio MAB-modeled</text>
<text top="299" left="79" width="453" height="9" font="font3" id="p29_t16" reading_order_no="15" segment_no="5" tag_type="text">problems in [46] and for the opportunistic spectrum access (OSA) scenario in [47]. In these examples,</text>
<text top="311" left="79" width="453" height="9" font="font3" id="p29_t17" reading_order_no="16" segment_no="5" tag_type="text">UCB1 [21] algorithm is mostly used as a proof of the pertinence of bandit algorithms for free spectrum-<a href="deeplearning_paper3.html#28">(9.55)</a></text>
<text top="323" left="79" width="453" height="9" font="font3" id="p29_t18" reading_order_no="17" segment_no="5" tag_type="text">access problems, but other bandit algorithms have also been considered and could be implemented as</text>
<text top="335" left="79" width="20" height="9" font="font3" id="p29_t19" reading_order_no="18" segment_no="5" tag_type="text">well.</text>
<text top="347" left="94" width="438" height="9" font="font3" id="p29_t20" reading_order_no="19" segment_no="6" tag_type="text">In the spectrum access context, the goal is to be able to manage a large spectrum without adding</text>
<text top="359" left="79" width="453" height="9" font="font3" id="p29_t21" reading_order_no="20" segment_no="6" tag_type="text">complexity to the radio system by enlarging its bandwidth (and consequently without adding complexity</text>
<text top="371" left="79" width="453" height="9" font="font3" id="p29_t22" reading_order_no="21" segment_no="6" tag_type="text">to the transceivers). Figure 9.6 shows that learning is a mean to decrease the receivers architecture<a href="deeplearning_paper3.html#26">(9.52)</a></text>
<text top="383" left="79" width="453" height="9" font="font3" id="p29_t23" reading_order_no="22" segment_no="6" tag_type="text">complexity while maintaining a legacy bandwidth to the OSA radio system with extended capabilities.</text>
<text top="395" left="79" width="453" height="9" font="font3" id="p29_t24" reading_order_no="23" segment_no="6" tag_type="text">RL enables to reconstruct the global bandwidth knowledge, thanks to successive small scale investigations.</text>
<text top="406" left="79" width="453" height="9" font="font3" id="p29_t25" reading_order_no="24" segment_no="6" tag_type="text">RL offers a light solution, in terms of implementation complexity cost compared to wide (full)band OSA</text>
<text top="418" left="79" width="36" height="9" font="font3" id="p29_t26" reading_order_no="25" segment_no="6" tag_type="text">systems.</text>
<text top="703" left="127" width="359" height="9" font="font3" id="p29_t27" reading_order_no="26" segment_no="7" tag_type="text">Figure 9.6: Comparison of the receiver architectures for OSA with or without RL.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p29_t28" reading_order_no="27" segment_no="8" tag_type="text">29</text>
</page>
<page number="30" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p30_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="220" height="9" font="font6" id="p30_t2" reading_order_no="1" segment_no="1" tag_type="title">Implementation at the post-processing level</text>
<text top="130" left="79" width="453" height="9" font="font3" id="p30_t3" reading_order_no="2" segment_no="2" tag_type="text">Due to the unique world wide covering characteristic of high frequency (HF) transmissions, there is a high</text>
<text top="142" left="79" width="453" height="9" font="font3" id="p30_t4" reading_order_no="3" segment_no="2" tag_type="text">necessity to reduce collisions between users which act in a decentralized manner. RL algorithms have been</text>
<text top="154" left="79" width="453" height="9" font="font3" id="p30_t5" reading_order_no="4" segment_no="2" tag_type="text">applied on real measurements of the HF spectrum that has been recorded during a radio-ham contest</text>
<text top="166" left="79" width="453" height="9" font="font3" id="p30_t6" reading_order_no="5" segment_no="2" tag_type="text">[48], i.e. when HF radio traffic is at its highest. Figure 9.7 shows the subset of the spectrum data the<a href="deeplearning_paper3.html#46">[48], </a>i.e. when HF radio traffic is at its highest. Figure <a href="deeplearning_paper3.html#30">9.7 </a>shows the subset of the spectrum data the</text>
<text top="178" left="79" width="453" height="9" font="font3" id="p30_t7" reading_order_no="6" segment_no="2" tag_type="text">MAB algorithms considered in [48]. The goal of learning is to enable users to find yellow unoccupied slots<a href="deeplearning_paper3.html#46">[48]. </a>The goal of learning is to enable users to find yellow unoccupied slots</text>
<text top="190" left="79" width="453" height="9" font="font3" id="p30_t8" reading_order_no="7" segment_no="2" tag_type="text">in time and frequency before transmitting and to maximize the probability that no collision occurs during</text>
<text top="202" left="79" width="453" height="9" font="font3" id="p30_t9" reading_order_no="8" segment_no="2" tag_type="text">the transmission duration. For instance, the cognitive device should avoid the frequencies delimited by</text>
<text top="214" left="79" width="341" height="9" font="font3" id="p30_t10" reading_order_no="9" segment_no="2" tag_type="text">the rectangle in Figure 9.7, as it is a highly used bandwidth by primary users.<a href="deeplearning_paper3.html#30">9.7, </a>as it is a highly used bandwidth by primary users.</text>
<text top="508" left="79" width="453" height="9" font="font3" id="p30_t11" reading_order_no="10" segment_no="3" tag_type="text">Figure 9.7: HFSA IDeTIC F1 V01 database extraction for HF channel traffic during a radio-ham contest</text>
<text top="520" left="79" width="228" height="9" font="font3" id="p30_t12" reading_order_no="11" segment_no="3" tag_type="text">at 14.1 MHz (area highlighted in the rectangle) [48].<a href="deeplearning_paper3.html#46">[48].</a></text>
<text top="556" left="94" width="438" height="9" font="font3" id="p30_t13" reading_order_no="12" segment_no="4" tag_type="text">The solution in [48] proposes a new hybrid system which combines two types of machine learning tech-<a href="deeplearning_paper3.html#46">[48] </a>proposes a new hybrid system which combines two types of machine learning tech-</text>
<text top="568" left="79" width="453" height="9" font="font3" id="p30_t14" reading_order_no="13" segment_no="4" tag_type="text">niques based on MAB and Hidden Markov Models (HMM). This system can be seen as a meta-cognitive</text>
<text top="580" left="79" width="453" height="9" font="font3" id="p30_t15" reading_order_no="14" segment_no="4" tag_type="text">engine that automatically adapts its data transmission strategy according to the HF environment’s be-</text>
<text top="592" left="79" width="453" height="9" font="font3" id="p30_t16" reading_order_no="15" segment_no="4" tag_type="text">haviour to efficiently use the spectrum holes. The proposed hybrid algorithm, i.e. combining a UCB</text>
<text top="604" left="79" width="453" height="9" font="font3" id="p30_t17" reading_order_no="16" segment_no="4" tag_type="text">algorithm and HMM, allows to increase the time the opportunistic user transmits when conditions are</text>
<text top="616" left="79" width="453" height="9" font="font3" id="p30_t18" reading_order_no="17" segment_no="4" tag_type="text">favourable, and is also able to reduce the required signalling transmissions between the transmitter and</text>
<text top="628" left="79" width="345" height="9" font="font3" id="p30_t19" reading_order_no="18" segment_no="4" tag_type="text">the receiver to inform which channels have been selected for data transmission.</text>
<text top="640" left="94" width="438" height="9" font="font3" id="p30_t20" reading_order_no="19" segment_no="5" tag_type="text">Table 9.1 sums-up the characteristics of the RL algorithm implementation for this use case. The reward<a href="deeplearning_paper3.html#31">9.1 </a>sums-up the characteristics of the RL algorithm implementation for this use case. The reward</text>
<text top="652" left="79" width="453" height="9" font="font3" id="p30_t21" reading_order_no="20" segment_no="5" tag_type="text">is the channel availability detected by the cognitive users. More details about these measurements can</text>
<text top="664" left="79" width="71" height="9" font="font3" id="p30_t22" reading_order_no="21" segment_no="5" tag_type="text">be found in [48].<a href="deeplearning_paper3.html#46">[48].</a></text>
<text top="694" left="79" width="192" height="9" font="font6" id="p30_t23" reading_order_no="22" segment_no="6" tag_type="title">Implementation in a Proof-of-Concept</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p30_t24" reading_order_no="23" segment_no="7" tag_type="text">First real-time RL implementation on real radio signal took place in 2013 for OSA scenario and was first</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p30_t25" reading_order_no="24" segment_no="7" tag_type="text">published in 2014 [49] at the Karlshruhe Workshop of Prof. Friedrich Jondral and extended hereafter in<a href="deeplearning_paper3.html#46">[49] </a>at the Karlshruhe Workshop of Prof. Friedrich Jondral and extended hereafter in</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p30_t26" reading_order_no="25" segment_no="8" tag_type="text">30</text>
</page>
<page number="31" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font37" size="8" family="CMR10" color="#000000"/>
	<fontspec id="font38" size="8" family="CMTT8" color="#000000"/>
<text top="72" left="79" width="117" height="9" font="font3" id="p31_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="109" left="206" width="65" height="9" font="font3" id="p31_t2" reading_order_no="1" segment_no="1" tag_type="table">Characteristics</text>
<text top="109" left="355" width="42" height="9" font="font3" id="p31_t3" reading_order_no="2" segment_no="1" tag_type="table">Comment</text>
<text top="124" left="209" width="59" height="9" font="font3" id="p31_t4" reading_order_no="3" segment_no="1" tag_type="table">RL algorithm</text>
<text top="124" left="314" width="123" height="9" font="font3" id="p31_t5" reading_order_no="4" segment_no="1" tag_type="table">HMM combined with UCB1</text>
<text top="136" left="222" width="33" height="9" font="font3" id="p31_t6" reading_order_no="5" segment_no="1" tag_type="table">Reward</text>
<text top="136" left="334" width="84" height="9" font="font3" id="p31_t7" reading_order_no="6" segment_no="1" tag_type="table">channel availability</text>
<text top="148" left="194" width="89" height="9" font="font3" id="p31_t8" reading_order_no="7" segment_no="1" tag_type="table">Implementation side</text>
<text top="148" left="344" width="64" height="9" font="font3" id="p31_t9" reading_order_no="8" segment_no="1" tag_type="table">HF transceiver</text>
<text top="161" left="175" width="128" height="9" font="font3" id="p31_t10" reading_order_no="9" segment_no="1" tag_type="table">method for RL feedback loop</text>
<text top="161" left="360" width="33" height="9" font="font3" id="p31_t11" reading_order_no="10" segment_no="1" tag_type="table">Sensing</text>
<text top="182" left="193" width="227" height="9" font="font3" id="p31_t12" reading_order_no="11" segment_no="2" tag_type="text">Table 9.1: Summary for HF signals post-processing.</text>
<text top="223" left="79" width="453" height="11" font="font3" id="p31_t13" reading_order_no="12" segment_no="3" tag_type="text">[50]. This consisted of a proof-of-concept (PoC) in the laboratory conditions with one USRP 17 platform<a href="deeplearning_paper3.html#46">[50]. </a>This consisted of a proof-of-concept (PoC) in the laboratory conditions with one US<a href="deeplearning_paper3.html#31">RP</a></text>
<text top="237" left="79" width="453" height="9" font="font3" id="p31_t14" reading_order_no="13" segment_no="3" tag_type="text">emulating the traffic generated by a set of primary users, i.e. users that own the frequency bands, and<a href="deeplearning_paper3.html#31">17</a></text>
<text top="249" left="79" width="453" height="9" font="font3" id="p31_t15" reading_order_no="14" segment_no="3" tag_type="text">another USRP platform running the sensing and learning algorithm of one secondary user, i.e. user that</text>
<text top="261" left="79" width="453" height="9" font="font3" id="p31_t16" reading_order_no="15" segment_no="3" tag_type="text">opportunistically exploits the licensed band. Both i.i.d and Markovian MAB traffic models have been</text>
<text top="273" left="79" width="453" height="9" font="font3" id="p31_t17" reading_order_no="16" segment_no="3" tag_type="text">tested. UCB1 algorithm was used first in order to validate the RL approach, but then other bandit</text>
<text top="285" left="79" width="453" height="9" font="font3" id="p31_t18" reading_order_no="17" segment_no="3" tag_type="text">algorithms have been implemented later, e.g. Thompson Sampling, KL-UCB. The multi-user version</text>
<text top="297" left="79" width="453" height="9" font="font3" id="p31_t19" reading_order_no="18" segment_no="3" tag_type="text">has been implemented in [51], moreover, several videos implementing the main UCB algorithms in a</text>
<text top="308" left="79" width="453" height="9" font="font3" id="p31_t20" reading_order_no="19" segment_no="3" tag_type="text">USRP-based platform demonstrating the real-time learning evolution under various traffic models (i.i.d.</text>
<text top="319" left="79" width="454" height="10" font="font3" id="p31_t21" reading_order_no="20" segment_no="3" tag_type="text">and Markovian) can be found on Internet 18 . In order to help the experimental community to verify and<a href="deeplearning_paper3.html#46">[51], </a>moreover, several videos implementing the main UCB algorithms in a</text>
<text top="332" left="79" width="453" height="9" font="font3" id="p31_t22" reading_order_no="21" segment_no="3" tag_type="text">develop new learning algorithms, an exhaustive Python code library and framework for simulations have</text>
<text top="343" left="79" width="453" height="10" font="font3" id="p31_t23" reading_order_no="22" segment_no="3" tag_type="text">been provided on GithHub 19 20 21 that encompasses a lot of MAB algorithms published until mid-2019.<a href="deeplearning_paper3.html#31">ternet</a></text>
<text top="357" left="94" width="438" height="9" font="font3" id="p31_t24" reading_order_no="23" segment_no="4" tag_type="text">Table 9.2 summarizes the main characteristics of the RL algorithm implementation for this use case.<a href="deeplearning_paper3.html#31">18</a></text>
<text top="368" left="79" width="266" height="9" font="font3" id="p31_t25" reading_order_no="24" segment_no="4" tag_type="text">More details about these measurements can be found in [49].<a href="deeplearning_paper3.html#31">. </a>In order to help the experimental community to verify and</text>
<text top="392" left="202" width="55" height="8" font="font37" id="p31_t26" reading_order_no="25" segment_no="5" tag_type="table">Characteristics</text>
<text top="392" left="347" width="36" height="8" font="font37" id="p31_t27" reading_order_no="26" segment_no="5" tag_type="table">Comment<a href="deeplearning_paper3.html#31">Hub</a></text>
<text top="404" left="204" width="50" height="8" font="font37" id="p31_t28" reading_order_no="27" segment_no="5" tag_type="table">RL algorithm<a href="deeplearning_paper3.html#31">19 20 21</a></text>
<text top="404" left="294" width="143" height="8" font="font37" id="p31_t29" reading_order_no="28" segment_no="5" tag_type="table">UCB1 (or any other bandit algorithm)</text>
<text top="415" left="215" width="28" height="8" font="font37" id="p31_t30" reading_order_no="29" segment_no="5" tag_type="table">Reward<a href="deeplearning_paper3.html#31">9.2 </a>summarizes the main characteristics of the RL algorithm implementation for this use case.</text>
<text top="415" left="327" width="76" height="8" font="font37" id="p31_t31" reading_order_no="30" segment_no="5" tag_type="table">channel ’availability’<a href="deeplearning_paper3.html#46">[49].</a></text>
<text top="425" left="191" width="75" height="8" font="font37" id="p31_t32" reading_order_no="31" segment_no="5" tag_type="table">Implementation side</text>
<text top="425" left="338" width="54" height="8" font="font37" id="p31_t33" reading_order_no="32" segment_no="5" tag_type="table">secondary user</text>
<text top="436" left="175" width="108" height="8" font="font37" id="p31_t34" reading_order_no="33" segment_no="5" tag_type="table">method for RL feedback loop</text>
<text top="436" left="352" width="28" height="8" font="font37" id="p31_t35" reading_order_no="34" segment_no="5" tag_type="table">Sensing</text>
<text top="456" left="203" width="205" height="9" font="font3" id="p31_t36" reading_order_no="35" segment_no="6" tag_type="text">Table 9.2: Summary for OSA proof-of-concept.</text>
<text top="490" left="94" width="438" height="9" font="font3" id="p31_t37" reading_order_no="36" segment_no="7" tag_type="text">A PoC implementing MAB algorithms for internet of things (IoT) access has been done in [52]. It</text>
<text top="502" left="79" width="453" height="9" font="font3" id="p31_t38" reading_order_no="37" segment_no="7" tag_type="text">consists in one gateway, one or several learning IoT devices, embedding UCB1 and Thompson Sampling</text>
<text top="514" left="79" width="453" height="9" font="font3" id="p31_t39" reading_order_no="38" segment_no="7" tag_type="text">algorithms, and a traffic generator that emulates radio interferences from many other IoT devices. The</text>
<text top="526" left="79" width="453" height="9" font="font3" id="p31_t40" reading_order_no="39" segment_no="7" tag_type="text">IoT network access is modeled as a discrete sequential decision making problem. No specific IoT stan-</text>
<text top="538" left="79" width="453" height="9" font="font3" id="p31_t41" reading_order_no="40" segment_no="7" tag_type="text">dard is implemented in order to stay agnostic to any specific IoT implementation. The PoC shows that</text>
<text top="550" left="79" width="453" height="9" font="font3" id="p31_t42" reading_order_no="41" segment_no="7" tag_type="text">intelligent IoT devices can improve their network access by using low complexity and decentralized algo-</text>
<text top="562" left="79" width="453" height="9" font="font3" id="p31_t43" reading_order_no="42" segment_no="7" tag_type="text">rithms which can be added in a straightforward and cost-less manner in any IoT network (such as Sigfox,<a href="deeplearning_paper3.html#46">[52]. </a>It</text>
<text top="574" left="79" width="453" height="9" font="font3" id="p31_t44" reading_order_no="43" segment_no="7" tag_type="text">LoRaWAN, etc.), without any modification at the network side. Table 9.3 summarizes the characteristics</text>
<text top="586" left="79" width="453" height="9" font="font3" id="p31_t45" reading_order_no="44" segment_no="7" tag_type="text">of the RL algorithm implementation for this use case. More details about these measurements can be</text>
<text top="598" left="79" width="236" height="9" font="font3" id="p31_t46" reading_order_no="45" segment_no="7" tag_type="text">found in [52], but we present the main outcomes here.</text>
<text top="610" left="94" width="438" height="9" font="font3" id="p31_t47" reading_order_no="46" segment_no="8" tag_type="text">Figure 9.8 shows the UCB parameters during an execution on 4 channels numbered as channels #2,</text>
<text top="620" left="79" width="453" height="11" font="font3" id="p31_t48" reading_order_no="47" segment_no="8" tag_type="text">#4, #6 and #8 22 23 . We can see on top left of Fig. 9.8 that the left channel (channel #2) has been only</text>
<text top="641" left="87" width="135" height="9" font="font38" id="p31_t49" reading_order_no="48" segment_no="9" tag_type="footnote">17 https://www.ettusresearch.com/</text>
<text top="650" left="87" width="275" height="9" font="font38" id="p31_t50" reading_order_no="49" segment_no="10" tag_type="footnote">18 https://www.youtube.com/channel/UC5UFCuH4jQ_s_4UQb4spt7Q/videos<a href="deeplearning_paper3.html#33">9.3 </a>summarizes the characteristics</text>
<text top="660" left="87" width="446" height="8" font="font21" id="p31_t51" reading_order_no="50" segment_no="11" tag_type="footnote">19 “SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits (MAB) Algo-</text>
<text top="671" left="79" width="68" height="7" font="font21" id="p31_t52" reading_order_no="51" segment_no="11" tag_type="footnote">rithms in Python”<a href="deeplearning_paper3.html#46">[52], </a>but we present the main outcomes here.</text>
<text top="679" left="87" width="217" height="9" font="font21" id="p31_t53" reading_order_no="52" segment_no="12" tag_type="footnote">20 code on https://GitHub.com/SMPyBandits/SMPyBandits<a href="deeplearning_paper3.html#32">9.8 </a>shows the UCB parameters during an execution on 4 channels numbered as channels #2,</text>
<text top="688" left="87" width="204" height="9" font="font21" id="p31_t54" reading_order_no="53" segment_no="13" tag_type="footnote">21 documentation on https://SMPyBandits.GitHub.io/<a href="deeplearning_paper3.html#31">#8</a></text>
<text top="698" left="87" width="445" height="9" font="font21" id="p31_t55" reading_order_no="54" segment_no="14" tag_type="footnote">22 The entire source code for this demo is available on-line, open-sourced under GPLv3 license, at https://bitbucket.org/<a href="deeplearning_paper3.html#31">22 23</a></text>
<text top="709" left="79" width="454" height="7" font="font38" id="p31_t56" reading_order_no="55" segment_no="14" tag_type="footnote">scee_ietr/malin-multi-armed-bandit-learning-for-iot-networks-with-grc/src/master/ . It contains both the GNU<a href="deeplearning_paper3.html#31">. </a>We can see on top left of Fig. <a href="deeplearning_paper3.html#32">9.8 </a>that the left channel (channel #2) has been only</text>
<text top="718" left="79" width="453" height="7" font="font21" id="p31_t57" reading_order_no="56" segment_no="14" tag_type="footnote">Radio Companion flowcharts and blocks, with ready-to-use Makefiles to easily compile, install and launch the demonstration.<a href="https://www.ettusresearch.com/">17</a></text>
<text top="726" left="87" width="446" height="9" font="font21" id="p31_t58" reading_order_no="57" segment_no="15" tag_type="footnote">23 A 6-minute video showing the demonstration is at https://www.youtube.com/watch?v=HospLNQhcMk&amp;feature=youtu.be<a href="https://www.ettusresearch.com/">https://www.ettusresearch.com/</a></text>
<text top="756" left="301" width="10" height="9" font="font3" id="p31_t59" reading_order_no="58" segment_no="16" tag_type="text">31<a href="https://www.youtube.com/channel/UC5UFCuH4jQ_s_4UQb4spt7Q/videos">18</a></text>
</page>
<page number="32" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p32_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p32_t2" reading_order_no="1" segment_no="1" tag_type="text">tried twice over 63 trials by the IoT device and it did not receive the ACK from the network at both</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p32_t3" reading_order_no="2" segment_no="1" tag_type="text">times, as it can be seen at the top right panel which represents the number of successes on each channel.</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p32_t4" reading_order_no="3" segment_no="1" tag_type="text">So the success rate for channel #2 is null, as seen at the bottom right panel. The more the channel index</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p32_t5" reading_order_no="4" segment_no="1" tag_type="text">increases, i.e. from the left to the right, the better the success rate. That is why channel #8, with 90%</text>
<text top="158" left="79" width="453" height="9" font="font3" id="p32_t6" reading_order_no="5" segment_no="1" tag_type="text">of success rate has been preferably used by the algorithm with 35 trials over 63 (top left panel) and 30</text>
<text top="170" left="79" width="453" height="9" font="font3" id="p32_t7" reading_order_no="6" segment_no="1" tag_type="text">successes (top right panel) over the 49 total successes obtained until the caption of this figure during the</text>
<text top="182" left="79" width="51" height="9" font="font3" id="p32_t8" reading_order_no="7" segment_no="1" tag_type="text">experiment.</text>
<text top="582" left="160" width="292" height="9" font="font3" id="p32_t9" reading_order_no="8" segment_no="2" tag_type="text">Figure 9.8: UCB parameters monitored during PoC execution [52].<a href="deeplearning_paper3.html#46">[52].</a></text>
<text top="635" left="79" width="144" height="9" font="font6" id="p32_t10" reading_order_no="9" segment_no="3" tag_type="title">Real world experimentations</text>
<text top="654" left="79" width="453" height="9" font="font3" id="p32_t11" reading_order_no="10" segment_no="4" tag_type="text">The ultimate step of experimentation is the real world, validating the MAB approaches for intelligent</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p32_t12" reading_order_no="11" segment_no="4" tag_type="text">spectrum access. It has been done on a LoRaWAN network deployed in the licence free 868 MHz ISM</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p32_t13" reading_order_no="12" segment_no="4" tag_type="text">band for Europe. The experiment has been conducted in two steps. The first step consists of emulating</text>
<text top="690" left="79" width="453" height="9" font="font3" id="p32_t14" reading_order_no="13" segment_no="4" tag_type="text">an artificial IoT traffic in controlled radio conditions, i.e. inside an anechoic chamber, in order to validate</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p32_t15" reading_order_no="14" segment_no="4" tag_type="text">the devices and the gateway implementation itself [53]. The second step is to make it run in real world<a href="deeplearning_paper3.html#46">[53]. </a>The second step is to make it run in real world</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p32_t16" reading_order_no="15" segment_no="4" tag_type="text">conditions without being able to neither control the spectrum use, nor the propagation conditions in the</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p32_t17" reading_order_no="16" segment_no="4" tag_type="text">area [54]. On step 1, seven channels have been considered, whereas on step 2, only three channels were<a href="deeplearning_paper3.html#46">[54]. </a>On step 1, seven channels have been considered, whereas on step 2, only three channels were</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p32_t18" reading_order_no="17" segment_no="5" tag_type="text">32</text>
</page>
<page number="33" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="117" height="9" font="font3" id="p33_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="109" left="216" width="55" height="8" font="font37" id="p33_t2" reading_order_no="1" segment_no="1" tag_type="table">Characteristics</text>
<text top="109" left="347" width="36" height="8" font="font37" id="p33_t3" reading_order_no="2" segment_no="1" tag_type="table">Comment</text>
<text top="121" left="219" width="50" height="8" font="font37" id="p33_t4" reading_order_no="3" segment_no="1" tag_type="table">RL algorithm</text>
<text top="121" left="309" width="114" height="8" font="font37" id="p33_t5" reading_order_no="4" segment_no="1" tag_type="table">UCB1 and Thomson Sampling</text>
<text top="132" left="230" width="28" height="8" font="font37" id="p33_t6" reading_order_no="5" segment_no="1" tag_type="table">Reward</text>
<text top="132" left="327" width="76" height="8" font="font37" id="p33_t7" reading_order_no="6" segment_no="1" tag_type="table">channel ’availability’</text>
<text top="142" left="206" width="75" height="8" font="font37" id="p33_t8" reading_order_no="7" segment_no="1" tag_type="table">Implementation side</text>
<text top="142" left="328" width="75" height="8" font="font37" id="p33_t9" reading_order_no="8" segment_no="1" tag_type="table">embedded on device</text>
<text top="153" left="190" width="108" height="8" font="font37" id="p33_t10" reading_order_no="9" segment_no="1" tag_type="table">method for RL feedback loop</text>
<text top="153" left="337" width="57" height="8" font="font37" id="p33_t11" reading_order_no="10" segment_no="1" tag_type="table">Emulated ACK</text>
<text top="173" left="206" width="200" height="9" font="font3" id="p33_t12" reading_order_no="11" segment_no="2" tag_type="text">Table 9.3: Summary for IoT proof-of-concept.</text>
<text top="207" left="215" width="31" height="8" font="font37" id="p33_t13" reading_order_no="12" segment_no="3" tag_type="table">Channel</text>
<text top="207" left="257" width="52" height="8" font="font37" id="p33_t14" reading_order_no="13" segment_no="3" tag_type="table">% of jamming</text>
<text top="207" left="319" width="75" height="8" font="font37" id="p33_t15" reading_order_no="14" segment_no="3" tag_type="table">Frequency (in MHz)</text>
<text top="217" left="225" width="11" height="8" font="font37" id="p33_t16" reading_order_no="15" segment_no="3" tag_type="table">#0</text>
<text top="217" left="275" width="16" height="8" font="font37" id="p33_t17" reading_order_no="16" segment_no="3" tag_type="table">30%</text>
<text top="217" left="347" width="19" height="8" font="font37" id="p33_t18" reading_order_no="17" segment_no="3" tag_type="table">866.9</text>
<text top="228" left="225" width="11" height="8" font="font37" id="p33_t19" reading_order_no="18" segment_no="3" tag_type="table">#1</text>
<text top="228" left="275" width="16" height="8" font="font37" id="p33_t20" reading_order_no="19" segment_no="3" tag_type="table">25%</text>
<text top="228" left="347" width="19" height="8" font="font37" id="p33_t21" reading_order_no="20" segment_no="3" tag_type="table">867.1</text>
<text top="238" left="225" width="11" height="8" font="font37" id="p33_t22" reading_order_no="21" segment_no="3" tag_type="table">#2</text>
<text top="238" left="275" width="16" height="8" font="font37" id="p33_t23" reading_order_no="22" segment_no="3" tag_type="table">20%</text>
<text top="238" left="347" width="19" height="8" font="font37" id="p33_t24" reading_order_no="23" segment_no="3" tag_type="table">867.3</text>
<text top="249" left="225" width="11" height="8" font="font37" id="p33_t25" reading_order_no="24" segment_no="3" tag_type="table">#3</text>
<text top="249" left="275" width="16" height="8" font="font37" id="p33_t26" reading_order_no="25" segment_no="3" tag_type="table">15%</text>
<text top="249" left="347" width="19" height="8" font="font37" id="p33_t27" reading_order_no="26" segment_no="3" tag_type="table">867.5</text>
<text top="259" left="225" width="11" height="8" font="font37" id="p33_t28" reading_order_no="27" segment_no="3" tag_type="table">#4</text>
<text top="259" left="275" width="16" height="8" font="font37" id="p33_t29" reading_order_no="28" segment_no="3" tag_type="table">10%</text>
<text top="259" left="347" width="19" height="8" font="font37" id="p33_t30" reading_order_no="29" segment_no="3" tag_type="table">867.7</text>
<text top="270" left="225" width="11" height="8" font="font37" id="p33_t31" reading_order_no="30" segment_no="3" tag_type="table">#5</text>
<text top="270" left="277" width="11" height="8" font="font37" id="p33_t32" reading_order_no="31" segment_no="3" tag_type="table">5%</text>
<text top="270" left="347" width="19" height="8" font="font37" id="p33_t33" reading_order_no="32" segment_no="3" tag_type="table">867.9</text>
<text top="280" left="225" width="11" height="8" font="font37" id="p33_t34" reading_order_no="33" segment_no="3" tag_type="table">#6</text>
<text top="280" left="277" width="11" height="8" font="font37" id="p33_t35" reading_order_no="34" segment_no="3" tag_type="table">0%</text>
<text top="280" left="347" width="19" height="8" font="font37" id="p33_t36" reading_order_no="35" segment_no="3" tag_type="table">868.1</text>
<text top="300" left="178" width="256" height="9" font="font3" id="p33_t37" reading_order_no="36" segment_no="4" tag_type="text">Table 9.4: Channels characteristics for step 1 experiments.</text>
<text top="343" left="79" width="453" height="9" font="font3" id="p33_t38" reading_order_no="37" segment_no="5" tag_type="text">used due to the configuration of the gateway, which was controlled by the LoRaWAN network provider.</text>
<text top="354" left="79" width="454" height="10" font="font3" id="p33_t39" reading_order_no="38" segment_no="5" tag_type="text">For the two measurement campaigns, Pycom equipped with Lopy4 24 shields have been used as devices<a href="deeplearning_paper3.html#33">y4</a></text>
<text top="367" left="79" width="176" height="9" font="font3" id="p33_t40" reading_order_no="39" segment_no="5" tag_type="text">and a standard gateway from Mutlitech.<a href="deeplearning_paper3.html#33">24</a></text>
<text top="380" left="94" width="438" height="9" font="font3" id="p33_t41" reading_order_no="40" segment_no="6" tag_type="text">STEP 1 - The characteristics of the seven channels are given in Table 9.4 which gives the index of</text>
<text top="392" left="79" width="453" height="9" font="font3" id="p33_t42" reading_order_no="41" segment_no="6" tag_type="text">the channel, the percentage of time occupancy (or jamming) the channels experience due to other IoT</text>
<text top="404" left="79" width="453" height="9" font="font3" id="p33_t43" reading_order_no="42" segment_no="6" tag_type="text">devices in the area, the center frequency of each channel (channel bandwidth is set to 125 kHz). USRP<a href="deeplearning_paper3.html#33">9.4 </a>which gives the index of</text>
<text top="416" left="79" width="286" height="9" font="font3" id="p33_t44" reading_order_no="43" segment_no="6" tag_type="text">platforms have been used to generate the surrounding IoT traffic.</text>
<text top="428" left="94" width="438" height="9" font="font3" id="p33_t45" reading_order_no="44" segment_no="7" tag_type="text">Figure 9.9 shows that, due to the surrounding IoT traffic, the channel number #6, i.e. the curve with</text>
<text top="440" left="79" width="453" height="9" font="font3" id="p33_t46" reading_order_no="45" segment_no="7" tag_type="text">star markers, has been much more played by the cognitive device, thanks to the learning algorithm it</text>
<text top="452" left="79" width="453" height="9" font="font3" id="p33_t47" reading_order_no="46" segment_no="7" tag_type="text">embeds. This device is therefore named IoTligent and is hence able to maximise the success rate of its<a href="deeplearning_paper3.html#34">9.9 </a>shows that, due to the surrounding IoT traffic, the channel number #6, i.e. the curve with</text>
<text top="464" left="79" width="453" height="9" font="font3" id="p33_t48" reading_order_no="47" segment_no="7" tag_type="text">transmission. A transmission is called a ”success” when a message has been sent by the IoT device in</text>
<text top="476" left="79" width="453" height="9" font="font3" id="p33_t49" reading_order_no="48" segment_no="7" tag_type="text">uplink, received by the LoRaWAN gateway, transmitted to the application server which sends an ACK</text>
<text top="488" left="79" width="453" height="9" font="font3" id="p33_t50" reading_order_no="49" segment_no="7" tag_type="text">back towards the IoT device, which is transmitted by the gateway in downlink at the same frequency</text>
<text top="500" left="79" width="453" height="9" font="font3" id="p33_t51" reading_order_no="50" segment_no="7" tag_type="text">used in uplink and finally received by the IoT device. A normal device following standard LoRaWAN</text>
<text top="512" left="79" width="453" height="9" font="font3" id="p33_t52" reading_order_no="51" segment_no="7" tag_type="text">features used by default, served as reference and the results in term of number of channel uses and success</text>
<text top="524" left="79" width="177" height="9" font="font3" id="p33_t53" reading_order_no="52" segment_no="7" tag_type="text">rate have been summarized in Table 9.5.</text>
<text top="537" left="94" width="438" height="9" font="font3" id="p33_t54" reading_order_no="53" segment_no="8" tag_type="text">As we can see on Table 9.5, whereas the reference IoT device uniformly transmits on all channels</text>
<text top="549" left="79" width="453" height="9" font="font3" id="p33_t55" reading_order_no="54" segment_no="8" tag_type="text">(around 75 times during this experiment), we can see that the IoTligent device concentrates its trans-<a href="deeplearning_paper3.html#34">9.5.</a></text>
<text top="561" left="79" width="453" height="9" font="font3" id="p33_t56" reading_order_no="55" segment_no="8" tag_type="text">missions on the most vacant channels, with a clear choice for channel #6. Over a total of 528 iterations,<a href="deeplearning_paper3.html#34">9.5, </a>whereas the reference IoT device uniformly transmits on all channels</text>
<text top="573" left="79" width="453" height="9" font="font3" id="p33_t57" reading_order_no="56" segment_no="8" tag_type="text">323 transmissions have been done in this channel for IoTligent, which is more than 4 times compared to</text>
<text top="585" left="79" width="453" height="9" font="font3" id="p33_t58" reading_order_no="57" segment_no="8" tag_type="text">the reference IoT, with 75 transmissions. Moreover, IoTligent selects the channel #6 much more than</text>
<text top="596" left="79" width="453" height="9" font="font3" id="p33_t59" reading_order_no="58" segment_no="8" tag_type="text">channel #0, i.e. almost 27 times more. Hence, the IoT device with learning capability is able to increase</text>
<text top="608" left="79" width="453" height="9" font="font3" id="p33_t60" reading_order_no="59" segment_no="8" tag_type="text">its global success rate drastically that reaches 80% (420 successful ACK received over 528) compared to</text>
<text top="620" left="79" width="301" height="9" font="font3" id="p33_t61" reading_order_no="60" segment_no="8" tag_type="text">50% for the reference IoT device (266 successful ACK received only).</text>
<text top="633" left="94" width="438" height="9" font="font3" id="p33_t62" reading_order_no="61" segment_no="9" tag_type="text">On this example, due to its ability to favor the use of less occupied channels, IoTligent demonstrates</text>
<text top="645" left="79" width="453" height="9" font="font3" id="p33_t63" reading_order_no="62" segment_no="9" tag_type="text">2.5 times improvement in performance compared to standard IoT LoRaWAN device, in terms of number</text>
<text top="657" left="79" width="453" height="9" font="font3" id="p33_t64" reading_order_no="63" segment_no="9" tag_type="text">of successes. Note that in this experimental setup, radio collisions are only considered as obstacles to the</text>
<text top="669" left="79" width="164" height="9" font="font3" id="p33_t65" reading_order_no="64" segment_no="9" tag_type="text">reception of ACK by the IoT devices.</text>
<text top="682" left="94" width="438" height="9" font="font3" id="p33_t66" reading_order_no="65" segment_no="10" tag_type="text">It is worth mentioning that adding this learning capability does not impose changes to LoRaWAN</text>
<text top="693" left="79" width="453" height="9" font="font3" id="p33_t67" reading_order_no="66" segment_no="10" tag_type="text">protocols: no additional re-transmissions to be sent, no additional power consumed, no data to be added</text>
<text top="705" left="79" width="453" height="9" font="font3" id="p33_t68" reading_order_no="67" segment_no="10" tag_type="text">in frames. The only condition is that the proposed solution should work with the acknowledged (ACK)</text>
<text top="726" left="87" width="75" height="9" font="font21" id="p33_t69" reading_order_no="68" segment_no="11" tag_type="footnote">24 https://pycom.io/</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p33_t70" reading_order_no="69" segment_no="12" tag_type="text">33</text>
</page>
<page number="34" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p34_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="410" left="81" width="449" height="9" font="font3" id="p34_t2" reading_order_no="1" segment_no="1" tag_type="text">Figure 9.9: Number of selections for each of the seven channels used in step 1 for IoTligent device [53].<a href="deeplearning_paper3.html#46">[53].</a></text>
<text top="553" left="230" width="52" height="8" font="font37" id="p34_t3" reading_order_no="2" segment_no="2" tag_type="table">Reference IoT</text>
<text top="553" left="341" width="34" height="8" font="font37" id="p34_t4" reading_order_no="3" segment_no="2" tag_type="table">IoTligent</text>
<text top="563" left="189" width="31" height="8" font="font37" id="p34_t5" reading_order_no="4" segment_no="2" tag_type="table">Channel</text>
<text top="563" left="233" width="46" height="8" font="font37" id="p34_t6" reading_order_no="5" segment_no="2" tag_type="table">% of success</text>
<text top="563" left="292" width="32" height="8" font="font37" id="p34_t7" reading_order_no="6" segment_no="2" tag_type="table">nb of Tx</text>
<text top="563" left="335" width="46" height="8" font="font37" id="p34_t8" reading_order_no="7" segment_no="2" tag_type="table">% of success</text>
<text top="563" left="391" width="32" height="8" font="font37" id="p34_t9" reading_order_no="8" segment_no="2" tag_type="table">nb of Tx</text>
<text top="574" left="198" width="11" height="8" font="font37" id="p34_t10" reading_order_no="9" segment_no="2" tag_type="table">#0</text>
<text top="574" left="248" width="16" height="8" font="font37" id="p34_t11" reading_order_no="10" segment_no="2" tag_type="table">21%</text>
<text top="574" left="304" width="8" height="8" font="font37" id="p34_t12" reading_order_no="11" segment_no="2" tag_type="table">76</text>
<text top="574" left="352" width="11" height="8" font="font37" id="p34_t13" reading_order_no="12" segment_no="2" tag_type="table">8%</text>
<text top="574" left="403" width="8" height="8" font="font37" id="p34_t14" reading_order_no="13" segment_no="2" tag_type="table">12</text>
<text top="584" left="198" width="11" height="8" font="font37" id="p34_t15" reading_order_no="14" segment_no="2" tag_type="table">#1</text>
<text top="584" left="248" width="16" height="8" font="font37" id="p34_t16" reading_order_no="15" segment_no="2" tag_type="table">20%</text>
<text top="584" left="304" width="8" height="8" font="font37" id="p34_t17" reading_order_no="16" segment_no="2" tag_type="table">76</text>
<text top="584" left="350" width="16" height="8" font="font37" id="p34_t18" reading_order_no="17" segment_no="2" tag_type="table">25%</text>
<text top="584" left="403" width="8" height="8" font="font37" id="p34_t19" reading_order_no="18" segment_no="2" tag_type="table">16</text>
<text top="595" left="198" width="11" height="8" font="font37" id="p34_t20" reading_order_no="19" segment_no="2" tag_type="table">#2</text>
<text top="595" left="248" width="16" height="8" font="font37" id="p34_t21" reading_order_no="20" segment_no="2" tag_type="table">24%</text>
<text top="595" left="304" width="8" height="8" font="font37" id="p34_t22" reading_order_no="21" segment_no="2" tag_type="table">75</text>
<text top="595" left="350" width="16" height="8" font="font37" id="p34_t23" reading_order_no="22" segment_no="2" tag_type="table">25%</text>
<text top="595" left="403" width="8" height="8" font="font37" id="p34_t24" reading_order_no="23" segment_no="2" tag_type="table">16</text>
<text top="605" left="198" width="11" height="8" font="font37" id="p34_t25" reading_order_no="24" segment_no="2" tag_type="table">#3</text>
<text top="605" left="248" width="16" height="8" font="font37" id="p34_t26" reading_order_no="25" segment_no="2" tag_type="table">49%</text>
<text top="605" left="304" width="8" height="8" font="font37" id="p34_t27" reading_order_no="26" segment_no="2" tag_type="table">76</text>
<text top="605" left="350" width="16" height="8" font="font37" id="p34_t28" reading_order_no="27" segment_no="2" tag_type="table">50%</text>
<text top="605" left="403" width="8" height="8" font="font37" id="p34_t29" reading_order_no="28" segment_no="2" tag_type="table">32</text>
<text top="616" left="198" width="11" height="8" font="font37" id="p34_t30" reading_order_no="29" segment_no="2" tag_type="table">#4</text>
<text top="616" left="248" width="16" height="8" font="font37" id="p34_t31" reading_order_no="30" segment_no="2" tag_type="table">62%</text>
<text top="616" left="304" width="8" height="8" font="font37" id="p34_t32" reading_order_no="31" segment_no="2" tag_type="table">74</text>
<text top="616" left="350" width="16" height="8" font="font37" id="p34_t33" reading_order_no="32" segment_no="2" tag_type="table">62%</text>
<text top="616" left="403" width="8" height="8" font="font37" id="p34_t34" reading_order_no="33" segment_no="2" tag_type="table">47</text>
<text top="626" left="198" width="11" height="8" font="font37" id="p34_t35" reading_order_no="34" segment_no="2" tag_type="table">#5</text>
<text top="626" left="248" width="16" height="8" font="font37" id="p34_t36" reading_order_no="35" segment_no="2" tag_type="table">76%</text>
<text top="626" left="304" width="8" height="8" font="font37" id="p34_t37" reading_order_no="36" segment_no="2" tag_type="table">76</text>
<text top="626" left="350" width="16" height="8" font="font37" id="p34_t38" reading_order_no="37" segment_no="2" tag_type="table">74%</text>
<text top="626" left="403" width="8" height="8" font="font37" id="p34_t39" reading_order_no="38" segment_no="2" tag_type="table">82</text>
<text top="637" left="198" width="11" height="8" font="font37" id="p34_t40" reading_order_no="39" segment_no="2" tag_type="table">#6</text>
<text top="637" left="248" width="16" height="8" font="font37" id="p34_t41" reading_order_no="40" segment_no="2" tag_type="table">96%</text>
<text top="637" left="304" width="8" height="8" font="font37" id="p34_t42" reading_order_no="41" segment_no="2" tag_type="table">75</text>
<text top="637" left="350" width="16" height="8" font="font37" id="p34_t43" reading_order_no="42" segment_no="2" tag_type="table">94%</text>
<text top="637" left="401" width="13" height="8" font="font37" id="p34_t44" reading_order_no="43" segment_no="2" tag_type="table">323</text>
<text top="657" left="85" width="443" height="9" font="font3" id="p34_t45" reading_order_no="44" segment_no="3" tag_type="text">Table 9.5: Success rate and number of attempts for each channel of regular IoT device and IoTligent.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p34_t46" reading_order_no="45" segment_no="4" tag_type="text">34</text>
</page>
<page number="35" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="117" height="9" font="font3" id="p35_t1" reading_order_no="0" segment_no="0" tag_type="title">9.3. RL AT PHY LAYER</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p35_t2" reading_order_no="1" segment_no="1" tag_type="text">mode for IoT. The underlying hypothesis, however, is that the channels occupancy by surrounding radio</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p35_t3" reading_order_no="2" segment_no="1" tag_type="text">signals (IoT or not) is not equally balanced. In other words, some ISM sub-bands are less occupied or</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p35_t4" reading_order_no="3" segment_no="1" tag_type="text">jammed than others, but it is not possible to predict it in time and space, so the need to learn on the</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p35_t5" reading_order_no="4" segment_no="1" tag_type="text">field. Table 9.6 sums-up the characteristics of the RL algorithm implementation for this use case. More<a href="deeplearning_paper3.html#35">9.6 </a>sums-up the characteristics of the RL algorithm implementation for this use case. More</text>
<text top="158" left="79" width="240" height="9" font="font3" id="p35_t6" reading_order_no="5" segment_no="1" tag_type="text">details about these measurements can be found in [53].<a href="deeplearning_paper3.html#46">[53].</a></text>
<text top="181" left="202" width="55" height="8" font="font37" id="p35_t7" reading_order_no="6" segment_no="2" tag_type="table">Characteristics</text>
<text top="181" left="347" width="36" height="8" font="font37" id="p35_t8" reading_order_no="7" segment_no="2" tag_type="table">Comment</text>
<text top="193" left="204" width="50" height="8" font="font37" id="p35_t9" reading_order_no="8" segment_no="2" tag_type="table">RL algorithm</text>
<text top="193" left="294" width="143" height="8" font="font37" id="p35_t10" reading_order_no="9" segment_no="2" tag_type="table">UCB1 (or any other bandit algorithm)</text>
<text top="204" left="215" width="28" height="8" font="font37" id="p35_t11" reading_order_no="10" segment_no="2" tag_type="table">Reward</text>
<text top="204" left="340" width="51" height="8" font="font37" id="p35_t12" reading_order_no="11" segment_no="2" tag_type="table">received ACK</text>
<text top="214" left="191" width="75" height="8" font="font37" id="p35_t13" reading_order_no="12" segment_no="2" tag_type="table">Implementation side</text>
<text top="214" left="328" width="75" height="8" font="font37" id="p35_t14" reading_order_no="13" segment_no="2" tag_type="table">embedded on device</text>
<text top="225" left="175" width="108" height="8" font="font37" id="p35_t15" reading_order_no="14" segment_no="2" tag_type="table">method for RL feedback loop</text>
<text top="225" left="316" width="99" height="8" font="font37" id="p35_t16" reading_order_no="15" segment_no="2" tag_type="table">Standard LoRaWAN ACK</text>
<text top="245" left="211" width="189" height="9" font="font3" id="p35_t17" reading_order_no="16" segment_no="3" tag_type="text">Table 9.6: Summary for step 1 experiments</text>
<text top="278" left="94" width="438" height="9" font="font3" id="p35_t18" reading_order_no="17" segment_no="4" tag_type="text">STEP 2 - Real world experiments have been done on a LoRa network deployed in the town of</text>
<text top="290" left="79" width="453" height="9" font="font3" id="p35_t19" reading_order_no="18" segment_no="4" tag_type="text">Rennes, France, with 3 channels 868.1 MHz, 868.3 MHz, 868.5 MHz. IoTligent is completely agnostic</text>
<text top="302" left="79" width="453" height="9" font="font3" id="p35_t20" reading_order_no="19" segment_no="4" tag_type="text">to the number of channels and can be used in any country or ITU Region (i.e. 866 MHz and 915 MHz</text>
<text top="314" left="79" width="453" height="9" font="font3" id="p35_t21" reading_order_no="20" segment_no="4" tag_type="text">ISM bands as well). Since this experiment is run in real conditions, we have no means to determine</text>
<text top="326" left="79" width="453" height="9" font="font3" id="p35_t22" reading_order_no="21" segment_no="4" tag_type="text">exactly which of the four following possible phenomenon influences the IoTligent devices behavior: i)</text>
<text top="338" left="79" width="453" height="9" font="font3" id="p35_t23" reading_order_no="22" segment_no="4" tag_type="text">collisions with other LoRaWAN IoT devices, ii) collisions with IoT devices running other IoT standards,</text>
<text top="350" left="79" width="348" height="9" font="font3" id="p35_t24" reading_order_no="23" segment_no="4" tag_type="text">iii) collisions with other radio jammers in the ISM band, iv) propagation issues.</text>
<text top="362" left="94" width="438" height="9" font="font3" id="p35_t25" reading_order_no="24" segment_no="5" tag_type="text">We now look at the results obtained by IoTligent, for 129 transmissions done every 2 hours, over</text>
<text top="374" left="79" width="453" height="9" font="font3" id="p35_t26" reading_order_no="25" segment_no="5" tag_type="text">an 11 days period. Figure 9.10 shows the empirical mean experienced by the device on each of the 3<a href="deeplearning_paper3.html#36">9.10 </a>shows the empirical mean experienced by the device on each of the 3</text>
<text top="386" left="79" width="453" height="9" font="font3" id="p35_t27" reading_order_no="26" segment_no="5" tag_type="text">channels. This represents the average success rate achieved in each channel since the beginning of the</text>
<text top="398" left="79" width="453" height="9" font="font3" id="p35_t28" reading_order_no="27" segment_no="5" tag_type="text">experiment. The average success rates for the three channels, i.e. #0 (868,1 MHz), #1 (868.3 MHz)</text>
<text top="410" left="79" width="453" height="9" font="font3" id="p35_t29" reading_order_no="28" segment_no="5" tag_type="text">and #2 (868.5 MHz), are represented by the curves with squares, stars and bullets, respectively. Each</text>
<text top="422" left="79" width="453" height="9" font="font3" id="p35_t30" reading_order_no="29" segment_no="5" tag_type="text">peak corresponds to a LoRa successful bi-directional exchange between the device and the application</text>
<text top="434" left="79" width="453" height="9" font="font3" id="p35_t31" reading_order_no="30" segment_no="5" tag_type="text">server: from device transmission, to ACK reception by the device. Each peak in Figure 9.10 reveals a<a href="deeplearning_paper3.html#36">9.10 </a>reveals a</text>
<text top="446" left="79" width="453" height="9" font="font3" id="p35_t32" reading_order_no="31" segment_no="5" tag_type="text">successful transmission where ACK has been received by IoTligent device. We can see that channel #1,</text>
<text top="458" left="79" width="453" height="9" font="font3" id="p35_t33" reading_order_no="32" segment_no="5" tag_type="text">star markers, has been the most successful, before channel #2, while channel #0 always failed to send</text>
<text top="470" left="79" width="453" height="9" font="font3" id="p35_t34" reading_order_no="33" segment_no="5" tag_type="text">back an ACK to the device. During the experiment indeed, channel #0 has been tried 29 times with</text>
<text top="482" left="79" width="453" height="9" font="font3" id="p35_t35" reading_order_no="34" segment_no="5" tag_type="text">0 success. IoTligent device uses channel #1 61 times with 7 successful bi-directional exchanges, i.e. 7</text>
<text top="494" left="79" width="453" height="9" font="font3" id="p35_t36" reading_order_no="35" segment_no="5" tag_type="text">peaks on the curve with stars, and channel #2 39 times with 2 successes, i.e. 2 peaks on the curve with</text>
<text top="506" left="79" width="453" height="9" font="font3" id="p35_t37" reading_order_no="36" segment_no="5" tag_type="text">bullets. At the end of the experiment, one can observe 11.5% successful bi-directional connections for</text>
<text top="518" left="79" width="453" height="9" font="font3" id="p35_t38" reading_order_no="37" segment_no="5" tag_type="text">channel #1 and 5% for channel #2, whereas channel #0 never worked from the device point of view. For</text>
<text top="529" left="79" width="453" height="9" font="font3" id="p35_t39" reading_order_no="38" segment_no="5" tag_type="text">comparison, a regular IoT device, performing a random access, achieves a global average successful rate</text>
<text top="541" left="79" width="453" height="9" font="font3" id="p35_t40" reading_order_no="39" segment_no="5" tag_type="text">of 5.5%. Table 9.7 summarizes the characteristics of the RL algorithm implementation for this use case.<a href="deeplearning_paper3.html#35">9.7 </a>summarizes the characteristics of the RL algorithm implementation for this use case.</text>
<text top="553" left="79" width="266" height="9" font="font3" id="p35_t41" reading_order_no="40" segment_no="5" tag_type="text">More details about these measurements can be found in [54].<a href="deeplearning_paper3.html#46">[54].</a></text>
<text top="650" left="163" width="55" height="8" font="font37" id="p35_t42" reading_order_no="41" segment_no="6" tag_type="table">Characteristics</text>
<text top="650" left="347" width="36" height="8" font="font37" id="p35_t43" reading_order_no="42" segment_no="6" tag_type="table">Comment</text>
<text top="663" left="166" width="50" height="8" font="font37" id="p35_t44" reading_order_no="43" segment_no="6" tag_type="table">RL algorithm</text>
<text top="663" left="285" width="160" height="8" font="font37" id="p35_t45" reading_order_no="44" segment_no="6" tag_type="table">UCB1 (but could be any bandit algorithm)</text>
<text top="673" left="177" width="28" height="8" font="font37" id="p35_t46" reading_order_no="45" segment_no="6" tag_type="table">Reward</text>
<text top="673" left="256" width="219" height="8" font="font37" id="p35_t47" reading_order_no="46" segment_no="6" tag_type="table">channel ’availability’ (collisions, jamming and propagation)</text>
<text top="684" left="153" width="75" height="8" font="font37" id="p35_t48" reading_order_no="47" segment_no="6" tag_type="table">Implementation side</text>
<text top="684" left="328" width="75" height="8" font="font37" id="p35_t49" reading_order_no="48" segment_no="6" tag_type="table">embedded on device</text>
<text top="694" left="137" width="108" height="8" font="font37" id="p35_t50" reading_order_no="49" segment_no="6" tag_type="table">method for RL feedback loop</text>
<text top="694" left="316" width="99" height="8" font="font37" id="p35_t51" reading_order_no="50" segment_no="6" tag_type="table">Standard LoRaWAN ACK</text>
<text top="714" left="210" width="192" height="9" font="font3" id="p35_t52" reading_order_no="51" segment_no="7" tag_type="text">Table 9.7: Summary for step 2 experiments.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p35_t53" reading_order_no="52" segment_no="8" tag_type="text">35</text>
</page>
<page number="36" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p36_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="532" left="154" width="305" height="9" font="font3" id="p36_t2" reading_order_no="1" segment_no="1" tag_type="text">Figure 9.10: Empirical mean evolution through time over 11 days [54]<a href="deeplearning_paper3.html#46">[54]</a></text>
<text top="756" left="301" width="10" height="9" font="font3" id="p36_t3" reading_order_no="2" segment_no="2" tag_type="text">36</text>
</page>
<page number="37" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="214" height="9" font="font3" id="p37_t1" reading_order_no="0" segment_no="0" tag_type="title">9.4. CONCLUSIONS AND FUTURE TRENDS</text>
<text top="110" left="79" width="21" height="13" font="font0" id="p37_t2" reading_order_no="1" segment_no="1" tag_type="title">9.4</text>
<text top="110" left="116" width="221" height="13" font="font0" id="p37_t3" reading_order_no="2" segment_no="1" tag_type="title">Conclusions and Future Trends</text>
<text top="136" left="79" width="453" height="9" font="font3" id="p37_t4" reading_order_no="3" segment_no="2" tag_type="text">During recent years, we have witnessed a shift in the way wireless communication networks are designed,</text>
<text top="148" left="79" width="453" height="9" font="font3" id="p37_t5" reading_order_no="4" segment_no="2" tag_type="text">at least in the academic community, by introducing ML techniques to optimize the networks. With the</text>
<text top="160" left="79" width="453" height="9" font="font3" id="p37_t6" reading_order_no="5" segment_no="2" tag_type="text">complexity of wireless systems increasing, it becomes more and more difficult to build explainable math-</text>
<text top="172" left="79" width="453" height="9" font="font3" id="p37_t7" reading_order_no="6" segment_no="2" tag_type="text">ematical model to predict the performance of large scale systems. Exhaustive and complex simulations</text>
<text top="184" left="79" width="453" height="9" font="font3" id="p37_t8" reading_order_no="7" segment_no="2" tag_type="text">are not always an option because they are highly resource-intensive. In this sense, ML in general and</text>
<text top="196" left="79" width="453" height="9" font="font3" id="p37_t9" reading_order_no="8" segment_no="2" tag_type="text">RL in particular have the potential to overcome the network modelling deficit by learning the optimal</text>
<text top="207" left="79" width="453" height="9" font="font3" id="p37_t10" reading_order_no="9" segment_no="2" tag_type="text">network functioning with minimal assumptions on the underlying phenomena. This is of particular im-</text>
<text top="219" left="79" width="453" height="9" font="font3" id="p37_t11" reading_order_no="10" segment_no="2" tag_type="text">portance when non linear phenomena needs to be taken into account and for which it is very difficult to</text>
<text top="231" left="79" width="98" height="9" font="font3" id="p37_t12" reading_order_no="11" segment_no="2" tag_type="text">get analytical insights.</text>
<text top="243" left="94" width="438" height="9" font="font3" id="p37_t13" reading_order_no="12" segment_no="3" tag_type="text">In this Chapter, we focused on RL for PHY layer communications. Even if this domain enjoys a long</text>
<text top="255" left="79" width="453" height="9" font="font3" id="p37_t14" reading_order_no="13" segment_no="3" tag_type="text">successful history of well-defined mathematical models, the heterogeneity of the use-cases envisaged in 5G</text>
<text top="267" left="79" width="453" height="9" font="font3" id="p37_t15" reading_order_no="14" segment_no="3" tag_type="text">for instance, advocates for adding smartness in the PHY layer of upcoming wireless systems. All along</text>
<text top="279" left="79" width="453" height="9" font="font3" id="p37_t16" reading_order_no="15" segment_no="3" tag_type="text">this Chapter, we have provided some PHY examples in which RL or deep RL can help to achieve good</text>
<text top="291" left="79" width="453" height="9" font="font3" id="p37_t17" reading_order_no="16" segment_no="3" tag_type="text">performance results. The key point to apply RL algorithms to a PHY layer problem is when one cannot</text>
<text top="303" left="79" width="453" height="9" font="font3" id="p37_t18" reading_order_no="17" segment_no="3" tag_type="text">solve the problem analytically or by writing an explicit program to do so. There are, without any doubt,</text>
<text top="315" left="79" width="454" height="9" font="font3" id="p37_t19" reading_order_no="18" segment_no="3" tag_type="text">many practical situations where one cannot explicitly solve the associated optimization problem related</text>
<text top="327" left="79" width="453" height="9" font="font3" id="p37_t20" reading_order_no="19" segment_no="3" tag_type="text">to the communication system design, e.g. non linear peak to average power ratio reduction, acoustic</text>
<text top="339" left="79" width="178" height="9" font="font3" id="p37_t21" reading_order_no="20" segment_no="3" tag_type="text">transmission, non linear power-amplifier.</text>
<text top="351" left="94" width="438" height="9" font="font3" id="p37_t22" reading_order_no="21" segment_no="4" tag_type="text">The key benefit of RL is its adaptability to unknown system dynamics. However, the convergence</text>
<text top="363" left="79" width="453" height="9" font="font3" id="p37_t23" reading_order_no="22" segment_no="4" tag_type="text">time is crucial in wireless communications and real networks cannot afford to spend too much time to</text>
<text top="373" left="79" width="454" height="11" font="font3" id="p37_t24" reading_order_no="23" segment_no="4" tag_type="text">learn a good strategy. For instance, the deep Q-learning in [37] converges around 10 4 iterations, for two</text>
<text top="387" left="79" width="453" height="9" font="font3" id="p37_t25" reading_order_no="24" segment_no="4" tag_type="text">popularity profile states. This cannot be directly converted into time delay because it depends on the</text>
<text top="399" left="79" width="453" height="9" font="font3" id="p37_t26" reading_order_no="25" segment_no="4" tag_type="text">number of requests the users do, but if the network records 1 request per second, it takes almost 3 hours</text>
<text top="409" left="79" width="454" height="11" font="font3" id="p37_t27" reading_order_no="26" segment_no="4" tag_type="text">to converge toward the optimal strategy. The converging time is also around 10 3 -10 4 iterations in the</text>
<text top="423" left="79" width="453" height="9" font="font3" id="p37_t28" reading_order_no="27" segment_no="4" tag_type="text">problem of optimal buffer state and transmission power dealt with in [32]. In case of non stationary<a href="deeplearning_paper3.html#45">[37] </a>converges around 10</text>
<text top="435" left="79" width="152" height="9" font="font3" id="p37_t29" reading_order_no="28" segment_no="4" tag_type="text">environment, it can be prohibitive.</text>
<text top="447" left="94" width="438" height="9" font="font3" id="p37_t30" reading_order_no="29" segment_no="5" tag_type="text">The feasibility of a PHY layer data-driven designed has been proved in [55, 56]. However, even if</text>
<text top="459" left="79" width="454" height="9" font="font3" id="p37_t31" reading_order_no="30" segment_no="5" tag_type="text">possible, learning an entire PHY layer without any expert knowledge is not necessarily desirable. For</text>
<text top="471" left="79" width="453" height="9" font="font3" id="p37_t32" reading_order_no="31" segment_no="5" tag_type="text">instance, the synchronisation task requires an huge amount of computation and a dedicated NN in</text>
<text top="482" left="79" width="453" height="9" font="font3" id="p37_t33" reading_order_no="32" segment_no="5" tag_type="text">[56] to synchronize the transmission while it can be simply performed using well-known OFDM signal</text>
<text top="494" left="79" width="453" height="9" font="font3" id="p37_t34" reading_order_no="33" segment_no="5" tag_type="text">structures [57]. Hence it is apparent that DRL can be an efficient tool to design the PHY layer of</text>
<text top="506" left="79" width="453" height="9" font="font3" id="p37_t35" reading_order_no="34" segment_no="5" tag_type="text">future wireless systems, however, it would may gain in efficiency if cross-fertilization research between</text>
<text top="518" left="79" width="453" height="9" font="font3" id="p37_t36" reading_order_no="35" segment_no="5" tag_type="text">ML and model-based approach would be undertaken. In order to achieve this goal, explainable ANN with</text>
<text top="530" left="79" width="454" height="9" font="font3" id="p37_t37" reading_order_no="36" segment_no="5" tag_type="text">information-theoretic tools is a promising research direction, as attempted in [58]. The transfer learning</text>
<text top="542" left="79" width="453" height="9" font="font3" id="p37_t38" reading_order_no="37" segment_no="5" tag_type="text">concept, which consists in transferring the knowledge acquired in a previous task to a current one in order<a href="deeplearning_paper3.html#45">[32]. </a>In case of non stationary</text>
<text top="554" left="79" width="453" height="9" font="font3" id="p37_t39" reading_order_no="38" segment_no="5" tag_type="text">to speed up the converge, is also a promising research direction if a mixed transfer, i.e. model-based to</text>
<text top="566" left="79" width="114" height="9" font="font3" id="p37_t40" reading_order_no="39" segment_no="5" tag_type="text">data-driven, is considered.<a href="deeplearning_paper3.html#46">[55, 56]. </a>However, even if</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p37_t41" reading_order_no="40" segment_no="6" tag_type="text">37</text>
</page>
<page number="38" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p38_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="21" height="13" font="font0" id="p38_t2" reading_order_no="1" segment_no="1" tag_type="title">9.5</text>
<text top="110" left="116" width="168" height="13" font="font0" id="p38_t3" reading_order_no="2" segment_no="1" tag_type="title">Bibliographical remarks</text>
<text top="137" left="79" width="453" height="9" font="font3" id="p38_t4" reading_order_no="3" segment_no="2" tag_type="text">This Chapter aimed at being the most self-content as possible and at presenting, in a tutorial way, the</text>
<text top="149" left="79" width="453" height="9" font="font3" id="p38_t5" reading_order_no="4" segment_no="2" tag_type="text">theoretical background of RL and some application examples, drawn from the PHY layer communication</text>
<text top="161" left="79" width="453" height="9" font="font3" id="p38_t6" reading_order_no="5" segment_no="2" tag_type="text">domain, where RL techniques can be applied. The theoretical part of Section 9.2 revisits the fundamentals<a href="deeplearning_paper3.html#4">9.2 </a>revisits the fundamentals</text>
<text top="173" left="79" width="453" height="9" font="font3" id="p38_t7" reading_order_no="6" segment_no="2" tag_type="text">of RL and is largely based on the great books of Sutton and Barto [5] and Csaba Szepesvari [6] that<a href="deeplearning_paper3.html#43">[5] </a>and Csaba Szepesvari <a href="deeplearning_paper3.html#43">[6] </a>that</text>
<text top="185" left="79" width="453" height="9" font="font3" id="p38_t8" reading_order_no="7" segment_no="2" tag_type="text">gathered in a comprehensive way the original results of Bellman [7] who introduced the notion of dynamic<a href="deeplearning_paper3.html#43">[7] </a>who introduced the notion of dynamic</text>
<text top="197" left="79" width="309" height="9" font="font3" id="p38_t9" reading_order_no="8" segment_no="2" tag_type="text">programming and Watkins [9] who proposed the Q-learning algorithm.<a href="deeplearning_paper3.html#43">[9] </a>who proposed the Q-learning algorithm.</text>
<text top="209" left="94" width="438" height="9" font="font3" id="p38_t10" reading_order_no="9" segment_no="3" tag_type="text">Example in Section 9.3.1 follows from [31, 32]. The problem of joint power and delay management can<a href="deeplearning_paper3.html#20">9.3.1 </a>follows from <a href="deeplearning_paper3.html#45">[31, 32]. </a>The problem of joint power and delay management can</text>
<text top="221" left="79" width="453" height="9" font="font3" id="p38_t11" reading_order_no="10" segment_no="3" tag_type="text">be dated back to at least the early 2000s. In [59] the authors studied the power adaptation and transmis-<a href="deeplearning_paper3.html#46">[59] </a>the authors studied the power adaptation and transmis-</text>
<text top="233" left="79" width="453" height="9" font="font3" id="p38_t12" reading_order_no="11" segment_no="3" tag_type="text">sion rate to optimize the power consumption and the buffer occupancy as well. They characterized the</text>
<text top="245" left="79" width="453" height="9" font="font3" id="p38_t13" reading_order_no="12" segment_no="3" tag_type="text">optimal power-delay tradeoff and provided the former steps of a dynamic scheduling strategy achieving</text>
<text top="257" left="79" width="453" height="9" font="font3" id="p38_t14" reading_order_no="13" segment_no="3" tag_type="text">the Pareto front derived earlier. While the previous work investigated the power-delay Pareto front in</text>
<text top="269" left="79" width="453" height="9" font="font3" id="p38_t15" reading_order_no="14" segment_no="3" tag_type="text">block fading channels, authors in [60] came back to AWGN channel but proposed more explicit schedulers<a href="deeplearning_paper3.html#46">[60] </a>came back to AWGN channel but proposed more explicit schedulers</text>
<text top="281" left="79" width="453" height="9" font="font3" id="p38_t16" reading_order_no="15" segment_no="3" tag_type="text">to operate close to the fundamental bound. The authors in [35] revisited the same problem as in [59] and<a href="deeplearning_paper3.html#45">[35] </a>revisited the same problem as in <a href="deeplearning_paper3.html#46">[59] </a>and</text>
<text top="293" left="79" width="453" height="9" font="font3" id="p38_t17" reading_order_no="16" segment_no="3" tag_type="text">proved the existence of a stationary policy to achieve the power-delay Pareto front. The formulation of</text>
<text top="305" left="79" width="443" height="9" font="font3" id="p38_t18" reading_order_no="17" segment_no="3" tag_type="text">the joint delay-power management problem as a constrained MDP can be reported in [35, 36, 31, 32].<a href="deeplearning_paper3.html#45">[35, 36, 31, 32]</a>.</text>
<text top="317" left="94" width="438" height="9" font="font3" id="p38_t19" reading_order_no="18" segment_no="4" tag_type="text">Example in Section 9.3.2 has been considered in [37] with a more complex setting, where the authors<a href="deeplearning_paper3.html#22">9.3.2 </a>has been considered in <a href="deeplearning_paper3.html#45">[37] </a>with a more complex setting, where the authors</text>
<text top="329" left="79" width="453" height="9" font="font3" id="p38_t20" reading_order_no="19" segment_no="4" tag_type="text">introduced two kind of popularity profiles, i.e. local and global. The local popularity profile allows to</text>
<text top="341" left="79" width="453" height="9" font="font3" id="p38_t21" reading_order_no="20" segment_no="4" tag_type="text">cache the requested files according to the local demand while the global one allows to anticipate the local</text>
<text top="353" left="79" width="453" height="9" font="font3" id="p38_t22" reading_order_no="21" segment_no="4" tag_type="text">demand by monitoring the most wanted files over the network. We simplified the system model in order</text>
<text top="365" left="79" width="453" height="9" font="font3" id="p38_t23" reading_order_no="22" segment_no="4" tag_type="text">to make a toy example easily with this application. Due to the potentially large number of files to be</text>
<text top="377" left="79" width="425" height="9" font="font3" id="p38_t24" reading_order_no="23" segment_no="4" tag_type="text">cached, this problem can be addressed with deep neural network as well, see for instance [61, 62].<a href="deeplearning_paper3.html#46">[61, </a><a href="deeplearning_paper3.html#47">62].</a></text>
<text top="390" left="94" width="438" height="9" font="font3" id="p38_t25" reading_order_no="24" segment_no="5" tag_type="text">Examples of Section 9.3.3 follow from [23, 42] for the OSA problem with quality of the transmission<a href="deeplearning_paper3.html#25">9.3.3 </a>follow from <a href="deeplearning_paper3.html#44">[23, </a><a href="deeplearning_paper3.html#45">42] </a>for the OSA problem with quality of the transmission</text>
<text top="402" left="79" width="453" height="9" font="font3" id="p38_t26" reading_order_no="25" segment_no="5" tag_type="text">and from [45] for the switch ON/OFF base station problem. The opportunistic spectrum access problem<a href="deeplearning_paper3.html#46">[45] </a>for the switch ON/OFF base station problem. The opportunistic spectrum access problem</text>
<text top="414" left="79" width="453" height="9" font="font3" id="p38_t27" reading_order_no="26" segment_no="5" tag_type="text">has been considered as the typical use-case for the application of MAB and RL algorithms in wireless</text>
<text top="426" left="79" width="453" height="9" font="font3" id="p38_t28" reading_order_no="27" segment_no="5" tag_type="text">communications since their inception in the field. This matter of fact comes from the RL framework</text>
<text top="438" left="79" width="453" height="9" font="font3" id="p38_t29" reading_order_no="28" segment_no="5" tag_type="text">in general, and MAB in particular, are well suited for describing the success and the failure of a user</text>
<text top="449" left="79" width="453" height="9" font="font3" id="p38_t30" reading_order_no="29" segment_no="5" tag_type="text">that tries to opportunistically access to a spectral resource that is sporadically occupied. The second</text>
<text top="461" left="79" width="453" height="9" font="font3" id="p38_t31" reading_order_no="30" segment_no="5" tag_type="text">reason is that the performance of the learning algorithm strongly depends on the ability of the device to</text>
<text top="473" left="79" width="453" height="9" font="font3" id="p38_t32" reading_order_no="31" segment_no="5" tag_type="text">detect good channels, that motivates the reborn of the research on signal detection algorithms. A broad</text>
<text top="485" left="79" width="453" height="9" font="font3" id="p38_t33" reading_order_no="32" segment_no="5" tag_type="text">overview on signal detection and algorithms to opportunistically exploit the frequency resource has been</text>
<text top="497" left="79" width="71" height="9" font="font3" id="p38_t34" reading_order_no="33" segment_no="5" tag_type="text">provided in [63].<a href="deeplearning_paper3.html#47">[63].</a></text>
<text top="510" left="94" width="438" height="9" font="font3" id="p38_t35" reading_order_no="34" segment_no="6" tag_type="text">The examples we proposed in this chapter are not exhaustive since it was not the objective and at</text>
<text top="522" left="79" width="453" height="9" font="font3" id="p38_t36" reading_order_no="35" segment_no="6" tag_type="text">least two very complete surveys have been provided recently on that topic [64, 4]. We invite the interested<a href="deeplearning_paper3.html#47">[64, </a><a href="deeplearning_paper3.html#43">4]. </a>We invite the interested</text>
<text top="534" left="79" width="453" height="9" font="font3" id="p38_t37" reading_order_no="36" segment_no="6" tag_type="text">reader to consult these articles and the references therein to go further in the application of (deep)-RL</text>
<text top="546" left="79" width="453" height="9" font="font3" id="p38_t38" reading_order_no="37" segment_no="6" tag_type="text">techniques to some specific wireless communication problems. In this section, we give some articles that</text>
<text top="558" left="79" width="370" height="9" font="font3" id="p38_t39" reading_order_no="38" segment_no="6" tag_type="text">may be interesting to consult when addressing PHY layer communication challenges.</text>
<text top="570" left="94" width="438" height="9" font="font3" id="p38_t40" reading_order_no="39" segment_no="7" tag_type="text">An interesting use-case in which DRL can be applied is the IoT. The huge number of cheap devices to</text>
<text top="582" left="79" width="453" height="9" font="font3" id="p38_t41" reading_order_no="40" segment_no="7" tag_type="text">be connected with small signalling overheads make this application appealing for some learning approaches</text>
<text top="594" left="79" width="453" height="9" font="font3" id="p38_t42" reading_order_no="41" segment_no="7" tag_type="text">implemented at BS for instance. Authors in [65] revisited the problem of dynamic spectrum access for one<a href="deeplearning_paper3.html#47">[65] </a>revisited the problem of dynamic spectrum access for one</text>
<text top="606" left="79" width="453" height="9" font="font3" id="p38_t43" reading_order_no="42" segment_no="7" tag_type="text">user. Even if this set up has been widely studied in literature, the authors modeled the dynamic access</text>
<text top="618" left="79" width="453" height="9" font="font3" id="p38_t44" reading_order_no="43" segment_no="7" tag_type="text">issue as a POMDP with correlated states and they introduce a deep Q-learning algorithm to choose what</text>
<text top="630" left="79" width="453" height="9" font="font3" id="p38_t45" reading_order_no="44" segment_no="7" tag_type="text">are the best channels to access at each time slot. An extension to multiple devices has been provided</text>
<text top="642" left="79" width="453" height="9" font="font3" id="p38_t46" reading_order_no="45" segment_no="7" tag_type="text">in [66]. A relay with buffering capability is considered to forward the packets of the other nodes to the<a href="deeplearning_paper3.html#47">[66]</a>. A relay with buffering capability is considered to forward the packets of the other nodes to the</text>
<text top="654" left="79" width="453" height="9" font="font3" id="p38_t47" reading_order_no="46" segment_no="7" tag_type="text">sink. At the beginning of each frame, the relay chooses packets from buffers to transmit on some channels</text>
<text top="666" left="79" width="453" height="9" font="font3" id="p38_t48" reading_order_no="47" segment_no="7" tag_type="text">with the suitable power and rate. They propose a deep Q-learning approach with a FNN to optimize</text>
<text top="678" left="79" width="453" height="9" font="font3" id="p38_t49" reading_order_no="48" segment_no="7" tag_type="text">the packet transmission rate. IoT networks with energy harvesting capability can also be addressed with</text>
<text top="690" left="79" width="363" height="9" font="font3" id="p38_t50" reading_order_no="49" segment_no="7" tag_type="text">deep Q-learning in order to accurately predict the battery state of each sensor [67].<a href="deeplearning_paper3.html#47">[67].</a></text>
<text top="702" left="94" width="438" height="9" font="font3" id="p38_t51" reading_order_no="50" segment_no="8" tag_type="text">The works above consider centralized resource allocation, i.e. the agent is run in the BS, relay or</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p38_t52" reading_order_no="51" segment_no="8" tag_type="text">another server but is located at one place. In large scale heterogeneous networks, with multiple kind of</text>
<text top="726" left="79" width="453" height="9" font="font3" id="p38_t53" reading_order_no="52" segment_no="8" tag_type="text">BS, e.g. macro, small or pico BS, this approach is no longer valid and distributed learning has to be</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p38_t54" reading_order_no="53" segment_no="9" tag_type="text">38</text>
</page>
<page number="39" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="175" height="9" font="font3" id="p39_t1" reading_order_no="0" segment_no="0" tag_type="title">9.5. BIBLIOGRAPHICAL REMARKS</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p39_t2" reading_order_no="1" segment_no="1" tag_type="text">implemented. In [68], the authors considered the problem of LTE access through WiFi small cells by<a href="deeplearning_paper3.html#47">[68], </a>the authors considered the problem of LTE access through WiFi small cells by</text>
<text top="122" left="79" width="453" height="9" font="font3" id="p39_t3" reading_order_no="2" segment_no="1" tag_type="text">allocating the communication channels and managing the interference. The small BS are in competition</text>
<text top="134" left="79" width="453" height="9" font="font3" id="p39_t4" reading_order_no="3" segment_no="1" tag_type="text">to access the resources and hence the problem is formulated as a non cooperative game which is solved</text>
<text top="146" left="79" width="453" height="9" font="font3" id="p39_t5" reading_order_no="4" segment_no="1" tag_type="text">using deep RL techniques. The same kind of problem has been tackled in [69] while in a different context<a href="deeplearning_paper3.html#47">[69] </a>while in a different context</text>
<text top="158" left="79" width="127" height="9" font="font3" id="p39_t6" reading_order_no="5" segment_no="1" tag_type="text">and different utility function.</text>
<text top="170" left="94" width="438" height="9" font="font3" id="p39_t7" reading_order_no="6" segment_no="2" tag_type="text">DRL has also been successfully applied to complex and changing radio environments with multiple</text>
<text top="182" left="79" width="453" height="9" font="font3" id="p39_t8" reading_order_no="7" segment_no="2" tag_type="text">conflicting metrics such as in satellite communications in [70]. Indeed, the orbital dynamics, the variable<a href="deeplearning_paper3.html#47">[70]. </a>Indeed, the orbital dynamics, the variable</text>
<text top="194" left="79" width="453" height="9" font="font3" id="p39_t9" reading_order_no="8" segment_no="2" tag_type="text">propagation environment, e.g. cloudy, clear sky, the multiple optimization objectives to handle, e.g. low</text>
<text top="206" left="79" width="453" height="9" font="font3" id="p39_t10" reading_order_no="9" segment_no="2" tag_type="text">bit error rate, throughput improvement, power and spectral efficiencies, make the analytical optimization</text>
<text top="218" left="79" width="453" height="9" font="font3" id="p39_t11" reading_order_no="10" segment_no="2" tag_type="text">of the global system untractable. Authors used a deep Q-learning with a deep NN to choose the actions to</text>
<text top="230" left="79" width="453" height="9" font="font3" id="p39_t12" reading_order_no="11" segment_no="2" tag_type="text">perform at each cognitive cycle, e.g. modulation and encoding rate, power transmission, and demonstrate</text>
<text top="242" left="79" width="453" height="9" font="font3" id="p39_t13" reading_order_no="12" segment_no="2" tag_type="text">that the proposed solution achieved very good performance compared to the ideal case obtained with a</text>
<text top="253" left="79" width="80" height="9" font="font3" id="p39_t14" reading_order_no="13" segment_no="2" tag_type="text">brute force search.</text>
<text top="265" left="94" width="438" height="9" font="font3" id="p39_t15" reading_order_no="14" segment_no="3" tag_type="text">There are still lots of papers dealing with RL and deep-RL for PHY layer communications, ranging</text>
<text top="277" left="79" width="453" height="9" font="font3" id="p39_t16" reading_order_no="15" segment_no="3" tag_type="text">from resource allocations to PHY layer security for instance. Since this is a hot topic rapidly evolving</text>
<text top="289" left="79" width="453" height="9" font="font3" id="p39_t17" reading_order_no="16" segment_no="3" tag_type="text">at the time we write this book, one can expect very interesting and important contributions in this field</text>
<text top="301" left="79" width="453" height="9" font="font3" id="p39_t18" reading_order_no="17" segment_no="3" tag_type="text">in the upcoming years. Since the feasibility and the potential of RL techniques has been demonstrated</text>
<text top="313" left="79" width="453" height="9" font="font3" id="p39_t19" reading_order_no="18" segment_no="3" tag_type="text">for PHY layer communications, we encourage the research community to also address important issues</text>
<text top="325" left="79" width="453" height="9" font="font3" id="p39_t20" reading_order_no="19" segment_no="3" tag_type="text">such as the convergence time reduction of learning algorithms or the energy consumption reduction for</text>
<text top="337" left="79" width="296" height="9" font="font3" id="p39_t21" reading_order_no="20" segment_no="3" tag_type="text">training a deep NN instead of an expert-based design of PHY layer.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p39_t22" reading_order_no="21" segment_no="4" tag_type="text">39</text>
</page>
<page number="40" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="262" width="271" height="9" font="font3" id="p40_t1" reading_order_no="0" segment_no="0" tag_type="title">CHAPTER 9. RL FOR PHY LAYER COMMUNICATIONS</text>
<text top="110" left="79" width="71" height="13" font="font0" id="p40_t2" reading_order_no="1" segment_no="1" tag_type="title">Acronyms</text>
<text top="160" left="85" width="22" height="9" font="font3" id="p40_t3" reading_order_no="2" segment_no="2" tag_type="list">ACK</text>
<text top="160" left="182" width="78" height="9" font="font3" id="p40_t4" reading_order_no="3" segment_no="2" tag_type="list">Acknowledgement</text>
<text top="175" left="85" width="22" height="9" font="font3" id="p40_t5" reading_order_no="4" segment_no="3" tag_type="list">ANN</text>
<text top="175" left="182" width="111" height="9" font="font3" id="p40_t6" reading_order_no="5" segment_no="3" tag_type="list">Artificial Neural Network</text>
<text top="190" left="85" width="32" height="9" font="font3" id="p40_t7" reading_order_no="6" segment_no="4" tag_type="list">AWGN</text>
<text top="190" left="182" width="137" height="9" font="font3" id="p40_t8" reading_order_no="7" segment_no="4" tag_type="list">Additive White Gaussian Noise</text>
<text top="205" left="85" width="27" height="9" font="font3" id="p40_t9" reading_order_no="8" segment_no="5" tag_type="list">BPSK</text>
<text top="205" left="182" width="117" height="9" font="font3" id="p40_t10" reading_order_no="9" segment_no="5" tag_type="list">Binary Phase Shift Keying</text>
<text top="220" left="85" width="13" height="9" font="font3" id="p40_t11" reading_order_no="10" segment_no="6" tag_type="list">BS</text>
<text top="220" left="182" width="55" height="9" font="font3" id="p40_t12" reading_order_no="11" segment_no="6" tag_type="list">Base Station</text>
<text top="235" left="85" width="14" height="9" font="font3" id="p40_t13" reading_order_no="12" segment_no="7" tag_type="list">DL</text>
<text top="235" left="182" width="64" height="9" font="font3" id="p40_t14" reading_order_no="13" segment_no="7" tag_type="list">Deep Learning</text>
<text top="250" left="85" width="21" height="9" font="font3" id="p40_t15" reading_order_no="14" segment_no="8" tag_type="list">DRL</text>
<text top="250" left="182" width="130" height="9" font="font3" id="p40_t16" reading_order_no="15" segment_no="8" tag_type="list">Reinforcement Deep Learning</text>
<text top="265" left="85" width="14" height="9" font="font3" id="p40_t17" reading_order_no="16" segment_no="9" tag_type="list">HF</text>
<text top="265" left="182" width="69" height="9" font="font3" id="p40_t18" reading_order_no="17" segment_no="9" tag_type="list">High Frequency</text>
<text top="280" left="85" width="26" height="9" font="font3" id="p40_t19" reading_order_no="18" segment_no="10" tag_type="list">HMM</text>
<text top="280" left="182" width="98" height="9" font="font3" id="p40_t20" reading_order_no="19" segment_no="10" tag_type="list">Hidden Markov Model</text>
<text top="295" left="85" width="24" height="9" font="font3" id="p40_t21" reading_order_no="20" segment_no="11" tag_type="list">MAB</text>
<text top="295" left="182" width="90" height="9" font="font3" id="p40_t22" reading_order_no="21" segment_no="11" tag_type="list">Multi-Armed Bandit</text>
<text top="310" left="85" width="24" height="9" font="font3" id="p40_t23" reading_order_no="22" segment_no="12" tag_type="list">MDP</text>
<text top="310" left="182" width="109" height="9" font="font3" id="p40_t24" reading_order_no="23" segment_no="12" tag_type="list">Markov Decision Process</text>
<text top="325" left="85" width="15" height="9" font="font3" id="p40_t25" reading_order_no="24" segment_no="13" tag_type="list">NN</text>
<text top="325" left="182" width="69" height="9" font="font3" id="p40_t26" reading_order_no="25" segment_no="13" tag_type="list">Neural Network</text>
<text top="340" left="85" width="31" height="9" font="font3" id="p40_t27" reading_order_no="26" segment_no="14" tag_type="list">OFDM</text>
<text top="340" left="182" width="208" height="9" font="font3" id="p40_t28" reading_order_no="27" segment_no="14" tag_type="list">Orthogonal Frequency Division Multiple Access</text>
<text top="354" left="85" width="22" height="9" font="font3" id="p40_t29" reading_order_no="28" segment_no="15" tag_type="list">PHY</text>
<text top="354" left="182" width="135" height="9" font="font3" id="p40_t30" reading_order_no="29" segment_no="15" tag_type="list">Physical layer of the OSI stack</text>
<text top="369" left="85" width="38" height="9" font="font3" id="p40_t31" reading_order_no="30" segment_no="16" tag_type="list">POMDP</text>
<text top="369" left="182" width="201" height="9" font="font3" id="p40_t32" reading_order_no="31" segment_no="16" tag_type="list">Partially Observable Markov Decision Process</text>
<text top="384" left="85" width="15" height="9" font="font3" id="p40_t33" reading_order_no="32" segment_no="17" tag_type="list">ML</text>
<text top="384" left="182" width="78" height="9" font="font3" id="p40_t34" reading_order_no="33" segment_no="17" tag_type="list">Machine Learning</text>
<text top="399" left="85" width="18" height="9" font="font3" id="p40_t35" reading_order_no="34" segment_no="18" tag_type="list">QoS</text>
<text top="399" left="182" width="78" height="9" font="font3" id="p40_t36" reading_order_no="35" segment_no="18" tag_type="list">Quality of Service</text>
<text top="414" left="85" width="14" height="9" font="font3" id="p40_t37" reading_order_no="36" segment_no="19" tag_type="list">RL</text>
<text top="414" left="182" width="104" height="9" font="font3" id="p40_t38" reading_order_no="37" segment_no="19" tag_type="list">Reinforcement Learning</text>
<text top="429" left="85" width="24" height="9" font="font3" id="p40_t39" reading_order_no="38" segment_no="20" tag_type="list">SINR</text>
<text top="429" left="182" width="169" height="9" font="font3" id="p40_t40" reading_order_no="39" segment_no="20" tag_type="list">Signal to Interference plus Noise Ratio</text>
<text top="444" left="85" width="13" height="9" font="font3" id="p40_t41" reading_order_no="40" segment_no="21" tag_type="list">SU</text>
<text top="444" left="182" width="79" height="9" font="font3" id="p40_t42" reading_order_no="41" segment_no="21" tag_type="list">Secondary User(s)</text>
<text top="459" left="85" width="22" height="9" font="font3" id="p40_t43" reading_order_no="42" segment_no="22" tag_type="list">UCB</text>
<text top="459" left="182" width="110" height="9" font="font3" id="p40_t44" reading_order_no="43" segment_no="22" tag_type="list">Upper Confidence Bound</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p40_t45" reading_order_no="44" segment_no="23" tag_type="text">40</text>
</page>
<page number="41" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="175" height="9" font="font3" id="p41_t1" reading_order_no="0" segment_no="0" tag_type="title">9.5. BIBLIOGRAPHICAL REMARKS</text>
<text top="110" left="79" width="155" height="13" font="font0" id="p41_t2" reading_order_no="1" segment_no="1" tag_type="title">Notation and symbols</text>
<text top="150" left="85" width="19" height="9" font="font7" id="p41_t3" reading_order_no="2" segment_no="2" tag_type="list">A ( t )</text>
<text top="150" left="180" width="122" height="9" font="font3" id="p41_t4" reading_order_no="3" segment_no="2" tag_type="list">Action random variable at t</text>
<text top="165" left="85" width="5" height="9" font="font7" id="p41_t5" reading_order_no="4" segment_no="3" tag_type="list">a</text>
<text top="165" left="180" width="199" height="9" font="font3" id="p41_t6" reading_order_no="5" segment_no="3" tag_type="list">A realization of the random variable action A</text>
<text top="180" left="85" width="42" height="10" font="font14" id="p41_t7" reading_order_no="6" segment_no="4" tag_type="list">E P [ X ( t )]</text>
<text top="180" left="180" width="197" height="9" font="font3" id="p41_t8" reading_order_no="7" segment_no="4" tag_type="list">Expectation of X ( t ) under the distribution P</text>
<text top="195" left="85" width="61" height="10" font="font14" id="p41_t9" reading_order_no="8" segment_no="5" tag_type="list">P X [ X ( t ) = x ]</text>
<text top="195" left="180" width="310" height="9" font="font3" id="p41_t10" reading_order_no="9" segment_no="5" tag_type="list">Probability measure of the event X ( t ) = x , under the distribution of X</text>
<text top="208" left="85" width="83" height="13" font="font7" id="p41_t11" reading_order_no="10" segment_no="6" tag_type="list">P X ( t +1) | X ( t ) ( x 0 | x )</text>
<text top="210" left="180" width="271" height="9" font="font3" id="p41_t12" reading_order_no="11" segment_no="6" tag_type="list">Conditional probability measure of X at t + 1 knowing X at t</text>
<text top="224" left="85" width="31" height="10" font="font7" id="p41_t13" reading_order_no="12" segment_no="7" tag_type="list">π ( a | s )</text>
<text top="225" left="180" width="294" height="9" font="font3" id="p41_t14" reading_order_no="13" segment_no="7" tag_type="list">Policy π , i.e. probability to choose action a , while observing state s</text>
<text top="238" left="85" width="10" height="11" font="font7" id="p41_t15" reading_order_no="14" segment_no="8" tag_type="list">π ∗</text>
<text top="240" left="180" width="65" height="9" font="font3" id="p41_t16" reading_order_no="15" segment_no="8" tag_type="list">Optimal policy</text>
<text top="255" left="85" width="32" height="9" font="font7" id="p41_t17" reading_order_no="16" segment_no="9" tag_type="list">q π ( a, s )</text>
<text top="255" left="180" width="312" height="9" font="font3" id="p41_t18" reading_order_no="17" segment_no="9" tag_type="list">Action-state value of the pair ( a, s ) and following hereafter the policy π</text>
<text top="270" left="85" width="32" height="10" font="font7" id="p41_t19" reading_order_no="18" segment_no="10" tag_type="list">q ∗ ( a, s )</text>
<text top="268" left="180" width="307" height="11" font="font3" id="p41_t20" reading_order_no="19" segment_no="10" tag_type="list">Optimal action-state value of the pair ( a, s ) and following hereafter π ∗</text>
<text top="285" left="85" width="19" height="9" font="font7" id="p41_t21" reading_order_no="20" segment_no="11" tag_type="list">R ( t )</text>
<text top="285" left="180" width="126" height="9" font="font3" id="p41_t22" reading_order_no="21" segment_no="11" tag_type="list">Reward random variable at t</text>
<text top="300" left="85" width="4" height="9" font="font7" id="p41_t23" reading_order_no="22" segment_no="12" tag_type="list">r</text>
<text top="300" left="180" width="203" height="9" font="font3" id="p41_t24" reading_order_no="23" segment_no="12" tag_type="list">A realization of the random variable reward R</text>
<text top="315" left="85" width="19" height="9" font="font7" id="p41_t25" reading_order_no="24" segment_no="13" tag_type="list">S ( t )</text>
<text top="315" left="180" width="116" height="9" font="font3" id="p41_t26" reading_order_no="25" segment_no="13" tag_type="list">State random variable at t</text>
<text top="330" left="85" width="5" height="9" font="font7" id="p41_t27" reading_order_no="26" segment_no="14" tag_type="list">s</text>
<text top="330" left="180" width="192" height="9" font="font3" id="p41_t28" reading_order_no="27" segment_no="14" tag_type="list">A realization of the random variable state S</text>
<text top="345" left="85" width="23" height="9" font="font7" id="p41_t29" reading_order_no="28" segment_no="15" tag_type="list">v π ( s )</text>
<text top="345" left="180" width="208" height="9" font="font3" id="p41_t30" reading_order_no="29" segment_no="15" tag_type="list">Value function of the state s under the policy π</text>
<text top="359" left="85" width="22" height="11" font="font7" id="p41_t31" reading_order_no="30" segment_no="16" tag_type="list">v ∗ ( s )</text>
<text top="358" left="180" width="203" height="10" font="font3" id="p41_t32" reading_order_no="31" segment_no="16" tag_type="list">Optimal value function of the state s under π ∗</text>
<text top="386" left="85" width="15" height="12" font="font3" id="p41_t33" reading_order_no="32" segment_no="17" tag_type="list">[ · ] +</text>
<text top="389" left="180" width="40" height="9" font="font3" id="p41_t34" reading_order_no="33" segment_no="17" tag_type="list">max ( · , 0)</text>
<text top="404" left="85" width="23" height="10" font="font12" id="p41_t35" reading_order_no="34" segment_no="18" tag_type="list">1 { a }</text>
<text top="404" left="180" width="250" height="9" font="font3" id="p41_t36" reading_order_no="35" segment_no="18" tag_type="list">Indicator function equals to 1 if a is true and 0 otherwise</text>
<text top="419" left="85" width="11" height="10" font="font6" id="p41_t37" reading_order_no="36" segment_no="19" tag_type="list">1 n</text>
<text top="419" left="180" width="166" height="9" font="font3" id="p41_t38" reading_order_no="37" segment_no="19" tag_type="list">Column vector full of ones of length n</text>
<text top="449" left="85" width="8" height="9" font="font11" id="p41_t39" reading_order_no="38" segment_no="20" tag_type="list">A</text>
<text top="449" left="180" width="96" height="9" font="font3" id="p41_t40" reading_order_no="39" segment_no="20" tag_type="list">Set of possible actions</text>
<text top="466" left="85" width="7" height="8" font="font14" id="p41_t41" reading_order_no="40" segment_no="21" tag_type="list">N</text>
<text top="464" left="180" width="99" height="9" font="font3" id="p41_t42" reading_order_no="41" segment_no="21" tag_type="list">Set of positive integers</text>
<text top="478" left="85" width="8" height="9" font="font11" id="p41_t43" reading_order_no="42" segment_no="22" tag_type="list">R</text>
<text top="479" left="180" width="100" height="9" font="font3" id="p41_t44" reading_order_no="43" segment_no="22" tag_type="list">Set of possible rewards</text>
<text top="496" left="85" width="7" height="8" font="font14" id="p41_t45" reading_order_no="44" segment_no="23" tag_type="list">R</text>
<text top="494" left="180" width="85" height="9" font="font3" id="p41_t46" reading_order_no="45" segment_no="23" tag_type="list">Set of real numbers</text>
<text top="511" left="85" width="14" height="8" font="font14" id="p41_t47" reading_order_no="46" segment_no="24" tag_type="list">R +</text>
<text top="509" left="180" width="141" height="9" font="font3" id="p41_t48" reading_order_no="47" segment_no="24" tag_type="list">Set of real and positive numbers</text>
<text top="523" left="85" width="6" height="9" font="font11" id="p41_t49" reading_order_no="48" segment_no="25" tag_type="list">S</text>
<text top="524" left="180" width="91" height="9" font="font3" id="p41_t50" reading_order_no="49" segment_no="25" tag_type="list">Set of possible states</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p41_t51" reading_order_no="50" segment_no="26" tag_type="text">41</text>
</page>
<page number="42" position="absolute" top="0" left="0" height="792" width="612">
<text top="182" left="79" width="63" height="13" font="font0" id="p42_t1" reading_order_no="0" segment_no="0" tag_type="title">Contents</text>
<text top="225" left="455" width="72" height="9" font="font2" id="p42_t2" reading_order_no="1" segment_no="1" tag_type="text">Philippe Mary</text>
<text top="237" left="452" width="75" height="9" font="font2" id="p42_t3" reading_order_no="2" segment_no="1" tag_type="text">Visa Koivunen</text>
<text top="249" left="446" width="81" height="9" font="font2" id="p42_t4" reading_order_no="3" segment_no="1" tag_type="text">Christophe Moy</text>
<text top="310" left="79" width="6" height="9" font="font6" id="p42_t5" reading_order_no="4" segment_no="2" tag_type="list">9</text>
<text top="310" left="94" width="175" height="9" font="font6" id="p42_t6" reading_order_no="5" segment_no="2" tag_type="list">RL for PHY layer communications</text>
<text top="310" left="527" width="6" height="9" font="font6" id="p42_t7" reading_order_no="6" segment_no="2" tag_type="list">1<a href="deeplearning_paper3.html#1">1</a></text>
<text top="322" left="94" width="13" height="9" font="font3" id="p42_t8" reading_order_no="7" segment_no="3" tag_type="list">9.1</text>
<text top="322" left="117" width="394" height="9" font="font3" id="p42_t9" reading_order_no="8" segment_no="3" tag_type="list">Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="322" left="528" width="5" height="9" font="font3" id="p42_t10" reading_order_no="9" segment_no="3" tag_type="list">1<a href="deeplearning_paper3.html#1">1</a></text>
<text top="334" left="94" width="13" height="9" font="font3" id="p42_t11" reading_order_no="10" segment_no="4" tag_type="list">9.2</text>
<text top="334" left="117" width="394" height="9" font="font3" id="p42_t12" reading_order_no="11" segment_no="4" tag_type="list">Reinforcement Learning: background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="334" left="528" width="5" height="9" font="font3" id="p42_t13" reading_order_no="12" segment_no="4" tag_type="list">4<a href="deeplearning_paper3.html#4">4</a></text>
<text top="346" left="117" width="20" height="9" font="font3" id="p42_t14" reading_order_no="13" segment_no="5" tag_type="list">9.2.1</text>
<text top="346" left="149" width="362" height="9" font="font3" id="p42_t15" reading_order_no="14" segment_no="5" tag_type="list">Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="346" left="528" width="5" height="9" font="font3" id="p42_t16" reading_order_no="15" segment_no="5" tag_type="list">4<a href="deeplearning_paper3.html#4">4</a></text>
<text top="358" left="117" width="20" height="9" font="font3" id="p42_t17" reading_order_no="16" segment_no="6" tag_type="list">9.2.2</text>
<text top="358" left="149" width="109" height="9" font="font3" id="p42_t18" reading_order_no="17" segment_no="6" tag_type="list">Markov Decision Process</text>
<text top="358" left="268" width="243" height="9" font="font3" id="p42_t19" reading_order_no="18" segment_no="6" tag_type="list">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="358" left="528" width="5" height="9" font="font3" id="p42_t20" reading_order_no="19" segment_no="6" tag_type="list">4<a href="deeplearning_paper3.html#4">4</a></text>
<text top="370" left="117" width="20" height="9" font="font3" id="p42_t21" reading_order_no="20" segment_no="7" tag_type="list">9.2.3</text>
<text top="370" left="149" width="195" height="9" font="font3" id="p42_t22" reading_order_no="21" segment_no="7" tag_type="list">Partially observable Markov decision process</text>
<text top="370" left="353" width="158" height="9" font="font3" id="p42_t23" reading_order_no="22" segment_no="7" tag_type="list">. . . . . . . . . . . . . . . . . . . . .</text>
<text top="370" left="528" width="5" height="9" font="font3" id="p42_t24" reading_order_no="23" segment_no="7" tag_type="list">8<a href="deeplearning_paper3.html#8">8</a></text>
<text top="382" left="117" width="20" height="9" font="font3" id="p42_t25" reading_order_no="24" segment_no="8" tag_type="list">9.2.4</text>
<text top="382" left="149" width="147" height="9" font="font3" id="p42_t26" reading_order_no="25" segment_no="8" tag_type="list">Q-learning and SARSA algorithm</text>
<text top="382" left="307" width="204" height="9" font="font3" id="p42_t27" reading_order_no="26" segment_no="8" tag_type="list">. . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="382" left="523" width="10" height="9" font="font3" id="p42_t28" reading_order_no="27" segment_no="8" tag_type="list">11<a href="deeplearning_paper3.html#11">11</a></text>
<text top="394" left="117" width="20" height="9" font="font3" id="p42_t29" reading_order_no="28" segment_no="9" tag_type="list">9.2.5</text>
<text top="394" left="149" width="39" height="9" font="font3" id="p42_t30" reading_order_no="29" segment_no="9" tag_type="list">Deep RL</text>
<text top="394" left="198" width="313" height="9" font="font3" id="p42_t31" reading_order_no="30" segment_no="9" tag_type="list">. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="394" left="523" width="10" height="9" font="font3" id="p42_t32" reading_order_no="31" segment_no="9" tag_type="list">12<a href="deeplearning_paper3.html#12">12</a></text>
<text top="406" left="117" width="20" height="9" font="font3" id="p42_t33" reading_order_no="32" segment_no="10" tag_type="list">9.2.6</text>
<text top="406" left="149" width="362" height="9" font="font3" id="p42_t34" reading_order_no="33" segment_no="10" tag_type="list">Multi-armed bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="406" left="523" width="10" height="9" font="font3" id="p42_t35" reading_order_no="34" segment_no="10" tag_type="list">15<a href="deeplearning_paper3.html#15">15</a></text>
<text top="418" left="117" width="20" height="9" font="font3" id="p42_t36" reading_order_no="35" segment_no="11" tag_type="list">9.2.7</text>
<text top="418" left="149" width="362" height="9" font="font3" id="p42_t37" reading_order_no="36" segment_no="11" tag_type="list">Markovian MAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="418" left="523" width="10" height="9" font="font3" id="p42_t38" reading_order_no="37" segment_no="11" tag_type="list">17<a href="deeplearning_paper3.html#17">17</a></text>
<text top="430" left="94" width="13" height="9" font="font3" id="p42_t39" reading_order_no="38" segment_no="12" tag_type="list">9.3</text>
<text top="430" left="117" width="394" height="9" font="font3" id="p42_t40" reading_order_no="39" segment_no="12" tag_type="list">RL at PHY layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="430" left="523" width="10" height="9" font="font3" id="p42_t41" reading_order_no="40" segment_no="12" tag_type="list">20<a href="deeplearning_paper3.html#20">20</a></text>
<text top="442" left="117" width="20" height="9" font="font3" id="p42_t42" reading_order_no="41" segment_no="13" tag_type="list">9.3.1</text>
<text top="442" left="149" width="362" height="9" font="font3" id="p42_t43" reading_order_no="42" segment_no="13" tag_type="list">Example with Q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="442" left="523" width="10" height="9" font="font3" id="p42_t44" reading_order_no="43" segment_no="13" tag_type="list">20<a href="deeplearning_paper3.html#20">20</a></text>
<text top="454" left="117" width="20" height="9" font="font3" id="p42_t45" reading_order_no="44" segment_no="14" tag_type="list">9.3.2</text>
<text top="454" left="149" width="362" height="9" font="font3" id="p42_t46" reading_order_no="45" segment_no="14" tag_type="list">Example with Deep-RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="454" left="523" width="10" height="9" font="font3" id="p42_t47" reading_order_no="46" segment_no="14" tag_type="list">22<a href="deeplearning_paper3.html#22">22</a></text>
<text top="466" left="117" width="20" height="9" font="font3" id="p42_t48" reading_order_no="47" segment_no="15" tag_type="list">9.3.3</text>
<text top="466" left="149" width="362" height="9" font="font3" id="p42_t49" reading_order_no="48" segment_no="15" tag_type="list">Examples with MAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="466" left="523" width="10" height="9" font="font3" id="p42_t50" reading_order_no="49" segment_no="15" tag_type="list">25<a href="deeplearning_paper3.html#25">25</a></text>
<text top="478" left="117" width="20" height="9" font="font3" id="p42_t51" reading_order_no="50" segment_no="16" tag_type="list">9.3.4</text>
<text top="478" left="149" width="362" height="9" font="font3" id="p42_t52" reading_order_no="51" segment_no="16" tag_type="list">Real world examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="478" left="523" width="10" height="9" font="font3" id="p42_t53" reading_order_no="52" segment_no="16" tag_type="list">29<a href="deeplearning_paper3.html#29">29</a></text>
<text top="490" left="94" width="13" height="9" font="font3" id="p42_t54" reading_order_no="53" segment_no="17" tag_type="list">9.4</text>
<text top="490" left="117" width="394" height="9" font="font3" id="p42_t55" reading_order_no="54" segment_no="17" tag_type="list">Conclusions and Future Trends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="490" left="523" width="10" height="9" font="font3" id="p42_t56" reading_order_no="55" segment_no="17" tag_type="list">37<a href="deeplearning_paper3.html#37">37</a></text>
<text top="502" left="94" width="13" height="9" font="font3" id="p42_t57" reading_order_no="56" segment_no="18" tag_type="list">9.5</text>
<text top="502" left="117" width="394" height="9" font="font3" id="p42_t58" reading_order_no="57" segment_no="18" tag_type="list">Bibliographical remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="502" left="523" width="10" height="9" font="font3" id="p42_t59" reading_order_no="58" segment_no="18" tag_type="list">38<a href="deeplearning_paper3.html#38">38</a></text>
<text top="514" left="94" width="416" height="9" font="font3" id="p42_t60" reading_order_no="59" segment_no="19" tag_type="list">Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="514" left="523" width="10" height="9" font="font3" id="p42_t61" reading_order_no="60" segment_no="19" tag_type="list">40<a href="deeplearning_paper3.html#38">40</a></text>
<text top="526" left="94" width="416" height="9" font="font3" id="p42_t62" reading_order_no="61" segment_no="20" tag_type="list">Notation and symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</text>
<text top="526" left="523" width="10" height="9" font="font3" id="p42_t63" reading_order_no="62" segment_no="20" tag_type="list">41<a href="deeplearning_paper3.html#38">41</a></text>
<text top="756" left="301" width="10" height="9" font="font3" id="p42_t64" reading_order_no="63" segment_no="21" tag_type="text">42</text>
</page>
<page number="43" position="absolute" top="0" left="0" height="792" width="612">
<text top="183" left="79" width="90" height="13" font="font0" id="p43_t1" reading_order_no="0" segment_no="0" tag_type="title">Bibliography</text>
<text top="226" left="455" width="72" height="9" font="font2" id="p43_t2" reading_order_no="1" segment_no="1" tag_type="text">Philippe Mary</text>
<text top="238" left="452" width="75" height="9" font="font2" id="p43_t3" reading_order_no="2" segment_no="1" tag_type="text">Visa Koivunen</text>
<text top="250" left="446" width="81" height="9" font="font2" id="p43_t4" reading_order_no="3" segment_no="1" tag_type="text">Christophe Moy</text>
<text top="303" left="84" width="269" height="9" font="font3" id="p43_t5" reading_order_no="4" segment_no="2" tag_type="text">[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning .</text>
<text top="303" left="362" width="171" height="9" font="font3" id="p43_t6" reading_order_no="5" segment_no="2" tag_type="text">Cambridge, MA, USA: The MIT Press,</text>
<text top="315" left="100" width="23" height="9" font="font3" id="p43_t7" reading_order_no="6" segment_no="2" tag_type="text">2016.</text>
<text top="337" left="84" width="448" height="9" font="font3" id="p43_t8" reading_order_no="7" segment_no="3" tag_type="text">[2] A. Zappone, M. Di Renzo, and M. Debbah, “Wireless networks design in the era of deep learning:</text>
<text top="349" left="100" width="150" height="9" font="font3" id="p43_t9" reading_order_no="8" segment_no="3" tag_type="text">Model-based, ai-based, or both?”</text>
<text top="349" left="259" width="274" height="9" font="font5" id="p43_t10" reading_order_no="9" segment_no="3" tag_type="text">IEEE Transactions on Communications , vol. 67, no. 10, pp.</text>
<text top="361" left="100" width="74" height="9" font="font3" id="p43_t11" reading_order_no="10" segment_no="3" tag_type="text">7331–7376, 2019.</text>
<text top="383" left="84" width="448" height="9" font="font3" id="p43_t12" reading_order_no="11" segment_no="4" tag_type="text">[3] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artificial neural networks-based machine</text>
<text top="395" left="100" width="432" height="9" font="font3" id="p43_t13" reading_order_no="12" segment_no="4" tag_type="text">learning for wireless networks: A tutorial,” IEEE Communications Surveys Tutorials , vol. 21, no. 4,</text>
<text top="407" left="100" width="91" height="9" font="font3" id="p43_t14" reading_order_no="13" segment_no="4" tag_type="text">pp. 3039–3071, 2019.</text>
<text top="430" left="84" width="448" height="9" font="font3" id="p43_t15" reading_order_no="14" segment_no="5" tag_type="text">[4] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y. Liang, and D. I. Kim, “Applications of</text>
<text top="442" left="100" width="432" height="9" font="font3" id="p43_t16" reading_order_no="15" segment_no="5" tag_type="text">deep reinforcement learning in communications and networking: A survey,” IEEE Communications</text>
<text top="453" left="100" width="235" height="9" font="font5" id="p43_t17" reading_order_no="16" segment_no="5" tag_type="text">Surveys Tutorials , vol. 21, no. 4, pp. 3133–3174, 2019.</text>
<text top="476" left="84" width="367" height="9" font="font3" id="p43_t18" reading_order_no="17" segment_no="6" tag_type="text">[5] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction , 2nd ed.</text>
<text top="476" left="460" width="73" height="9" font="font3" id="p43_t19" reading_order_no="18" segment_no="6" tag_type="text">Cambridge, MA,</text>
<text top="488" left="100" width="102" height="9" font="font3" id="p43_t20" reading_order_no="19" segment_no="6" tag_type="text">USA: MIT Press, 2018.</text>
<text top="510" left="84" width="243" height="9" font="font3" id="p43_t21" reading_order_no="20" segment_no="7" tag_type="text">[6] C. Szepsvari, Algorithms for reinforcement learning .</text>
<text top="510" left="336" width="124" height="9" font="font3" id="p43_t22" reading_order_no="21" segment_no="7" tag_type="text">Morgan and Claypool, 2010.</text>
<text top="532" left="84" width="186" height="9" font="font3" id="p43_t23" reading_order_no="22" segment_no="8" tag_type="text">[7] R. E. Bellman, Dynamic programming .</text>
<text top="532" left="279" width="145" height="9" font="font3" id="p43_t24" reading_order_no="23" segment_no="8" tag_type="text">Princeton University Press, 1957.</text>
<text top="555" left="84" width="448" height="9" font="font3" id="p43_t25" reading_order_no="24" segment_no="9" tag_type="text">[8] R. D. Smallwood and E. J. Sondik, “The Optimal Control of Partially Observable Markov Processes</text>
<text top="567" left="100" width="351" height="9" font="font3" id="p43_t26" reading_order_no="25" segment_no="9" tag_type="text">over a Finite Horizon,” Operations Research , vol. 21, no. 5, pp. 1071–1088, 1973.</text>
<text top="589" left="84" width="398" height="9" font="font3" id="p43_t27" reading_order_no="26" segment_no="10" tag_type="text">[9] C. Watkins, “Learning from delayed rewards,” Ph.D. dissertation, King’s College, 1989.</text>
<text top="611" left="79" width="454" height="9" font="font3" id="p43_t28" reading_order_no="27" segment_no="11" tag_type="text">[10] C. Szepsvari, “Learning and exploitation do not conflict under minimax optimality,” in In Proc. of</text>
<text top="623" left="100" width="335" height="9" font="font5" id="p43_t29" reading_order_no="28" segment_no="11" tag_type="text">the 9th European Conference on Machine Learning , April 1997, pp. 242–249.</text>
<text top="646" left="79" width="454" height="9" font="font3" id="p43_t30" reading_order_no="29" segment_no="12" tag_type="text">[11] E. Even-Dar and Y. Mansour, “Learning rates for q-learning,” Journal of Machine Learning Re-</text>
<text top="658" left="100" width="131" height="9" font="font5" id="p43_t31" reading_order_no="30" segment_no="12" tag_type="text">search , vol. 5, December 2004.</text>
<text top="680" left="79" width="453" height="9" font="font3" id="p43_t32" reading_order_no="31" segment_no="13" tag_type="text">[12] G. A. Rummery and M. Niranjan, “On-line Q-learning using connectionist systems,” Cambridge</text>
<text top="692" left="100" width="411" height="9" font="font3" id="p43_t33" reading_order_no="32" segment_no="13" tag_type="text">University Engineering Department, Tech. Rep. CUED/F-INFENG/TR 166, September 1994.</text>
<text top="714" left="79" width="284" height="9" font="font3" id="p43_t34" reading_order_no="33" segment_no="14" tag_type="text">[13] L. Bottou, On-line Learning and Stochastic Approximations .</text>
<text top="714" left="371" width="161" height="9" font="font3" id="p43_t35" reading_order_no="34" segment_no="14" tag_type="text">Cambridge University Press, 1999, p.</text>
<text top="726" left="100" width="23" height="9" font="font3" id="p43_t36" reading_order_no="35" segment_no="14" tag_type="text">9–42.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p43_t37" reading_order_no="36" segment_no="15" tag_type="text">43</text>
</page>
<page number="44" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="453" width="80" height="9" font="font3" id="p44_t1" reading_order_no="0" segment_no="0" tag_type="title">BIBLIOGRAPHY</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p44_t2" reading_order_no="1" segment_no="1" tag_type="text">[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-</text>
<text top="122" left="100" width="433" height="9" font="font3" id="p44_t3" reading_order_no="2" segment_no="1" tag_type="text">miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,</text>
<text top="134" left="100" width="433" height="9" font="font3" id="p44_t4" reading_order_no="3" segment_no="1" tag_type="text">D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through deep reinforce-</text>
<text top="146" left="100" width="269" height="9" font="font3" id="p44_t5" reading_order_no="4" segment_no="1" tag_type="text">ment learning,” Nature , vol. 518, no. 7540, pp. 529–533, 2015.</text>
<text top="167" left="79" width="453" height="9" font="font3" id="p44_t6" reading_order_no="5" segment_no="2" tag_type="text">[15] H. V. Hasselt, “Double q-learning,” in Advances in Neural Information Processing Systems 23 , J. D.</text>
<text top="179" left="100" width="342" height="9" font="font3" id="p44_t7" reading_order_no="6" segment_no="2" tag_type="text">Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, Eds.</text>
<text top="179" left="451" width="82" height="9" font="font3" id="p44_t8" reading_order_no="7" segment_no="2" tag_type="text">Curran Associates,</text>
<text top="191" left="100" width="433" height="9" font="font3" id="p44_t9" reading_order_no="8" segment_no="2" tag_type="text">Inc., 2010, pp. 2613–2621. [Online]. Available: http://papers.nips.cc/paper/3964-double-q-learning.</text>
<text top="203" left="100" width="14" height="9" font="font3" id="p44_t10" reading_order_no="9" segment_no="2" tag_type="text">pdf</text>
<text top="224" left="79" width="453" height="9" font="font3" id="p44_t11" reading_order_no="10" segment_no="3" tag_type="text">[16] H. v. Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double q-learning,” in</text>
<text top="236" left="100" width="392" height="9" font="font5" id="p44_t12" reading_order_no="11" segment_no="3" tag_type="text">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence , ser. AAAI’16.</text>
<text top="236" left="507" width="26" height="9" font="font3" id="p44_t13" reading_order_no="12" segment_no="3" tag_type="text">AAAI<a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf">http://papers.nips.cc/paper/3964-double-q-learning.</a></text>
<text top="248" left="100" width="114" height="9" font="font3" id="p44_t14" reading_order_no="13" segment_no="3" tag_type="text">Press, 2016, p. 2094–2100.<a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf">pdf</a></text>
<text top="269" left="79" width="453" height="9" font="font3" id="p44_t15" reading_order_no="14" segment_no="4" tag_type="text">[17] W. R. Thompson, “On the likelihood that one unknown probability exceeds another in view of the</text>
<text top="281" left="100" width="369" height="9" font="font3" id="p44_t16" reading_order_no="15" segment_no="4" tag_type="text">evidence of two samples,” Biometrika , vol. 25, no. 3/4, pp. 285–294, December 1933.</text>
<text top="302" left="79" width="454" height="9" font="font3" id="p44_t17" reading_order_no="16" segment_no="5" tag_type="text">[18] H. Robbins, “Some aspects of the sequential design of experiments,” Bull. Amer. Math. Soc. , vol. 58,</text>
<text top="314" left="100" width="428" height="9" font="font3" id="p44_t18" reading_order_no="17" segment_no="5" tag_type="text">no. 5, pp. 527–535, 09 1952. [Online]. Available: http://projecteuclid.org/euclid.bams/1183517370</text>
<text top="335" left="79" width="454" height="9" font="font3" id="p44_t19" reading_order_no="18" segment_no="6" tag_type="text">[19] T. L. Lai and H. Robbins, “Asymptotically efficient adaptive allocation rules,” Advances in Applied</text>
<text top="347" left="100" width="185" height="9" font="font5" id="p44_t20" reading_order_no="19" segment_no="6" tag_type="text">Mathematics , vol. 6, no. 1, pp. 4–22, 1985.</text>
<text top="368" left="79" width="454" height="9" font="font3" id="p44_t21" reading_order_no="20" segment_no="7" tag_type="text">[20] E. Kaufmann, “Analyse de strat´ egies bay´ esiennes et fr´ equentistes pour l’allocation s´ equentielle de</text>
<text top="380" left="100" width="250" height="9" font="font3" id="p44_t22" reading_order_no="21" segment_no="7" tag_type="text">ressources,” Ph.D. dissertation, Telecom ParisTech, 2014.</text>
<text top="402" left="79" width="453" height="9" font="font3" id="p44_t23" reading_order_no="22" segment_no="8" tag_type="text">[21] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the multiarmed bandit</text>
<text top="414" left="100" width="433" height="9" font="font3" id="p44_t24" reading_order_no="23" segment_no="8" tag_type="text">problem,” Machine Learning , vol. 47, no. 2-3, pp. 235–256, May 2002. [Online]. Available:</text>
<text top="425" left="100" width="192" height="9" font="font3" id="p44_t25" reading_order_no="24" segment_no="8" tag_type="text">http://dx.doi.org/10.1023/A:1013689704352</text>
<text top="447" left="79" width="454" height="9" font="font3" id="p44_t26" reading_order_no="25" segment_no="9" tag_type="text">[22] O. Capp´ e, A. Garivier, O.-A. Maillard, R. Munos, and G. Stoltz, “Kullback-Leibler Upper</text>
<text top="459" left="100" width="433" height="9" font="font3" id="p44_t27" reading_order_no="26" segment_no="9" tag_type="text">Confidence Bounds for Optimal Sequential Allocation,” Annals of Statistics , vol. 41, no. 3,<a href="http://projecteuclid.org/euclid.bams/1183517370">http://projecteuclid.org/euclid.bams/1183517370</a></text>
<text top="471" left="100" width="70" height="9" font="font3" id="p44_t28" reading_order_no="27" segment_no="9" tag_type="text">pp. 1516–1541,</text>
<text top="471" left="180" width="23" height="9" font="font3" id="p44_t29" reading_order_no="28" segment_no="9" tag_type="text">2013,</text>
<text top="471" left="213" width="40" height="9" font="font3" id="p44_t30" reading_order_no="29" segment_no="9" tag_type="text">accepted,</text>
<text top="471" left="263" width="269" height="9" font="font3" id="p44_t31" reading_order_no="30" segment_no="9" tag_type="text">to appear in Annals of Statistics. [Online]. Available:</text>
<text top="482" left="100" width="197" height="9" font="font3" id="p44_t32" reading_order_no="31" segment_no="9" tag_type="text">https://hal.archives-ouvertes.fr/hal-00738209</text>
<text top="504" left="79" width="453" height="9" font="font3" id="p44_t33" reading_order_no="32" segment_no="10" tag_type="text">[23] J. Oksanen and V. Koivunen, “An order optimal policy for exploiting idle spectrum in cognitive</text>
<text top="516" left="100" width="432" height="9" font="font3" id="p44_t34" reading_order_no="33" segment_no="10" tag_type="text">radio networks,” IEEE Transactions on Signal Processing , vol. 63, no. 5, pp. 1214–1227, March</text>
<text top="528" left="100" width="23" height="9" font="font3" id="p44_t35" reading_order_no="34" segment_no="10" tag_type="text">2015.</text>
<text top="549" left="79" width="453" height="9" font="font3" id="p44_t36" reading_order_no="35" segment_no="11" tag_type="text">[24] V. Anantharam, P. Varaiya, and J. Walrand, “Asymptotically efficient allocation rules for the mul-</text>
<text top="561" left="100" width="432" height="9" font="font3" id="p44_t37" reading_order_no="36" segment_no="11" tag_type="text">tiarmed bandit problem with multiple plays-part ii: Markovian rewards,” Automatic Control, IEEE</text>
<text top="573" left="100" width="245" height="9" font="font5" id="p44_t38" reading_order_no="37" segment_no="11" tag_type="text">Transactions on , vol. 32, no. 11, pp. 977–982, Nov 1987.</text>
<text top="594" left="79" width="454" height="9" font="font3" id="p44_t39" reading_order_no="38" segment_no="12" tag_type="text">[25] C. Tekin and M. Liu, “Online learning of rested and restless bandits,” IEEE Transactions on Infor-</text>
<text top="606" left="100" width="245" height="9" font="font5" id="p44_t40" reading_order_no="39" segment_no="12" tag_type="text">mation Theory , vol. 58, no. 8, pp. 5588–5611, Aug 2012.</text>
<text top="627" left="79" width="454" height="9" font="font3" id="p44_t41" reading_order_no="40" segment_no="13" tag_type="text">[26] ——, “Online learning in opportunistic spectrum access: A restless bandit approach,” in INFOCOM,</text>
<text top="639" left="100" width="104" height="9" font="font5" id="p44_t42" reading_order_no="41" segment_no="13" tag_type="text">2011 Proceedings IEEE .<a href="http://dx.doi.org/10.1023/A:1013689704352">http://dx.doi.org/10.1023/A:1013689704352</a></text>
<text top="639" left="214" width="121" height="9" font="font3" id="p44_t43" reading_order_no="42" segment_no="13" tag_type="text">IEEE, 2011, pp. 2462–2470.</text>
<text top="660" left="79" width="243" height="9" font="font3" id="p44_t44" reading_order_no="43" segment_no="14" tag_type="text">[27] H. V. Poor and O. Hadjiliadis, Quickest Detection .</text>
<text top="660" left="331" width="150" height="9" font="font3" id="p44_t45" reading_order_no="44" segment_no="14" tag_type="text">Cambridge University Press, 2008.</text>
<text top="681" left="79" width="453" height="9" font="font3" id="p44_t46" reading_order_no="45" segment_no="15" tag_type="text">[28] E. Nitzan, T. Halme, and V. Koivunen, “Bayesian methods for multiple change-point detection with</text>
<text top="693" left="100" width="422" height="9" font="font3" id="p44_t47" reading_order_no="46" segment_no="15" tag_type="text">reduced communication,” IEEE Transactions on Signal Processing , vol. 68, pp. 4871–4886, 2020.</text>
<text top="714" left="79" width="453" height="9" font="font3" id="p44_t48" reading_order_no="47" segment_no="16" tag_type="text">[29] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire, “The nonstochastic multiarmed bandit</text>
<text top="726" left="100" width="286" height="9" font="font3" id="p44_t49" reading_order_no="48" segment_no="16" tag_type="text">problem,” SIAM J. Comput. , vol. 32, no. 1, pp. 48–77, Jan. 2003.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p44_t50" reading_order_no="49" segment_no="17" tag_type="text">44</text>
</page>
<page number="45" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="80" height="9" font="font3" id="p45_t1" reading_order_no="0" segment_no="0" tag_type="title">BIBLIOGRAPHY</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p45_t2" reading_order_no="1" segment_no="1" tag_type="text">[30] G. Stoltz, “Incomplete information and internal regret in prediction of individual sequences,” Ph.D.</text>
<text top="122" left="100" width="169" height="9" font="font3" id="p45_t3" reading_order_no="2" segment_no="1" tag_type="text">dissertation, Universit´ e Paris XI, 2005.</text>
<text top="141" left="79" width="453" height="9" font="font3" id="p45_t4" reading_order_no="3" segment_no="2" tag_type="text">[31] M. H. Ngo and V. Krishnamurthy, “Monotonicity of constrained optimal transmission policies in</text>
<text top="153" left="100" width="433" height="9" font="font3" id="p45_t5" reading_order_no="4" segment_no="2" tag_type="text">correlated fading channels with arq,” IEEE Transactions on Signal Processing , vol. 58, no. 1, pp.</text>
<text top="165" left="100" width="64" height="9" font="font3" id="p45_t6" reading_order_no="5" segment_no="2" tag_type="text">438–451, 2010.</text>
<text top="184" left="79" width="453" height="9" font="font3" id="p45_t7" reading_order_no="6" segment_no="3" tag_type="text">[32] N. Mastronarde and M. van der Schaar, “Joint physical-layer and system-level power management for</text>
<text top="196" left="100" width="432" height="9" font="font3" id="p45_t8" reading_order_no="7" segment_no="3" tag_type="text">delay-sensitive wireless communications,” IEEE Transactions on Mobile Computing , vol. 12, no. 4,</text>
<text top="208" left="100" width="81" height="9" font="font3" id="p45_t9" reading_order_no="8" segment_no="3" tag_type="text">pp. 694–709, 2013.</text>
<text top="228" left="79" width="454" height="9" font="font3" id="p45_t10" reading_order_no="9" segment_no="4" tag_type="text">[33] Y. Polyanskiy, H. Poor, and S. Verd´ u, “Channel coding rate in the finite blocklength regime,” IEEE</text>
<text top="239" left="100" width="365" height="9" font="font5" id="p45_t11" reading_order_no="10" segment_no="4" tag_type="text">Transactions on Information Theory , vol. 56, no. 5, pp. 2307–2359, December 2010.</text>
<text top="259" left="79" width="453" height="9" font="font3" id="p45_t12" reading_order_no="11" segment_no="5" tag_type="text">[34] D. Anade, J.-M. Gorce, P. Mary, and S. M. Perlaza, “An upper bound on the error induced by</text>
<text top="271" left="100" width="432" height="9" font="font3" id="p45_t13" reading_order_no="12" segment_no="5" tag_type="text">saddlepoint approximations – applications to information theory.” Entropy , vol. 22, no. 6, p. 690,</text>
<text top="283" left="100" width="47" height="9" font="font3" id="p45_t14" reading_order_no="13" segment_no="5" tag_type="text">June 2020.</text>
<text top="302" left="79" width="453" height="9" font="font3" id="p45_t15" reading_order_no="14" segment_no="6" tag_type="text">[35] Munish Goyal, Anurag Kumar, and Vinod Sharma, “Power constrained and delay optimal policies</text>
<text top="314" left="100" width="432" height="9" font="font3" id="p45_t16" reading_order_no="15" segment_no="6" tag_type="text">for scheduling transmission over a fading channel,” in IEEE INFOCOM 2003. Twenty-second Annual</text>
<text top="326" left="100" width="433" height="9" font="font5" id="p45_t17" reading_order_no="16" segment_no="6" tag_type="text">Joint Conference of the IEEE Computer and Communications Societies , vol. 1, 2003, pp. 311–320</text>
<text top="338" left="100" width="23" height="9" font="font3" id="p45_t18" reading_order_no="17" segment_no="6" tag_type="text">vol.1.</text>
<text top="357" left="79" width="453" height="9" font="font3" id="p45_t19" reading_order_no="18" segment_no="7" tag_type="text">[36] N. Salodkar, A. Bhorkar, A. Karandikar, and V. S. Borkar, “An on-line learning algorithm for energy</text>
<text top="369" left="100" width="433" height="9" font="font3" id="p45_t20" reading_order_no="19" segment_no="7" tag_type="text">efficient delay constrained scheduling over a fading channel,” IEEE Journal on Selected Areas in</text>
<text top="381" left="100" width="222" height="9" font="font5" id="p45_t21" reading_order_no="20" segment_no="7" tag_type="text">Communications , vol. 26, no. 4, pp. 732–742, 2008.</text>
<text top="400" left="79" width="453" height="9" font="font3" id="p45_t22" reading_order_no="21" segment_no="8" tag_type="text">[37] A. Sadeghi, F. Sheikholeslami, and G. B. Giannakis, “Optimal and scalable caching for 5g using rein-</text>
<text top="412" left="100" width="433" height="9" font="font3" id="p45_t23" reading_order_no="22" segment_no="8" tag_type="text">forcement learning of space-time popularities,” IEEE Journal of Selected Topics in Signal Processing ,</text>
<text top="424" left="100" width="143" height="9" font="font3" id="p45_t24" reading_order_no="23" segment_no="8" tag_type="text">vol. 12, no. 1, pp. 180–190, 2018.</text>
<text top="443" left="79" width="454" height="9" font="font3" id="p45_t25" reading_order_no="24" segment_no="9" tag_type="text">[38] M. Maaz, P. Mary, and M. H´ elard, “Energy minimization in harq-i relay-assisted networks with</text>
<text top="455" left="100" width="433" height="9" font="font3" id="p45_t26" reading_order_no="25" segment_no="9" tag_type="text">delay-limited users,” IEEE Transactions on Vehicular Technology , vol. 66, no. 8, pp. 6887–6898,</text>
<text top="467" left="100" width="23" height="9" font="font3" id="p45_t27" reading_order_no="26" segment_no="9" tag_type="text">2017.</text>
<text top="487" left="79" width="453" height="9" font="font3" id="p45_t28" reading_order_no="27" segment_no="10" tag_type="text">[39] D. S. Wing Hui, V. K. Nang Lau, and W. H. Lam, “Cross-layer design for ofdma wireless systems</text>
<text top="498" left="100" width="433" height="9" font="font3" id="p45_t29" reading_order_no="28" segment_no="10" tag_type="text">with heterogeneous delay requirements,” IEEE Transactions on Wireless Communications , vol. 6,</text>
<text top="510" left="100" width="118" height="9" font="font3" id="p45_t30" reading_order_no="29" segment_no="10" tag_type="text">no. 8, pp. 2872–2880, 2007.</text>
<text top="530" left="79" width="453" height="9" font="font3" id="p45_t31" reading_order_no="30" segment_no="11" tag_type="text">[40] E. Axell, G. Leus, E. G. Larsson, and H. V. Poor, “Spectrum sensing for cognitive radio : State-of-</text>
<text top="542" left="100" width="433" height="9" font="font3" id="p45_t32" reading_order_no="31" segment_no="11" tag_type="text">the-art and recent advances,” IEEE Signal Processing Magazine , vol. 29, no. 3, pp. 101–116, May</text>
<text top="554" left="100" width="23" height="9" font="font3" id="p45_t33" reading_order_no="32" segment_no="11" tag_type="text">2012.</text>
<text top="573" left="79" width="453" height="9" font="font3" id="p45_t34" reading_order_no="33" segment_no="12" tag_type="text">[41] J. Oksanen, V. Koivunen, and H. V. Poor, “A sensing policy based on confidence bounds and a rest-</text>
<text top="585" left="100" width="433" height="9" font="font3" id="p45_t35" reading_order_no="34" segment_no="12" tag_type="text">less multi-armed bandit model,” in 2012 Conference Record of the Forty Sixth Asilomar Conference</text>
<text top="597" left="100" width="309" height="9" font="font5" id="p45_t36" reading_order_no="35" segment_no="12" tag_type="text">on Signals, Systems and Computers (ASILOMAR) , 2012, pp. 318–323.</text>
<text top="616" left="79" width="453" height="9" font="font3" id="p45_t37" reading_order_no="36" segment_no="13" tag_type="text">[42] N. Modi, P. Mary, and C. Moy, “Qos driven channel selection algorithm for cognitive radio network:</text>
<text top="628" left="100" width="432" height="9" font="font3" id="p45_t38" reading_order_no="37" segment_no="13" tag_type="text">Multi-user multi-armed bandit approach,” IEEE Transactions on Cognitive Communications and</text>
<text top="640" left="100" width="205" height="9" font="font5" id="p45_t39" reading_order_no="38" segment_no="13" tag_type="text">Networking , vol. 3, no. 1, pp. 49–66, Mar 2017.</text>
<text top="659" left="79" width="453" height="9" font="font3" id="p45_t40" reading_order_no="39" segment_no="14" tag_type="text">[43] A. Anandkumar, N. Michael, A. K. Tang, and A. Swami, “Distributed algorithms for learning and</text>
<text top="671" left="100" width="432" height="9" font="font3" id="p45_t41" reading_order_no="40" segment_no="14" tag_type="text">cognitive medium access with logarithmic regret,” IEEE Journal on Selected Areas in Communica-</text>
<text top="683" left="100" width="196" height="9" font="font5" id="p45_t42" reading_order_no="41" segment_no="14" tag_type="text">tions , vol. 29, no. 4, pp. 731–745, April 2011.</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p45_t43" reading_order_no="42" segment_no="15" tag_type="text">[44] E. Oh, K. Son, and B. Krishnamachari, “Dynamic base station switching-on/off strategies for green</text>
<text top="714" left="100" width="433" height="9" font="font3" id="p45_t44" reading_order_no="43" segment_no="15" tag_type="text">cellular networks,” IEEE Transactions on Wireless Communications , vol. 12, no. 5, pp. 2126–2136,</text>
<text top="726" left="100" width="45" height="9" font="font3" id="p45_t45" reading_order_no="44" segment_no="15" tag_type="text">May 2013.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p45_t46" reading_order_no="45" segment_no="16" tag_type="text">45</text>
</page>
<page number="46" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="453" width="80" height="9" font="font3" id="p46_t1" reading_order_no="0" segment_no="0" tag_type="title">BIBLIOGRAPHY</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p46_t2" reading_order_no="1" segment_no="1" tag_type="text">[45] N. Modi, P. Mary, and C. Moy, “Transfer restless multi-armed bandit policy for energy-efficient</text>
<text top="122" left="100" width="433" height="9" font="font3" id="p46_t3" reading_order_no="2" segment_no="1" tag_type="text">heterogeneous cellular network,” EURASIP Journal on Advances in Signal Processing , vol. 2019,</text>
<text top="134" left="100" width="55" height="9" font="font3" id="p46_t4" reading_order_no="3" segment_no="1" tag_type="text">no. 46, 2019.</text>
<text top="154" left="79" width="453" height="9" font="font3" id="p46_t5" reading_order_no="4" segment_no="2" tag_type="text">[46] W. Jouini, D. Ernst, C. Moy, and J. Palicot, “Multi-Armed Bandit Based Policies for Cognitive</text>
<text top="166" left="100" width="411" height="9" font="font3" id="p46_t6" reading_order_no="5" segment_no="2" tag_type="text">Radio’s Decision Making Issues,” in 3rd conference on Signal Circuits and Systems , Nov 2009.</text>
<text top="186" left="79" width="453" height="9" font="font3" id="p46_t7" reading_order_no="6" segment_no="3" tag_type="text">[47] ——, “Upper confidence bound based decision making strategies and dynamic spectrum access,” in</text>
<text top="197" left="100" width="288" height="9" font="font5" id="p46_t8" reading_order_no="7" segment_no="3" tag_type="text">International Conference on Communications, ICC’10 , May 2010.</text>
<text top="217" left="79" width="453" height="9" font="font3" id="p46_t9" reading_order_no="8" segment_no="4" tag_type="text">[48] L. Melian-Guttierrez, N. Modi, C. Moy, F. Bader, I. Perez-Alvarez, and S. Zazo, “Hybrid ucb-</text>
<text top="229" left="100" width="433" height="9" font="font3" id="p46_t10" reading_order_no="9" segment_no="4" tag_type="text">hmm: A machine learning strategy for cognitive radio in hf band,” IEEE Transactions on Cognitive</text>
<text top="241" left="100" width="251" height="9" font="font5" id="p46_t11" reading_order_no="10" segment_no="4" tag_type="text">Communications and Networking , vol. 2016, no. 99, 2016.</text>
<text top="261" left="79" width="454" height="9" font="font3" id="p46_t12" reading_order_no="11" segment_no="5" tag_type="text">[49] C. Moy, “Reinforcement learning real experiments for opportunistic spectrum access,” in Karlsruhe</text>
<text top="273" left="100" width="233" height="9" font="font5" id="p46_t13" reading_order_no="12" segment_no="5" tag_type="text">Workshop on Software Radios, WSR’14 , March 2014.</text>
<text top="293" left="79" width="453" height="9" font="font3" id="p46_t14" reading_order_no="13" segment_no="6" tag_type="text">[50] S. J. Darak, C. Moy, and J. Palicot, “Proof-of-concept system for opportunistic spectrum access</text>
<text top="305" left="100" width="433" height="9" font="font3" id="p46_t15" reading_order_no="14" segment_no="6" tag_type="text">in multi-user decentralized networks,” EAI Endorsed Transactions on Cognitive Communications ,</text>
<text top="317" left="100" width="135" height="9" font="font3" id="p46_t16" reading_order_no="15" segment_no="6" tag_type="text">vol. 16, no. 7, September 2016.</text>
<text top="337" left="79" width="453" height="9" font="font3" id="p46_t17" reading_order_no="16" segment_no="7" tag_type="text">[51] S. Darak, N. Modi, A. Nafkha, and C. Moy, “Spectrum utilization and reconfiguration cost compari-</text>
<text top="349" left="100" width="433" height="9" font="font3" id="p46_t18" reading_order_no="17" segment_no="7" tag_type="text">son of various decision making policies for opportunistic spectrum access using real radio signals,” in</text>
<text top="361" left="100" width="432" height="9" font="font5" id="p46_t19" reading_order_no="18" segment_no="7" tag_type="text">Cognitive Radio Oriented Wireless Networks and Communications 2016, CrownCom’16 , May 2016.</text>
<text top="380" left="79" width="453" height="9" font="font3" id="p46_t20" reading_order_no="19" segment_no="8" tag_type="text">[52] L. Besson, R. Bonnefoi, and C. Moy, “Gnu radio implementation of malin: Multi-armed ban-</text>
<text top="392" left="100" width="432" height="9" font="font3" id="p46_t21" reading_order_no="20" segment_no="8" tag_type="text">dits learning for iot networks,” in IEEE Wireless Communications and Networking Conference,</text>
<text top="404" left="100" width="100" height="9" font="font5" id="p46_t22" reading_order_no="21" segment_no="8" tag_type="text">WCNC’19 , April 2019.</text>
<text top="424" left="79" width="453" height="9" font="font3" id="p46_t23" reading_order_no="22" segment_no="9" tag_type="text">[53] C. Moy, L. Besson, G. Delbarre, and L. Toutain, “Decentralized spectrum learning for radio collision</text>
<text top="436" left="100" width="432" height="9" font="font3" id="p46_t24" reading_order_no="23" segment_no="9" tag_type="text">mitigation in ultra-dense iot networks: Lorawan case study and experiments,” Annals of Telecom-</text>
<text top="448" left="100" width="175" height="9" font="font5" id="p46_t25" reading_order_no="24" segment_no="9" tag_type="text">munications, SPRINGER , August 2020.</text>
<text top="468" left="79" width="453" height="9" font="font3" id="p46_t26" reading_order_no="25" segment_no="10" tag_type="text">[54] C. Moy, “Iotligent: First world-wide implementation of decentralized spectrum learning for iot</text>
<text top="480" left="100" width="231" height="9" font="font3" id="p46_t27" reading_order_no="26" segment_no="10" tag_type="text">wireless networks,” in URSI AP-RASC , March 2019.</text>
<text top="500" left="79" width="454" height="9" font="font3" id="p46_t28" reading_order_no="27" segment_no="11" tag_type="text">[55] T. O’Shea and J. Hoydis, “An introduction to deep learning for the physical layer,” IEEE Transac-</text>
<text top="512" left="100" width="372" height="9" font="font5" id="p46_t29" reading_order_no="28" segment_no="11" tag_type="text">tions on Cognitive Communications and Networking , vol. 3, no. 4, pp. 563–575, 2017.</text>
<text top="531" left="79" width="453" height="9" font="font3" id="p46_t30" reading_order_no="29" segment_no="12" tag_type="text">[56] S. D¨ orner, S. Cammerer, J. Hoydis, and S. t. Brink, “Deep learning based communication over the</text>
<text top="543" left="100" width="404" height="9" font="font3" id="p46_t31" reading_order_no="30" segment_no="12" tag_type="text">air,” IEEE Journal of Selected Topics in Signal Processing , vol. 12, no. 1, pp. 132–143, 2018.</text>
<text top="563" left="79" width="454" height="9" font="font3" id="p46_t32" reading_order_no="31" segment_no="13" tag_type="text">[57] A. Felix, S. Cammerer, S. D¨ orner, J. Hoydis, and S. Ten Brink, “Ofdm-autoencoder for end-to-</text>
<text top="575" left="100" width="433" height="9" font="font3" id="p46_t33" reading_order_no="32" segment_no="13" tag_type="text">end learning of communications systems,” in 2018 IEEE 19th International Workshop on Signal</text>
<text top="587" left="100" width="331" height="9" font="font5" id="p46_t34" reading_order_no="33" segment_no="13" tag_type="text">Processing Advances in Wireless Communications (SPAWC) , 2018, pp. 1–5.</text>
<text top="607" left="79" width="453" height="9" font="font3" id="p46_t35" reading_order_no="34" segment_no="14" tag_type="text">[58] C. Louart, Z. Liao, and R. Couillet, “A random matrix approach to neural networks,” Annals of</text>
<text top="619" left="100" width="267" height="9" font="font5" id="p46_t36" reading_order_no="35" segment_no="14" tag_type="text">Applied Probability , vol. 28, no. 2, pp. 1190–1248, April 2018.</text>
<text top="639" left="79" width="453" height="9" font="font3" id="p46_t37" reading_order_no="36" segment_no="15" tag_type="text">[59] R. A. Berry and R. G. Gallager, “Communication over fading channels with delay constraints,”</text>
<text top="651" left="100" width="346" height="9" font="font5" id="p46_t38" reading_order_no="37" segment_no="15" tag_type="text">IEEE Transactions on Information Theory , vol. 48, no. 5, pp. 1135–1149, 2002.</text>
<text top="671" left="79" width="453" height="9" font="font3" id="p46_t39" reading_order_no="38" segment_no="16" tag_type="text">[60] D. Rajan, A. Sabharwal, and B. Aazhang, “Delay-bounded packet scheduling of bursty traffic over</text>
<text top="682" left="100" width="421" height="9" font="font3" id="p46_t40" reading_order_no="39" segment_no="16" tag_type="text">wireless channels,” IEEE Transactions on Information Theory , vol. 50, no. 1, pp. 125–144, 2004.</text>
<text top="702" left="79" width="453" height="9" font="font3" id="p46_t41" reading_order_no="40" segment_no="17" tag_type="text">[61] C. Zhong, M. C. Gursoy, and S. Velipasalar, “Deep reinforcement learning-based edge caching in</text>
<text top="714" left="100" width="433" height="9" font="font3" id="p46_t42" reading_order_no="41" segment_no="17" tag_type="text">wireless networks,” IEEE Transactions on Cognitive Communications and Networking , vol. 6, no. 1,</text>
<text top="726" left="100" width="71" height="9" font="font3" id="p46_t43" reading_order_no="42" segment_no="17" tag_type="text">pp. 48–61, 2020.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p46_t44" reading_order_no="43" segment_no="18" tag_type="text">46</text>
</page>
<page number="47" position="absolute" top="0" left="0" height="792" width="612">
<text top="72" left="79" width="80" height="9" font="font3" id="p47_t1" reading_order_no="0" segment_no="0" tag_type="title">BIBLIOGRAPHY</text>
<text top="110" left="79" width="453" height="9" font="font3" id="p47_t2" reading_order_no="1" segment_no="1" tag_type="text">[62] Y. He, Z. Zhang, F. R. Yu, N. Zhao, H. Yin, V. C. M. Leung, and Y. Zhang, “Deep-reinforcement-</text>
<text top="122" left="100" width="433" height="9" font="font3" id="p47_t3" reading_order_no="2" segment_no="1" tag_type="text">learning-based optimization for cache-enabled opportunistic interference alignment wireless net-</text>
<text top="134" left="100" width="407" height="9" font="font3" id="p47_t4" reading_order_no="3" segment_no="1" tag_type="text">works,” IEEE Transactions on Vehicular Technology , vol. 66, no. 11, pp. 10 433–10 445, 2017.</text>
<text top="154" left="79" width="453" height="9" font="font3" id="p47_t5" reading_order_no="4" segment_no="2" tag_type="text">[63] J. Lunden, V. Koivunen, and H. V. Poor, “Spectrum exploration and exploitation for cognitive radio:</text>
<text top="166" left="100" width="402" height="9" font="font3" id="p47_t6" reading_order_no="5" segment_no="2" tag_type="text">Recent advances,” IEEE Signal Processing Magazine , vol. 32, no. 3, pp. 123–140, May 2015.</text>
<text top="186" left="79" width="453" height="9" font="font3" id="p47_t7" reading_order_no="6" segment_no="3" tag_type="text">[64] Q. Mao, F. Hu, and Q. Hao, “Deep learning for intelligent wireless networks: A comprehensive</text>
<text top="198" left="100" width="377" height="9" font="font3" id="p47_t8" reading_order_no="7" segment_no="3" tag_type="text">survey,” IEEE Communications Surveys Tutorials , vol. 20, no. 4, pp. 2595–2621, 2018.</text>
<text top="218" left="79" width="453" height="9" font="font3" id="p47_t9" reading_order_no="8" segment_no="4" tag_type="text">[65] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep reinforcement learning for dynamic</text>
<text top="230" left="100" width="432" height="9" font="font3" id="p47_t10" reading_order_no="9" segment_no="4" tag_type="text">multichannel access in wireless networks,” IEEE Transactions on Cognitive Communications and</text>
<text top="242" left="100" width="193" height="9" font="font5" id="p47_t11" reading_order_no="10" segment_no="4" tag_type="text">Networking , vol. 4, no. 2, pp. 257–265, 2018.</text>
<text top="261" left="79" width="453" height="9" font="font3" id="p47_t12" reading_order_no="11" segment_no="5" tag_type="text">[66] J. Zhu, Y. Song, D. Jiang, and H. Song, “A new deep-q-learning-based transmission scheduling</text>
<text top="273" left="100" width="432" height="9" font="font3" id="p47_t13" reading_order_no="12" segment_no="5" tag_type="text">mechanism for the cognitive internet of things,” IEEE Internet of Things Journal , vol. 5, no. 4, pp.</text>
<text top="285" left="100" width="74" height="9" font="font3" id="p47_t14" reading_order_no="13" segment_no="5" tag_type="text">2375–2385, 2018.</text>
<text top="305" left="79" width="453" height="9" font="font3" id="p47_t15" reading_order_no="14" segment_no="6" tag_type="text">[67] M. Chu, H. Li, X. Liao, and S. Cui, “Reinforcement learning-based multiaccess control and battery</text>
<text top="317" left="100" width="432" height="9" font="font3" id="p47_t16" reading_order_no="15" segment_no="6" tag_type="text">prediction with energy harvesting in iot systems,” IEEE Internet of Things Journal , vol. 6, no. 2,</text>
<text top="329" left="100" width="91" height="9" font="font3" id="p47_t17" reading_order_no="16" segment_no="6" tag_type="text">pp. 2009–2020, 2019.</text>
<text top="349" left="79" width="453" height="9" font="font3" id="p47_t18" reading_order_no="17" segment_no="7" tag_type="text">[68] U. Challita, L. Dong, and W. Saad, “Proactive resource management for lte in unlicensed spectrum:</text>
<text top="361" left="100" width="432" height="9" font="font3" id="p47_t19" reading_order_no="18" segment_no="7" tag_type="text">A deep learning perspective,” IEEE Transactions on Wireless Communications , vol. 17, no. 7, pp.</text>
<text top="373" left="100" width="74" height="9" font="font3" id="p47_t20" reading_order_no="19" segment_no="7" tag_type="text">4674–4689, 2018.</text>
<text top="393" left="79" width="453" height="9" font="font3" id="p47_t21" reading_order_no="20" segment_no="8" tag_type="text">[69] O. Naparstek and K. Cohen, “Deep multi-user reinforcement learning for distributed dynamic spec-</text>
<text top="405" left="100" width="425" height="9" font="font3" id="p47_t22" reading_order_no="21" segment_no="8" tag_type="text">trum access,” IEEE Transactions on Wireless Communications , vol. 18, no. 1, pp. 310–323, 2019.</text>
<text top="425" left="79" width="453" height="9" font="font3" id="p47_t23" reading_order_no="22" segment_no="9" tag_type="text">[70] P. V. R. Ferreira, R. Paffenroth, A. M. Wyglinski, T. M. Hackett, S. G. Bil´ en, R. C. Reinhart, and</text>
<text top="437" left="100" width="433" height="9" font="font3" id="p47_t24" reading_order_no="23" segment_no="9" tag_type="text">D. J. Mortensen, “Multiobjective reinforcement learning for cognitive satellite communications using</text>
<text top="449" left="100" width="432" height="9" font="font3" id="p47_t25" reading_order_no="24" segment_no="9" tag_type="text">deep neural network ensembles,” IEEE Journal on Selected Areas in Communications , vol. 36, no. 5,</text>
<text top="461" left="100" width="91" height="9" font="font3" id="p47_t26" reading_order_no="25" segment_no="9" tag_type="text">pp. 1030–1041, 2018.</text>
<text top="756" left="301" width="10" height="9" font="font3" id="p47_t27" reading_order_no="26" segment_no="10" tag_type="text">47</text>
</page>
<outline>
<item page="1">9 RL for PHY layer communications</item>
<outline>
<item page="1">9.1 Introduction</item>
<item page="4">9.2 Reinforcement Learning: background</item>
<outline>
<item page="4">9.2.1 Overview</item>
<item page="4">9.2.2 Markov Decision Process</item>
<item page="8">9.2.3 Partially observable Markov decision process</item>
<item page="11">9.2.4 Q-learning and SARSA algorithm</item>
<item page="12">9.2.5 Deep RL</item>
<item page="15">9.2.6 Multi-armed bandits</item>
<item page="17">9.2.7 Markovian MAB</item>
</outline>
<item page="20">9.3 RL at PHY layer</item>
<outline>
<item page="20">9.3.1 Example with Q-learning</item>
<item page="22">9.3.2 Example with Deep-RL</item>
<item page="25">9.3.3 Examples with MAB</item>
<item page="29">9.3.4 Real world examples</item>
</outline>
<item page="37">9.4 Conclusions and Future Trends</item>
<item page="38">9.5 Bibliographical remarks</item>
<item page="38">Acronyms</item>
<item page="38">Notation and symbols</item>
</outline>
</outline>
</pdf2xml>
