<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="14" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font1" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font2" size="7" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="12" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font4" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font5" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font6" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font7" size="7" family="NimbusRomNo9L-Regu" color="#001472"/>
	<fontspec id="font8" size="10" family="NimbusRomNo9L-Regu" color="#001472"/>
	<fontspec id="font9" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font10" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font11" size="9" family="CMMI9" color="#000000"/>
	<fontspec id="font12" size="9" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font13" size="9" family="CMTI9" color="#000000"/>
	<fontspec id="font14" size="6" family="CMMI6" color="#000000"/>
	<fontspec id="font15" size="20" family="Times" color="#7f7f7f"/>
<text top="91" left="60" width="477" height="13" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="109" left="216" width="165" height="13" font="font0" id="p1_t2" reading_order_no="2" segment_no="0" tag_type="title">for Few-Shot Classification</text>
<text top="158" left="218" width="71" height="10" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="text">Dong Hoon Lee 1</text>
<text top="158" left="296" width="81" height="10" font="font1" id="p1_t4" reading_order_no="4" segment_no="1" tag_type="text">Sae-Young Chung 1</text>
<text top="192" left="150" width="44" height="11" font="font3" id="p1_t5" reading_order_no="5" segment_no="2" tag_type="title">Abstract</text>
<text top="210" left="75" width="194" height="9" font="font4" id="p1_t6" reading_order_no="6" segment_no="4" tag_type="text">We propose unsupervised embedding adaptation</text>
<text top="222" left="75" width="196" height="9" font="font4" id="p1_t7" reading_order_no="7" segment_no="4" tag_type="text">for the downstream few-shot classification task.</text>
<text top="233" left="75" width="194" height="9" font="font4" id="p1_t8" reading_order_no="8" segment_no="4" tag_type="text">Based on findings that deep neural networks learn</text>
<text top="245" left="75" width="194" height="9" font="font4" id="p1_t9" reading_order_no="9" segment_no="4" tag_type="text">to generalize before memorizing, we develop</text>
<text top="257" left="75" width="194" height="9" font="font4" id="p1_t10" reading_order_no="10" segment_no="4" tag_type="text">Early-Stage Feature Reconstruction (ESFR) — a</text>
<text top="269" left="75" width="196" height="9" font="font4" id="p1_t11" reading_order_no="11" segment_no="4" tag_type="text">novel adaptation scheme with feature reconstruc-</text>
<text top="281" left="75" width="194" height="9" font="font4" id="p1_t12" reading_order_no="12" segment_no="4" tag_type="text">tion and dimensionality-driven early stopping that</text>
<text top="293" left="75" width="194" height="9" font="font4" id="p1_t13" reading_order_no="13" segment_no="4" tag_type="text">finds generalizable features. Incorporating ESFR</text>
<text top="305" left="75" width="196" height="9" font="font4" id="p1_t14" reading_order_no="14" segment_no="4" tag_type="text">consistently improves the performance of base-</text>
<text top="317" left="75" width="194" height="9" font="font4" id="p1_t15" reading_order_no="15" segment_no="4" tag_type="text">line methods on all standard settings, including</text>
<text top="329" left="75" width="194" height="9" font="font4" id="p1_t16" reading_order_no="16" segment_no="4" tag_type="text">the recently proposed transductive method. ESFR</text>
<text top="341" left="75" width="194" height="9" font="font4" id="p1_t17" reading_order_no="17" segment_no="4" tag_type="text">used in conjunction with the transductive method</text>
<text top="353" left="75" width="194" height="9" font="font4" id="p1_t18" reading_order_no="18" segment_no="4" tag_type="text">further achieves state-of-the-art performance on</text>
<text top="365" left="75" width="196" height="9" font="font5" id="p1_t19" reading_order_no="19" segment_no="4" tag_type="text">mini -ImageNet, tiered -ImageNet, and CUB; es-</text>
<text top="376" left="75" width="197" height="10" font="font4" id="p1_t20" reading_order_no="20" segment_no="4" tag_type="text">pecially with 1.2% ∼ 2.0% improvements in accu-</text>
<text top="389" left="75" width="194" height="9" font="font4" id="p1_t21" reading_order_no="21" segment_no="4" tag_type="text">racy over the previous best performing method on</text>
<text top="399" left="75" width="59" height="11" font="font4" id="p1_t22" reading_order_no="22" segment_no="4" tag_type="text">1-shot setting. 1</text>
<text top="434" left="55" width="77" height="11" font="font3" id="p1_t23" reading_order_no="23" segment_no="6" tag_type="title">1. Introduction</text>
<text top="455" left="55" width="234" height="9" font="font4" id="p1_t24" reading_order_no="24" segment_no="7" tag_type="text">Deep learning has achieved impressive results on visual</text>
<text top="467" left="55" width="234" height="9" font="font4" id="p1_t25" reading_order_no="25" segment_no="7" tag_type="text">recognition tasks. However, it still has difficulty generalizing</text>
<text top="479" left="55" width="234" height="9" font="font4" id="p1_t26" reading_order_no="26" segment_no="7" tag_type="text">to novel classes with few examples; while humans can learn</text>
<text top="491" left="55" width="234" height="9" font="font4" id="p1_t27" reading_order_no="27" segment_no="7" tag_type="text">to recognize from few experiences. Few-shot classification</text>
<text top="503" left="55" width="235" height="9" font="font4" id="p1_t28" reading_order_no="28" segment_no="7" tag_type="text">( Miller et al. , 2000 ; Vinyals et al. , 2016 ; Ravi &amp; Larochelle ,</text>
<text top="515" left="55" width="234" height="9" font="font8" id="p1_t29" reading_order_no="29" segment_no="7" tag_type="text">2017 ) is designed to bridge the gap between the two and has<a href="deeplearning_paper6.html#1">setting.</a></text>
<text top="527" left="55" width="154" height="9" font="font4" id="p1_t30" reading_order_no="30" segment_no="7" tag_type="text">recently attracted substantial attention.<a href="deeplearning_paper6.html#1">1</a></text>
<text top="545" left="55" width="235" height="9" font="font4" id="p1_t31" reading_order_no="31" segment_no="8" tag_type="text">Several works ( Liu et al. , 2019 ; Hou et al. , 2019 ; Qiao</text>
<text top="557" left="55" width="234" height="9" font="font8" id="p1_t32" reading_order_no="32" segment_no="8" tag_type="text">et al. , 2019 ; Hu et al. , 2020 ; Dhillon et al. , 2020 ; Ziko</text>
<text top="569" left="55" width="237" height="9" font="font8" id="p1_t33" reading_order_no="33" segment_no="8" tag_type="text">et al. , 2020 ; Boudiaf et al. , 2020 ) have shown the effec-</text>
<text top="581" left="55" width="235" height="9" font="font4" id="p1_t34" reading_order_no="34" segment_no="8" tag_type="text">tiveness of transductive methods in few-shot classification,</text>
<text top="593" left="55" width="236" height="9" font="font4" id="p1_t35" reading_order_no="35" segment_no="8" tag_type="text">showing a significant improvement over inductive methods.</text>
<text top="605" left="55" width="234" height="9" font="font4" id="p1_t36" reading_order_no="36" segment_no="8" tag_type="text">While test samples are inaccessible in an inductive few-shot<a href="deeplearning_paper6.html#10">(</a></text>
<text top="617" left="55" width="234" height="9" font="font4" id="p1_t37" reading_order_no="37" segment_no="8" tag_type="text">classification setting, one can utilize all the unlabeled test<a href="deeplearning_paper6.html#10">Miller et al.</a></text>
<text top="629" left="55" width="234" height="9" font="font4" id="p1_t38" reading_order_no="38" segment_no="8" tag_type="text">samples together to make an inference in a transductive<a href="deeplearning_paper6.html#10">,</a></text>
<text top="647" left="67" width="223" height="9" font="font10" id="p1_t39" reading_order_no="80" segment_no="11" tag_type="footnote">1 School of Electrical Engineering, Korea Advanced Institute<a href="deeplearning_paper6.html#10">2000</a></text>
<text top="658" left="55" width="235" height="8" font="font10" id="p1_t40" reading_order_no="81" segment_no="11" tag_type="footnote">of Science and Technology (KAIST), Daejeon, Korea. Correspon-<a href="deeplearning_paper6.html#10">;</a></text>
<text top="668" left="55" width="203" height="8" font="font10" id="p1_t41" reading_order_no="82" segment_no="11" tag_type="footnote">dence to: Dong Hoon Lee &lt; donghoonlee@kaist.ac.kr &gt; .<a href="deeplearning_paper6.html#11">Vinyals et al.</a></text>
<text top="686" left="55" width="234" height="10" font="font12" id="p1_t42" reading_order_no="83" segment_no="12" tag_type="footnote">Proceedings of the 38 th International Conference on Machine<a href="deeplearning_paper6.html#11">,</a></text>
<text top="698" left="55" width="222" height="8" font="font12" id="p1_t43" reading_order_no="84" segment_no="12" tag_type="footnote">Learning , PMLR 139, 2021. Copyright 2021 by the author(s).<a href="deeplearning_paper6.html#11">2016</a></text>
<text top="707" left="68" width="213" height="10" font="font10" id="p1_t44" reading_order_no="85" segment_no="13" tag_type="footnote">1 Code is available at https://github.com/movinghoon/ESFR<a href="deeplearning_paper6.html#11">;</a></text>
<text top="194" left="307" width="234" height="9" font="font4" id="p1_t45" reading_order_no="39" segment_no="3" tag_type="text">setting. The co-existence of labeled- and unlabeled-data in<a href="deeplearning_paper6.html#10">Ravi &amp; Larochelle</a></text>
<text top="206" left="307" width="234" height="9" font="font4" id="p1_t46" reading_order_no="40" segment_no="3" tag_type="text">this setting motivates the use of transductive inference or<a href="deeplearning_paper6.html#10">,</a></text>
<text top="218" left="307" width="234" height="9" font="font4" id="p1_t47" reading_order_no="41" segment_no="3" tag_type="text">semi-supervised learning. A popular transductive approach<a href="deeplearning_paper6.html#10">2017</a></text>
<text top="230" left="307" width="234" height="9" font="font4" id="p1_t48" reading_order_no="42" segment_no="3" tag_type="text">is pseudo-label-based methods that progressively update the<a href="deeplearning_paper6.html#10">) </a>is designed to bridge the gap between the two and has</text>
<text top="242" left="307" width="234" height="9" font="font4" id="p1_t49" reading_order_no="43" segment_no="3" tag_type="text">labels or inference models by predicted test samples. For</text>
<text top="254" left="307" width="234" height="9" font="font4" id="p1_t50" reading_order_no="44" segment_no="3" tag_type="text">instance, Liu et al. ( 2019 ); Kim et al. ( 2019 ) use a neighbor<a href="deeplearning_paper6.html#10">(</a></text>
<text top="265" left="307" width="236" height="9" font="font4" id="p1_t51" reading_order_no="45" segment_no="3" tag_type="text">graph for label propagation, Hou et al. ( 2019 ); Liu et al.<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="277" left="307" width="235" height="9" font="font4" id="p1_t52" reading_order_no="46" segment_no="3" tag_type="text">( 2020 ) use predicted labels on test samples to update the<a href="deeplearning_paper6.html#10">,</a></text>
<text top="289" left="307" width="236" height="9" font="font4" id="p1_t53" reading_order_no="47" segment_no="3" tag_type="text">class prototypes, and Antoniou &amp; Storkey ( 2019 ); Hu et al.<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="301" left="307" width="236" height="9" font="font4" id="p1_t54" reading_order_no="48" segment_no="3" tag_type="text">( 2020 ) use prediction to produce an intrinsic loss or syn-<a href="deeplearning_paper6.html#10">;</a></text>
<text top="313" left="307" width="234" height="9" font="font4" id="p1_t55" reading_order_no="49" segment_no="3" tag_type="text">thetic gradient. Another line of works utilizes regularization<a href="deeplearning_paper6.html#9">Hou et al.</a></text>
<text top="325" left="307" width="236" height="9" font="font4" id="p1_t56" reading_order_no="50" segment_no="3" tag_type="text">terms on unlabeled test samples. To list a few, Dhillon et al.<a href="deeplearning_paper6.html#9">,</a></text>
<text top="337" left="307" width="235" height="9" font="font4" id="p1_t57" reading_order_no="51" segment_no="3" tag_type="text">( 2020 ); Boudiaf et al. ( 2020 ) use entropy minimization of<a href="deeplearning_paper6.html#9">2019</a></text>
<text top="349" left="307" width="234" height="9" font="font4" id="p1_t58" reading_order_no="52" segment_no="3" tag_type="text">the prediction on unlabeled data, Ziko et al. ( 2020 ) uses the<a href="deeplearning_paper6.html#9">;</a></text>
<text top="361" left="307" width="234" height="9" font="font4" id="p1_t59" reading_order_no="53" segment_no="3" tag_type="text">Laplacian regularization term for graph clustering. These<a href="deeplearning_paper6.html#10">Qiao</a></text>
<text top="373" left="307" width="236" height="9" font="font4" id="p1_t60" reading_order_no="54" segment_no="3" tag_type="text">methods are mostly originated from semi-supervised learn-<a href="deeplearning_paper6.html#10">et al.</a></text>
<text top="385" left="307" width="234" height="9" font="font4" id="p1_t61" reading_order_no="55" segment_no="3" tag_type="text">ing studies; approaches in semi-supervised learning are still<a href="deeplearning_paper6.html#10">,</a></text>
<text top="397" left="307" width="172" height="9" font="font4" id="p1_t62" reading_order_no="56" segment_no="3" tag_type="text">motivating few-shot classification research.<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="415" left="307" width="234" height="9" font="font4" id="p1_t63" reading_order_no="57" segment_no="5" tag_type="text">On the other hand, semi-supervised learning research has<a href="deeplearning_paper6.html#10">;</a></text>
<text top="427" left="307" width="236" height="9" font="font4" id="p1_t64" reading_order_no="58" segment_no="5" tag_type="text">recently benefited from unsupervised learning. Rapidly ad-<a href="deeplearning_paper6.html#10">Hu et al.</a></text>
<text top="439" left="307" width="236" height="9" font="font4" id="p1_t65" reading_order_no="59" segment_no="5" tag_type="text">vancing self-supervised learning methods ( Caron et al. ,<a href="deeplearning_paper6.html#10">,</a></text>
<text top="451" left="307" width="234" height="9" font="font8" id="p1_t66" reading_order_no="60" segment_no="5" tag_type="text">2020 ; Grill et al. , 2020 ; Chen et al. , 2020 ) have shown strong<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="463" left="307" width="236" height="9" font="font4" id="p1_t67" reading_order_no="61" segment_no="5" tag_type="text">performance on semi-supervised image classification tasks.<a href="deeplearning_paper6.html#10">;</a></text>
<text top="475" left="307" width="236" height="9" font="font4" id="p1_t68" reading_order_no="62" segment_no="5" tag_type="text">A popular approach is to use a self-supervision loss for repre-<a href="deeplearning_paper6.html#9">Dhillon et al.</a></text>
<text top="487" left="307" width="234" height="9" font="font4" id="p1_t69" reading_order_no="63" segment_no="5" tag_type="text">sentation learning to acquire more general features. Learned<a href="deeplearning_paper6.html#9">,</a></text>
<text top="499" left="307" width="235" height="9" font="font4" id="p1_t70" reading_order_no="64" segment_no="5" tag_type="text">representations are then used with fine-tuning ( Caron et al. ,<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="511" left="307" width="236" height="9" font="font8" id="p1_t71" reading_order_no="65" segment_no="5" tag_type="text">2020 ; Grill et al. , 2020 ; Chen et al. , 2020 ) or other semi-<a href="deeplearning_paper6.html#9">;</a></text>
<text top="523" left="307" width="236" height="9" font="font4" id="p1_t72" reading_order_no="66" segment_no="5" tag_type="text">supervised learning methods ( Zhai et al. , 2019 ; Kim et al. ,<a href="deeplearning_paper6.html#11">Ziko</a></text>
<text top="534" left="307" width="236" height="9" font="font8" id="p1_t73" reading_order_no="67" segment_no="5" tag_type="text">2021 ) for downstream tasks. These studies achieved state-<a href="deeplearning_paper6.html#11">et al.</a></text>
<text top="546" left="307" width="235" height="9" font="font4" id="p1_t74" reading_order_no="68" segment_no="5" tag_type="text">of-the-art performance on semi-supervised learning tasks,<a href="deeplearning_paper6.html#11">,</a></text>
<text top="558" left="307" width="191" height="9" font="font4" id="p1_t75" reading_order_no="69" segment_no="5" tag_type="text">especially in settings with extremely few labels.<a href="deeplearning_paper6.html#11">2020</a></text>
<text top="576" left="307" width="234" height="9" font="font4" id="p1_t76" reading_order_no="70" segment_no="10" tag_type="text">The success of unsupervised learning on semi-supervised<a href="deeplearning_paper6.html#11">;</a></text>
<text top="588" left="307" width="234" height="9" font="font4" id="p1_t77" reading_order_no="71" segment_no="10" tag_type="text">tasks suggests the potential benefit of finding shared features<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="600" left="307" width="236" height="9" font="font4" id="p1_t78" reading_order_no="72" segment_no="10" tag_type="text">or patterns without labels in relevant research areas. In few-<a href="deeplearning_paper6.html#9">,</a></text>
<text top="612" left="307" width="235" height="9" font="font4" id="p1_t79" reading_order_no="73" segment_no="10" tag_type="text">shot classification, several works ( Gidaris et al. , 2019 ; Su<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="624" left="307" width="235" height="9" font="font8" id="p1_t80" reading_order_no="74" segment_no="10" tag_type="text">et al. , 2020 ) use additional self-supervision loss during the<a href="deeplearning_paper6.html#9">) </a>have shown the effec-</text>
<text top="636" left="307" width="236" height="9" font="font4" id="p1_t81" reading_order_no="75" segment_no="10" tag_type="text">training of base datasets to learn more general features.</text>
<text top="648" left="307" width="234" height="9" font="font4" id="p1_t82" reading_order_no="76" segment_no="10" tag_type="text">However, the use of unsupervised learning on unlabeled</text>
<text top="660" left="307" width="235" height="9" font="font4" id="p1_t83" reading_order_no="77" segment_no="10" tag_type="text">data that appears in test-time is less studied. In this work,</text>
<text top="672" left="307" width="234" height="9" font="font4" id="p1_t84" reading_order_no="78" segment_no="10" tag_type="text">we study unsupervised learning for adaptation to satisfy the</text>
<text top="684" left="307" width="23" height="9" font="font4" id="p1_t85" reading_order_no="79" segment_no="10" tag_type="text">thirst.</text>
<text top="546" left="32" width="0" height="18" font="font15" id="p1_t86" reading_order_no="0" segment_no="9" tag_type="title">arXiv:2106.11486v1  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font16" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font17" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font18" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font19" size="7" family="CMMI7" color="#000000"/>
	<fontspec id="font20" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font21" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font22" size="9" family="CMR9" color="#000000"/>
	<fontspec id="font23" size="5" family="CMR5" color="#000000"/>
	<fontspec id="font24" size="10" family="CMEX10" color="#000000"/>
	<fontspec id="font25" size="9" family="NimbusRomNo9L-Regu" color="#001472"/>
<text top="48" left="149" width="298" height="8" font="font16" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="70" left="55" width="182" height="9" font="font4" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">Our contributions are summarized as follows:</text>
<text top="97" left="67" width="224" height="9" font="font4" id="p2_t3" reading_order_no="2" segment_no="4" tag_type="list">• We find that early generalized features during unsu-</text>
<text top="109" left="75" width="214" height="9" font="font4" id="p2_t4" reading_order_no="3" segment_no="4" tag_type="list">pervised training are valuable for recognizing novel</text>
<text top="121" left="75" width="216" height="9" font="font4" id="p2_t5" reading_order_no="4" segment_no="4" tag_type="list">classes of few-shot classification. Based on recent stud-</text>
<text top="133" left="75" width="214" height="9" font="font4" id="p2_t6" reading_order_no="5" segment_no="4" tag_type="list">ies of deep neural network’s training dynamics, we</text>
<text top="144" left="75" width="202" height="9" font="font4" id="p2_t7" reading_order_no="6" segment_no="4" tag_type="list">explain the finding with experiments. (Section 3.1 )</text>
<text top="165" left="67" width="223" height="9" font="font4" id="p2_t8" reading_order_no="7" segment_no="5" tag_type="list">• Based on the finding, we construct a novel embedding<a href="deeplearning_paper6.html#3">3.1</a></text>
<text top="177" left="75" width="216" height="9" font="font4" id="p2_t9" reading_order_no="8" segment_no="5" tag_type="list">adaptation scheme with (1) feature reconstruction train-<a href="deeplearning_paper6.html#3">)</a></text>
<text top="189" left="75" width="214" height="9" font="font4" id="p2_t10" reading_order_no="9" segment_no="5" tag_type="list">ing and (2) dimensionality-driven early stopping. Our</text>
<text top="201" left="75" width="214" height="9" font="font4" id="p2_t11" reading_order_no="10" segment_no="5" tag_type="list">method provides task-adapted embeddings composed</text>
<text top="213" left="75" width="214" height="9" font="font4" id="p2_t12" reading_order_no="11" segment_no="5" tag_type="list">of desirable-shared features, which are more likely to</text>
<text top="225" left="75" width="216" height="9" font="font4" id="p2_t13" reading_order_no="12" segment_no="5" tag_type="list">be task-relevant and valuable for the few-shot classifi-</text>
<text top="237" left="75" width="214" height="9" font="font4" id="p2_t14" reading_order_no="13" segment_no="5" tag_type="list">cation. Our method is used as a plug-and-play module</text>
<text top="249" left="75" width="141" height="9" font="font4" id="p2_t15" reading_order_no="14" segment_no="5" tag_type="list">for few-shot methods. (Section 3.3 )</text>
<text top="270" left="67" width="223" height="9" font="font4" id="p2_t16" reading_order_no="15" segment_no="8" tag_type="list">• We test our method, ESFR, used in conjunction with</text>
<text top="282" left="75" width="214" height="9" font="font4" id="p2_t17" reading_order_no="16" segment_no="8" tag_type="list">baseline methods in the standard few-shot classification</text>
<text top="294" left="75" width="216" height="9" font="font4" id="p2_t18" reading_order_no="17" segment_no="8" tag_type="list">benchmarks. ESFR consistently improves the perfor-<a href="deeplearning_paper6.html#5">3.3</a></text>
<text top="306" left="75" width="214" height="9" font="font4" id="p2_t19" reading_order_no="18" segment_no="8" tag_type="list">mance of baselines; adding ESFR to the transductive<a href="deeplearning_paper6.html#5">)</a></text>
<text top="318" left="75" width="214" height="9" font="font4" id="p2_t20" reading_order_no="19" segment_no="8" tag_type="list">method achieves the state-of-the-art performance on</text>
<text top="330" left="75" width="216" height="9" font="font5" id="p2_t21" reading_order_no="20" segment_no="8" tag_type="list">mini -ImageNet, tiered -ImageNet, and CUB. Particu-</text>
<text top="342" left="75" width="214" height="9" font="font4" id="p2_t22" reading_order_no="21" segment_no="8" tag_type="list">larly in the scarce-label setting of 1-shot, our method</text>
<text top="354" left="75" width="216" height="9" font="font4" id="p2_t23" reading_order_no="22" segment_no="8" tag_type="list">outperforms the previous state-of-the-art with accura-</text>
<text top="365" left="75" width="135" height="10" font="font4" id="p2_t24" reading_order_no="23" segment_no="8" tag_type="list">cies of 1.2% ∼ 2.0%. (Section 5.3 )</text>
<text top="393" left="55" width="81" height="11" font="font3" id="p2_t25" reading_order_no="24" segment_no="12" tag_type="title">2. Preliminaries</text>
<text top="413" left="55" width="86" height="9" font="font1" id="p2_t26" reading_order_no="25" segment_no="13" tag_type="title">2.1. Problem Setting</text>
<text top="432" left="55" width="234" height="9" font="font4" id="p2_t27" reading_order_no="26" segment_no="15" tag_type="text">In a few-shot classification task, a small labeled support</text>
<text top="446" left="55" width="22" height="9" font="font4" id="p2_t28" reading_order_no="27" segment_no="15" tag_type="text">set S</text>
<text top="442" left="85" width="66" height="14" font="font18" id="p2_t29" reading_order_no="28" segment_no="15" tag_type="text">= { ( x i s , y i s ) } | S |</text>
<text top="446" left="141" width="149" height="10" font="font4" id="p2_t30" reading_order_no="29" segment_no="15" tag_type="text">i =1 and unlabeled query set Q =</text>
<text top="456" left="55" width="43" height="14" font="font6" id="p2_t31" reading_order_no="30" segment_no="15" tag_type="text">{ ( x i q , ) } | Q |</text>
<text top="458" left="88" width="202" height="13" font="font4" id="p2_t32" reading_order_no="31" segment_no="15" tag_type="text">i =1 are given. 2 Both support samples and query</text>
<text top="472" left="55" width="234" height="9" font="font4" id="p2_t33" reading_order_no="32" segment_no="15" tag_type="text">samples are from the same novel classes that are never seen</text>
<text top="484" left="55" width="234" height="9" font="font4" id="p2_t34" reading_order_no="33" segment_no="15" tag_type="text">during training. The goal of few-shot classification is to<a href="deeplearning_paper6.html#7">5.3</a></text>
<text top="496" left="55" width="234" height="9" font="font4" id="p2_t35" reading_order_no="34" segment_no="15" tag_type="text">classify query (test) samples by few examples in the support<a href="deeplearning_paper6.html#7">)</a></text>
<text top="508" left="55" width="236" height="9" font="font4" id="p2_t36" reading_order_no="35" segment_no="15" tag_type="text">set. In the usual setting, the support set has K = 1 or 5 ex-</text>
<text top="519" left="55" width="234" height="10" font="font4" id="p2_t37" reading_order_no="36" segment_no="15" tag_type="text">amples per N = 5 novel classes, and we call this an N -way</text>
<text top="531" left="55" width="68" height="10" font="font17" id="p2_t38" reading_order_no="37" segment_no="15" tag_type="text">K -shot problem.</text>
<text top="550" left="55" width="234" height="9" font="font4" id="p2_t39" reading_order_no="38" segment_no="18" tag_type="text">We address a transductive few-shot classification task where</text>
<text top="561" left="55" width="234" height="9" font="font4" id="p2_t40" reading_order_no="39" segment_no="18" tag_type="text">all query samples are accessible. Since learning from few</text>
<text top="573" left="55" width="236" height="9" font="font4" id="p2_t41" reading_order_no="40" segment_no="18" tag_type="text">samples without prior is extremely hard, we use a pre-</text>
<text top="585" left="55" width="235" height="9" font="font4" id="p2_t42" reading_order_no="41" segment_no="18" tag_type="text">trained embedding network f that is trained on the base</text>
<text top="597" left="55" width="235" height="9" font="font4" id="p2_t43" reading_order_no="42" segment_no="18" tag_type="text">dataset as in Rusu et al. ( 2019 ); Wang et al. ( 2019 );</text>
<text top="609" left="55" width="219" height="10" font="font8" id="p2_t44" reading_order_no="43" segment_no="18" tag_type="text">Hu et al. ( 2020 ); Ziko et al. ( 2020 ). We denote S f</text>
<text top="609" left="282" width="8" height="9" font="font18" id="p2_t45" reading_order_no="44" segment_no="18" tag_type="text">=</text>
<text top="620" left="55" width="65" height="14" font="font6" id="p2_t46" reading_order_no="45" segment_no="18" tag_type="text">{ ( f ( x i s ) , y i s ) } | S |</text>
<text top="620" left="110" width="119" height="14" font="font4" id="p2_t47" reading_order_no="46" segment_no="18" tag_type="text">i =1 and Q f = { ( f ( x i q ) , · ) } | Q |</text>
<text top="624" left="218" width="71" height="10" font="font4" id="p2_t48" reading_order_no="47" segment_no="18" tag_type="text">i =1 as the support</text>
<text top="635" left="55" width="236" height="9" font="font4" id="p2_t49" reading_order_no="48" segment_no="18" tag_type="text">set and the query set in the embedding domain, respectively.</text>
<text top="653" left="55" width="235" height="9" font="font4" id="p2_t50" reading_order_no="49" segment_no="21" tag_type="text">Our interest is in task-adapted embeddings (representations)</text>
<text top="665" left="55" width="234" height="9" font="font4" id="p2_t51" reading_order_no="50" segment_no="21" tag_type="text">that are useful in the given few-shot task. We construct the</text>
<text top="677" left="55" width="235" height="10" font="font4" id="p2_t52" reading_order_no="51" segment_no="21" tag_type="text">embeddings with a module g φ on top of f by training on the</text>
<text top="689" left="55" width="152" height="9" font="font4" id="p2_t53" reading_order_no="52" segment_no="21" tag_type="text">union of the support set and query set.</text>
<text top="707" left="68" width="137" height="10" font="font22" id="p2_t54" reading_order_no="112" segment_no="24" tag_type="footnote">2 ( x, y ) denotes an image and its label.</text>
<text top="70" left="307" width="174" height="9" font="font1" id="p2_t55" reading_order_no="53" segment_no="2" tag_type="title">2.2. Local Intrinsic Dimensionality (LID)</text>
<text top="89" left="307" width="236" height="9" font="font4" id="p2_t56" reading_order_no="54" segment_no="3" tag_type="text">We briefly explain LID that our method uses as early stop-</text>
<text top="101" left="307" width="236" height="9" font="font4" id="p2_t57" reading_order_no="55" segment_no="3" tag_type="text">ping criterion. LID is a statistical version of an expansion-</text>
<text top="113" left="307" width="234" height="9" font="font4" id="p2_t58" reading_order_no="56" segment_no="3" tag_type="text">based Intrinsic Dimension (ID) that provides an estimated</text>
<text top="125" left="307" width="234" height="9" font="font4" id="p2_t59" reading_order_no="57" segment_no="3" tag_type="text">subspace dimension of local regions. Recently, substantial</text>
<text top="137" left="307" width="234" height="9" font="font4" id="p2_t60" reading_order_no="58" segment_no="3" tag_type="text">redundant dimensions of modern deep neural networks lead</text>
<text top="149" left="307" width="234" height="9" font="font4" id="p2_t61" reading_order_no="59" segment_no="3" tag_type="text">to the wide use of ID and LID to track and analyze training</text>
<text top="161" left="307" width="236" height="9" font="font4" id="p2_t62" reading_order_no="60" segment_no="3" tag_type="text">( Amsaleg et al. , 2017 ; Ma et al. , 2018a ; b ; Ansuini et al. ,</text>
<text top="173" left="307" width="235" height="9" font="font8" id="p2_t63" reading_order_no="61" segment_no="3" tag_type="text">2019 ; Gong et al. , 2019 ). The formal definition of LID is</text>
<text top="185" left="307" width="193" height="9" font="font4" id="p2_t64" reading_order_no="62" segment_no="3" tag_type="text">given as ( Amsaleg et al. , 2015 ; Houle , 2017a ; b ):</text>
<text top="201" left="307" width="235" height="10" font="font1" id="p2_t65" reading_order_no="63" segment_no="6" tag_type="text">Definition 1 (Local Intrinsic Dimensionality) . Given a data</text>
<text top="213" left="307" width="235" height="9" font="font5" id="p2_t66" reading_order_no="64" segment_no="6" tag_type="text">point x , let r &gt; 0 be a continuous random distance variable</text>
<text top="225" left="307" width="235" height="10" font="font5" id="p2_t67" reading_order_no="65" segment_no="6" tag_type="text">from x . For the cumulative density function F x ( r ) , the LID</text>
<text top="237" left="307" width="123" height="9" font="font5" id="p2_t68" reading_order_no="66" segment_no="6" tag_type="text">of x at distance r is defined as :</text>
<text top="264" left="320" width="57" height="12" font="font4" id="p2_t69" reading_order_no="67" segment_no="7" tag_type="formula">LID ( r ; F x ) def</text>
<text top="266" left="369" width="29" height="9" font="font18" id="p2_t70" reading_order_no="68" segment_no="7" tag_type="formula">= lim</text>
<text top="275" left="384" width="17" height="7" font="font20" id="p2_t71" reading_order_no="69" segment_no="7" tag_type="formula">→ 0 +</text>
<text top="259" left="404" width="109" height="10" font="font18" id="p2_t72" reading_order_no="70" segment_no="7" tag_type="formula">ln F x ((1 + ) r ) − ln F x ( r )</text>
<text top="273" left="440" width="37" height="9" font="font18" id="p2_t73" reading_order_no="71" segment_no="7" tag_type="formula">ln(1 + )<a href="deeplearning_paper6.html#2">en.</a></text>
<text top="266" left="514" width="3" height="9" font="font17" id="p2_t74" reading_order_no="72" segment_no="7" tag_type="formula">,<a href="deeplearning_paper6.html#2">2</a></text>
<text top="266" left="530" width="12" height="9" font="font4" id="p2_t75" reading_order_no="73" segment_no="7" tag_type="text">(1)</text>
<text top="295" left="307" width="234" height="9" font="font5" id="p2_t76" reading_order_no="74" segment_no="9" tag_type="text">whenever the limit exists. The LID at x is then defined as</text>
<text top="306" left="307" width="117" height="10" font="font5" id="p2_t77" reading_order_no="75" segment_no="9" tag_type="text">the limit of distance r → 0 + :</text>
<text top="331" left="367" width="41" height="12" font="font4" id="p2_t78" reading_order_no="76" segment_no="10" tag_type="formula">LID ( x ) def</text>
<text top="333" left="400" width="29" height="9" font="font18" id="p2_t79" reading_order_no="77" segment_no="10" tag_type="formula">= lim</text>
<text top="342" left="412" width="20" height="7" font="font19" id="p2_t80" reading_order_no="78" segment_no="10" tag_type="formula">r → 0 +</text>
<text top="333" left="435" width="48" height="10" font="font4" id="p2_t81" reading_order_no="79" segment_no="10" tag_type="formula">LID ( r ; F x ) .</text>
<text top="334" left="530" width="12" height="9" font="font4" id="p2_t82" reading_order_no="80" segment_no="10" tag_type="text">(2)</text>
<text top="367" left="307" width="234" height="9" font="font4" id="p2_t83" reading_order_no="81" segment_no="11" tag_type="text">Since the density function of the distance variable is usually</text>
<text top="379" left="307" width="234" height="9" font="font4" id="p2_t84" reading_order_no="82" segment_no="11" tag_type="text">unknown, the exact value of LID is hard to acquire. We</text>
<text top="391" left="307" width="237" height="9" font="font4" id="p2_t85" reading_order_no="83" segment_no="11" tag_type="text">use the maximum likelihood estimation by Amsaleg et al.</text>
<text top="403" left="307" width="193" height="9" font="font4" id="p2_t86" reading_order_no="84" segment_no="11" tag_type="text">( 2015 ) to calculate the LID estimates as follows:</text>
<text top="441" left="352" width="14" height="4" font="font24" id="p2_t87" reading_order_no="85" segment_no="14" tag_type="formula">d</text>
<text top="436" left="351" width="52" height="10" font="font4" id="p2_t88" reading_order_no="86" segment_no="14" tag_type="formula">LID ( x ) = −</text>
<text top="426" left="404" width="14" height="13" font="font24" id="p2_t89" reading_order_no="87" segment_no="14" tag_type="formula">" 1</text>
<text top="444" left="411" width="9" height="9" font="font17" id="p2_t90" reading_order_no="88" segment_no="14" tag_type="formula">m</text>
<text top="427" left="426" width="7" height="6" font="font19" id="p2_t91" reading_order_no="89" segment_no="14" tag_type="formula">m</text>
<text top="434" left="423" width="14" height="4" font="font24" id="p2_t92" reading_order_no="90" segment_no="14" tag_type="formula">X</text>
<text top="451" left="423" width="13" height="6" font="font19" id="p2_t93" reading_order_no="91" segment_no="14" tag_type="formula">i =1</text>
<text top="430" left="439" width="34" height="16" font="font18" id="p2_t94" reading_order_no="92" segment_no="14" tag_type="formula">ln r i ( x )</text>
<text top="444" left="450" width="26" height="9" font="font17" id="p2_t95" reading_order_no="93" segment_no="14" tag_type="formula">r m ( x )</text>
<text top="424" left="477" width="16" height="7" font="font24" id="p2_t96" reading_order_no="94" segment_no="14" tag_type="formula"># − 1</text>
<text top="437" left="495" width="3" height="9" font="font17" id="p2_t97" reading_order_no="95" segment_no="14" tag_type="formula">,</text>
<text top="437" left="530" width="12" height="9" font="font4" id="p2_t98" reading_order_no="96" segment_no="14" tag_type="text">(3)</text>
<text top="469" left="307" width="234" height="11" font="font4" id="p2_t99" reading_order_no="97" segment_no="16" tag_type="text">where r i ( x ) indicates the distance 3 between x and its i -th</text>
<text top="483" left="307" width="235" height="9" font="font4" id="p2_t100" reading_order_no="98" segment_no="16" tag_type="text">nearest neighbor. The number of the nearest neighbor m<a href="deeplearning_paper6.html#10">Rusu et al.</a></text>
<text top="495" left="307" width="234" height="9" font="font4" id="p2_t101" reading_order_no="99" segment_no="16" tag_type="text">should be chosen appropriately to make estimation local but<a href="deeplearning_paper6.html#10">(</a></text>
<text top="505" left="307" width="44" height="11" font="font4" id="p2_t102" reading_order_no="100" segment_no="16" tag_type="text">stabilized. 4<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="525" left="307" width="234" height="9" font="font4" id="p2_t103" reading_order_no="101" segment_no="17" tag_type="text">We refer to Amsaleg et al. ( 2015 ); Houle ( 2017a ; b ) for more<a href="deeplearning_paper6.html#10">);</a></text>
<text top="537" left="307" width="181" height="9" font="font4" id="p2_t104" reading_order_no="102" segment_no="17" tag_type="text">details about LID and its estimation methods.<a href="deeplearning_paper6.html#11">Wang et al.</a></text>
<text top="564" left="307" width="79" height="11" font="font3" id="p2_t105" reading_order_no="103" segment_no="19" tag_type="title">3. Methodology<a href="deeplearning_paper6.html#11">(</a></text>
<text top="585" left="307" width="236" height="9" font="font4" id="p2_t106" reading_order_no="104" segment_no="20" tag_type="text">Figure 1 illustrates the usage and overview of our method.<a href="deeplearning_paper6.html#11">2019</a></text>
<text top="597" left="307" width="236" height="9" font="font4" id="p2_t107" reading_order_no="105" segment_no="20" tag_type="text">As a plug-and-play module, our method provides task-<a href="deeplearning_paper6.html#11">);</a></text>
<text top="609" left="307" width="236" height="9" font="font4" id="p2_t108" reading_order_no="106" segment_no="20" tag_type="text">adapted embeddings to other few-shot classification meth-<a href="deeplearning_paper6.html#10">Hu et al.</a></text>
<text top="621" left="307" width="236" height="9" font="font4" id="p2_t109" reading_order_no="107" segment_no="20" tag_type="text">ods. Our method is mainly composed of feature recon-<a href="deeplearning_paper6.html#10">(</a></text>
<text top="633" left="307" width="234" height="9" font="font4" id="p2_t110" reading_order_no="108" segment_no="20" tag_type="text">struction training and LID-based early stopping. We will<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="645" left="307" width="236" height="9" font="font4" id="p2_t111" reading_order_no="109" segment_no="20" tag_type="text">explain feature reconstruction training (Section 3.1 ); LID-<a href="deeplearning_paper6.html#10">);</a></text>
<text top="657" left="307" width="236" height="9" font="font4" id="p2_t112" reading_order_no="110" segment_no="20" tag_type="text">based early stopping (Section 3.2 ); the overall method (Sec-<a href="deeplearning_paper6.html#11">Ziko et al.</a></text>
<text top="668" left="307" width="105" height="9" font="font4" id="p2_t113" reading_order_no="111" segment_no="20" tag_type="text">tion 3.3 ), in the following.<a href="deeplearning_paper6.html#11">(</a></text>
<text top="686" left="320" width="102" height="10" font="font10" id="p2_t114" reading_order_no="113" segment_no="22" tag_type="footnote">3 We use Euclidean distance.<a href="deeplearning_paper6.html#11">2020</a></text>
<text top="697" left="320" width="223" height="10" font="font10" id="p2_t115" reading_order_no="114" segment_no="23" tag_type="footnote">4 We set m = 20 throughout experiments as in Ma et al.<a href="deeplearning_paper6.html#11">). </a>We denote</text>
<text top="709" left="307" width="30" height="8" font="font10" id="p2_t116" reading_order_no="115" segment_no="23" tag_type="footnote">( 2018a ).</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font26" size="12" family="CambriaMath" color="#000000"/>
	<fontspec id="font27" size="8" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font28" size="7" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font29" size="9" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font30" size="8" family="CambriaMath" color="#000000"/>
	<fontspec id="font31" size="7" family="CambriaMath" color="#000000"/>
	<fontspec id="font32" size="18" family="Calibri" color="#000000"/>
	<fontspec id="font33" size="6" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font34" size="9" family="NimbusRomNo9L-Regu-Slant_167" color="#000000"/>
	<fontspec id="font35" size="5" family="CMMI5" color="#000000"/>
	<fontspec id="font36" size="7" family="Helvetica" color="#000000"/>
	<fontspec id="font37" size="7" family="Helvetica" color="#000000"/>
<text top="48" left="149" width="298" height="8" font="font16" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="139" left="224" width="6" height="12" font="font26" id="p3_t2" reading_order_no="16" segment_no="1" tag_type="figure">𝑧</text>
<text top="137" left="100" width="36" height="8" font="font27" id="p3_t3" reading_order_no="4" segment_no="1" tag_type="figure">Embedding<b>Embedding</b></text>
<text top="146" left="104" width="28" height="8" font="font27" id="p3_t4" reading_order_no="5" segment_no="1" tag_type="figure">Network<b>Network</b></text>
<text top="137" left="158" width="36" height="8" font="font27" id="p3_t5" reading_order_no="6" segment_no="1" tag_type="figure">Embedding<b>Embedding</b></text>
<text top="146" left="162" width="28" height="8" font="font27" id="p3_t6" reading_order_no="7" segment_no="1" tag_type="figure">Network<b>Network</b></text>
<text top="162" left="159" width="35" height="7" font="font28" id="p3_t7" reading_order_no="8" segment_no="1" tag_type="figure">Embedding<b>Embedding </b></text>
<text top="171" left="159" width="33" height="7" font="font28" id="p3_t8" reading_order_no="9" segment_no="1" tag_type="figure">Adaptation<b>Adaptation</b></text>
<text top="191" left="161" width="29" height="8" font="font27" id="p3_t9" reading_order_no="11" segment_no="1" tag_type="figure">Classifier<b>Classifier</b></text>
<text top="191" left="103" width="29" height="8" font="font27" id="p3_t10" reading_order_no="10" segment_no="1" tag_type="figure">Classifier<b>Classifier</b></text>
<text top="226" left="113" width="10" height="9" font="font29" id="p3_t11" reading_order_no="14" segment_no="1" tag_type="figure">(a)<b>(a)</b></text>
<text top="213" left="101" width="33" height="8" font="font27" id="p3_t12" reading_order_no="12" segment_no="1" tag_type="figure">Prediction<b>Prediction</b></text>
<text top="213" left="159" width="33" height="8" font="font27" id="p3_t13" reading_order_no="13" segment_no="1" tag_type="figure">Prediction<b>Prediction</b></text>
<text top="226" left="170" width="10" height="9" font="font29" id="p3_t14" reading_order_no="15" segment_no="1" tag_type="figure">(b)<b>(b)</b></text>
<text top="115" left="251" width="17" height="14" font="font26" id="p3_t15" reading_order_no="20" segment_no="1" tag_type="figure">𝑔 𝜙 0</text>
<text top="154" left="250" width="18" height="13" font="font26" id="p3_t16" reading_order_no="21" segment_no="1" tag_type="figure">ℒ 𝐹𝑅</text>
<text top="114" left="306" width="18" height="15" font="font26" id="p3_t17" reading_order_no="23" segment_no="1" tag_type="figure">𝑔 𝜙 1</text>
<text top="154" left="306" width="18" height="12" font="font26" id="p3_t18" reading_order_no="24" segment_no="1" tag_type="figure">ℒ 𝐹𝑅</text>
<text top="114" left="384" width="21" height="16" font="font26" id="p3_t19" reading_order_no="31" segment_no="1" tag_type="figure">𝑔 𝜙 𝑘∗</text>
<text top="108" left="349" width="12" height="18" font="font32" id="p3_t20" reading_order_no="25" segment_no="1" tag_type="figure">…</text>
<text top="147" left="349" width="12" height="18" font="font32" id="p3_t21" reading_order_no="27" segment_no="1" tag_type="figure">…</text>
<text top="70" left="261" width="24" height="7" font="font28" id="p3_t22" reading_order_no="17" segment_no="1" tag_type="figure">Feature</text>
<text top="79" left="250" width="44" height="7" font="font28" id="p3_t23" reading_order_no="18" segment_no="1" tag_type="figure">Reconstruction</text>
<text top="87" left="261" width="24" height="7" font="font28" id="p3_t24" reading_order_no="19" segment_no="1" tag_type="figure">Training</text>
<text top="70" left="380" width="46" height="7" font="font28" id="p3_t25" reading_order_no="28" segment_no="1" tag_type="figure">Dimensionality-</text>
<text top="78" left="385" width="36" height="7" font="font28" id="p3_t26" reading_order_no="29" segment_no="1" tag_type="figure">driven Early</text>
<text top="87" left="390" width="26" height="7" font="font28" id="p3_t27" reading_order_no="30" segment_no="1" tag_type="figure">Stopping</text>
<text top="149" left="391" width="14" height="6" font="font33" id="p3_t28" reading_order_no="32" segment_no="1" tag_type="figure">New</text>
<text top="157" left="379" width="36" height="6" font="font33" id="p3_t29" reading_order_no="33" segment_no="1" tag_type="figure">Task-adapted</text>
<text top="164" left="382" width="32" height="6" font="font33" id="p3_t30" reading_order_no="34" segment_no="1" tag_type="figure">Embeddings<b>Feature </b></text>
<text top="110" left="427" width="12" height="18" font="font32" id="p3_t31" reading_order_no="36" segment_no="1" tag_type="figure">…<b>Reconstruction</b></text>
<text top="149" left="427" width="12" height="18" font="font32" id="p3_t32" reading_order_no="37" segment_no="1" tag_type="figure">…<b>Training</b></text>
<text top="213" left="299" width="80" height="8" font="font27" id="p3_t33" reading_order_no="42" segment_no="1" tag_type="figure">Shared Features Learning<b>Dimensionality-</b></text>
<text top="200" left="380" width="95" height="8" font="font27" id="p3_t34" reading_order_no="41" segment_no="1" tag_type="figure">Non-shared Features Learning<b>driven Early </b></text>
<text top="70" left="103" width="89" height="8" font="font27" id="p3_t35" reading_order_no="1" segment_no="1" tag_type="figure">Few-Shot Classification Task<b>Stopping</b></text>
<text top="136" left="335" width="23" height="8" font="font27" id="p3_t36" reading_order_no="26" segment_no="1" tag_type="figure">update<b>New </b></text>
<text top="136" left="277" width="23" height="8" font="font27" id="p3_t37" reading_order_no="22" segment_no="1" tag_type="figure">update<b>Task-adapted</b></text>
<text top="84" left="103" width="37" height="8" font="font27" id="p3_t38" reading_order_no="2" segment_no="1" tag_type="figure">Support Set<b>Embeddings</b></text>
<text top="84" left="158" width="31" height="8" font="font27" id="p3_t39" reading_order_no="3" segment_no="1" tag_type="figure">Query Set</text>
<text top="115" left="462" width="20" height="14" font="font26" id="p3_t40" reading_order_no="38" segment_no="1" tag_type="figure">𝑔 𝜙 ∞</text>
<text top="152" left="462" width="22" height="6" font="font33" id="p3_t41" reading_order_no="39" segment_no="1" tag_type="figure">Original<b>Shared Features Learning</b></text>
<text top="160" left="456" width="32" height="6" font="font33" id="p3_t42" reading_order_no="40" segment_no="1" tag_type="figure">Embeddings<b>Non-shared Features Learning</b></text>
<text top="110" left="421" width="10" height="8" font="font27" id="p3_t43" reading_order_no="35" segment_no="1" tag_type="figure">LID<b>Few-Shot Classification Task</b></text>
<text top="245" left="55" width="486" height="8" font="font34" id="p3_t44" reading_order_no="43" segment_no="2" tag_type="text">Figure 1. Overview of our method. 1-(a) shows the case without embedding adaptation, and 1-(b) shows the case with embedding<b>update</b></text>
<text top="256" left="55" width="487" height="8" font="font10" id="p3_t45" reading_order_no="44" segment_no="2" tag_type="text">adaptation. Our scheme mainly consists of feature reconstruction training and dimensionality-driven early stopping , and provides new<b>update</b></text>
<text top="267" left="55" width="258" height="8" font="font10" id="p3_t46" reading_order_no="45" segment_no="2" tag_type="text">embeddings of generalizable features for the downstream few-shot task.<b>Support Set</b></text>
<text top="292" left="55" width="117" height="9" font="font1" id="p3_t47" reading_order_no="46" segment_no="4" tag_type="title">3.1. Feature Reconstruction<b>Query Set</b></text>
<text top="311" left="55" width="234" height="9" font="font4" id="p3_t48" reading_order_no="47" segment_no="5" tag_type="text">Our main idea is based on the findings that deep neural</text>
<text top="322" left="55" width="234" height="10" font="font5" id="p3_t49" reading_order_no="48" segment_no="5" tag_type="text">networks learn to generalize before memorizing to abstract</text>
<text top="334" left="55" width="234" height="9" font="font4" id="p3_t50" reading_order_no="49" segment_no="5" tag_type="text">task-useful features. We design feature-level reconstruction</text>
<text top="346" left="55" width="234" height="9" font="font4" id="p3_t51" reading_order_no="50" segment_no="5" tag_type="text">training, which is unsupervised learning and builds on prior<b>Original </b></text>
<text top="358" left="55" width="236" height="9" font="font4" id="p3_t52" reading_order_no="51" segment_no="5" tag_type="text">knowledge given by embeddings. As mentioned before, un-<b>Embeddings</b></text>
<text top="370" left="55" width="234" height="9" font="font4" id="p3_t53" reading_order_no="52" segment_no="5" tag_type="text">supervised learning for few-shot adaptation tends to have<b>LID</b></text>
<text top="382" left="55" width="234" height="9" font="font4" id="p3_t54" reading_order_no="53" segment_no="5" tag_type="text">less attention; and we find that naively applied unsupervised</text>
<text top="393" left="55" width="234" height="10" font="font4" id="p3_t55" reading_order_no="54" segment_no="5" tag_type="text">learning for few-shot adaptation often fails. 5 Moreover, the</text>
<text top="406" left="55" width="236" height="9" font="font4" id="p3_t56" reading_order_no="55" segment_no="5" tag_type="text">behavior of unsupervised learning with a pre-trained embed-</text>
<text top="418" left="55" width="234" height="9" font="font4" id="p3_t57" reading_order_no="56" segment_no="5" tag_type="text">ding network is often ambiguous. For instance, contrastive</text>
<text top="430" left="55" width="236" height="9" font="font4" id="p3_t58" reading_order_no="57" segment_no="5" tag_type="text">learning heavily relies on augmentations, while augmenta-</text>
<text top="442" left="55" width="234" height="9" font="font4" id="p3_t59" reading_order_no="58" segment_no="5" tag_type="text">tions affect embeddings differently depending on how the</text>
<text top="454" left="55" width="234" height="9" font="font4" id="p3_t60" reading_order_no="59" segment_no="5" tag_type="text">embedding network is pre-trained. Instead, we find that our</text>
<text top="466" left="55" width="236" height="9" font="font4" id="p3_t61" reading_order_no="60" segment_no="5" tag_type="text">feature reconstruction training can be used to adapt embed-</text>
<text top="478" left="55" width="130" height="9" font="font4" id="p3_t62" reading_order_no="61" segment_no="5" tag_type="text">dings for few-shot classification.</text>
<text top="496" left="55" width="236" height="9" font="font4" id="p3_t63" reading_order_no="62" segment_no="7" tag_type="text">We explain our feature reconstruction training. For a few-</text>
<text top="508" left="55" width="234" height="9" font="font4" id="p3_t64" reading_order_no="63" segment_no="7" tag_type="text">shot classification task with embedding support set S f and</text>
<text top="520" left="55" width="234" height="9" font="font4" id="p3_t65" reading_order_no="64" segment_no="7" tag_type="text">query set Q f ; we train a reconstruction module g φ using the</text>
<text top="532" left="55" width="170" height="9" font="font4" id="p3_t66" reading_order_no="65" segment_no="7" tag_type="text">following feature level reconstruction loss:</text>
<text top="557" left="86" width="30" height="10" font="font6" id="p3_t67" reading_order_no="66" segment_no="9" tag_type="formula">L ( φ ) =</text>
<text top="551" left="139" width="5" height="9" font="font18" id="p3_t68" reading_order_no="67" segment_no="9" tag_type="formula">1</text>
<text top="564" left="121" width="41" height="10" font="font6" id="p3_t69" reading_order_no="68" segment_no="9" tag_type="formula">| S f ∪ Q f |</text>
<text top="555" left="175" width="14" height="4" font="font24" id="p3_t70" reading_order_no="69" segment_no="9" tag_type="formula">X</text>
<text top="571" left="165" width="33" height="7" font="font19" id="p3_t71" reading_order_no="70" segment_no="9" tag_type="formula">z ∈ S f ∪ Q f</text>
<text top="558" left="201" width="58" height="9" font="font17" id="p3_t72" reading_order_no="71" segment_no="9" tag_type="formula">d cos ( z, g φ ( z )) ,<a href="deeplearning_paper6.html#3">.</a></text>
<text top="558" left="278" width="12" height="9" font="font4" id="p3_t73" reading_order_no="72" segment_no="9" tag_type="text">(4)<a href="deeplearning_paper6.html#3">5</a></text>
<text top="591" left="55" width="235" height="9" font="font4" id="p3_t74" reading_order_no="73" segment_no="10" tag_type="text">where d cos denotes the cosine distance. Both z and g φ ( z ) are</text>
<text top="601" left="55" width="234" height="11" font="font4" id="p3_t75" reading_order_no="74" segment_no="10" tag_type="text">preprocessed 6 embeddings, but their expressions are omitted</text>
<text top="615" left="55" width="236" height="9" font="font4" id="p3_t76" reading_order_no="75" segment_no="10" tag_type="text">for notational simplicity. We note that for a newly given few-</text>
<text top="626" left="55" width="234" height="10" font="font4" id="p3_t77" reading_order_no="76" segment_no="10" tag_type="text">shot classification task, the weight φ of the reconstruction</text>
<text top="639" left="55" width="136" height="9" font="font4" id="p3_t78" reading_order_no="77" segment_no="10" tag_type="text">module is randomly re-initialized.</text>
<text top="657" left="55" width="236" height="9" font="font4" id="p3_t79" reading_order_no="78" segment_no="12" tag_type="text">We investigate the behavior of reconstruction modules dur-</text>
<text top="668" left="55" width="234" height="10" font="font4" id="p3_t80" reading_order_no="79" segment_no="12" tag_type="text">ing feature reconstruction training w.r.t. the downstream</text>
<text top="686" left="68" width="222" height="10" font="font10" id="p3_t81" reading_order_no="132" segment_no="13" tag_type="footnote">5 We test self-supervised learning models of rotation ( Gidaris</text>
<text top="698" left="55" width="234" height="8" font="font25" id="p3_t82" reading_order_no="133" segment_no="13" tag_type="footnote">et al. , 2018 ) and jigsaw ( Noroozi &amp; Favaro , 2016 ) in Appendix E.</text>
<text top="707" left="68" width="174" height="10" font="font10" id="p3_t83" reading_order_no="134" segment_no="14" tag_type="footnote">6 Details are described in the experiment section.</text>
<text top="394" left="333" width="4" height="7" font="font36" id="p3_t84" reading_order_no="86" segment_no="3" tag_type="figure">0</text>
<text top="394" left="365" width="8" height="7" font="font36" id="p3_t85" reading_order_no="87" segment_no="3" tag_type="figure">20</text>
<text top="394" left="400" width="8" height="7" font="font36" id="p3_t86" reading_order_no="88" segment_no="3" tag_type="figure">40</text>
<text top="402" left="365" width="27" height="7" font="font36" id="p3_t87" reading_order_no="89" segment_no="3" tag_type="figure">Iteration</text>
<text top="383" left="320" width="8" height="7" font="font36" id="p3_t88" reading_order_no="85" segment_no="3" tag_type="figure">50</text>
<text top="358" left="320" width="8" height="7" font="font36" id="p3_t89" reading_order_no="84" segment_no="3" tag_type="figure">55</text>
<text top="334" left="320" width="8" height="7" font="font36" id="p3_t90" reading_order_no="83" segment_no="3" tag_type="figure">60</text>
<text top="309" left="320" width="8" height="7" font="font36" id="p3_t91" reading_order_no="82" segment_no="3" tag_type="figure">65</text>
<text top="285" left="320" width="8" height="7" font="font36" id="p3_t92" reading_order_no="81" segment_no="3" tag_type="figure">70</text>
<text top="356" left="316" width="0" height="7" font="font37" id="p3_t93" reading_order_no="80" segment_no="3" tag_type="figure">Accuracy (%)</text>
<text top="345" left="390" width="9" height="7" font="font36" id="p3_t94" reading_order_no="90" segment_no="3" tag_type="figure">A0</text>
<text top="356" left="390" width="9" height="7" font="font36" id="p3_t95" reading_order_no="91" segment_no="3" tag_type="figure">A1</text>
<text top="366" left="390" width="9" height="7" font="font36" id="p3_t96" reading_order_no="92" segment_no="3" tag_type="figure">B1</text>
<text top="377" left="390" width="28" height="7" font="font36" id="p3_t97" reading_order_no="93" segment_no="3" tag_type="figure">Baseline</text>
<text top="394" left="446" width="4" height="7" font="font36" id="p3_t98" reading_order_no="100" segment_no="3" tag_type="figure">0</text>
<text top="394" left="478" width="8" height="7" font="font36" id="p3_t99" reading_order_no="101" segment_no="3" tag_type="figure">20</text>
<text top="394" left="513" width="8" height="7" font="font36" id="p3_t100" reading_order_no="102" segment_no="3" tag_type="figure">40</text>
<text top="402" left="478" width="27" height="7" font="font36" id="p3_t101" reading_order_no="103" segment_no="3" tag_type="figure">Iteration</text>
<text top="388" left="432" width="8" height="7" font="font36" id="p3_t102" reading_order_no="99" segment_no="3" tag_type="figure">72</text>
<text top="367" left="432" width="8" height="7" font="font36" id="p3_t103" reading_order_no="98" segment_no="3" tag_type="figure">74</text>
<text top="347" left="432" width="8" height="7" font="font36" id="p3_t104" reading_order_no="97" segment_no="3" tag_type="figure">76</text>
<text top="326" left="432" width="8" height="7" font="font36" id="p3_t105" reading_order_no="96" segment_no="3" tag_type="figure">78</text>
<text top="305" left="432" width="8" height="7" font="font36" id="p3_t106" reading_order_no="95" segment_no="3" tag_type="figure">80</text>
<text top="285" left="432" width="8" height="7" font="font36" id="p3_t107" reading_order_no="94" segment_no="3" tag_type="figure">82</text>
<text top="345" left="503" width="9" height="7" font="font36" id="p3_t108" reading_order_no="104" segment_no="3" tag_type="figure">A0</text>
<text top="356" left="503" width="9" height="7" font="font36" id="p3_t109" reading_order_no="105" segment_no="3" tag_type="figure">A1</text>
<text top="366" left="503" width="9" height="7" font="font36" id="p3_t110" reading_order_no="106" segment_no="3" tag_type="figure">B1</text>
<text top="377" left="503" width="28" height="7" font="font36" id="p3_t111" reading_order_no="107" segment_no="3" tag_type="figure">Baseline</text>
<text top="416" left="307" width="235" height="9" font="font34" id="p3_t112" reading_order_no="108" segment_no="6" tag_type="text">Figure 2. Few-shot classification accuracy with A0 , A1 , and B1</text>
<text top="427" left="307" width="235" height="8" font="font10" id="p3_t113" reading_order_no="109" segment_no="6" tag_type="text">during feature reconstruction training. We use ResNet-18 back-</text>
<text top="438" left="307" width="234" height="9" font="font10" id="p3_t114" reading_order_no="110" segment_no="6" tag_type="text">bone, mini -ImageNet dataset, and nearest neighbor classifier. For</text>
<text top="449" left="307" width="236" height="9" font="font11" id="p3_t115" reading_order_no="111" segment_no="6" tag_type="text">g φ 1 , we use a 4-layer neural network with 256-128-256-512 units.</text>
<text top="460" left="307" width="234" height="8" font="font10" id="p3_t116" reading_order_no="112" segment_no="6" tag_type="text">Other settings are described in the experiment section. (left) shows</text>
<text top="471" left="307" width="234" height="8" font="font10" id="p3_t117" reading_order_no="113" segment_no="6" tag_type="text">the accuracy on 1-shot and (right) shows the accuracy on 5-shot</text>
<text top="482" left="307" width="26" height="8" font="font10" id="p3_t118" reading_order_no="114" segment_no="6" tag_type="text">setting.</text>
<text top="511" left="307" width="236" height="9" font="font4" id="p3_t119" reading_order_no="115" segment_no="8" tag_type="text">few-shot task. We train two types of reconstruction mod-</text>
<text top="523" left="307" width="235" height="10" font="font4" id="p3_t120" reading_order_no="116" segment_no="8" tag_type="text">ules, g φ 1 with compressed (bottleneck) hidden layer as in</text>
<text top="535" left="307" width="236" height="10" font="font4" id="p3_t121" reading_order_no="117" segment_no="8" tag_type="text">conventional auto-encoders, and g φ 2 ; without compression.</text>
<text top="547" left="307" width="236" height="9" font="font4" id="p3_t122" reading_order_no="118" segment_no="8" tag_type="text">We evaluate few-shot classification accuracy with three em-</text>
<text top="559" left="307" width="234" height="9" font="font4" id="p3_t123" reading_order_no="119" segment_no="8" tag_type="text">beddings A0 , A1 , B1 ; where A0 is the middle compressed</text>
<text top="571" left="307" width="234" height="10" font="font4" id="p3_t124" reading_order_no="120" segment_no="8" tag_type="text">hidden layer output of g φ 1 ; A1 and B1 are the reconstructed</text>
<text top="583" left="307" width="141" height="10" font="font4" id="p3_t125" reading_order_no="121" segment_no="8" tag_type="text">output of g φ 1 and g φ 2 , respectively.</text>
<text top="601" left="307" width="236" height="9" font="font4" id="p3_t126" reading_order_no="122" segment_no="11" tag_type="text">Figure 2 shows an interesting behavior that few-shot classi-</text>
<text top="613" left="307" width="236" height="9" font="font4" id="p3_t127" reading_order_no="123" segment_no="11" tag_type="text">fication accuracies, with embeddings of reconstruction mod-</text>
<text top="625" left="307" width="234" height="9" font="font4" id="p3_t128" reading_order_no="124" segment_no="11" tag_type="text">ules, initially increase then decrease. Moreover, the peak</text>
<text top="636" left="307" width="234" height="10" font="font4" id="p3_t129" reading_order_no="125" segment_no="11" tag_type="text">accuracy of B1 exceeds the baseline accuracy of the original</text>
<text top="649" left="307" width="234" height="9" font="font4" id="p3_t130" reading_order_no="126" segment_no="11" tag_type="text">embedding, on both 1- and 5-shot settings. We believe that</text>
<text top="660" left="307" width="234" height="9" font="font4" id="p3_t131" reading_order_no="127" segment_no="11" tag_type="text">recent studies of Deep Neural Networks (DNNs) explain</text>
<text top="672" left="307" width="234" height="9" font="font4" id="p3_t132" reading_order_no="128" segment_no="11" tag_type="text">such behavior. Several works ( Arpit et al. , 2017 ; Lampinen</text>
<text top="684" left="307" width="237" height="9" font="font8" id="p3_t133" reading_order_no="129" segment_no="11" tag_type="text">&amp; Ganguli , 2019 ; Stephenson et al. , 2021 ) observe a prop-</text>
<text top="696" left="307" width="234" height="9" font="font4" id="p3_t134" reading_order_no="130" segment_no="11" tag_type="text">erty that DNNs learn to generalize before memorizing. To</text>
<text top="708" left="307" width="235" height="9" font="font4" id="p3_t135" reading_order_no="131" segment_no="11" tag_type="text">be more specific, Arpit et al. ( 2017 ) reports that DNNs learn</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font38" size="10" family="MSBM10" color="#000000"/>
	<fontspec id="font39" size="9" family="CMSY9" color="#000000"/>
<text top="48" left="149" width="298" height="8" font="font16" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="70" left="55" width="234" height="9" font="font4" id="p4_t2" reading_order_no="1" segment_no="2" tag_type="text">patterns first before memorization, with experiments on the</text>
<text top="82" left="55" width="234" height="9" font="font4" id="p4_t3" reading_order_no="2" segment_no="2" tag_type="text">mixture of well-structured real data and noisy data. Further</text>
<text top="94" left="55" width="237" height="9" font="font4" id="p4_t4" reading_order_no="3" segment_no="2" tag_type="text">research Lampinen &amp; Ganguli ( 2019 ); Stephenson et al.</text>
<text top="106" left="55" width="234" height="9" font="font4" id="p4_t5" reading_order_no="4" segment_no="2" tag_type="text">( 2021 ) provide analytical explanations on the property that<a href="deeplearning_paper6.html#10">Lampinen &amp; Ganguli</a></text>
<text top="118" left="55" width="236" height="9" font="font4" id="p4_t6" reading_order_no="5" segment_no="2" tag_type="text">learning speeds between generalization of patterns and mem-<a href="deeplearning_paper6.html#10">(</a></text>
<text top="128" left="55" width="235" height="11" font="font4" id="p4_t7" reading_order_no="6" segment_no="2" tag_type="text">orization of noise are different. 7 We argue that the behavior,<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="142" left="55" width="234" height="9" font="font4" id="p4_t8" reading_order_no="7" segment_no="2" tag_type="text">shown in Figure 2 , is a result of the DNNs’ property. By the<a href="deeplearning_paper6.html#10">);</a></text>
<text top="154" left="55" width="234" height="9" font="font4" id="p4_t9" reading_order_no="8" segment_no="2" tag_type="text">property, reconstruction modules learn shared features faster<a href="deeplearning_paper6.html#10">Stephenson et al.</a></text>
<text top="166" left="55" width="235" height="9" font="font4" id="p4_t10" reading_order_no="9" segment_no="2" tag_type="text">since they form certain patterns or correlations among data,<a href="deeplearning_paper6.html#10">(</a></text>
<text top="178" left="55" width="234" height="9" font="font4" id="p4_t11" reading_order_no="10" segment_no="2" tag_type="text">while non-shared features are learned later since they are<a href="deeplearning_paper6.html#10">2021</a></text>
<text top="190" left="55" width="234" height="9" font="font4" id="p4_t12" reading_order_no="11" segment_no="2" tag_type="text">less generalizable. Generalizable shared features are more<a href="deeplearning_paper6.html#10">) </a>provide analytical explanations on the property that</text>
<text top="202" left="55" width="234" height="9" font="font4" id="p4_t13" reading_order_no="12" segment_no="2" tag_type="text">likely to be task-relevant in classification; the difference</text>
<text top="214" left="55" width="234" height="9" font="font4" id="p4_t14" reading_order_no="13" segment_no="2" tag_type="text">in learning speeds between shared features and non-shared<a href="deeplearning_paper6.html#4">erent.</a></text>
<text top="226" left="55" width="200" height="9" font="font4" id="p4_t15" reading_order_no="14" segment_no="2" tag_type="text">features explains the initial increase of accuracies.<a href="deeplearning_paper6.html#4">7</a></text>
<text top="244" left="55" width="234" height="9" font="font4" id="p4_t16" reading_order_no="15" segment_no="7" tag_type="text">Our main idea is to use the behavior shown in Figure 2</text>
<text top="256" left="55" width="236" height="9" font="font4" id="p4_t17" reading_order_no="16" segment_no="7" tag_type="text">for embedding adaptation. The behavior provides an oppor-</text>
<text top="268" left="55" width="236" height="9" font="font4" id="p4_t18" reading_order_no="17" segment_no="7" tag_type="text">tunity to achieve improved few-shot classification perfor-<a href="deeplearning_paper6.html#3">2</a></text>
<text top="279" left="55" width="236" height="9" font="font4" id="p4_t19" reading_order_no="18" segment_no="7" tag_type="text">mance by acquiring new embeddings of generalized features.<a href="deeplearning_paper6.html#3">, </a>is a result of the DNNs’ property. By the</text>
<text top="291" left="55" width="234" height="9" font="font4" id="p4_t20" reading_order_no="19" segment_no="7" tag_type="text">To do this, we find that non-compressed and non-encoded</text>
<text top="303" left="55" width="235" height="9" font="font4" id="p4_t21" reading_order_no="20" segment_no="7" tag_type="text">embedding ( B1 ) performs the best. Throughout the rest of</text>
<text top="315" left="55" width="237" height="10" font="font4" id="p4_t22" reading_order_no="21" segment_no="7" tag_type="text">our work, we use the reconstructed output g φ that is non-</text>
<text top="327" left="55" width="164" height="9" font="font4" id="p4_t23" reading_order_no="22" segment_no="7" tag_type="text">compressed as task-adapted embeddings.</text>
<text top="464" left="79" width="4" height="6" font="font36" id="p4_t24" reading_order_no="28" segment_no="8" tag_type="figure">0</text>
<text top="464" left="111" width="8" height="6" font="font36" id="p4_t25" reading_order_no="29" segment_no="8" tag_type="figure">20</text>
<text top="464" left="145" width="8" height="6" font="font36" id="p4_t26" reading_order_no="30" segment_no="8" tag_type="figure">40</text>
<text top="472" left="111" width="25" height="6" font="font36" id="p4_t27" reading_order_no="31" segment_no="8" tag_type="figure">Iteration</text>
<text top="459" left="67" width="8" height="6" font="font36" id="p4_t28" reading_order_no="27" segment_no="8" tag_type="figure">50<a href="deeplearning_paper6.html#3">2</a></text>
<text top="432" left="67" width="8" height="6" font="font36" id="p4_t29" reading_order_no="26" segment_no="8" tag_type="figure">60</text>
<text top="405" left="67" width="8" height="6" font="font36" id="p4_t30" reading_order_no="25" segment_no="8" tag_type="figure">70</text>
<text top="378" left="67" width="8" height="6" font="font36" id="p4_t31" reading_order_no="24" segment_no="8" tag_type="figure">80</text>
<text top="429" left="63" width="0" height="6" font="font37" id="p4_t32" reading_order_no="23" segment_no="8" tag_type="figure">Accuracy (%)</text>
<text top="464" left="177" width="4" height="6" font="font36" id="p4_t33" reading_order_no="36" segment_no="8" tag_type="figure">0</text>
<text top="464" left="209" width="8" height="6" font="font36" id="p4_t34" reading_order_no="37" segment_no="8" tag_type="figure">20</text>
<text top="464" left="243" width="8" height="6" font="font36" id="p4_t35" reading_order_no="38" segment_no="8" tag_type="figure">40</text>
<text top="472" left="209" width="25" height="6" font="font36" id="p4_t36" reading_order_no="39" segment_no="8" tag_type="figure">Iteration</text>
<text top="373" left="128" width="11" height="6" font="font36" id="p4_t37" reading_order_no="32" segment_no="8" tag_type="figure">Acc</text>
<text top="383" left="128" width="36" height="6" font="font36" id="p4_t38" reading_order_no="33" segment_no="8" tag_type="figure">Acc:dropout</text>
<text top="392" left="128" width="11" height="6" font="font36" id="p4_t39" reading_order_no="34" segment_no="8" tag_type="figure">LID</text>
<text top="402" left="128" width="36" height="6" font="font36" id="p4_t40" reading_order_no="35" segment_no="8" tag_type="figure">LID:dropout</text>
<text top="450" left="271" width="4" height="6" font="font36" id="p4_t41" reading_order_no="49" segment_no="8" tag_type="figure">8</text>
<text top="433" left="271" width="8" height="6" font="font36" id="p4_t42" reading_order_no="48" segment_no="8" tag_type="figure">10</text>
<text top="416" left="271" width="8" height="6" font="font36" id="p4_t43" reading_order_no="47" segment_no="8" tag_type="figure">12</text>
<text top="400" left="271" width="8" height="6" font="font36" id="p4_t44" reading_order_no="46" segment_no="8" tag_type="figure">14</text>
<text top="383" left="271" width="8" height="6" font="font36" id="p4_t45" reading_order_no="45" segment_no="8" tag_type="figure">16</text>
<text top="366" left="271" width="8" height="6" font="font36" id="p4_t46" reading_order_no="44" segment_no="8" tag_type="figure">18</text>
<text top="449" left="285" width="0" height="6" font="font37" id="p4_t47" reading_order_no="50" segment_no="8" tag_type="figure">LID (2nd last hidden layer)</text>
<text top="420" left="225" width="11" height="6" font="font36" id="p4_t48" reading_order_no="40" segment_no="8" tag_type="figure">Acc</text>
<text top="429" left="225" width="36" height="6" font="font36" id="p4_t49" reading_order_no="41" segment_no="8" tag_type="figure">Acc:dropout</text>
<text top="439" left="225" width="11" height="6" font="font36" id="p4_t50" reading_order_no="42" segment_no="8" tag_type="figure">LID</text>
<text top="449" left="225" width="36" height="6" font="font36" id="p4_t51" reading_order_no="43" segment_no="8" tag_type="figure">LID:dropout</text>
<text top="486" left="55" width="235" height="8" font="font34" id="p4_t52" reading_order_no="51" segment_no="11" tag_type="text">Figure 3. The figure shows the accuracy and the LID curve during</text>
<text top="497" left="55" width="235" height="8" font="font10" id="p4_t53" reading_order_no="52" segment_no="11" tag_type="text">feature reconstruction training, with (Acc:dropout, LID:dropout)</text>
<text top="508" left="55" width="234" height="8" font="font10" id="p4_t54" reading_order_no="53" segment_no="11" tag_type="text">and without (Acc, LID) dropout perturbation. We use the same</text>
<text top="519" left="55" width="234" height="8" font="font10" id="p4_t55" reading_order_no="54" segment_no="11" tag_type="text">setting as in Figure 2 . The vertical lines indicate the maximum</text>
<text top="530" left="55" width="234" height="8" font="font10" id="p4_t56" reading_order_no="55" segment_no="11" tag_type="text">accuracy (blue) and the minimum LID (red) points; for 1-shot, they</text>
<text top="541" left="55" width="235" height="8" font="font10" id="p4_t57" reading_order_no="56" segment_no="11" tag_type="text">are overlapped. (left) shows the results in the 1-shot and (right)</text>
<text top="552" left="55" width="137" height="8" font="font10" id="p4_t58" reading_order_no="57" segment_no="11" tag_type="text">shows the results in the 5-shot setting.</text>
<text top="594" left="55" width="236" height="9" font="font4" id="p4_t59" reading_order_no="58" segment_no="14" tag_type="text">We propose to perturb the training to further discard the non-</text>
<text top="606" left="55" width="236" height="9" font="font4" id="p4_t60" reading_order_no="59" segment_no="14" tag_type="text">shared features. Perturbation with noise does not form gen-</text>
<text top="618" left="55" width="234" height="9" font="font4" id="p4_t61" reading_order_no="60" segment_no="14" tag_type="text">eralizable patterns or correlations; thus, perturbation tends</text>
<text top="630" left="55" width="234" height="9" font="font4" id="p4_t62" reading_order_no="61" segment_no="14" tag_type="text">to make training harder for less generalizable non-shared</text>
<text top="642" left="55" width="234" height="9" font="font4" id="p4_t63" reading_order_no="62" segment_no="14" tag_type="text">features while shared features are still learnable by pattern</text>
<text top="654" left="55" width="234" height="9" font="font4" id="p4_t64" reading_order_no="63" segment_no="14" tag_type="text">learning. While additive perturbation is hard to design, due</text>
<text top="665" left="55" width="236" height="9" font="font4" id="p4_t65" reading_order_no="64" segment_no="14" tag_type="text">to the unknown distribution of z , we find that multiplica-</text>
<text top="677" left="55" width="235" height="9" font="font4" id="p4_t66" reading_order_no="65" segment_no="14" tag_type="text">tive perturbation by dropout ( Srivastava et al. , 2014 ) is well</text>
<text top="689" left="55" width="234" height="9" font="font4" id="p4_t67" reading_order_no="66" segment_no="14" tag_type="text">suited for our purpose. Our feature reconstruction training</text>
<text top="707" left="68" width="159" height="10" font="font10" id="p4_t68" reading_order_no="145" segment_no="15" tag_type="footnote">7 Generalization is faster than memorization.</text>
<text top="69" left="307" width="210" height="10" font="font4" id="p4_t69" reading_order_no="67" segment_no="1" tag_type="text">with input dropout uses the following loss function 8 :</text>
<text top="97" left="312" width="40" height="10" font="font6" id="p4_t70" reading_order_no="68" segment_no="3" tag_type="formula">L FR ( φ ) =</text>
<text top="91" left="374" width="5" height="9" font="font18" id="p4_t71" reading_order_no="69" segment_no="3" tag_type="formula">1</text>
<text top="104" left="356" width="42" height="10" font="font6" id="p4_t72" reading_order_no="70" segment_no="3" tag_type="formula">| S f ∪ Q f |</text>
<text top="94" left="411" width="14" height="4" font="font24" id="p4_t73" reading_order_no="71" segment_no="3" tag_type="formula">X</text>
<text top="111" left="400" width="34" height="7" font="font19" id="p4_t74" reading_order_no="72" segment_no="3" tag_type="formula">z ∈ S f ∪ Q f<a href="deeplearning_paper6.html#3">2</a></text>
<text top="97" left="437" width="105" height="11" font="font38" id="p4_t75" reading_order_no="73" segment_no="3" tag_type="formula">E µ [ d cos ( z, g φ ( z ◦ µ ))] , (5)<a href="deeplearning_paper6.html#3">. </a>The vertical lines indicate the maximum</text>
<text top="131" left="307" width="236" height="9" font="font4" id="p4_t76" reading_order_no="74" segment_no="4" tag_type="text">where µ is a multiplicative noise implemented with dropout.</text>
<text top="143" left="307" width="235" height="9" font="font4" id="p4_t77" reading_order_no="75" segment_no="4" tag_type="text">Figure 3 shows the effect of dropout on the training curve;</text>
<text top="155" left="307" width="234" height="9" font="font4" id="p4_t78" reading_order_no="76" segment_no="4" tag_type="text">we can observe that the dropout perturbation results in</text>
<text top="167" left="307" width="93" height="9" font="font4" id="p4_t79" reading_order_no="77" segment_no="4" tag_type="text">higher peak accuracies.</text>
<text top="192" left="307" width="180" height="9" font="font1" id="p4_t80" reading_order_no="78" segment_no="5" tag_type="title">3.2. Dimensionality Driven Early Stopping</text>
<text top="211" left="307" width="234" height="9" font="font4" id="p4_t81" reading_order_no="79" segment_no="6" tag_type="text">Utilization of early retained generalizable features seems a</text>
<text top="223" left="307" width="236" height="9" font="font4" id="p4_t82" reading_order_no="80" segment_no="6" tag_type="text">promising idea for the downstream few-shot classification.</text>
<text top="235" left="307" width="234" height="9" font="font4" id="p4_t83" reading_order_no="81" segment_no="6" tag_type="text">However, determining the optimal early stopping time is</text>
<text top="247" left="307" width="234" height="9" font="font4" id="p4_t84" reading_order_no="82" segment_no="6" tag_type="text">not straightforward. Here, we suggest using Local Intrinsic</text>
<text top="259" left="307" width="236" height="9" font="font4" id="p4_t85" reading_order_no="83" segment_no="6" tag_type="text">Dimensionality (LID) as an early stopping criterion. Re-</text>
<text top="271" left="307" width="234" height="9" font="font4" id="p4_t86" reading_order_no="84" segment_no="6" tag_type="text">cent work by Ma et al. ( 2018b ) used LID to monitor the</text>
<text top="283" left="307" width="236" height="9" font="font4" id="p4_t87" reading_order_no="85" segment_no="6" tag_type="text">internal generalization and memorization during training.</text>
<text top="295" left="307" width="234" height="9" font="font4" id="p4_t88" reading_order_no="86" segment_no="6" tag_type="text">They argued that LID tends to decrease while DNNs learn<a href="deeplearning_paper6.html#10">(</a></text>
<text top="307" left="307" width="236" height="9" font="font4" id="p4_t89" reading_order_no="87" segment_no="6" tag_type="text">to generalize, due to summarizing effect; LID tends to in-<a href="deeplearning_paper6.html#10">Srivastava et al.</a></text>
<text top="319" left="307" width="234" height="9" font="font4" id="p4_t90" reading_order_no="88" segment_no="6" tag_type="text">crease while memorizing, as DNNs try to encode detailed<a href="deeplearning_paper6.html#10">,</a></text>
<text top="331" left="307" width="234" height="9" font="font4" id="p4_t91" reading_order_no="89" segment_no="6" tag_type="text">information individually. We observe the tendency in our<a href="deeplearning_paper6.html#10">2014</a></text>
<text top="342" left="307" width="234" height="9" font="font4" id="p4_t92" reading_order_no="90" segment_no="6" tag_type="text">reconstruction training, and we find that LID can be used as<a href="deeplearning_paper6.html#10">) </a>is well</text>
<text top="354" left="307" width="107" height="9" font="font4" id="p4_t93" reading_order_no="91" segment_no="6" tag_type="text">an early stopping criterion.</text>
<text top="372" left="307" width="234" height="9" font="font4" id="p4_t94" reading_order_no="92" segment_no="9" tag_type="text">For a given few-shot classification task with embedding</text>
<text top="384" left="307" width="236" height="10" font="font4" id="p4_t95" reading_order_no="93" segment_no="9" tag_type="text">support set S f and query set Q f , we use estimated LID,</text>
<text top="396" left="307" width="162" height="10" font="font4" id="p4_t96" reading_order_no="94" segment_no="9" tag_type="text">with reconstruction module g φ , given as:<a href="deeplearning_paper6.html#4">function</a></text>
<text top="426" left="314" width="14" height="4" font="font24" id="p4_t97" reading_order_no="95" segment_no="10" tag_type="formula">d<a href="deeplearning_paper6.html#4">8</a></text>
<text top="421" left="313" width="40" height="10" font="font4" id="p4_t98" reading_order_no="96" segment_no="10" tag_type="formula">LID ( φ ) =<a href="deeplearning_paper6.html#4">:</a></text>
<text top="418" left="367" width="14" height="4" font="font24" id="p4_t99" reading_order_no="97" segment_no="10" tag_type="formula">X</text>
<text top="435" left="356" width="34" height="7" font="font19" id="p4_t100" reading_order_no="98" segment_no="10" tag_type="formula">z ∈ S f ∪ Q f</text>
<text top="426" left="394" width="14" height="4" font="font24" id="p4_t101" reading_order_no="99" segment_no="10" tag_type="formula">d</text>
<text top="419" left="393" width="41" height="12" font="font4" id="p4_t102" reading_order_no="100" segment_no="10" tag_type="formula">LID ( g L − 2</text>
<text top="426" left="418" width="5" height="6" font="font19" id="p4_t103" reading_order_no="101" segment_no="10" tag_type="formula">φ</text>
<text top="421" left="435" width="17" height="9" font="font18" id="p4_t104" reading_order_no="102" segment_no="10" tag_type="formula">( z ))</text>
<text top="459" left="346" width="18" height="10" font="font18" id="p4_t105" reading_order_no="103" segment_no="10" tag_type="formula">= −</text>
<text top="457" left="376" width="14" height="4" font="font24" id="p4_t106" reading_order_no="104" segment_no="10" tag_type="formula">X</text>
<text top="473" left="366" width="33" height="7" font="font19" id="p4_t107" reading_order_no="105" segment_no="10" tag_type="formula">z ∈ S f ∪ Q f</text>
<text top="449" left="403" width="13" height="13" font="font24" id="p4_t108" reading_order_no="106" segment_no="10" tag_type="formula">" 1</text>
<text top="467" left="410" width="9" height="9" font="font17" id="p4_t109" reading_order_no="107" segment_no="10" tag_type="formula">m</text>
<text top="449" left="425" width="7" height="6" font="font19" id="p4_t110" reading_order_no="108" segment_no="10" tag_type="formula">m</text>
<text top="457" left="421" width="14" height="4" font="font24" id="p4_t111" reading_order_no="109" segment_no="10" tag_type="formula">X</text>
<text top="474" left="422" width="13" height="6" font="font19" id="p4_t112" reading_order_no="110" segment_no="10" tag_type="formula">i =1</text>
<text top="460" left="437" width="8" height="9" font="font18" id="p4_t113" reading_order_no="111" segment_no="10" tag_type="formula">ln</text>
<text top="449" left="450" width="33" height="12" font="font17" id="p4_t114" reading_order_no="112" segment_no="10" tag_type="formula">r i ( g L − 2</text>
<text top="457" left="467" width="5" height="6" font="font19" id="p4_t115" reading_order_no="113" segment_no="10" tag_type="formula">φ</text>
<text top="452" left="483" width="17" height="9" font="font18" id="p4_t116" reading_order_no="114" segment_no="10" tag_type="formula">( z ))</text>
<text top="465" left="448" width="37" height="12" font="font17" id="p4_t117" reading_order_no="115" segment_no="10" tag_type="formula">r m ( g L − 2</text>
<text top="473" left="469" width="5" height="6" font="font19" id="p4_t118" reading_order_no="116" segment_no="10" tag_type="formula">φ</text>
<text top="468" left="486" width="16" height="9" font="font18" id="p4_t119" reading_order_no="117" segment_no="10" tag_type="formula">( z ))</text>
<text top="446" left="503" width="16" height="7" font="font24" id="p4_t120" reading_order_no="118" segment_no="10" tag_type="formula"># − 1</text>
<text top="460" left="522" width="20" height="9" font="font17" id="p4_t121" reading_order_no="119" segment_no="10" tag_type="text">, (6)</text>
<text top="493" left="307" width="48" height="12" font="font4" id="p4_t122" reading_order_no="120" segment_no="12" tag_type="text">where g L − 2</text>
<text top="501" left="339" width="5" height="6" font="font19" id="p4_t123" reading_order_no="121" segment_no="12" tag_type="text">φ</text>
<text top="496" left="359" width="184" height="9" font="font4" id="p4_t124" reading_order_no="122" segment_no="12" tag_type="text">is the hidden representation of the second-to-</text>
<text top="508" left="307" width="106" height="12" font="font4" id="p4_t125" reading_order_no="123" segment_no="12" tag_type="text">last layer of g φ ; r i ( g L − 2</text>
<text top="515" left="397" width="5" height="6" font="font19" id="p4_t126" reading_order_no="124" segment_no="12" tag_type="text">φ</text>
<text top="510" left="413" width="130" height="9" font="font18" id="p4_t127" reading_order_no="125" segment_no="12" tag_type="text">( z )) denotes the Eucidean dis-</text>
<text top="522" left="307" width="85" height="12" font="font4" id="p4_t128" reading_order_no="126" segment_no="12" tag_type="text">tance between g L − 2</text>
<text top="530" left="376" width="5" height="6" font="font19" id="p4_t129" reading_order_no="127" segment_no="12" tag_type="text">φ</text>
<text top="525" left="392" width="149" height="9" font="font18" id="p4_t130" reading_order_no="128" segment_no="12" tag_type="text">( z ) and its i -th nearest neighbor in</text>
<text top="537" left="307" width="26" height="12" font="font6" id="p4_t131" reading_order_no="129" segment_no="12" tag_type="text">{ g L − 2</text>
<text top="545" left="317" width="5" height="6" font="font19" id="p4_t132" reading_order_no="130" segment_no="12" tag_type="text">φ</text>
<text top="538" left="334" width="119" height="11" font="font18" id="p4_t133" reading_order_no="131" segment_no="12" tag_type="text">( z 0 ) | z 0 ∈ S f ∪ Q f } . Our d</text>
<text top="540" left="438" width="103" height="9" font="font4" id="p4_t134" reading_order_no="132" segment_no="12" tag_type="text">LID ( φ ) is a proxy for the</text>
<text top="552" left="307" width="234" height="9" font="font4" id="p4_t135" reading_order_no="133" segment_no="12" tag_type="text">hidden layer subspace dimensionality of the reconstruction</text>
<text top="564" left="307" width="32" height="9" font="font4" id="p4_t136" reading_order_no="134" segment_no="12" tag_type="text">module.</text>
<text top="582" left="307" width="236" height="9" font="font4" id="p4_t137" reading_order_no="135" segment_no="13" tag_type="text">In Figure 3 , we empirically investigate the relationship be-</text>
<text top="594" left="307" width="236" height="9" font="font4" id="p4_t138" reading_order_no="136" segment_no="13" tag_type="text">tween the LID and accuracy during reconstruction training.</text>
<text top="606" left="307" width="234" height="9" font="font4" id="p4_t139" reading_order_no="137" segment_no="13" tag_type="text">The result shows that the change of LID can be used to find<a href="deeplearning_paper6.html#4">3</a></text>
<text top="618" left="307" width="235" height="9" font="font4" id="p4_t140" reading_order_no="138" segment_no="13" tag_type="text">the early stopping time of the best possible new embeddings;</text>
<text top="630" left="307" width="234" height="9" font="font4" id="p4_t141" reading_order_no="139" segment_no="13" tag_type="text">when the LID becomes the lowest or starts to increase. To</text>
<text top="641" left="307" width="234" height="9" font="font4" id="p4_t142" reading_order_no="140" segment_no="13" tag_type="text">be more specific, in 1-shot settings, we observe that the</text>
<text top="653" left="307" width="234" height="9" font="font4" id="p4_t143" reading_order_no="141" segment_no="13" tag_type="text">LID behaves exactly the opposite of the accuracy curve, for</text>
<text top="665" left="307" width="234" height="9" font="font4" id="p4_t144" reading_order_no="142" segment_no="13" tag_type="text">both with and without dropout. For 5-shot settings, the LID</text>
<text top="677" left="307" width="236" height="9" font="font4" id="p4_t145" reading_order_no="143" segment_no="13" tag_type="text">behaves almost the opposite of the accuracy curve; how-</text>
<text top="689" left="307" width="234" height="9" font="font4" id="p4_t146" reading_order_no="144" segment_no="13" tag_type="text">ever, it has a small misalignment, especially for the case</text>
<text top="707" left="320" width="137" height="10" font="font39" id="p4_t147" reading_order_no="146" segment_no="16" tag_type="footnote">8 ◦ indicates the element-wise product.</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font40" size="5" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font41" size="5" family="CMSY5" color="#000000"/>
<text top="48" left="149" width="298" height="8" font="font16" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="70" left="55" width="78" height="9" font="font1" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="title">Algorithm 1 ESFR</text>
<text top="83" left="65" width="224" height="10" font="font1" id="p5_t3" reading_order_no="2" segment_no="3" tag_type="code">Input: embedding support set S f , embedding query set</text>
<text top="96" left="65" width="190" height="10" font="font17" id="p5_t4" reading_order_no="3" segment_no="3" tag_type="code">Q f , and few-shot classifier Alg : S f , Q f → b Y Q</text>
<text top="107" left="65" width="73" height="11" font="font1" id="p5_t5" reading_order_no="4" segment_no="3" tag_type="code">Initialize: φ i =1: N e</text>
<text top="120" left="65" width="75" height="10" font="font1" id="p5_t6" reading_order_no="5" segment_no="3" tag_type="code">for i = 1 to N e do</text>
<text top="132" left="75" width="60" height="10" font="font4" id="p5_t7" reading_order_no="6" segment_no="3" tag_type="code">prev lid = d</text>
<text top="131" left="120" width="35" height="12" font="font4" id="p5_t8" reading_order_no="7" segment_no="3" tag_type="code">LID ( φ i 0 )</text>
<text top="144" left="75" width="82" height="10" font="font1" id="p5_t9" reading_order_no="8" segment_no="3" tag_type="code">Initialize: optimizer</text>
<text top="156" left="75" width="144" height="10" font="font1" id="p5_t10" reading_order_no="9" segment_no="3" tag_type="code">for j = 0 to MAX ITERATION do</text>
<text top="167" left="85" width="74" height="13" font="font17" id="p5_t11" reading_order_no="10" segment_no="3" tag_type="code">φ i j +1 ← φ i j − ∇ φ i j</text>
<text top="167" left="161" width="109" height="12" font="font6" id="p5_t12" reading_order_no="11" segment_no="3" tag_type="code">L ( φ i j ) from equation 5 or 7</text>
<text top="185" left="85" width="39" height="9" font="font4" id="p5_t13" reading_order_no="12" segment_no="3" tag_type="code">lid = d</text>
<text top="183" left="109" width="45" height="12" font="font4" id="p5_t14" reading_order_no="13" segment_no="3" tag_type="code">LID ( φ i j +1 )</text>
<text top="197" left="85" width="85" height="9" font="font1" id="p5_t15" reading_order_no="14" segment_no="3" tag_type="code">if lid &gt; prev lid then</text>
<text top="207" left="95" width="44" height="13" font="font17" id="p5_t16" reading_order_no="15" segment_no="3" tag_type="code">φ i ∗ = φ i j +1</text>
<text top="221" left="95" width="22" height="9" font="font4" id="p5_t17" reading_order_no="16" segment_no="3" tag_type="code">break</text>
<text top="233" left="85" width="24" height="9" font="font1" id="p5_t18" reading_order_no="17" segment_no="3" tag_type="code">end if</text>
<text top="244" left="85" width="56" height="10" font="font4" id="p5_t19" reading_order_no="18" segment_no="3" tag_type="code">prev lid = lid</text>
<text top="256" left="75" width="30" height="9" font="font1" id="p5_t20" reading_order_no="19" segment_no="3" tag_type="code">end for</text>
<text top="268" left="65" width="30" height="9" font="font1" id="p5_t21" reading_order_no="20" segment_no="3" tag_type="code">end for</text>
<text top="278" left="65" width="99" height="11" font="font17" id="p5_t22" reading_order_no="21" segment_no="3" tag_type="code">S ESFR = { ( z 0 , y ) | z 0 = 1</text>
<text top="286" left="158" width="8" height="6" font="font19" id="p5_t23" reading_order_no="22" segment_no="3" tag_type="code">N e</text>
<text top="277" left="170" width="39" height="15" font="font24" id="p5_t24" reading_order_no="23" segment_no="3" tag_type="code">P N e i =1 g φ i ∗</text>
<text top="280" left="210" width="68" height="10" font="font18" id="p5_t25" reading_order_no="24" segment_no="3" tag_type="code">( z ) , ( z, y ) ∈ S f }</text>
<text top="294" left="65" width="83" height="11" font="font17" id="p5_t26" reading_order_no="25" segment_no="3" tag_type="code">Q ESFR = { z 0 | z 0 = 1</text>
<text top="301" left="142" width="8" height="7" font="font19" id="p5_t27" reading_order_no="26" segment_no="3" tag_type="code">N e</text>
<text top="293" left="154" width="39" height="15" font="font24" id="p5_t28" reading_order_no="27" segment_no="3" tag_type="code">P N e i =1 g φ i ∗</text>
<text top="295" left="193" width="53" height="10" font="font18" id="p5_t29" reading_order_no="28" segment_no="3" tag_type="code">( z ) , z ∈ Q f }</text>
<text top="309" left="65" width="139" height="11" font="font1" id="p5_t30" reading_order_no="29" segment_no="3" tag_type="code">Output: b Y Q = Alg ( S ESFR , Q ESFR )</text>
<text top="347" left="55" width="234" height="9" font="font4" id="p5_t31" reading_order_no="30" segment_no="7" tag_type="text">with dropout. This seems reasonable since the classifier can</text>
<text top="359" left="55" width="234" height="9" font="font4" id="p5_t32" reading_order_no="31" segment_no="7" tag_type="text">further filter out non-shared features by multiple examples</text>
<text top="371" left="55" width="234" height="9" font="font4" id="p5_t33" reading_order_no="32" segment_no="7" tag_type="text">of novel classes in 5-shot settings; hence acquiring more</text>
<text top="383" left="55" width="234" height="9" font="font4" id="p5_t34" reading_order_no="33" segment_no="7" tag_type="text">shared features at the cost of learning non-shared ones can</text>
<text top="395" left="55" width="68" height="9" font="font4" id="p5_t35" reading_order_no="34" segment_no="7" tag_type="text">be advantageous.</text>
<text top="420" left="55" width="97" height="9" font="font1" id="p5_t36" reading_order_no="35" segment_no="8" tag_type="title">3.3. Proposed Methods</text>
<text top="438" left="55" width="234" height="10" font="font1" id="p5_t37" reading_order_no="36" segment_no="9" tag_type="text">ESFR: We describe our unsupervised adaptation scheme for</text>
<text top="450" left="55" width="234" height="9" font="font4" id="p5_t38" reading_order_no="37" segment_no="9" tag_type="text">few-shot classification: Early-Stage Feature Reconstruction</text>
<text top="462" left="55" width="234" height="9" font="font4" id="p5_t39" reading_order_no="38" segment_no="9" tag_type="text">(ESFR). Given a few-shot classification task and embedding</text>
<text top="474" left="55" width="235" height="10" font="font4" id="p5_t40" reading_order_no="39" segment_no="9" tag_type="text">network, we run feature reconstruction training by L FR ( 5 )</text>
<text top="486" left="55" width="234" height="9" font="font4" id="p5_t41" reading_order_no="40" segment_no="9" tag_type="text">with initialized φ . At every training iteration, we measure</text>
<text top="498" left="55" width="234" height="10" font="font4" id="p5_t42" reading_order_no="41" segment_no="9" tag_type="text">the LID of g φ with task samples by ( 6 ) and we early stop</text>
<text top="510" left="55" width="234" height="9" font="font4" id="p5_t43" reading_order_no="42" segment_no="9" tag_type="text">the training when the LID starts to increase. Task-adapted</text>
<text top="522" left="55" width="234" height="10" font="font4" id="p5_t44" reading_order_no="43" segment_no="9" tag_type="text">embeddings of g φ ( z ) are used for the classification of the</text>
<text top="534" left="55" width="236" height="9" font="font4" id="p5_t45" reading_order_no="44" segment_no="9" tag_type="text">given few-shot task. A wide range of metric-based and fine-</text>
<text top="546" left="55" width="234" height="9" font="font4" id="p5_t46" reading_order_no="45" segment_no="9" tag_type="text">tuning few-shot methods can be used with our embeddings</text>
<text top="558" left="55" width="234" height="9" font="font4" id="p5_t47" reading_order_no="46" segment_no="9" tag_type="text">for such classification. To reduce the variance by random</text>
<text top="570" left="55" width="235" height="9" font="font4" id="p5_t48" reading_order_no="47" segment_no="9" tag_type="text">initial weights of φ , we train N e reconstruction modules,</text>
<text top="582" left="55" width="234" height="9" font="font4" id="p5_t49" reading_order_no="48" segment_no="9" tag_type="text">separately, with different initial weights, and take the center</text>
<text top="594" left="55" width="176" height="9" font="font4" id="p5_t50" reading_order_no="49" segment_no="9" tag_type="text">of each sample’s reconstructed embeddings.</text>
<text top="612" left="55" width="234" height="9" font="font1" id="p5_t51" reading_order_no="50" segment_no="10" tag_type="text">ESFR-Semi: We further investigate the semi-supervised</text>
<text top="624" left="55" width="234" height="9" font="font4" id="p5_t52" reading_order_no="51" segment_no="10" tag_type="text">version of our scheme, ESFR-Semi, by adding the support</text>
<text top="636" left="55" width="190" height="9" font="font4" id="p5_t53" reading_order_no="52" segment_no="10" tag_type="text">classification loss to the reconstruction loss ( 5 ):</text>
<text top="654" left="67" width="163" height="12" font="font17" id="p5_t54" reading_order_no="53" segment_no="11" tag_type="formula">C j ( z, φ, W, b ) = softmax j [ W g φ ( z ) + b ]</text>
<text top="670" left="67" width="85" height="16" font="font6" id="p5_t55" reading_order_no="54" segment_no="11" tag_type="formula">L CE ( φ, W, b ) = − 1</text>
<text top="683" left="141" width="17" height="10" font="font6" id="p5_t56" reading_order_no="55" segment_no="11" tag_type="formula">| S f |</text>
<text top="674" left="172" width="14" height="4" font="font24" id="p5_t57" reading_order_no="56" segment_no="11" tag_type="formula">X</text>
<text top="691" left="161" width="36" height="7" font="font21" id="p5_t58" reading_order_no="57" segment_no="11" tag_type="formula">( z i ,y i ) ∈ S f</text>
<text top="675" left="200" width="78" height="11" font="font18" id="p5_t59" reading_order_no="58" segment_no="11" tag_type="formula">log C y i ( z i , φ, W, b )</text>
<text top="704" left="67" width="170" height="10" font="font6" id="p5_t60" reading_order_no="59" segment_no="11" tag_type="formula">L Semi ( φ, W, b ) = L FR ( φ ) + λ L CE ( φ, W, b )</text>
<text top="705" left="278" width="12" height="9" font="font4" id="p5_t61" reading_order_no="111" segment_no="11" tag_type="text">(7)</text>
<text top="70" left="307" width="236" height="10" font="font4" id="p5_t62" reading_order_no="60" segment_no="2" tag_type="text">where L CE is the cross-entropy loss given by an affine clas-</text>
<text top="82" left="307" width="236" height="9" font="font4" id="p5_t63" reading_order_no="61" segment_no="2" tag_type="text">sifier on new embeddings, and λ is a trade-off parameter.</text>
<text top="94" left="307" width="234" height="9" font="font4" id="p5_t64" reading_order_no="62" segment_no="2" tag_type="text">Additional weights W and b are jointly trained with φ . The</text>
<text top="106" left="307" width="235" height="9" font="font4" id="p5_t65" reading_order_no="63" segment_no="2" tag_type="text">trade-off parameter λ is tuned using few-shot classification</text>
<text top="118" left="307" width="234" height="9" font="font4" id="p5_t66" reading_order_no="64" segment_no="2" tag_type="text">tasks from validation datasets as in Ziko et al. ( 2020 ). Note</text>
<text top="130" left="307" width="235" height="9" font="font4" id="p5_t67" reading_order_no="65" segment_no="2" tag_type="text">that the only ESFR-Semi requires (while ESFR does not)</text>
<text top="142" left="307" width="236" height="9" font="font4" id="p5_t68" reading_order_no="66" segment_no="2" tag_type="text">few-shot classification task experiences to determine a cer-</text>
<text top="154" left="307" width="59" height="9" font="font4" id="p5_t69" reading_order_no="67" segment_no="2" tag_type="text">tain parameter.</text>
<text top="172" left="307" width="201" height="9" font="font4" id="p5_t70" reading_order_no="68" segment_no="4" tag_type="text">The overall methods are described in Algorithm 1 .<a href="deeplearning_paper6.html#4">5</a></text>
<text top="199" left="307" width="83" height="11" font="font3" id="p5_t71" reading_order_no="69" segment_no="5" tag_type="title">4. Related Work</text>
<text top="220" left="307" width="235" height="9" font="font1" id="p5_t72" reading_order_no="70" segment_no="6" tag_type="text">Few-Shot Classification (FSC): Few-shot learning or<a href="deeplearning_paper6.html#5">7</a></text>
<text top="232" left="307" width="235" height="9" font="font4" id="p5_t73" reading_order_no="71" segment_no="6" tag_type="text">few-shot classification methods have broad categories:</text>
<text top="244" left="307" width="235" height="9" font="font4" id="p5_t74" reading_order_no="72" segment_no="6" tag_type="text">optimization-based methods ( Ravi &amp; Larochelle , 2017 ; Finn</text>
<text top="256" left="307" width="235" height="9" font="font8" id="p5_t75" reading_order_no="73" segment_no="6" tag_type="text">et al. , 2017 ; Rusu et al. , 2019 ), distance-based approaches</text>
<text top="268" left="307" width="236" height="9" font="font4" id="p5_t76" reading_order_no="74" segment_no="6" tag_type="text">( Vinyals et al. , 2016 ; Snell et al. , 2017 ; Oreshkin et al. ,</text>
<text top="280" left="307" width="235" height="9" font="font8" id="p5_t77" reading_order_no="75" segment_no="6" tag_type="text">2018 ), fine-tunings ( Wang et al. , 2019 ; Tian et al. , 2020 ;</text>
<text top="292" left="307" width="236" height="9" font="font8" id="p5_t78" reading_order_no="76" segment_no="6" tag_type="text">Dhillon et al. , 2020 ), etc. A large portion of these meth-</text>
<text top="304" left="307" width="235" height="9" font="font4" id="p5_t79" reading_order_no="77" segment_no="6" tag_type="text">ods is based on meta-learning ( Thrun &amp; Pratt , 2012 ). In</text>
<text top="315" left="307" width="236" height="9" font="font4" id="p5_t80" reading_order_no="78" segment_no="6" tag_type="text">meta-learning, training is done in a series of few-shot classi-</text>
<text top="327" left="307" width="234" height="9" font="font4" id="p5_t81" reading_order_no="79" segment_no="6" tag_type="text">fication tasks (a.k.a. episodic training) to train the model in</text>
<text top="339" left="307" width="234" height="9" font="font4" id="p5_t82" reading_order_no="80" segment_no="6" tag_type="text">a way that reflects test-time scenarios. Several recent studies</text>
<text top="351" left="307" width="235" height="9" font="font4" id="p5_t83" reading_order_no="81" segment_no="6" tag_type="text">( Wang et al. , 2019 ; Tian et al. , 2020 ; Dhillon et al. , 2020 ;</text>
<text top="363" left="307" width="235" height="9" font="font8" id="p5_t84" reading_order_no="82" segment_no="6" tag_type="text">Ziko et al. , 2020 ; Boudiaf et al. , 2020 ) have questioned the</text>
<text top="375" left="307" width="236" height="9" font="font4" id="p5_t85" reading_order_no="83" segment_no="6" tag_type="text">necessity of meta-learning on few-shot classification, report-</text>
<text top="387" left="307" width="236" height="9" font="font4" id="p5_t86" reading_order_no="84" segment_no="6" tag_type="text">ing competitive performance on few-shot benchmarks with-</text>
<text top="399" left="307" width="236" height="9" font="font4" id="p5_t87" reading_order_no="85" segment_no="6" tag_type="text">out neither episodic training nor few-shot task experiences.</text>
<text top="410" left="307" width="236" height="10" font="font4" id="p5_t88" reading_order_no="86" segment_no="6" tag_type="text">These methods solve the few-shot task by fine-tuning 9 a pre-</text>
<text top="423" left="307" width="234" height="9" font="font4" id="p5_t89" reading_order_no="87" segment_no="6" tag_type="text">trained embedding network trained on the base dataset with</text>
<text top="435" left="307" width="234" height="9" font="font4" id="p5_t90" reading_order_no="88" segment_no="6" tag_type="text">standard cross-entropy loss. Our method lies in this line of</text>
<text top="447" left="307" width="236" height="9" font="font4" id="p5_t91" reading_order_no="89" segment_no="6" tag_type="text">research that doesn’t require meta-learning. It seems possi-</text>
<text top="459" left="307" width="234" height="9" font="font4" id="p5_t92" reading_order_no="90" segment_no="6" tag_type="text">ble to merge our method with meta-learning; we leave it as</text>
<text top="471" left="307" width="236" height="9" font="font4" id="p5_t93" reading_order_no="91" segment_no="6" tag_type="text">future work. Unsupervised adaptation for FSC: We men-</text>
<text top="483" left="307" width="234" height="9" font="font4" id="p5_t94" reading_order_no="92" segment_no="6" tag_type="text">tioned in the introduction that transductive methods, based</text>
<text top="495" left="307" width="234" height="9" font="font4" id="p5_t95" reading_order_no="93" segment_no="6" tag_type="text">on semi-supervised learning techniques, have been widely</text>
<text top="507" left="307" width="234" height="9" font="font4" id="p5_t96" reading_order_no="94" segment_no="6" tag_type="text">studied in few-shot classification. In contrast, unsupervised</text>
<text top="519" left="307" width="234" height="9" font="font4" id="p5_t97" reading_order_no="95" segment_no="6" tag_type="text">adaptation for few-shot classification is an open field. To</text>
<text top="531" left="307" width="234" height="9" font="font4" id="p5_t98" reading_order_no="96" segment_no="6" tag_type="text">the best of our knowledge, we are the first to propose deep</text>
<text top="543" left="307" width="234" height="9" font="font4" id="p5_t99" reading_order_no="97" segment_no="6" tag_type="text">unsupervised adaptation for few-shot classification. The</text>
<text top="555" left="307" width="236" height="9" font="font4" id="p5_t100" reading_order_no="98" segment_no="6" tag_type="text">closest related works are Rodr´ıguez et al. ( 2020 ) and Licht-</text>
<text top="567" left="307" width="235" height="9" font="font8" id="p5_t101" reading_order_no="99" segment_no="6" tag_type="text">enstein et al. ( 2020 ). Embedding Propagation proposed by</text>
<text top="578" left="307" width="234" height="9" font="font8" id="p5_t102" reading_order_no="100" segment_no="6" tag_type="text">Rodr´ıguez et al. ( 2020 ) iteratively updates embeddings by a</text>
<text top="590" left="307" width="234" height="9" font="font4" id="p5_t103" reading_order_no="101" segment_no="6" tag_type="text">linear combination with the nearest neighbors’ embeddings</text>
<text top="602" left="307" width="236" height="9" font="font4" id="p5_t104" reading_order_no="102" segment_no="6" tag_type="text">and improves few-shot classification performance. Licht-</text>
<text top="614" left="307" width="235" height="9" font="font8" id="p5_t105" reading_order_no="103" segment_no="6" tag_type="text">enstein et al. ( 2020 ) uses the principal component analysis</text>
<text top="626" left="307" width="235" height="9" font="font4" id="p5_t106" reading_order_no="104" segment_no="6" tag_type="text">to acquire major components from given task embeddings;</text>
<text top="638" left="307" width="236" height="9" font="font4" id="p5_t107" reading_order_no="105" segment_no="6" tag_type="text">this classical approach shows impressive performance gain.</text>
<text top="650" left="307" width="236" height="9" font="font4" id="p5_t108" reading_order_no="106" segment_no="6" tag_type="text">However, these works are limited to an affine transforma-</text>
<text top="662" left="307" width="235" height="9" font="font4" id="p5_t109" reading_order_no="107" segment_no="6" tag_type="text">tion of embeddings and do not benefit from deep learning;</text>
<text top="674" left="307" width="234" height="9" font="font4" id="p5_t110" reading_order_no="108" segment_no="6" tag_type="text">while our method finds non-linear patterns and correlations</text>
<text top="692" left="320" width="222" height="10" font="font10" id="p5_t111" reading_order_no="109" segment_no="12" tag_type="footnote">9 Including the only change of class prototypes or last layer</text>
<text top="704" left="307" width="42" height="8" font="font10" id="p5_t112" reading_order_no="110" segment_no="12" tag_type="footnote">parameters.</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="70" left="55" width="236" height="9" font="font4" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">by the benefit of deep unsupervised learning. Though un-</text>
<text top="82" left="55" width="234" height="9" font="font4" id="p6_t3" reading_order_no="2" segment_no="1" tag_type="text">supervised adaptation in few-shot classification had less</text>
<text top="94" left="55" width="236" height="9" font="font4" id="p6_t4" reading_order_no="3" segment_no="1" tag_type="text">attention, our experimental results show that it can outper-</text>
<text top="106" left="55" width="234" height="9" font="font4" id="p6_t5" reading_order_no="4" segment_no="1" tag_type="text">form conventional transductive baselines, especially when</text>
<text top="118" left="55" width="234" height="9" font="font4" id="p6_t6" reading_order_no="5" segment_no="1" tag_type="text">extremely few labeled samples are available in the 1-shot</text>
<text top="130" left="55" width="234" height="9" font="font4" id="p6_t7" reading_order_no="6" segment_no="1" tag_type="text">setting. Training behavior of DNNs: Our method is based</text>
<text top="142" left="55" width="234" height="9" font="font4" id="p6_t8" reading_order_no="7" segment_no="1" tag_type="text">on the property of Deep Neural Networks’ (DNNs) training</text>
<text top="154" left="55" width="236" height="9" font="font4" id="p6_t9" reading_order_no="8" segment_no="1" tag_type="text">behavior that DNNs learn to generalize before memorizing .</text>
<text top="166" left="55" width="235" height="9" font="font4" id="p6_t10" reading_order_no="9" segment_no="1" tag_type="text">The property is reported and studied in recent works ( Arpit</text>
<text top="178" left="55" width="235" height="9" font="font8" id="p6_t11" reading_order_no="10" segment_no="1" tag_type="text">et al. , 2017 ; Lampinen &amp; Ganguli , 2019 ; Stephenson et al. ,</text>
<text top="190" left="55" width="236" height="9" font="font8" id="p6_t12" reading_order_no="11" segment_no="1" tag_type="text">2021 ), as mentioned in Section 3.1 . Similar to our work, sev-</text>
<text top="202" left="55" width="234" height="9" font="font4" id="p6_t13" reading_order_no="12" segment_no="1" tag_type="text">eral methods of training DNNs with noisy labels are based</text>
<text top="214" left="55" width="234" height="9" font="font4" id="p6_t14" reading_order_no="13" segment_no="1" tag_type="text">on different training behaviors between generalization and<a href="deeplearning_paper6.html#9">(</a></text>
<text top="226" left="55" width="236" height="9" font="font4" id="p6_t15" reading_order_no="14" segment_no="1" tag_type="text">memorization. After the report of the property by Arpit et al.<a href="deeplearning_paper6.html#9">Arpit</a></text>
<text top="238" left="55" width="235" height="9" font="font4" id="p6_t16" reading_order_no="15" segment_no="1" tag_type="text">( 2017 ), several works ( Hendrycks et al. , 2019 ; Oymak et al. ,<a href="deeplearning_paper6.html#9">et al.</a></text>
<text top="250" left="55" width="234" height="9" font="font8" id="p6_t17" reading_order_no="16" segment_no="1" tag_type="text">2019 ; Song et al. , 2020 ) suggest using early stopping to<a href="deeplearning_paper6.html#9">,</a></text>
<text top="262" left="55" width="236" height="9" font="font4" id="p6_t18" reading_order_no="17" segment_no="1" tag_type="text">discard noisy information before memorization, Jiang et al.<a href="deeplearning_paper6.html#9">2017</a></text>
<text top="274" left="55" width="234" height="9" font="font4" id="p6_t19" reading_order_no="18" segment_no="1" tag_type="text">( 2018 ); Yu et al. ( 2019 ); Sugiyama ( 2018 ) propose to gather<a href="deeplearning_paper6.html#9">;</a></text>
<text top="285" left="55" width="234" height="9" font="font4" id="p6_t20" reading_order_no="19" segment_no="1" tag_type="text">the clean samples that exhibit high confident prediction<a href="deeplearning_paper6.html#10">Lampinen &amp; Ganguli</a></text>
<text top="297" left="55" width="237" height="9" font="font4" id="p6_t21" reading_order_no="20" segment_no="1" tag_type="text">in the early pattern learning for further training. Ma et al.<a href="deeplearning_paper6.html#10">,</a></text>
<text top="309" left="55" width="235" height="9" font="font4" id="p6_t22" reading_order_no="21" segment_no="1" tag_type="text">( 2018b ) offer to use LID to detect and correct noisy label<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="321" left="55" width="236" height="9" font="font4" id="p6_t23" reading_order_no="22" segment_no="1" tag_type="text">memorization, which strongly motivates our methodology.<a href="deeplearning_paper6.html#10">;</a></text>
<text top="333" left="55" width="234" height="9" font="font4" id="p6_t24" reading_order_no="23" segment_no="1" tag_type="text">However, these works are limited to the domain of learning<a href="deeplearning_paper6.html#10">Stephenson et al.</a></text>
<text top="345" left="55" width="234" height="9" font="font4" id="p6_t25" reading_order_no="24" segment_no="1" tag_type="text">from noisy labels and supervised learning. In our work, we<a href="deeplearning_paper6.html#10">,</a></text>
<text top="357" left="55" width="234" height="9" font="font4" id="p6_t26" reading_order_no="25" segment_no="1" tag_type="text">find the behavior also appears in unsupervised adaptation<a href="deeplearning_paper6.html#10">2021</a></text>
<text top="369" left="55" width="106" height="9" font="font4" id="p6_t27" reading_order_no="26" segment_no="1" tag_type="text">for few-shot classification.<a href="deeplearning_paper6.html#10">), </a>as mentioned in Section</text>
<text top="396" left="55" width="77" height="11" font="font3" id="p6_t28" reading_order_no="27" segment_no="6" tag_type="title">5. Experiments<a href="deeplearning_paper6.html#3">3.1</a></text>
<text top="417" left="55" width="112" height="9" font="font1" id="p6_t29" reading_order_no="28" segment_no="7" tag_type="title">5.1. Experimental Settings<a href="deeplearning_paper6.html#3">. </a>Similar to our work, sev-</text>
<text top="436" left="55" width="234" height="9" font="font1" id="p6_t30" reading_order_no="29" segment_no="8" tag_type="text">Datasets: We evaluate our method on three standard</text>
<text top="448" left="55" width="234" height="9" font="font4" id="p6_t31" reading_order_no="30" segment_no="8" tag_type="text">datasets of few-shot classification: (1) mini -ImageNet</text>
<text top="460" left="55" width="235" height="9" font="font4" id="p6_t32" reading_order_no="31" segment_no="8" tag_type="text">( Vinyals et al. , 2016 ) dataset as in Ravi &amp; Larochelle</text>
<text top="472" left="55" width="236" height="9" font="font4" id="p6_t33" reading_order_no="32" segment_no="8" tag_type="text">( 2017 ), (2) tiered -ImageNet dataset as in Ren et al. ( 2018 ),<a href="deeplearning_paper6.html#9">Arpit et al.</a></text>
<text top="484" left="55" width="236" height="9" font="font4" id="p6_t34" reading_order_no="33" segment_no="8" tag_type="text">and (3) Caltech-UCSD Birds 200 (CUB) ( Welinder et al. ,<a href="deeplearning_paper6.html#9">(</a></text>
<text top="496" left="55" width="234" height="9" font="font8" id="p6_t35" reading_order_no="34" segment_no="8" tag_type="text">2010 ) as in Chen et al. ( 2019 ). Each dataset is divided into<a href="deeplearning_paper6.html#9">2017</a></text>
<text top="507" left="55" width="234" height="9" font="font4" id="p6_t36" reading_order_no="35" segment_no="8" tag_type="text">train/val/test splits according to references. All the images<a href="deeplearning_paper6.html#9">), </a>several works <a href="deeplearning_paper6.html#9">(</a></text>
<text top="519" left="55" width="90" height="9" font="font4" id="p6_t37" reading_order_no="36" segment_no="8" tag_type="text">are resized to 84 × 84 .<a href="deeplearning_paper6.html#9">Hendrycks et al.</a></text>
<text top="537" left="55" width="234" height="9" font="font1" id="p6_t38" reading_order_no="37" segment_no="10" tag_type="text">Baseline Methods: We investigate three baseline few-shot<a href="deeplearning_paper6.html#9">,</a></text>
<text top="549" left="55" width="234" height="9" font="font4" id="p6_t39" reading_order_no="38" segment_no="10" tag_type="text">classification methods (classifiers) used in conjunction with<a href="deeplearning_paper6.html#9">2019</a></text>
<text top="561" left="55" width="48" height="9" font="font4" id="p6_t40" reading_order_no="39" segment_no="10" tag_type="text">our method.<a href="deeplearning_paper6.html#9">;</a></text>
<text top="579" left="55" width="234" height="9" font="font4" id="p6_t41" reading_order_no="40" segment_no="11" tag_type="list">• Linear : Train an affine classifier on labeled support set<a href="deeplearning_paper6.html#10">Oymak et al.</a></text>
<text top="591" left="64" width="226" height="9" font="font4" id="p6_t42" reading_order_no="41" segment_no="11" tag_type="list">with embeddings and use the trained classifier to classify<a href="deeplearning_paper6.html#10">,</a></text>
<text top="603" left="64" width="74" height="9" font="font4" id="p6_t43" reading_order_no="42" segment_no="11" tag_type="list">the query samples.<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="622" left="55" width="234" height="9" font="font4" id="p6_t44" reading_order_no="43" segment_no="12" tag_type="list">• Nearest Neighbor (NN): Compute the class prototype by<a href="deeplearning_paper6.html#10">;</a></text>
<text top="634" left="64" width="226" height="9" font="font4" id="p6_t45" reading_order_no="44" segment_no="12" tag_type="list">the centroid of support sample embeddings in each class<a href="deeplearning_paper6.html#10">Song et al.</a></text>
<text top="646" left="64" width="226" height="9" font="font4" id="p6_t46" reading_order_no="45" segment_no="12" tag_type="list">and classify the query sample to the class of the nearest<a href="deeplearning_paper6.html#10">,</a></text>
<text top="656" left="64" width="48" height="11" font="font4" id="p6_t47" reading_order_no="46" segment_no="12" tag_type="list">prototype. 10<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="677" left="55" width="234" height="9" font="font4" id="p6_t48" reading_order_no="47" segment_no="14" tag_type="list">• BD-CSPN : BD-CSPN ( Liu et al. , 2020 ) is chosen as a<a href="deeplearning_paper6.html#10">) </a>suggest using early stopping to</text>
<text top="689" left="64" width="227" height="9" font="font4" id="p6_t49" reading_order_no="48" segment_no="14" tag_type="list">baseline transductive method. It can be used with pre-</text>
<text top="707" left="65" width="146" height="10" font="font10" id="p6_t50" reading_order_no="98" segment_no="16" tag_type="footnote">10 We use Euclidean as a distance metric.<a href="deeplearning_paper6.html#10">Jiang et al.</a></text>
<text top="70" left="316" width="227" height="9" font="font4" id="p6_t51" reading_order_no="49" segment_no="2" tag_type="text">trained embeddings and achieves state-of-the-art perfor-<a href="deeplearning_paper6.html#10">(</a></text>
<text top="82" left="316" width="227" height="9" font="font4" id="p6_t52" reading_order_no="50" segment_no="2" tag_type="text">mance on 5-shot (Table 2 ). To briefly summarize, BD-<a href="deeplearning_paper6.html#10">2018</a></text>
<text top="94" left="316" width="226" height="9" font="font4" id="p6_t53" reading_order_no="51" segment_no="2" tag_type="text">CSPN consists of two components: (1) shifting-term for<a href="deeplearning_paper6.html#10">);</a></text>
<text top="106" left="316" width="226" height="9" font="font4" id="p6_t54" reading_order_no="52" segment_no="2" tag_type="text">removing the cross-class bias between the support set and<a href="deeplearning_paper6.html#11">Yu et al.</a></text>
<text top="118" left="316" width="225" height="9" font="font4" id="p6_t55" reading_order_no="53" segment_no="2" tag_type="text">query set, (2) Prototype Rectification (PR) that updates<a href="deeplearning_paper6.html#11">(</a></text>
<text top="130" left="316" width="227" height="9" font="font4" id="p6_t56" reading_order_no="54" segment_no="2" tag_type="text">class prototypes by accounting pseudo-labeled query sam-<a href="deeplearning_paper6.html#11">2019</a></text>
<text top="142" left="316" width="226" height="9" font="font4" id="p6_t57" reading_order_no="55" segment_no="2" tag_type="text">ples. Our particular interest is in PR since it is a baseline<a href="deeplearning_paper6.html#11">);</a></text>
<text top="154" left="316" width="226" height="9" font="font4" id="p6_t58" reading_order_no="56" segment_no="2" tag_type="text">approach of transductive methods; a similar scheme was<a href="deeplearning_paper6.html#10">Sugiyama</a></text>
<text top="166" left="316" width="195" height="9" font="font4" id="p6_t59" reading_order_no="57" segment_no="2" tag_type="text">suggested in Ren et al. ( 2018 ); Hou et al. ( 2019 ).<a href="deeplearning_paper6.html#10">(</a></text>
<text top="193" left="307" width="234" height="9" font="font1" id="p6_t60" reading_order_no="58" segment_no="3" tag_type="text">Evaluation Protocol: We evaluate our method on standard<a href="deeplearning_paper6.html#10">2018</a></text>
<text top="205" left="307" width="234" height="9" font="font4" id="p6_t61" reading_order_no="59" segment_no="3" tag_type="text">5-way 1- and 5-shot settings with 15 query samples per<a href="deeplearning_paper6.html#10">) </a>propose to gather</text>
<text top="217" left="307" width="236" height="9" font="font4" id="p6_t62" reading_order_no="60" segment_no="3" tag_type="text">class. We sampled 2,000 tasks for each experimental result.</text>
<text top="229" left="307" width="234" height="9" font="font4" id="p6_t63" reading_order_no="61" segment_no="3" tag_type="text">Our method uses the same fixed hyper-parameters for all</text>
<text top="241" left="307" width="234" height="9" font="font4" id="p6_t64" reading_order_no="62" segment_no="3" tag_type="text">experiments and settings. For the semi-supervised version<a href="deeplearning_paper6.html#10">Ma et al.</a></text>
<text top="253" left="307" width="235" height="9" font="font4" id="p6_t65" reading_order_no="63" segment_no="3" tag_type="text">of our method, with trade-off parameter λ , we tune λ by<a href="deeplearning_paper6.html#10">(</a></text>
<text top="264" left="307" width="234" height="10" font="font4" id="p6_t66" reading_order_no="64" segment_no="3" tag_type="text">selecting the best among λ ∈ [0 , 0 . 1 , 0 . 2 , 0 . 4 , 0 . 8 , 1 . 6] from<a href="deeplearning_paper6.html#10">2018b</a></text>
<text top="277" left="307" width="196" height="9" font="font4" id="p6_t67" reading_order_no="65" segment_no="3" tag_type="text">600 sampled few-shot tasks of validation dataset.<a href="deeplearning_paper6.html#10">) </a>offer to use LID to detect and correct noisy label</text>
<text top="302" left="307" width="117" height="9" font="font1" id="p6_t68" reading_order_no="66" segment_no="4" tag_type="title">5.2. Implementation Details</text>
<text top="321" left="307" width="235" height="9" font="font1" id="p6_t69" reading_order_no="67" segment_no="5" tag_type="text">Embedding Network: We follow the embedding network</text>
<text top="333" left="307" width="234" height="9" font="font4" id="p6_t70" reading_order_no="68" segment_no="5" tag_type="text">training procedure from Ziko et al. ( 2020 ), using backbone</text>
<text top="345" left="307" width="236" height="9" font="font4" id="p6_t71" reading_order_no="69" segment_no="5" tag_type="text">architectures: ResNet-18, WRN-28-10. We use from in-</text>
<text top="357" left="307" width="235" height="9" font="font5" id="p6_t72" reading_order_no="70" segment_no="5" tag_type="text">put to the average-pooled last residual block output as an</text>
<text top="369" left="307" width="234" height="9" font="font4" id="p6_t73" reading_order_no="71" segment_no="5" tag_type="text">embedding network. Embedding networks are trained for</text>
<text top="380" left="307" width="234" height="9" font="font4" id="p6_t74" reading_order_no="72" segment_no="5" tag_type="text">90 epochs, using stochastic gradient descent to minimize</text>
<text top="392" left="307" width="234" height="9" font="font4" id="p6_t75" reading_order_no="73" segment_no="5" tag_type="text">the standard cross-entropy on labeled base datasets. Label</text>
<text top="404" left="307" width="236" height="9" font="font4" id="p6_t76" reading_order_no="74" segment_no="5" tag_type="text">smoothing with parameter 0.1 is used for more general fea-</text>
<text top="416" left="307" width="236" height="9" font="font4" id="p6_t77" reading_order_no="75" segment_no="5" tag_type="text">tures. Random cropping, color jittering, and random horizon-</text>
<text top="427" left="307" width="234" height="10" font="font4" id="p6_t78" reading_order_no="76" segment_no="5" tag_type="text">tal flipping are applied for data augmentation. 11 The initial</text>
<text top="438" left="307" width="146" height="11" font="font4" id="p6_t79" reading_order_no="77" segment_no="5" tag_type="text">learning rate is 0.1 and shrank by 1</text>
<text top="440" left="447" width="96" height="12" font="font4" id="p6_t80" reading_order_no="78" segment_no="5" tag_type="text">10 at 45 and 66 epochs.<a href="deeplearning_paper6.html#11">(</a></text>
<text top="452" left="307" width="234" height="9" font="font4" id="p6_t81" reading_order_no="79" segment_no="5" tag_type="text">Mini-batch sizes of 256 and 128 are used for ResNet-18 and<a href="deeplearning_paper6.html#11">Vinyals et al.</a></text>
<text top="464" left="307" width="236" height="9" font="font4" id="p6_t82" reading_order_no="80" segment_no="5" tag_type="text">WRN-28-10 training, respectively. The best performing em-<a href="deeplearning_paper6.html#11">,</a></text>
<text top="476" left="307" width="234" height="9" font="font4" id="p6_t83" reading_order_no="81" segment_no="5" tag_type="text">bedding network on the 5-way 1-shot task of validation split<a href="deeplearning_paper6.html#11">2016</a></text>
<text top="488" left="307" width="234" height="9" font="font4" id="p6_t84" reading_order_no="82" segment_no="5" tag_type="text">dataset is chosen; while using the nearest neighbor classifier<a href="deeplearning_paper6.html#11">) </a>dataset as in</text>
<text top="500" left="307" width="195" height="9" font="font4" id="p6_t85" reading_order_no="83" segment_no="5" tag_type="text">and l2-normalized embeddings for classification.<a href="deeplearning_paper6.html#10">Ravi &amp; Larochelle</a></text>
<text top="518" left="307" width="236" height="9" font="font1" id="p6_t86" reading_order_no="84" segment_no="9" tag_type="text">Reconstruction Training: For the reconstruction module,<a href="deeplearning_paper6.html#10">(</a></text>
<text top="530" left="307" width="234" height="9" font="font4" id="p6_t87" reading_order_no="85" segment_no="9" tag_type="text">we use 4-layer fully connected deep neural network with<a href="deeplearning_paper6.html#10">2017</a></text>
<text top="542" left="307" width="234" height="9" font="font4" id="p6_t88" reading_order_no="86" segment_no="9" tag_type="text">ReLU activation. Each layer has the size of units equal to<a href="deeplearning_paper6.html#10">), </a>(2)</text>
<text top="554" left="307" width="234" height="9" font="font4" id="p6_t89" reading_order_no="87" segment_no="9" tag_type="text">the embedding dimension, and weight initialization follows</text>
<text top="566" left="307" width="236" height="9" font="font4" id="p6_t90" reading_order_no="88" segment_no="9" tag_type="text">the TensorFlow ( Abadi et al. , 2016 ) default setting (Gloro-</text>
<text top="578" left="307" width="236" height="9" font="font4" id="p6_t91" reading_order_no="89" segment_no="9" tag_type="text">tUniform). As an optimizer, we use Adam ( Kingma &amp; Ba ,<a href="deeplearning_paper6.html#10">Ren et al.</a></text>
<text top="590" left="307" width="236" height="9" font="font8" id="p6_t92" reading_order_no="90" segment_no="9" tag_type="text">2015 ) with a default learning rate of 1e-3. For dropout ( Sri-<a href="deeplearning_paper6.html#10">(</a></text>
<text top="602" left="307" width="235" height="9" font="font8" id="p6_t93" reading_order_no="91" segment_no="9" tag_type="text">vastava et al. , 2014 ), we use the maximum possible rate of<a href="deeplearning_paper6.html#10">2018</a></text>
<text top="613" left="307" width="235" height="10" font="font18" id="p6_t94" reading_order_no="92" segment_no="9" tag_type="text">0 . 5 . Finally, we take the ensemble of N e = 5 reconstruction<a href="deeplearning_paper6.html#10">),</a></text>
<text top="626" left="307" width="36" height="9" font="font4" id="p6_t95" reading_order_no="93" segment_no="9" tag_type="text">modules.<a href="deeplearning_paper6.html#11">(</a></text>
<text top="643" left="307" width="234" height="9" font="font1" id="p6_t96" reading_order_no="94" segment_no="13" tag_type="text">Preprocessing: Wang et al. ( 2019 ) reports the importance<a href="deeplearning_paper6.html#11">Welinder et al.</a></text>
<text top="655" left="307" width="236" height="9" font="font4" id="p6_t97" reading_order_no="95" segment_no="13" tag_type="text">of preprocessing in few-shot classification. We apply center-<a href="deeplearning_paper6.html#11">,</a></text>
<text top="667" left="307" width="234" height="9" font="font4" id="p6_t98" reading_order_no="96" segment_no="13" tag_type="text">ing and l2-normalization to embedding network’s output for<a href="deeplearning_paper6.html#11">2010</a></text>
<text top="679" left="307" width="235" height="9" font="font4" id="p6_t99" reading_order_no="97" segment_no="13" tag_type="text">reconstruction training and baseline methods. For centering,<a href="deeplearning_paper6.html#11">) </a>as in</text>
<text top="697" left="317" width="225" height="10" font="font10" id="p6_t100" reading_order_no="99" segment_no="15" tag_type="footnote">11 Data augmentations are used only for embedding network<a href="deeplearning_paper6.html#9">Chen et al.</a></text>
<text top="709" left="307" width="45" height="8" font="font10" id="p6_t101" reading_order_no="100" segment_no="15" tag_type="footnote">pre-training.<a href="deeplearning_paper6.html#9">(</a></text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font42" size="9" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font43" size="7" family="NimbusRomNo9L-Medi" color="#000000"/>
<text top="48" left="149" width="298" height="8" font="font16" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="77" left="55" width="486" height="8" font="font34" id="p7_t2" reading_order_no="1" segment_no="1" tag_type="text">Table 1. Improvement by incorporating our method into baseline methods with ResNet-18/WRN-28-10 backbone on mini -ImageNet and</text>
<text top="87" left="55" width="281" height="9" font="font12" id="p7_t3" reading_order_no="2" segment_no="1" tag_type="text">tiered -ImageNet. † indicates the use of shifting-term ( 8 ) during preprocessing.</text>
<text top="111" left="229" width="39" height="8" font="font16" id="p7_t4" reading_order_no="3" segment_no="2" tag_type="table">ResNet-18</text>
<text top="111" left="428" width="46" height="8" font="font16" id="p7_t5" reading_order_no="4" segment_no="2" tag_type="table">WRN-28-10</text>
<text top="121" left="169" width="57" height="8" font="font42" id="p7_t6" reading_order_no="5" segment_no="2" tag_type="table">mini -ImageNet</text>
<text top="121" left="268" width="61" height="8" font="font42" id="p7_t7" reading_order_no="6" segment_no="2" tag_type="table">tiered -ImageNet</text>
<text top="121" left="371" width="57" height="8" font="font42" id="p7_t8" reading_order_no="7" segment_no="2" tag_type="table">mini -ImageNet</text>
<text top="121" left="470" width="61" height="8" font="font42" id="p7_t9" reading_order_no="8" segment_no="2" tag_type="table">tiered -ImageNet<a href="deeplearning_paper6.html#7">(</a></text>
<text top="131" left="86" width="30" height="8" font="font16" id="p7_t10" reading_order_no="9" segment_no="2" tag_type="table">Method<a href="deeplearning_paper6.html#7">8</a></text>
<text top="131" left="161" width="23" height="8" font="font16" id="p7_t11" reading_order_no="10" segment_no="2" tag_type="table">1-shot<a href="deeplearning_paper6.html#7">) </a>during preprocessing.</text>
<text top="131" left="211" width="23" height="8" font="font16" id="p7_t12" reading_order_no="11" segment_no="2" tag_type="table">5-shot</text>
<text top="131" left="262" width="23" height="8" font="font16" id="p7_t13" reading_order_no="12" segment_no="2" tag_type="table">1-shot</text>
<text top="131" left="312" width="23" height="8" font="font16" id="p7_t14" reading_order_no="13" segment_no="2" tag_type="table">5-shot</text>
<text top="131" left="363" width="23" height="8" font="font16" id="p7_t15" reading_order_no="14" segment_no="2" tag_type="table">1-shot</text>
<text top="131" left="413" width="23" height="8" font="font16" id="p7_t16" reading_order_no="15" segment_no="2" tag_type="table">5-shot</text>
<text top="131" left="464" width="23" height="8" font="font16" id="p7_t17" reading_order_no="16" segment_no="2" tag_type="table">1-shot</text>
<text top="131" left="515" width="23" height="8" font="font16" id="p7_t18" reading_order_no="17" segment_no="2" tag_type="table">5-shot</text>
<text top="147" left="64" width="8" height="8" font="font10" id="p7_t19" reading_order_no="18" segment_no="2" tag_type="table">(i)</text>
<text top="142" left="102" width="23" height="8" font="font10" id="p7_t20" reading_order_no="19" segment_no="2" tag_type="table">Linear</text>
<text top="142" left="162" width="20" height="8" font="font10" id="p7_t21" reading_order_no="20" segment_no="2" tag_type="table">62.45</text>
<text top="142" left="213" width="20" height="8" font="font10" id="p7_t22" reading_order_no="21" segment_no="2" tag_type="table">79.32</text>
<text top="142" left="263" width="20" height="8" font="font10" id="p7_t23" reading_order_no="22" segment_no="2" tag_type="table">68.49</text>
<text top="142" left="314" width="20" height="8" font="font10" id="p7_t24" reading_order_no="23" segment_no="2" tag_type="table">83.77</text>
<text top="142" left="365" width="20" height="8" font="font10" id="p7_t25" reading_order_no="24" segment_no="2" tag_type="table">64.53</text>
<text top="142" left="415" width="20" height="8" font="font10" id="p7_t26" reading_order_no="25" segment_no="2" tag_type="table">80.81</text>
<text top="142" left="466" width="20" height="8" font="font10" id="p7_t27" reading_order_no="26" segment_no="2" tag_type="table">69.78</text>
<text top="142" left="516" width="20" height="8" font="font10" id="p7_t28" reading_order_no="27" segment_no="2" tag_type="table">84.91</text>
<text top="151" left="100" width="30" height="9" font="font10" id="p7_t29" reading_order_no="28" segment_no="2" tag_type="table">+ ESFR</text>
<text top="151" left="155" width="37" height="8" font="font16" id="p7_t30" reading_order_no="29" segment_no="2" tag_type="table">70.38 +7.93</text>
<text top="151" left="208" width="32" height="8" font="font16" id="p7_t31" reading_order_no="30" segment_no="2" tag_type="table">81.6 +2.28</text>
<text top="151" left="256" width="37" height="8" font="font16" id="p7_t32" reading_order_no="31" segment_no="2" tag_type="table">76.98 +8.49</text>
<text top="151" left="307" width="36" height="8" font="font16" id="p7_t33" reading_order_no="32" segment_no="2" tag_type="table">86.09 +2.32</text>
<text top="151" left="359" width="34" height="8" font="font16" id="p7_t34" reading_order_no="33" segment_no="2" tag_type="table">73.33 +8.8</text>
<text top="151" left="408" width="36" height="8" font="font16" id="p7_t35" reading_order_no="34" segment_no="2" tag_type="table">83.65 +2.84</text>
<text top="151" left="459" width="36" height="8" font="font16" id="p7_t36" reading_order_no="35" segment_no="2" tag_type="table">78.57 +8.79</text>
<text top="151" left="509" width="36" height="8" font="font16" id="p7_t37" reading_order_no="36" segment_no="2" tag_type="table">87.37 +2.46</text>
<text top="167" left="63" width="11" height="8" font="font10" id="p7_t38" reading_order_no="37" segment_no="2" tag_type="table">(ii)</text>
<text top="162" left="108" width="13" height="8" font="font10" id="p7_t39" reading_order_no="38" segment_no="2" tag_type="table">NN</text>
<text top="162" left="162" width="20" height="8" font="font10" id="p7_t40" reading_order_no="39" segment_no="2" tag_type="table">64.04</text>
<text top="162" left="213" width="20" height="8" font="font10" id="p7_t41" reading_order_no="40" segment_no="2" tag_type="table">79.71</text>
<text top="162" left="263" width="20" height="8" font="font10" id="p7_t42" reading_order_no="41" segment_no="2" tag_type="table">71.60</text>
<text top="162" left="314" width="20" height="8" font="font10" id="p7_t43" reading_order_no="42" segment_no="2" tag_type="table">84.62</text>
<text top="162" left="365" width="20" height="8" font="font10" id="p7_t44" reading_order_no="43" segment_no="2" tag_type="table">66.73</text>
<text top="162" left="415" width="20" height="8" font="font10" id="p7_t45" reading_order_no="44" segment_no="2" tag_type="table">81.85</text>
<text top="162" left="466" width="20" height="8" font="font10" id="p7_t46" reading_order_no="45" segment_no="2" tag_type="table">72.97</text>
<text top="162" left="516" width="20" height="8" font="font10" id="p7_t47" reading_order_no="46" segment_no="2" tag_type="table">85.74</text>
<text top="172" left="100" width="30" height="8" font="font10" id="p7_t48" reading_order_no="47" segment_no="2" tag_type="table">+ ESFR</text>
<text top="172" left="157" width="33" height="8" font="font16" id="p7_t49" reading_order_no="48" segment_no="2" tag_type="table">70.94 +6.9</text>
<text top="172" left="208" width="33" height="8" font="font16" id="p7_t50" reading_order_no="49" segment_no="2" tag_type="table">81.61 +1.9</text>
<text top="172" left="256" width="37" height="8" font="font16" id="p7_t51" reading_order_no="50" segment_no="2" tag_type="table">77.44 +5.84</text>
<text top="172" left="307" width="36" height="8" font="font16" id="p7_t52" reading_order_no="51" segment_no="2" tag_type="table">85.84 +1.22</text>
<text top="172" left="358" width="36" height="8" font="font16" id="p7_t53" reading_order_no="52" segment_no="2" tag_type="table">74.01 +7.28</text>
<text top="172" left="408" width="36" height="8" font="font16" id="p7_t54" reading_order_no="53" segment_no="2" tag_type="table">83.58 +1.73</text>
<text top="172" left="459" width="36" height="8" font="font16" id="p7_t55" reading_order_no="54" segment_no="2" tag_type="table">79.13 +6.16</text>
<text top="172" left="509" width="36" height="8" font="font16" id="p7_t56" reading_order_no="55" segment_no="2" tag_type="table">87.08 +1.34</text>
<text top="187" left="61" width="13" height="8" font="font10" id="p7_t57" reading_order_no="56" segment_no="2" tag_type="table">(iii)</text>
<text top="182" left="101" width="26" height="8" font="font10" id="p7_t58" reading_order_no="57" segment_no="2" tag_type="table">CSPN †</text>
<text top="182" left="162" width="20" height="8" font="font10" id="p7_t59" reading_order_no="58" segment_no="2" tag_type="table">64.54</text>
<text top="182" left="213" width="20" height="8" font="font10" id="p7_t60" reading_order_no="59" segment_no="2" tag_type="table">80.49</text>
<text top="182" left="263" width="20" height="8" font="font10" id="p7_t61" reading_order_no="60" segment_no="2" tag_type="table">71.89</text>
<text top="182" left="314" width="20" height="8" font="font10" id="p7_t62" reading_order_no="61" segment_no="2" tag_type="table">85.09</text>
<text top="182" left="365" width="20" height="8" font="font10" id="p7_t63" reading_order_no="62" segment_no="2" tag_type="table">67.52</text>
<text top="182" left="415" width="20" height="8" font="font10" id="p7_t64" reading_order_no="63" segment_no="2" tag_type="table">82.36</text>
<text top="182" left="466" width="20" height="8" font="font10" id="p7_t65" reading_order_no="64" segment_no="2" tag_type="table">73.00</text>
<text top="182" left="516" width="20" height="8" font="font10" id="p7_t66" reading_order_no="65" segment_no="2" tag_type="table">86.28</text>
<text top="192" left="100" width="30" height="8" font="font10" id="p7_t67" reading_order_no="66" segment_no="2" tag_type="table">+ ESFR</text>
<text top="192" left="155" width="37" height="8" font="font16" id="p7_t68" reading_order_no="67" segment_no="2" tag_type="table">71.71 +7.17</text>
<text top="192" left="206" width="36" height="8" font="font16" id="p7_t69" reading_order_no="68" segment_no="2" tag_type="table">82.22 +1.73</text>
<text top="192" left="256" width="37" height="8" font="font16" id="p7_t70" reading_order_no="69" segment_no="2" tag_type="table">78.17 +6.28</text>
<text top="192" left="307" width="36" height="8" font="font16" id="p7_t71" reading_order_no="70" segment_no="2" tag_type="table">86.38 +1.29</text>
<text top="192" left="358" width="36" height="8" font="font16" id="p7_t72" reading_order_no="71" segment_no="2" tag_type="table">74.83 +7.31</text>
<text top="192" left="408" width="36" height="8" font="font16" id="p7_t73" reading_order_no="72" segment_no="2" tag_type="table">84.17 +1.81</text>
<text top="192" left="459" width="36" height="8" font="font16" id="p7_t74" reading_order_no="73" segment_no="2" tag_type="table">79.65 +6.65</text>
<text top="192" left="509" width="36" height="8" font="font16" id="p7_t75" reading_order_no="74" segment_no="2" tag_type="table">87.57 +1.29</text>
<text top="213" left="62" width="13" height="8" font="font10" id="p7_t76" reading_order_no="75" segment_no="2" tag_type="table">(iv)</text>
<text top="202" left="93" width="42" height="9" font="font10" id="p7_t77" reading_order_no="76" segment_no="2" tag_type="table">BD-CSPN †</text>
<text top="203" left="162" width="20" height="8" font="font10" id="p7_t78" reading_order_no="77" segment_no="2" tag_type="table">70.00</text>
<text top="202" left="213" width="20" height="8" font="font16" id="p7_t79" reading_order_no="78" segment_no="2" tag_type="table">82.36</text>
<text top="203" left="263" width="20" height="8" font="font10" id="p7_t80" reading_order_no="79" segment_no="2" tag_type="table">77.28</text>
<text top="202" left="314" width="20" height="8" font="font16" id="p7_t81" reading_order_no="80" segment_no="2" tag_type="table">86.55</text>
<text top="203" left="365" width="20" height="8" font="font10" id="p7_t82" reading_order_no="81" segment_no="2" tag_type="table">72.74</text>
<text top="203" left="415" width="20" height="8" font="font10" id="p7_t83" reading_order_no="82" segment_no="2" tag_type="table">84.14</text>
<text top="203" left="466" width="20" height="8" font="font10" id="p7_t84" reading_order_no="83" segment_no="2" tag_type="table">78.89</text>
<text top="202" left="516" width="20" height="8" font="font16" id="p7_t85" reading_order_no="84" segment_no="2" tag_type="table">87.72</text>
<text top="212" left="100" width="30" height="9" font="font10" id="p7_t86" reading_order_no="85" segment_no="2" tag_type="table">+ ESFR</text>
<text top="212" left="155" width="37" height="8" font="font16" id="p7_t87" reading_order_no="86" segment_no="2" tag_type="table">73.98 +3.98</text>
<text top="213" left="207" width="35" height="8" font="font10" id="p7_t88" reading_order_no="87" segment_no="2" tag_type="table">82.32 -0.04</text>
<text top="212" left="256" width="37" height="8" font="font16" id="p7_t89" reading_order_no="88" segment_no="2" tag_type="table">80.13 +2.85</text>
<text top="213" left="308" width="35" height="8" font="font10" id="p7_t90" reading_order_no="89" segment_no="2" tag_type="table">86.34 -0.21</text>
<text top="212" left="358" width="36" height="8" font="font16" id="p7_t91" reading_order_no="90" segment_no="2" tag_type="table">76.84 +4.10</text>
<text top="212" left="408" width="36" height="8" font="font16" id="p7_t92" reading_order_no="91" segment_no="2" tag_type="table">84.36 +0.22</text>
<text top="212" left="459" width="36" height="8" font="font16" id="p7_t93" reading_order_no="92" segment_no="2" tag_type="table">81.77 +2.88</text>
<text top="213" left="510" width="35" height="8" font="font10" id="p7_t94" reading_order_no="93" segment_no="2" tag_type="table">87.61 -0.11</text>
<text top="222" left="89" width="52" height="9" font="font10" id="p7_t95" reading_order_no="94" segment_no="2" tag_type="table">+ ESFR-Semi</text>
<text top="222" left="206" width="36" height="8" font="font16" id="p7_t96" reading_order_no="95" segment_no="2" tag_type="table">82.89 +0.53</text>
<text top="222" left="307" width="36" height="8" font="font16" id="p7_t97" reading_order_no="96" segment_no="2" tag_type="table">86.83 +0.28</text>
<text top="222" left="408" width="36" height="8" font="font16" id="p7_t98" reading_order_no="97" segment_no="2" tag_type="table">84.97 +0.83</text>
<text top="222" left="509" width="36" height="8" font="font16" id="p7_t99" reading_order_no="98" segment_no="2" tag_type="table">88.10 +0.38</text>
<text top="260" left="55" width="234" height="9" font="font4" id="p7_t100" reading_order_no="99" segment_no="3" tag_type="text">we subtract the center of task sample embeddings from each</text>
<text top="272" left="55" width="235" height="9" font="font4" id="p7_t101" reading_order_no="100" segment_no="3" tag_type="text">embedding as in Lichtenstein et al. ( 2020 ) since it performs</text>
<text top="284" left="55" width="236" height="9" font="font4" id="p7_t102" reading_order_no="101" segment_no="3" tag_type="text">better on all baseline methods. For BD-CSPN ( Liu et al. ,</text>
<text top="296" left="55" width="205" height="9" font="font8" id="p7_t103" reading_order_no="102" segment_no="3" tag_type="text">2020 ), the aforementioned shifting-term defined as:</text>
<text top="312" left="93" width="32" height="16" font="font6" id="p7_t104" reading_order_no="103" segment_no="5" tag_type="formula">4 = 1</text>
<text top="325" left="117" width="12" height="10" font="font6" id="p7_t105" reading_order_no="104" segment_no="5" tag_type="formula">| S |</text>
<text top="316" left="134" width="14" height="4" font="font24" id="p7_t106" reading_order_no="105" segment_no="5" tag_type="formula">X</text>
<text top="333" left="132" width="19" height="7" font="font19" id="p7_t107" reading_order_no="106" segment_no="5" tag_type="formula">x s ∈ S</text>
<text top="318" left="153" width="33" height="11" font="font17" id="p7_t108" reading_order_no="107" segment_no="5" tag_type="formula">f ( x s ) −</text>
<text top="312" left="194" width="5" height="9" font="font18" id="p7_t109" reading_order_no="108" segment_no="5" tag_type="formula">1</text>
<text top="325" left="190" width="13" height="10" font="font6" id="p7_t110" reading_order_no="109" segment_no="5" tag_type="formula">| Q |</text>
<text top="316" left="209" width="14" height="4" font="font24" id="p7_t111" reading_order_no="110" segment_no="5" tag_type="formula">X</text>
<text top="333" left="206" width="20" height="7" font="font19" id="p7_t112" reading_order_no="111" segment_no="5" tag_type="formula">x q ∈ Q</text>
<text top="319" left="228" width="24" height="10" font="font17" id="p7_t113" reading_order_no="112" segment_no="5" tag_type="formula">f ( x q )</text>
<text top="319" left="278" width="12" height="9" font="font4" id="p7_t114" reading_order_no="113" segment_no="5" tag_type="text">(8)</text>
<text top="349" left="55" width="234" height="9" font="font4" id="p7_t115" reading_order_no="114" segment_no="6" tag_type="text">is added for query sample before centering as in Ziko</text>
<text top="361" left="55" width="236" height="9" font="font8" id="p7_t116" reading_order_no="115" segment_no="6" tag_type="text">et al. ( 2020 ); Liu et al. ( 2020 ). For reconstruction train-</text>
<text top="373" left="55" width="236" height="9" font="font4" id="p7_t117" reading_order_no="116" segment_no="6" tag_type="text">ing, when computing the reconstruction loss, output em-</text>
<text top="384" left="55" width="234" height="9" font="font4" id="p7_t118" reading_order_no="117" segment_no="6" tag_type="text">beddings of the reconstruction module and the pre-trained</text>
<text top="396" left="55" width="234" height="9" font="font4" id="p7_t119" reading_order_no="118" segment_no="6" tag_type="text">embedding network follows the same preprocessing. For</text>
<text top="408" left="55" width="234" height="9" font="font4" id="p7_t120" reading_order_no="119" segment_no="6" tag_type="text">few-shot classification with new embeddings, we apply only</text>
<text top="419" left="55" width="234" height="10" font="font4" id="p7_t121" reading_order_no="120" segment_no="6" tag_type="text">l2-normalization since it performs the best. 12 We refer to</text>
<text top="432" left="55" width="187" height="9" font="font4" id="p7_t122" reading_order_no="121" segment_no="6" tag_type="text">Appendix A for more details on preprocessing.</text>
<text top="457" left="55" width="48" height="9" font="font1" id="p7_t123" reading_order_no="122" segment_no="8" tag_type="title">5.3. Results</text>
<text top="476" left="55" width="235" height="9" font="font1" id="p7_t124" reading_order_no="123" segment_no="10" tag_type="text">Improvement by ESFR: We validate the improvement of</text>
<text top="488" left="55" width="234" height="9" font="font4" id="p7_t125" reading_order_no="124" segment_no="10" tag_type="text">our method when used in conjunction with commonly used</text>
<text top="500" left="55" width="234" height="9" font="font4" id="p7_t126" reading_order_no="125" segment_no="10" tag_type="text">few-shot classification methods. We evaluate our method in</text>
<text top="512" left="55" width="234" height="9" font="font4" id="p7_t127" reading_order_no="126" segment_no="10" tag_type="text">the most common 5-way 1- and 5-shot settings on standard</text>
<text top="524" left="55" width="236" height="9" font="font5" id="p7_t128" reading_order_no="127" segment_no="10" tag_type="text">mini -ImageNet, tiered -ImageNet datasets with ResNet-18,</text>
<text top="536" left="55" width="234" height="9" font="font4" id="p7_t129" reading_order_no="128" segment_no="10" tag_type="text">WRN-28-10 backbone networks. The results are listed in</text>
<text top="548" left="55" width="31" height="9" font="font4" id="p7_t130" reading_order_no="129" segment_no="10" tag_type="text">Table 1 .</text>
<text top="566" left="55" width="234" height="9" font="font4" id="p7_t131" reading_order_no="130" segment_no="11" tag_type="text">In Table 1 -(i, ii), we investigate our method with linear and</text>
<text top="578" left="55" width="234" height="9" font="font4" id="p7_t132" reading_order_no="131" segment_no="11" tag_type="text">NN classifiers. Each is a widely used baseline method in</text>
<text top="590" left="55" width="236" height="9" font="font4" id="p7_t133" reading_order_no="132" segment_no="11" tag_type="text">unsupervised representation learning and few-shot classifi-</text>
<text top="602" left="55" width="234" height="9" font="font4" id="p7_t134" reading_order_no="133" segment_no="11" tag_type="text">cation. Our method provides noticeable improvements in</text>
<text top="613" left="55" width="235" height="10" font="font4" id="p7_t135" reading_order_no="134" segment_no="11" tag_type="text">all settings; +5.9% ∼ 8.9% for 1-shot and +1.5% ∼ 3.1% for</text>
<text top="626" left="55" width="234" height="9" font="font4" id="p7_t136" reading_order_no="135" segment_no="11" tag_type="text">5-shot settings. This roughly indicates that our embedding</text>
<text top="637" left="55" width="234" height="9" font="font4" id="p7_t137" reading_order_no="136" segment_no="11" tag_type="text">adaptation provides well-clustered new embeddings; hence</text>
<text top="649" left="55" width="211" height="9" font="font4" id="p7_t138" reading_order_no="137" segment_no="11" tag_type="text">samples can be classified by simple affine classifiers.</text>
<text top="667" left="55" width="236" height="9" font="font4" id="p7_t139" reading_order_no="138" segment_no="14" tag_type="text">In Table 1 -(iii, iv), we compare our method with the semi-</text>
<text top="679" left="55" width="235" height="9" font="font4" id="p7_t140" reading_order_no="139" segment_no="14" tag_type="text">supervised learning approach, Prototype-Rectification (PR)</text>
<text top="697" left="65" width="225" height="10" font="font10" id="p7_t141" reading_order_no="173" segment_no="16" tag_type="footnote">12 Note that applying only l2-normalization for baseline methods</text>
<text top="709" left="55" width="89" height="8" font="font10" id="p7_t142" reading_order_no="174" segment_no="16" tag_type="footnote">worsen the performance.</text>
<text top="260" left="307" width="235" height="9" font="font4" id="p7_t143" reading_order_no="140" segment_no="4" tag_type="text">( Liu et al. , 2020 ). For a fair comparison, we use the same</text>
<text top="272" left="307" width="236" height="9" font="font4" id="p7_t144" reading_order_no="141" segment_no="4" tag_type="text">preprocessing with shifting-term ( 8 ) and cosine similarity-</text>
<text top="283" left="307" width="234" height="10" font="font4" id="p7_t145" reading_order_no="142" segment_no="4" tag_type="text">based nearest neighbor (CSPN) classifier as in BD-CSPN †</text>
<text top="295" left="307" width="234" height="10" font="font4" id="p7_t146" reading_order_no="143" segment_no="4" tag_type="text">(which includes PR); the result is described as CSPN †</text>
<text top="308" left="306" width="235" height="9" font="font4" id="p7_t147" reading_order_no="144" segment_no="4" tag_type="text">+ ESFR. We can observe that ESFR further improves</text>
<text top="319" left="307" width="236" height="10" font="font4" id="p7_t148" reading_order_no="145" segment_no="4" tag_type="text">accuracies on 1-shot by +0.8% ∼ 2.1% compared to BD-</text>
<text top="331" left="307" width="236" height="10" font="font4" id="p7_t149" reading_order_no="146" segment_no="4" tag_type="text">CSPN † while showing comparable improvements on 5-shot.</text>
<text top="344" left="307" width="234" height="9" font="font4" id="p7_t150" reading_order_no="147" segment_no="4" tag_type="text">This result implies that, though unsupervised adaptation</text>
<text top="356" left="307" width="236" height="9" font="font4" id="p7_t151" reading_order_no="148" segment_no="4" tag_type="text">has received less attention in few-shot classification, well-</text>
<text top="368" left="307" width="234" height="9" font="font4" id="p7_t152" reading_order_no="149" segment_no="4" tag_type="text">designed unsupervised learning can provide comparable</text>
<text top="380" left="307" width="210" height="9" font="font4" id="p7_t153" reading_order_no="150" segment_no="4" tag_type="text">improvements to conventional transductive methods.</text>
<text top="398" left="307" width="234" height="9" font="font4" id="p7_t154" reading_order_no="151" segment_no="7" tag_type="text">Performance can be further improved by incorporating our</text>
<text top="410" left="307" width="234" height="9" font="font4" id="p7_t155" reading_order_no="152" segment_no="7" tag_type="text">method into BD-CSPN, a transductive method. The result is</text>
<text top="421" left="307" width="234" height="9" font="font4" id="p7_t156" reading_order_no="153" segment_no="7" tag_type="text">described in Table 1 -(iv). For the 5-shot setting, there was</text>
<text top="433" left="307" width="234" height="9" font="font4" id="p7_t157" reading_order_no="154" segment_no="7" tag_type="text">no additional gain by combining our method; however, for<a href="deeplearning_paper6.html#10">Lichtenstein et al.</a></text>
<text top="445" left="307" width="234" height="9" font="font4" id="p7_t158" reading_order_no="155" segment_no="7" tag_type="text">the 1-shot setting, improvements of +2.9% ∼ 4.1% indicates<a href="deeplearning_paper6.html#10">(</a></text>
<text top="457" left="307" width="233" height="9" font="font4" id="p7_t159" reading_order_no="156" segment_no="7" tag_type="text">that ESFR can offer a complementary improvement to PR.<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="475" left="307" width="236" height="9" font="font4" id="p7_t160" reading_order_no="157" segment_no="9" tag_type="text">In the last row of Table 1 -(iv), we also investigate ESFR-<a href="deeplearning_paper6.html#10">) </a>since it performs</text>
<text top="487" left="307" width="236" height="10" font="font4" id="p7_t161" reading_order_no="158" segment_no="9" tag_type="text">semi that uses the semi-supervised loss L Semi ( 7 ) for em-<a href="deeplearning_paper6.html#10">(</a></text>
<text top="499" left="307" width="234" height="9" font="font4" id="p7_t162" reading_order_no="159" segment_no="9" tag_type="text">bedding adaptation. As mentioned in Section 5.2 , we chose<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="511" left="307" width="235" height="9" font="font4" id="p7_t163" reading_order_no="160" segment_no="9" tag_type="text">the trade-off parameter λ that best performs with validation<a href="deeplearning_paper6.html#10">,</a></text>
<text top="523" left="307" width="235" height="9" font="font4" id="p7_t164" reading_order_no="161" segment_no="9" tag_type="text">tasks. For the 1-shot setting, we find that λ = 0 performs<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="535" left="307" width="236" height="9" font="font4" id="p7_t165" reading_order_no="162" segment_no="9" tag_type="text">the best that does not use label information during adapta-<a href="deeplearning_paper6.html#10">), </a>the aforementioned shifting-term defined as:</text>
<text top="547" left="307" width="234" height="9" font="font4" id="p7_t166" reading_order_no="163" segment_no="9" tag_type="text">tion. For the 5-shot setting, ESFR-semi gives an additional</text>
<text top="558" left="306" width="141" height="10" font="font4" id="p7_t167" reading_order_no="164" segment_no="9" tag_type="text">+0.3% ∼ 0.8% gain over BD-CSPN.</text>
<text top="577" left="307" width="234" height="9" font="font1" id="p7_t168" reading_order_no="165" segment_no="12" tag_type="text">Comparison with prior work: We compare our method</text>
<text top="587" left="307" width="236" height="11" font="font4" id="p7_t169" reading_order_no="166" segment_no="12" tag_type="text">with prior few-shot classification methods 13 on mini -</text>
<text top="601" left="307" width="237" height="9" font="font4" id="p7_t170" reading_order_no="167" segment_no="12" tag_type="text">ImageNet, tiered -ImageNet, and CUB datasets in Table 2 .</text>
<text top="613" left="307" width="234" height="9" font="font4" id="p7_t171" reading_order_no="168" segment_no="12" tag_type="text">We use BD-CSPN + ESFR and BD-CSPN + ESFR-Semi as</text>
<text top="625" left="307" width="52" height="9" font="font4" id="p7_t172" reading_order_no="169" segment_no="12" tag_type="text">our methods.</text>
<text top="643" left="307" width="236" height="9" font="font4" id="p7_t173" reading_order_no="170" segment_no="13" tag_type="text">For the 1-shot setting, our method achieves new state-of-the-</text>
<text top="655" left="307" width="234" height="9" font="font4" id="p7_t174" reading_order_no="171" segment_no="13" tag_type="text">art performance on all datasets and backbone networks. To</text>
<text top="667" left="307" width="236" height="9" font="font4" id="p7_t175" reading_order_no="172" segment_no="13" tag_type="text">be specific, our method outperforms the previous state-of-</text>
<text top="684" left="317" width="224" height="10" font="font10" id="p7_t176" reading_order_no="175" segment_no="15" tag_type="footnote">13 We separately compared the work by ( Boudiaf et al. , 2020 ) in</text>
<text top="696" left="307" width="235" height="8" font="font10" id="p7_t177" reading_order_no="176" segment_no="15" tag_type="footnote">Appendix B; since they use strong prior that the number of query</text>
<text top="706" left="307" width="94" height="8" font="font10" id="p7_t178" reading_order_no="177" segment_no="15" tag_type="footnote">samples per class is equal.</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="77" left="55" width="488" height="8" font="font34" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="title">Table 2. Comparison with state-of-the-art methods of 5-way 1- and 5-shot accuracy (in %) on mini -ImageNet, tiered -ImageNet and CUB.</text>
<text top="88" left="55" width="132" height="8" font="font10" id="p8_t3" reading_order_no="2" segment_no="1" tag_type="title">The best results are reported in bold .</text>
<text top="103" left="307" width="57" height="8" font="font42" id="p8_t4" reading_order_no="3" segment_no="2" tag_type="table">mini -ImageNet</text>
<text top="103" left="377" width="61" height="8" font="font42" id="p8_t5" reading_order_no="4" segment_no="2" tag_type="table">tiered -ImageNet</text>
<text top="103" left="470" width="19" height="8" font="font16" id="p8_t6" reading_order_no="5" segment_no="2" tag_type="table">CUB</text>
<text top="113" left="88" width="30" height="8" font="font16" id="p8_t7" reading_order_no="6" segment_no="2" tag_type="table">Method</text>
<text top="113" left="256" width="38" height="8" font="font16" id="p8_t8" reading_order_no="7" segment_no="2" tag_type="table">Backbone</text>
<text top="113" left="306" width="23" height="8" font="font16" id="p8_t9" reading_order_no="8" segment_no="2" tag_type="table">1-shot</text>
<text top="113" left="342" width="23" height="8" font="font16" id="p8_t10" reading_order_no="9" segment_no="2" tag_type="table">5-shot</text>
<text top="113" left="377" width="23" height="8" font="font16" id="p8_t11" reading_order_no="10" segment_no="2" tag_type="table">1-shot</text>
<text top="113" left="414" width="23" height="8" font="font16" id="p8_t12" reading_order_no="11" segment_no="2" tag_type="table">5-shot</text>
<text top="113" left="450" width="23" height="8" font="font16" id="p8_t13" reading_order_no="12" segment_no="2" tag_type="table">1-shot</text>
<text top="113" left="486" width="23" height="8" font="font16" id="p8_t14" reading_order_no="13" segment_no="2" tag_type="table">5-shot</text>
<text top="123" left="88" width="94" height="8" font="font10" id="p8_t15" reading_order_no="14" segment_no="2" tag_type="table">MAML ( Finn et al. , 2017 )</text>
<text top="123" left="256" width="38" height="8" font="font10" id="p8_t16" reading_order_no="15" segment_no="2" tag_type="table">ResNet-18</text>
<text top="123" left="308" width="20" height="8" font="font10" id="p8_t17" reading_order_no="16" segment_no="2" tag_type="table">49.61</text>
<text top="123" left="343" width="20" height="8" font="font10" id="p8_t18" reading_order_no="17" segment_no="2" tag_type="table">65.72</text>
<text top="123" left="387" width="3" height="8" font="font10" id="p8_t19" reading_order_no="18" segment_no="2" tag_type="table">-</text>
<text top="123" left="424" width="3" height="8" font="font10" id="p8_t20" reading_order_no="19" segment_no="2" tag_type="table">-</text>
<text top="123" left="452" width="20" height="8" font="font10" id="p8_t21" reading_order_no="20" segment_no="2" tag_type="table">68.42</text>
<text top="123" left="487" width="20" height="8" font="font10" id="p8_t22" reading_order_no="21" segment_no="2" tag_type="table">83.47</text>
<text top="133" left="88" width="88" height="8" font="font10" id="p8_t23" reading_order_no="22" segment_no="2" tag_type="table">Chen ( Chen et al. , 2019 )</text>
<text top="133" left="256" width="38" height="8" font="font10" id="p8_t24" reading_order_no="23" segment_no="2" tag_type="table">ResNet-18<a href="deeplearning_paper6.html#9">(</a></text>
<text top="133" left="308" width="20" height="8" font="font10" id="p8_t25" reading_order_no="24" segment_no="2" tag_type="table">51.87<a href="deeplearning_paper6.html#9">Finn et al.</a></text>
<text top="133" left="343" width="20" height="8" font="font10" id="p8_t26" reading_order_no="25" segment_no="2" tag_type="table">75.68<a href="deeplearning_paper6.html#9">,</a></text>
<text top="133" left="387" width="3" height="8" font="font10" id="p8_t27" reading_order_no="26" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#9">2017</a></text>
<text top="133" left="424" width="3" height="8" font="font10" id="p8_t28" reading_order_no="27" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#9">)</a></text>
<text top="133" left="452" width="20" height="8" font="font10" id="p8_t29" reading_order_no="28" segment_no="2" tag_type="table">67.02</text>
<text top="133" left="487" width="20" height="8" font="font10" id="p8_t30" reading_order_no="29" segment_no="2" tag_type="table">83.58</text>
<text top="143" left="88" width="101" height="8" font="font10" id="p8_t31" reading_order_no="30" segment_no="2" tag_type="table">ProtoNet ( Snell et al. , 2017 )</text>
<text top="143" left="256" width="38" height="8" font="font10" id="p8_t32" reading_order_no="31" segment_no="2" tag_type="table">ResNet-18</text>
<text top="143" left="308" width="20" height="8" font="font10" id="p8_t33" reading_order_no="32" segment_no="2" tag_type="table">54.16</text>
<text top="143" left="343" width="20" height="8" font="font10" id="p8_t34" reading_order_no="33" segment_no="2" tag_type="table">73.68</text>
<text top="143" left="387" width="3" height="8" font="font10" id="p8_t35" reading_order_no="34" segment_no="2" tag_type="table">-</text>
<text top="143" left="424" width="3" height="8" font="font10" id="p8_t36" reading_order_no="35" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#9">(</a></text>
<text top="143" left="452" width="20" height="8" font="font10" id="p8_t37" reading_order_no="36" segment_no="2" tag_type="table">72.99<a href="deeplearning_paper6.html#9">Chen et al.</a></text>
<text top="143" left="487" width="20" height="8" font="font10" id="p8_t38" reading_order_no="37" segment_no="2" tag_type="table">86.64<a href="deeplearning_paper6.html#9">,</a></text>
<text top="153" left="88" width="79" height="8" font="font10" id="p8_t39" reading_order_no="38" segment_no="2" tag_type="table">TPN ( Liu et al. , 2019 )<a href="deeplearning_paper6.html#9">2019</a></text>
<text top="153" left="256" width="38" height="8" font="font10" id="p8_t40" reading_order_no="39" segment_no="2" tag_type="table">ResNet-12<a href="deeplearning_paper6.html#9">)</a></text>
<text top="153" left="308" width="20" height="8" font="font10" id="p8_t41" reading_order_no="40" segment_no="2" tag_type="table">59.46</text>
<text top="153" left="343" width="20" height="8" font="font10" id="p8_t42" reading_order_no="41" segment_no="2" tag_type="table">75.65</text>
<text top="153" left="387" width="3" height="8" font="font10" id="p8_t43" reading_order_no="42" segment_no="2" tag_type="table">-</text>
<text top="153" left="424" width="3" height="8" font="font10" id="p8_t44" reading_order_no="43" segment_no="2" tag_type="table">-</text>
<text top="153" left="461" width="3" height="8" font="font10" id="p8_t45" reading_order_no="44" segment_no="2" tag_type="table">-</text>
<text top="153" left="496" width="3" height="8" font="font10" id="p8_t46" reading_order_no="45" segment_no="2" tag_type="table">-</text>
<text top="163" left="88" width="93" height="8" font="font10" id="p8_t47" reading_order_no="46" segment_no="2" tag_type="table">TEAM ( Qiao et al. , 2019 )</text>
<text top="163" left="256" width="38" height="8" font="font10" id="p8_t48" reading_order_no="47" segment_no="2" tag_type="table">ResNet-18<a href="deeplearning_paper6.html#10">(</a></text>
<text top="163" left="308" width="20" height="8" font="font10" id="p8_t49" reading_order_no="48" segment_no="2" tag_type="table">60.07<a href="deeplearning_paper6.html#10">Snell et al.</a></text>
<text top="163" left="343" width="20" height="8" font="font10" id="p8_t50" reading_order_no="49" segment_no="2" tag_type="table">75.90<a href="deeplearning_paper6.html#10">,</a></text>
<text top="163" left="387" width="3" height="8" font="font10" id="p8_t51" reading_order_no="50" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#10">2017</a></text>
<text top="163" left="424" width="3" height="8" font="font10" id="p8_t52" reading_order_no="51" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#10">)</a></text>
<text top="163" left="452" width="20" height="8" font="font10" id="p8_t53" reading_order_no="52" segment_no="2" tag_type="table">80.16</text>
<text top="163" left="487" width="20" height="8" font="font10" id="p8_t54" reading_order_no="53" segment_no="2" tag_type="table">87.17</text>
<text top="173" left="88" width="113" height="8" font="font10" id="p8_t55" reading_order_no="54" segment_no="2" tag_type="table">SimpleShot ( Wang et al. , 2019 )</text>
<text top="173" left="256" width="38" height="8" font="font10" id="p8_t56" reading_order_no="55" segment_no="2" tag_type="table">ResNet-18</text>
<text top="173" left="308" width="20" height="8" font="font10" id="p8_t57" reading_order_no="56" segment_no="2" tag_type="table">63.10</text>
<text top="173" left="343" width="20" height="8" font="font10" id="p8_t58" reading_order_no="57" segment_no="2" tag_type="table">79.92</text>
<text top="173" left="379" width="20" height="8" font="font10" id="p8_t59" reading_order_no="58" segment_no="2" tag_type="table">69.68</text>
<text top="173" left="415" width="20" height="8" font="font10" id="p8_t60" reading_order_no="59" segment_no="2" tag_type="table">84.56<a href="deeplearning_paper6.html#10">(</a></text>
<text top="173" left="452" width="20" height="8" font="font10" id="p8_t61" reading_order_no="60" segment_no="2" tag_type="table">70.28<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="173" left="487" width="20" height="8" font="font10" id="p8_t62" reading_order_no="61" segment_no="2" tag_type="table">86.37<a href="deeplearning_paper6.html#10">,</a></text>
<text top="183" left="88" width="77" height="8" font="font10" id="p8_t63" reading_order_no="62" segment_no="2" tag_type="table">CTM ( Li et al. , 2019 )<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="183" left="256" width="38" height="8" font="font10" id="p8_t64" reading_order_no="63" segment_no="2" tag_type="table">ResNet-18<a href="deeplearning_paper6.html#10">)</a></text>
<text top="183" left="308" width="20" height="8" font="font10" id="p8_t65" reading_order_no="64" segment_no="2" tag_type="table">64.12</text>
<text top="183" left="343" width="20" height="8" font="font10" id="p8_t66" reading_order_no="65" segment_no="2" tag_type="table">78.64</text>
<text top="183" left="379" width="20" height="8" font="font10" id="p8_t67" reading_order_no="66" segment_no="2" tag_type="table">68.41</text>
<text top="183" left="415" width="20" height="8" font="font10" id="p8_t68" reading_order_no="67" segment_no="2" tag_type="table">84.28</text>
<text top="183" left="461" width="3" height="8" font="font10" id="p8_t69" reading_order_no="68" segment_no="2" tag_type="table">-</text>
<text top="183" left="496" width="3" height="8" font="font10" id="p8_t70" reading_order_no="69" segment_no="2" tag_type="table">-</text>
<text top="193" left="88" width="81" height="8" font="font10" id="p8_t71" reading_order_no="70" segment_no="2" tag_type="table">FEAT ( Ye et al. , 2020 )</text>
<text top="193" left="256" width="38" height="8" font="font10" id="p8_t72" reading_order_no="71" segment_no="2" tag_type="table">ResNet-18<a href="deeplearning_paper6.html#10">(</a></text>
<text top="193" left="308" width="20" height="8" font="font10" id="p8_t73" reading_order_no="72" segment_no="2" tag_type="table">66.78<a href="deeplearning_paper6.html#10">Qiao et al.</a></text>
<text top="193" left="343" width="20" height="8" font="font10" id="p8_t74" reading_order_no="73" segment_no="2" tag_type="table">82.05<a href="deeplearning_paper6.html#10">,</a></text>
<text top="193" left="379" width="20" height="8" font="font10" id="p8_t75" reading_order_no="74" segment_no="2" tag_type="table">70.80<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="193" left="415" width="20" height="8" font="font10" id="p8_t76" reading_order_no="75" segment_no="2" tag_type="table">84.79<a href="deeplearning_paper6.html#10">)</a></text>
<text top="193" left="461" width="3" height="8" font="font10" id="p8_t77" reading_order_no="76" segment_no="2" tag_type="table">-</text>
<text top="193" left="496" width="3" height="8" font="font10" id="p8_t78" reading_order_no="77" segment_no="2" tag_type="table">-</text>
<text top="203" left="88" width="100" height="8" font="font10" id="p8_t79" reading_order_no="78" segment_no="2" tag_type="table">BD-CSPN ( Liu et al. , 2020 )</text>
<text top="203" left="256" width="38" height="8" font="font10" id="p8_t80" reading_order_no="79" segment_no="2" tag_type="table">ResNet-18</text>
<text top="203" left="308" width="20" height="8" font="font10" id="p8_t81" reading_order_no="80" segment_no="2" tag_type="table">70.00</text>
<text top="203" left="343" width="20" height="8" font="font10" id="p8_t82" reading_order_no="81" segment_no="2" tag_type="table">82.36</text>
<text top="203" left="379" width="20" height="8" font="font10" id="p8_t83" reading_order_no="82" segment_no="2" tag_type="table">77.28</text>
<text top="203" left="415" width="20" height="8" font="font10" id="p8_t84" reading_order_no="83" segment_no="2" tag_type="table">86.55<a href="deeplearning_paper6.html#11">(</a></text>
<text top="203" left="452" width="20" height="8" font="font10" id="p8_t85" reading_order_no="84" segment_no="2" tag_type="table">78.89<a href="deeplearning_paper6.html#11">Wang et al.</a></text>
<text top="203" left="487" width="20" height="8" font="font10" id="p8_t86" reading_order_no="85" segment_no="2" tag_type="table">88.70<a href="deeplearning_paper6.html#11">,</a></text>
<text top="213" left="88" width="119" height="8" font="font10" id="p8_t87" reading_order_no="86" segment_no="2" tag_type="table">LaplacianShot ( Ziko et al. , 2020 )<a href="deeplearning_paper6.html#11">2019</a></text>
<text top="213" left="256" width="38" height="8" font="font10" id="p8_t88" reading_order_no="87" segment_no="2" tag_type="table">ResNet-18<a href="deeplearning_paper6.html#11">)</a></text>
<text top="213" left="308" width="20" height="8" font="font10" id="p8_t89" reading_order_no="88" segment_no="2" tag_type="table">72.11</text>
<text top="213" left="343" width="20" height="8" font="font10" id="p8_t90" reading_order_no="89" segment_no="2" tag_type="table">82.31</text>
<text top="213" left="379" width="20" height="8" font="font10" id="p8_t91" reading_order_no="90" segment_no="2" tag_type="table">78.98</text>
<text top="213" left="415" width="20" height="8" font="font10" id="p8_t92" reading_order_no="91" segment_no="2" tag_type="table">86.39</text>
<text top="213" left="452" width="20" height="8" font="font10" id="p8_t93" reading_order_no="92" segment_no="2" tag_type="table">80.96</text>
<text top="213" left="487" width="20" height="8" font="font10" id="p8_t94" reading_order_no="93" segment_no="2" tag_type="table">88.68</text>
<text top="223" left="88" width="94" height="8" font="font10" id="p8_t95" reading_order_no="94" segment_no="2" tag_type="table">BD-CSPN + ESFR (Ours)</text>
<text top="223" left="256" width="38" height="8" font="font10" id="p8_t96" reading_order_no="95" segment_no="2" tag_type="table">ResNet-18<a href="deeplearning_paper6.html#10">(</a></text>
<text top="223" left="308" width="20" height="8" font="font16" id="p8_t97" reading_order_no="96" segment_no="2" tag_type="table">73.98<a href="deeplearning_paper6.html#10">Li et al.</a></text>
<text top="223" left="343" width="20" height="8" font="font10" id="p8_t98" reading_order_no="97" segment_no="2" tag_type="table">82.32<a href="deeplearning_paper6.html#10">,</a></text>
<text top="223" left="379" width="20" height="8" font="font16" id="p8_t99" reading_order_no="98" segment_no="2" tag_type="table">80.13<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="223" left="415" width="20" height="8" font="font10" id="p8_t100" reading_order_no="99" segment_no="2" tag_type="table">86.34<a href="deeplearning_paper6.html#10">)</a></text>
<text top="223" left="452" width="20" height="8" font="font16" id="p8_t101" reading_order_no="100" segment_no="2" tag_type="table">82.68</text>
<text top="223" left="487" width="20" height="8" font="font10" id="p8_t102" reading_order_no="101" segment_no="2" tag_type="table">88.65</text>
<text top="233" left="88" width="116" height="8" font="font10" id="p8_t103" reading_order_no="102" segment_no="2" tag_type="table">BD-CSPN + ESFR-Semi (Ours)</text>
<text top="233" left="256" width="38" height="8" font="font10" id="p8_t104" reading_order_no="103" segment_no="2" tag_type="table">ResNet-18</text>
<text top="233" left="317" width="3" height="8" font="font10" id="p8_t105" reading_order_no="104" segment_no="2" tag_type="table">-</text>
<text top="233" left="343" width="20" height="8" font="font16" id="p8_t106" reading_order_no="105" segment_no="2" tag_type="table">82.89</text>
<text top="233" left="387" width="3" height="8" font="font10" id="p8_t107" reading_order_no="106" segment_no="2" tag_type="table">-</text>
<text top="233" left="415" width="20" height="8" font="font16" id="p8_t108" reading_order_no="107" segment_no="2" tag_type="table">86.83<a href="deeplearning_paper6.html#11">(</a></text>
<text top="233" left="461" width="3" height="8" font="font10" id="p8_t109" reading_order_no="108" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#11">Ye et al.</a></text>
<text top="233" left="487" width="20" height="8" font="font16" id="p8_t110" reading_order_no="109" segment_no="2" tag_type="table">89.10<a href="deeplearning_paper6.html#11">,</a></text>
<text top="243" left="88" width="86" height="8" font="font10" id="p8_t111" reading_order_no="110" segment_no="2" tag_type="table">LEO ( Rusu et al. , 2019 )<a href="deeplearning_paper6.html#11">2020</a></text>
<text top="243" left="265" width="21" height="8" font="font10" id="p8_t112" reading_order_no="111" segment_no="2" tag_type="table">WRN<a href="deeplearning_paper6.html#11">)</a></text>
<text top="243" left="308" width="20" height="8" font="font10" id="p8_t113" reading_order_no="112" segment_no="2" tag_type="table">61.76</text>
<text top="243" left="343" width="20" height="8" font="font10" id="p8_t114" reading_order_no="113" segment_no="2" tag_type="table">77.59</text>
<text top="243" left="379" width="20" height="8" font="font10" id="p8_t115" reading_order_no="114" segment_no="2" tag_type="table">66.33</text>
<text top="243" left="415" width="20" height="8" font="font10" id="p8_t116" reading_order_no="115" segment_no="2" tag_type="table">81.44</text>
<text top="243" left="461" width="3" height="8" font="font10" id="p8_t117" reading_order_no="116" segment_no="2" tag_type="table">-</text>
<text top="243" left="496" width="3" height="8" font="font10" id="p8_t118" reading_order_no="117" segment_no="2" tag_type="table">-</text>
<text top="253" left="88" width="156" height="8" font="font10" id="p8_t119" reading_order_no="118" segment_no="2" tag_type="table">wDAE-GNN ( Gidaris &amp; Komodakis , 2019 )</text>
<text top="253" left="265" width="21" height="8" font="font10" id="p8_t120" reading_order_no="119" segment_no="2" tag_type="table">WRN<a href="deeplearning_paper6.html#10">(</a></text>
<text top="253" left="308" width="20" height="8" font="font10" id="p8_t121" reading_order_no="120" segment_no="2" tag_type="table">62.96<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="253" left="343" width="20" height="8" font="font10" id="p8_t122" reading_order_no="121" segment_no="2" tag_type="table">78.85<a href="deeplearning_paper6.html#10">,</a></text>
<text top="253" left="379" width="20" height="8" font="font10" id="p8_t123" reading_order_no="122" segment_no="2" tag_type="table">68.18<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="253" left="415" width="20" height="8" font="font10" id="p8_t124" reading_order_no="123" segment_no="2" tag_type="table">83.09<a href="deeplearning_paper6.html#10">)</a></text>
<text top="253" left="461" width="3" height="8" font="font10" id="p8_t125" reading_order_no="124" segment_no="2" tag_type="table">-</text>
<text top="253" left="496" width="3" height="8" font="font10" id="p8_t126" reading_order_no="125" segment_no="2" tag_type="table">-</text>
<text top="263" left="88" width="81" height="8" font="font10" id="p8_t127" reading_order_no="126" segment_no="2" tag_type="table">FEAT ( Ye et al. , 2020 )</text>
<text top="263" left="265" width="21" height="8" font="font10" id="p8_t128" reading_order_no="127" segment_no="2" tag_type="table">WRN</text>
<text top="263" left="308" width="20" height="8" font="font10" id="p8_t129" reading_order_no="128" segment_no="2" tag_type="table">65.10</text>
<text top="263" left="343" width="20" height="8" font="font10" id="p8_t130" reading_order_no="129" segment_no="2" tag_type="table">81.11</text>
<text top="263" left="379" width="20" height="8" font="font10" id="p8_t131" reading_order_no="130" segment_no="2" tag_type="table">70.41</text>
<text top="263" left="415" width="20" height="8" font="font10" id="p8_t132" reading_order_no="131" segment_no="2" tag_type="table">84.38<a href="deeplearning_paper6.html#11">(</a></text>
<text top="263" left="461" width="3" height="8" font="font10" id="p8_t133" reading_order_no="132" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#11">Ziko et al.</a></text>
<text top="263" left="496" width="3" height="8" font="font10" id="p8_t134" reading_order_no="133" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#11">,</a></text>
<text top="273" left="88" width="129" height="8" font="font10" id="p8_t135" reading_order_no="134" segment_no="2" tag_type="table">Tran. Baseline ( Dhillon et al. , 2020 )<a href="deeplearning_paper6.html#11">2020</a></text>
<text top="273" left="265" width="21" height="8" font="font10" id="p8_t136" reading_order_no="135" segment_no="2" tag_type="table">WRN<a href="deeplearning_paper6.html#11">)</a></text>
<text top="273" left="308" width="20" height="8" font="font10" id="p8_t137" reading_order_no="136" segment_no="2" tag_type="table">65.73</text>
<text top="273" left="343" width="20" height="8" font="font10" id="p8_t138" reading_order_no="137" segment_no="2" tag_type="table">78.40</text>
<text top="273" left="379" width="20" height="8" font="font10" id="p8_t139" reading_order_no="138" segment_no="2" tag_type="table">73.34</text>
<text top="273" left="415" width="20" height="8" font="font10" id="p8_t140" reading_order_no="139" segment_no="2" tag_type="table">85.50</text>
<text top="273" left="461" width="3" height="8" font="font10" id="p8_t141" reading_order_no="140" segment_no="2" tag_type="table">-</text>
<text top="273" left="496" width="3" height="8" font="font10" id="p8_t142" reading_order_no="141" segment_no="2" tag_type="table">-</text>
<text top="283" left="88" width="113" height="8" font="font10" id="p8_t143" reading_order_no="142" segment_no="2" tag_type="table">SimpleShot ( Wang et al. , 2019 )</text>
<text top="283" left="265" width="21" height="8" font="font10" id="p8_t144" reading_order_no="143" segment_no="2" tag_type="table">WRN</text>
<text top="283" left="308" width="20" height="8" font="font10" id="p8_t145" reading_order_no="144" segment_no="2" tag_type="table">65.87</text>
<text top="283" left="343" width="20" height="8" font="font10" id="p8_t146" reading_order_no="145" segment_no="2" tag_type="table">82.09</text>
<text top="283" left="379" width="20" height="8" font="font10" id="p8_t147" reading_order_no="146" segment_no="2" tag_type="table">70.90</text>
<text top="283" left="415" width="20" height="8" font="font10" id="p8_t148" reading_order_no="147" segment_no="2" tag_type="table">85.76</text>
<text top="283" left="461" width="3" height="8" font="font10" id="p8_t149" reading_order_no="148" segment_no="2" tag_type="table">-</text>
<text top="283" left="496" width="3" height="8" font="font10" id="p8_t150" reading_order_no="149" segment_no="2" tag_type="table">-</text>
<text top="293" left="88" width="75" height="8" font="font10" id="p8_t151" reading_order_no="150" segment_no="2" tag_type="table">SIB ( Hu et al. , 2020 )</text>
<text top="293" left="265" width="21" height="8" font="font10" id="p8_t152" reading_order_no="151" segment_no="2" tag_type="table">WRN</text>
<text top="293" left="310" width="16" height="8" font="font10" id="p8_t153" reading_order_no="152" segment_no="2" tag_type="table">70.0</text>
<text top="293" left="346" width="16" height="8" font="font10" id="p8_t154" reading_order_no="153" segment_no="2" tag_type="table">79.2</text>
<text top="293" left="387" width="3" height="8" font="font10" id="p8_t155" reading_order_no="154" segment_no="2" tag_type="table">-</text>
<text top="293" left="424" width="3" height="8" font="font10" id="p8_t156" reading_order_no="155" segment_no="2" tag_type="table">-</text>
<text top="293" left="461" width="3" height="8" font="font10" id="p8_t157" reading_order_no="156" segment_no="2" tag_type="table">-</text>
<text top="293" left="496" width="3" height="8" font="font10" id="p8_t158" reading_order_no="157" segment_no="2" tag_type="table">-</text>
<text top="303" left="88" width="100" height="8" font="font10" id="p8_t159" reading_order_no="158" segment_no="2" tag_type="table">BD-CSPN ( Liu et al. , 2020 )</text>
<text top="303" left="265" width="21" height="8" font="font10" id="p8_t160" reading_order_no="159" segment_no="2" tag_type="table">WRN<a href="deeplearning_paper6.html#10">(</a></text>
<text top="303" left="308" width="20" height="8" font="font10" id="p8_t161" reading_order_no="160" segment_no="2" tag_type="table">72.74<a href="deeplearning_paper6.html#10">Rusu et al.</a></text>
<text top="303" left="343" width="20" height="8" font="font10" id="p8_t162" reading_order_no="161" segment_no="2" tag_type="table">84.14<a href="deeplearning_paper6.html#10">,</a></text>
<text top="303" left="379" width="20" height="8" font="font10" id="p8_t163" reading_order_no="162" segment_no="2" tag_type="table">78.89<a href="deeplearning_paper6.html#10">2019</a></text>
<text top="303" left="415" width="20" height="8" font="font10" id="p8_t164" reading_order_no="163" segment_no="2" tag_type="table">87.72<a href="deeplearning_paper6.html#10">)</a></text>
<text top="303" left="461" width="3" height="8" font="font10" id="p8_t165" reading_order_no="164" segment_no="2" tag_type="table">-</text>
<text top="303" left="496" width="3" height="8" font="font10" id="p8_t166" reading_order_no="165" segment_no="2" tag_type="table">-</text>
<text top="313" left="88" width="119" height="8" font="font10" id="p8_t167" reading_order_no="166" segment_no="2" tag_type="table">LaplacianShot ( Ziko et al. , 2020 )</text>
<text top="313" left="265" width="21" height="8" font="font10" id="p8_t168" reading_order_no="167" segment_no="2" tag_type="table">WRN</text>
<text top="313" left="308" width="20" height="8" font="font10" id="p8_t169" reading_order_no="168" segment_no="2" tag_type="table">74.86</text>
<text top="313" left="343" width="20" height="8" font="font10" id="p8_t170" reading_order_no="169" segment_no="2" tag_type="table">84.13</text>
<text top="313" left="379" width="20" height="8" font="font10" id="p8_t171" reading_order_no="170" segment_no="2" tag_type="table">80.18</text>
<text top="313" left="415" width="20" height="8" font="font10" id="p8_t172" reading_order_no="171" segment_no="2" tag_type="table">87.56<a href="deeplearning_paper6.html#9">(</a></text>
<text top="313" left="461" width="3" height="8" font="font10" id="p8_t173" reading_order_no="172" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#9">Gidaris &amp; Komodakis</a></text>
<text top="313" left="496" width="3" height="8" font="font10" id="p8_t174" reading_order_no="173" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#9">,</a></text>
<text top="323" left="88" width="94" height="8" font="font10" id="p8_t175" reading_order_no="174" segment_no="2" tag_type="table">BD-CSPN + ESFR (Ours)<a href="deeplearning_paper6.html#9">2019</a></text>
<text top="323" left="265" width="21" height="8" font="font10" id="p8_t176" reading_order_no="175" segment_no="2" tag_type="table">WRN<a href="deeplearning_paper6.html#9">)</a></text>
<text top="323" left="308" width="20" height="8" font="font16" id="p8_t177" reading_order_no="176" segment_no="2" tag_type="table">76.84</text>
<text top="323" left="343" width="20" height="8" font="font16" id="p8_t178" reading_order_no="177" segment_no="2" tag_type="table">84.36</text>
<text top="323" left="379" width="20" height="8" font="font16" id="p8_t179" reading_order_no="178" segment_no="2" tag_type="table">81.77</text>
<text top="323" left="415" width="20" height="8" font="font10" id="p8_t180" reading_order_no="179" segment_no="2" tag_type="table">87.61</text>
<text top="323" left="461" width="3" height="8" font="font10" id="p8_t181" reading_order_no="180" segment_no="2" tag_type="table">-</text>
<text top="323" left="496" width="3" height="8" font="font10" id="p8_t182" reading_order_no="181" segment_no="2" tag_type="table">-</text>
<text top="333" left="88" width="116" height="8" font="font10" id="p8_t183" reading_order_no="182" segment_no="2" tag_type="table">BD-CSPN + ESFR-Semi (Ours)</text>
<text top="333" left="265" width="21" height="8" font="font10" id="p8_t184" reading_order_no="183" segment_no="2" tag_type="table">WRN<a href="deeplearning_paper6.html#11">(</a></text>
<text top="333" left="317" width="3" height="8" font="font10" id="p8_t185" reading_order_no="184" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#11">Ye et al.</a></text>
<text top="333" left="343" width="20" height="8" font="font16" id="p8_t186" reading_order_no="185" segment_no="2" tag_type="table">84.97<a href="deeplearning_paper6.html#11">,</a></text>
<text top="333" left="387" width="3" height="8" font="font10" id="p8_t187" reading_order_no="186" segment_no="2" tag_type="table">-<a href="deeplearning_paper6.html#11">2020</a></text>
<text top="333" left="415" width="20" height="8" font="font16" id="p8_t188" reading_order_no="187" segment_no="2" tag_type="table">88.10<a href="deeplearning_paper6.html#11">)</a></text>
<text top="333" left="461" width="3" height="8" font="font10" id="p8_t189" reading_order_no="188" segment_no="2" tag_type="table">-</text>
<text top="333" left="496" width="3" height="8" font="font10" id="p8_t190" reading_order_no="189" segment_no="2" tag_type="table">-</text>
<text top="355" left="126" width="345" height="8" font="font34" id="p8_t191" reading_order_no="190" segment_no="3" tag_type="title">Table 3. Ablation study evaluating the effects of embedding ensemble and dropout perturbation.</text>
<text top="365" left="258" width="57" height="8" font="font42" id="p8_t192" reading_order_no="192" segment_no="4" tag_type="table">mini -ImageNet</text>
<text top="365" left="328" width="62" height="8" font="font42" id="p8_t193" reading_order_no="195" segment_no="4" tag_type="table">tiered -ImageNet</text>
<text top="365" left="422" width="19" height="8" font="font16" id="p8_t194" reading_order_no="198" segment_no="4" tag_type="table">CUB</text>
<text top="375" left="175" width="30" height="8" font="font16" id="p8_t195" reading_order_no="191" segment_no="4" tag_type="table">Method</text>
<text top="375" left="257" width="23" height="8" font="font16" id="p8_t196" reading_order_no="193" segment_no="4" tag_type="table">1-shot<a href="deeplearning_paper6.html#9">(</a></text>
<text top="375" left="293" width="23" height="8" font="font16" id="p8_t197" reading_order_no="194" segment_no="4" tag_type="table">5-shot<a href="deeplearning_paper6.html#9">Dhillon et al.</a></text>
<text top="375" left="328" width="23" height="8" font="font16" id="p8_t198" reading_order_no="196" segment_no="4" tag_type="table">1-shot<a href="deeplearning_paper6.html#9">,</a></text>
<text top="375" left="365" width="23" height="8" font="font16" id="p8_t199" reading_order_no="197" segment_no="4" tag_type="table">5-shot<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="375" left="402" width="23" height="8" font="font16" id="p8_t200" reading_order_no="199" segment_no="4" tag_type="table">1-shot<a href="deeplearning_paper6.html#9">)</a></text>
<text top="375" left="438" width="23" height="8" font="font16" id="p8_t201" reading_order_no="200" segment_no="4" tag_type="table">5-shot</text>
<text top="386" left="155" width="72" height="8" font="font10" id="p8_t202" reading_order_no="201" segment_no="4" tag_type="table">NN with ResNet-18</text>
<text top="386" left="259" width="20" height="8" font="font10" id="p8_t203" reading_order_no="202" segment_no="4" tag_type="table">64.04</text>
<text top="386" left="294" width="20" height="8" font="font10" id="p8_t204" reading_order_no="203" segment_no="4" tag_type="table">79.71</text>
<text top="386" left="330" width="20" height="8" font="font10" id="p8_t205" reading_order_no="204" segment_no="4" tag_type="table">71.60</text>
<text top="386" left="367" width="20" height="8" font="font10" id="p8_t206" reading_order_no="205" segment_no="4" tag_type="table">84.62</text>
<text top="386" left="404" width="20" height="8" font="font10" id="p8_t207" reading_order_no="206" segment_no="4" tag_type="table">71.43</text>
<text top="386" left="439" width="20" height="8" font="font10" id="p8_t208" reading_order_no="207" segment_no="4" tag_type="table">86.44<a href="deeplearning_paper6.html#11">(</a></text>
<text top="396" left="136" width="109" height="8" font="font10" id="p8_t209" reading_order_no="208" segment_no="4" tag_type="table">(i) w/o Dropout and Ensemble<a href="deeplearning_paper6.html#11">Wang et al.</a></text>
<text top="396" left="259" width="20" height="8" font="font10" id="p8_t210" reading_order_no="209" segment_no="4" tag_type="table">66.87<a href="deeplearning_paper6.html#11">,</a></text>
<text top="396" left="294" width="20" height="8" font="font10" id="p8_t211" reading_order_no="210" segment_no="4" tag_type="table">80.59<a href="deeplearning_paper6.html#11">2019</a></text>
<text top="396" left="330" width="20" height="8" font="font10" id="p8_t212" reading_order_no="211" segment_no="4" tag_type="table">73.39<a href="deeplearning_paper6.html#11">)</a></text>
<text top="396" left="367" width="20" height="8" font="font10" id="p8_t213" reading_order_no="212" segment_no="4" tag_type="table">84.60</text>
<text top="396" left="404" width="20" height="8" font="font10" id="p8_t214" reading_order_no="213" segment_no="4" tag_type="table">75.46</text>
<text top="396" left="439" width="20" height="8" font="font10" id="p8_t215" reading_order_no="214" segment_no="4" tag_type="table">87.02</text>
<text top="406" left="158" width="64" height="8" font="font10" id="p8_t216" reading_order_no="215" segment_no="4" tag_type="table">(ii) w/o Ensemble</text>
<text top="406" left="259" width="20" height="8" font="font10" id="p8_t217" reading_order_no="216" segment_no="4" tag_type="table">69.66</text>
<text top="406" left="294" width="20" height="8" font="font10" id="p8_t218" reading_order_no="217" segment_no="4" tag_type="table">81.10</text>
<text top="406" left="330" width="20" height="8" font="font10" id="p8_t219" reading_order_no="218" segment_no="4" tag_type="table">76.31</text>
<text top="406" left="367" width="20" height="8" font="font10" id="p8_t220" reading_order_no="219" segment_no="4" tag_type="table">85.33<a href="deeplearning_paper6.html#10">(</a></text>
<text top="406" left="404" width="20" height="8" font="font10" id="p8_t221" reading_order_no="220" segment_no="4" tag_type="table">78.32<a href="deeplearning_paper6.html#10">Hu et al.</a></text>
<text top="406" left="439" width="20" height="8" font="font10" id="p8_t222" reading_order_no="221" segment_no="4" tag_type="table">87.63<a href="deeplearning_paper6.html#10">,</a></text>
<text top="416" left="160" width="61" height="8" font="font10" id="p8_t223" reading_order_no="222" segment_no="4" tag_type="table">(iii) w/o Dropout<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="416" left="259" width="20" height="8" font="font10" id="p8_t224" reading_order_no="223" segment_no="4" tag_type="table">68.90<a href="deeplearning_paper6.html#10">)</a></text>
<text top="416" left="294" width="20" height="8" font="font10" id="p8_t225" reading_order_no="224" segment_no="4" tag_type="table">81.53</text>
<text top="416" left="330" width="20" height="8" font="font10" id="p8_t226" reading_order_no="225" segment_no="4" tag_type="table">75.39</text>
<text top="416" left="367" width="20" height="8" font="font10" id="p8_t227" reading_order_no="226" segment_no="4" tag_type="table">85.31</text>
<text top="416" left="404" width="20" height="8" font="font10" id="p8_t228" reading_order_no="227" segment_no="4" tag_type="table">77.32</text>
<text top="416" left="439" width="20" height="8" font="font10" id="p8_t229" reading_order_no="228" segment_no="4" tag_type="table">87.66</text>
<text top="426" left="168" width="45" height="8" font="font16" id="p8_t230" reading_order_no="229" segment_no="4" tag_type="table">NN + ESFR</text>
<text top="426" left="259" width="20" height="8" font="font16" id="p8_t231" reading_order_no="230" segment_no="4" tag_type="table">70.94</text>
<text top="426" left="294" width="20" height="8" font="font16" id="p8_t232" reading_order_no="231" segment_no="4" tag_type="table">81.61<a href="deeplearning_paper6.html#10">(</a></text>
<text top="426" left="330" width="20" height="8" font="font16" id="p8_t233" reading_order_no="232" segment_no="4" tag_type="table">77.44<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="426" left="367" width="20" height="8" font="font16" id="p8_t234" reading_order_no="233" segment_no="4" tag_type="table">85.84<a href="deeplearning_paper6.html#10">,</a></text>
<text top="426" left="404" width="20" height="8" font="font16" id="p8_t235" reading_order_no="234" segment_no="4" tag_type="table">79.44<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="426" left="439" width="20" height="8" font="font16" id="p8_t236" reading_order_no="235" segment_no="4" tag_type="table">88.02<a href="deeplearning_paper6.html#10">)</a></text>
<text top="448" left="55" width="237" height="10" font="font4" id="p8_t237" reading_order_no="236" segment_no="5" tag_type="text">the-art LaplacianShot ( Ziko et al. , 2020 ) by +1.2% ∼ 2.0%.</text>
<text top="461" left="55" width="236" height="9" font="font4" id="p8_t238" reading_order_no="237" segment_no="5" tag_type="text">We note that our implementation is based on Laplacian-</text>
<text top="473" left="55" width="234" height="9" font="font4" id="p8_t239" reading_order_no="238" segment_no="5" tag_type="text">Shot; BD-CSPN + ESFR shares many aspects including</text>
<text top="485" left="55" width="234" height="9" font="font4" id="p8_t240" reading_order_no="239" segment_no="5" tag_type="text">the baseline method BD-CSPN and embedding network</text>
<text top="497" left="55" width="50" height="9" font="font4" id="p8_t241" reading_order_no="240" segment_no="5" tag_type="text">pre-training.</text>
<text top="515" left="55" width="236" height="9" font="font4" id="p8_t242" reading_order_no="241" segment_no="7" tag_type="text">For the 5-shot setting, our method shows comparable per-</text>
<text top="527" left="55" width="234" height="9" font="font4" id="p8_t243" reading_order_no="242" segment_no="7" tag_type="text">formance to the state-of-the-art. Note that in Table 1 , CSPN</text>
<text top="539" left="54" width="237" height="9" font="font4" id="p8_t244" reading_order_no="243" segment_no="7" tag_type="text">+ ESFR, which does not benefit from transductive infer-<a href="deeplearning_paper6.html#11">(</a></text>
<text top="551" left="55" width="236" height="9" font="font4" id="p8_t245" reading_order_no="244" segment_no="7" tag_type="text">ence, shows subequal performance with BD-CSPN + ESFR.<a href="deeplearning_paper6.html#11">Ziko et al.</a></text>
<text top="563" left="55" width="234" height="9" font="font4" id="p8_t246" reading_order_no="245" segment_no="7" tag_type="text">For BD-CSPN + ESFR-Semi, our method provides further<a href="deeplearning_paper6.html#11">,</a></text>
<text top="574" left="55" width="128" height="9" font="font4" id="p8_t247" reading_order_no="246" segment_no="7" tag_type="text">improvements of +0.3% ∼ 0.8%.<a href="deeplearning_paper6.html#11">2020</a></text>
<text top="592" left="55" width="235" height="9" font="font1" id="p8_t248" reading_order_no="247" segment_no="9" tag_type="text">Ablation study: We conduct an ablation analysis on the<a href="deeplearning_paper6.html#11">)</a></text>
<text top="604" left="55" width="234" height="9" font="font4" id="p8_t249" reading_order_no="248" segment_no="9" tag_type="text">effects of different components of the proposed method. We</text>
<text top="616" left="55" width="234" height="9" font="font4" id="p8_t250" reading_order_no="249" segment_no="9" tag_type="text">use a NN classifier with ResNet-18 backbone network for</text>
<text top="628" left="55" width="236" height="9" font="font4" id="p8_t251" reading_order_no="250" segment_no="9" tag_type="text">these experiments; our experimental results are on mini -</text>
<text top="640" left="55" width="234" height="9" font="font4" id="p8_t252" reading_order_no="251" segment_no="9" tag_type="text">ImageNet, tiered -ImageNet, and CUB with 5-way 1- and</text>
<text top="652" left="55" width="234" height="9" font="font4" id="p8_t253" reading_order_no="252" segment_no="9" tag_type="text">5-shot tasks. Table 3 shows the influences of the embedding</text>
<text top="664" left="55" width="142" height="9" font="font4" id="p8_t254" reading_order_no="253" segment_no="9" tag_type="text">ensemble and dropout perturbation.</text>
<text top="682" left="55" width="235" height="9" font="font4" id="p8_t255" reading_order_no="254" segment_no="11" tag_type="text">In Section 3.3 , we proposed the embedding ensemble to</text>
<text top="694" left="55" width="234" height="9" font="font4" id="p8_t256" reading_order_no="255" segment_no="11" tag_type="text">reduce the variance by random initialization. The empirical</text>
<text top="706" left="55" width="234" height="9" font="font4" id="p8_t257" reading_order_no="256" segment_no="11" tag_type="text">result in Table 3 -(iii) shows that the ensemble consistently</text>
<text top="449" left="307" width="234" height="9" font="font4" id="p8_t258" reading_order_no="257" segment_no="6" tag_type="text">provides improvements in all settings. In Section 3.1 , we</text>
<text top="461" left="307" width="234" height="9" font="font4" id="p8_t259" reading_order_no="258" segment_no="6" tag_type="text">expected the effectiveness of dropout perturbation since</text>
<text top="473" left="307" width="234" height="9" font="font4" id="p8_t260" reading_order_no="259" segment_no="6" tag_type="text">noise perturbations tend to make memorization harder. The</text>
<text top="485" left="307" width="235" height="9" font="font4" id="p8_t261" reading_order_no="260" segment_no="6" tag_type="text">result in Table 3 -(ii) shows consistent improvements by</text>
<text top="497" left="307" width="234" height="9" font="font4" id="p8_t262" reading_order_no="261" segment_no="6" tag_type="text">dropout; supports our expectation. Finally, we observe that</text>
<text top="509" left="307" width="234" height="9" font="font4" id="p8_t263" reading_order_no="262" segment_no="6" tag_type="text">the complete version with both the embedding ensemble and</text>
<text top="521" left="307" width="233" height="9" font="font4" id="p8_t264" reading_order_no="263" segment_no="6" tag_type="text">dropout perturbation outperforms the other configurations.</text>
<text top="538" left="307" width="236" height="10" font="font1" id="p8_t265" reading_order_no="264" segment_no="8" tag_type="text">Comparison with prior embedding adaptation : We com-</text>
<text top="551" left="307" width="234" height="9" font="font4" id="p8_t266" reading_order_no="265" segment_no="8" tag_type="text">pare our method with prior embedding adaptation methods</text>
<text top="563" left="307" width="236" height="9" font="font4" id="p8_t267" reading_order_no="266" segment_no="8" tag_type="text">that are based on affine transformations: 1) Embedding Prop-</text>
<text top="574" left="307" width="237" height="9" font="font4" id="p8_t268" reading_order_no="267" segment_no="8" tag_type="text">agation (EP) from Rodr´ıguez et al. ( 2020 ), 2) Principal Com-</text>
<text top="586" left="307" width="236" height="9" font="font4" id="p8_t269" reading_order_no="268" segment_no="8" tag_type="text">ponent Analysis (PCA)- and Independent Component Anal-</text>
<text top="597" left="307" width="236" height="10" font="font4" id="p8_t270" reading_order_no="269" segment_no="8" tag_type="text">ysis (ICA) 14 -based methods from Lichtenstein et al. ( 2020 ).</text>
<text top="610" left="307" width="234" height="9" font="font4" id="p8_t271" reading_order_no="270" segment_no="8" tag_type="text">For a fair comparison, we experiment with the same nearest</text>
<text top="622" left="307" width="234" height="9" font="font4" id="p8_t272" reading_order_no="271" segment_no="8" tag_type="text">neighbor classifier and pre-trained embeddings. We use the</text>
<text top="634" left="307" width="235" height="9" font="font4" id="p8_t273" reading_order_no="272" segment_no="8" tag_type="text">released official code of each method. Table 4 describes</text>
<text top="646" left="307" width="235" height="9" font="font4" id="p8_t274" reading_order_no="273" segment_no="8" tag_type="text">the results of 1- and 5-shot settings on mini -ImageNet with</text>
<text top="658" left="307" width="66" height="9" font="font4" id="p8_t275" reading_order_no="274" segment_no="8" tag_type="text">WRN backbone.</text>
<text top="674" left="317" width="226" height="10" font="font25" id="p8_t276" reading_order_no="275" segment_no="10" tag_type="footnote">14 Lichtenstein et al. ( 2020 ) explained their method as Indepen-</text>
<text top="686" left="307" width="235" height="8" font="font10" id="p8_t277" reading_order_no="276" segment_no="10" tag_type="footnote">dent Component Analysis (ICA), but it seems the difference be-</text>
<text top="696" left="307" width="235" height="8" font="font10" id="p8_t278" reading_order_no="277" segment_no="10" tag_type="footnote">tween their PCA- and ICA-based methods is only in the whitening;</text>
<text top="704" left="307" width="184" height="10" font="font10" id="p8_t279" reading_order_no="278" segment_no="10" tag_type="footnote">for ICA, they whiten by the projection of 1 T z = 0 .</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="66" left="55" width="227" height="8" font="font34" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="title">Table 4. Comparison with prior embedding adaptation methods</text>
<text top="81" left="174" width="23" height="8" font="font16" id="p9_t3" reading_order_no="2" segment_no="3" tag_type="table">1-shot</text>
<text top="81" left="233" width="23" height="8" font="font16" id="p9_t4" reading_order_no="3" segment_no="3" tag_type="table">5-shot</text>
<text top="92" left="76" width="13" height="8" font="font10" id="p9_t5" reading_order_no="4" segment_no="3" tag_type="table">NN</text>
<text top="91" left="161" width="48" height="9" font="font10" id="p9_t6" reading_order_no="5" segment_no="3" tag_type="table">66.73 ± 0.44</text>
<text top="91" left="221" width="48" height="9" font="font10" id="p9_t7" reading_order_no="6" segment_no="3" tag_type="table">81.85 ± 0.31</text>
<text top="102" left="76" width="40" height="8" font="font10" id="p9_t8" reading_order_no="7" segment_no="3" tag_type="table">NN + PCA</text>
<text top="101" left="161" width="48" height="9" font="font10" id="p9_t9" reading_order_no="8" segment_no="3" tag_type="table">69.63 ± 0.50</text>
<text top="101" left="221" width="48" height="9" font="font10" id="p9_t10" reading_order_no="9" segment_no="3" tag_type="table">82.28 ± 0.32</text>
<text top="112" left="76" width="33" height="8" font="font10" id="p9_t11" reading_order_no="10" segment_no="3" tag_type="table">NN + EP</text>
<text top="111" left="161" width="48" height="9" font="font10" id="p9_t12" reading_order_no="11" segment_no="3" tag_type="table">70.58 ± 0.47</text>
<text top="111" left="221" width="48" height="9" font="font10" id="p9_t13" reading_order_no="12" segment_no="3" tag_type="table">82.73 ± 0.30</text>
<text top="122" left="76" width="38" height="8" font="font10" id="p9_t14" reading_order_no="13" segment_no="3" tag_type="table">NN + ICA</text>
<text top="121" left="161" width="48" height="9" font="font10" id="p9_t15" reading_order_no="14" segment_no="3" tag_type="table">72.19 ± 0.54</text>
<text top="121" left="221" width="48" height="9" font="font10" id="p9_t16" reading_order_no="15" segment_no="3" tag_type="table">83.12 ± 0.32</text>
<text top="132" left="76" width="73" height="8" font="font16" id="p9_t17" reading_order_no="16" segment_no="3" tag_type="table">NN + ESFR (Ours)</text>
<text top="131" left="161" width="48" height="9" font="font16" id="p9_t18" reading_order_no="17" segment_no="3" tag_type="table">74.01 ± 0.51</text>
<text top="131" left="221" width="48" height="9" font="font16" id="p9_t19" reading_order_no="18" segment_no="3" tag_type="table">83.58 ± 0.31</text>
<text top="159" left="55" width="234" height="9" font="font4" id="p9_t20" reading_order_no="19" segment_no="6" tag_type="text">Our method outperforms both Rodr´ıguez et al. ( 2020 ) and</text>
<text top="171" left="55" width="235" height="9" font="font8" id="p9_t21" reading_order_no="20" segment_no="6" tag_type="text">Lichtenstein et al. ( 2020 ) on all settings. Extracting shared</text>
<text top="183" left="55" width="235" height="9" font="font4" id="p9_t22" reading_order_no="21" segment_no="6" tag_type="text">or correlated features reminds us of the concept of PCA;</text>
<text top="195" left="55" width="234" height="9" font="font4" id="p9_t23" reading_order_no="22" segment_no="6" tag_type="text">however, compared to PCA, our method advantage from</text>
<text top="206" left="55" width="184" height="9" font="font4" id="p9_t24" reading_order_no="23" segment_no="6" tag_type="text">discovering non-linear patterns and correlates.</text>
<text top="234" left="55" width="69" height="11" font="font3" id="p9_t25" reading_order_no="24" segment_no="8" tag_type="title">6. Conclusion</text>
<text top="254" left="55" width="234" height="9" font="font4" id="p9_t26" reading_order_no="25" segment_no="10" tag_type="text">We propose the novel unsupervised embedding adaptation</text>
<text top="266" left="55" width="235" height="9" font="font4" id="p9_t27" reading_order_no="26" segment_no="10" tag_type="text">scheme based on the finding that deep neural networks learn</text>
<text top="278" left="55" width="236" height="9" font="font5" id="p9_t28" reading_order_no="27" segment_no="10" tag_type="text">to generalize before memorizing . Our method, ESFR, pro-</text>
<text top="290" left="55" width="235" height="9" font="font4" id="p9_t29" reading_order_no="28" segment_no="10" tag_type="text">vides task-adapted embeddings of generalizable features by</text>
<text top="302" left="55" width="236" height="9" font="font4" id="p9_t30" reading_order_no="29" segment_no="10" tag_type="text">feature reconstruction training and LID-based early stop-</text>
<text top="314" left="55" width="236" height="9" font="font4" id="p9_t31" reading_order_no="30" segment_no="10" tag_type="text">ping. Experimental results show that well-designed unsuper-</text>
<text top="326" left="55" width="235" height="9" font="font4" id="p9_t32" reading_order_no="31" segment_no="10" tag_type="text">vised adaptation can consistently improve baseline methods;</text>
<text top="338" left="55" width="234" height="9" font="font4" id="p9_t33" reading_order_no="32" segment_no="10" tag_type="text">outperform conventional transductive methods; be further</text>
<text top="350" left="55" width="234" height="9" font="font4" id="p9_t34" reading_order_no="33" segment_no="10" tag_type="text">improved by joint usage with a transductive method. ESFR</text>
<text top="362" left="55" width="234" height="9" font="font4" id="p9_t35" reading_order_no="34" segment_no="10" tag_type="text">used in conjunction with the transductive method achieves</text>
<text top="374" left="55" width="234" height="9" font="font4" id="p9_t36" reading_order_no="35" segment_no="10" tag_type="text">new state-of-the-art performance on the 1-shot setting. We</text>
<text top="386" left="55" width="234" height="9" font="font4" id="p9_t37" reading_order_no="36" segment_no="10" tag_type="text">hope that our work will become a starting point for future</text>
<text top="398" left="55" width="224" height="9" font="font4" id="p9_t38" reading_order_no="37" segment_no="10" tag_type="text">unsupervised learning studies on few-shot classification.</text>
<text top="425" left="55" width="99" height="11" font="font3" id="p9_t39" reading_order_no="38" segment_no="15" tag_type="title">Acknowledgements</text>
<text top="446" left="55" width="236" height="9" font="font4" id="p9_t40" reading_order_no="39" segment_no="16" tag_type="text">This work was supported by the National Research Founda-</text>
<text top="458" left="55" width="234" height="9" font="font4" id="p9_t41" reading_order_no="40" segment_no="16" tag_type="text">tion of Korea (NRF) grant funded by the Korea government</text>
<text top="470" left="55" width="127" height="9" font="font4" id="p9_t42" reading_order_no="41" segment_no="16" tag_type="text">(MSIT) [2021R1A2C2007518].<a href="deeplearning_paper6.html#10">Rodr´ıguez et al.</a></text>
<text top="497" left="55" width="56" height="11" font="font3" id="p9_t43" reading_order_no="42" segment_no="18" tag_type="title">References<a href="deeplearning_paper6.html#10">(</a></text>
<text top="516" left="55" width="235" height="9" font="font4" id="p9_t44" reading_order_no="43" segment_no="20" tag_type="text">Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="528" left="65" width="226" height="9" font="font4" id="p9_t45" reading_order_no="44" segment_no="20" tag_type="text">J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.<a href="deeplearning_paper6.html#10">) </a>and</text>
<text top="540" left="65" width="226" height="9" font="font4" id="p9_t46" reading_order_no="45" segment_no="20" tag_type="text">Tensorflow: A system for large-scale machine learning.<a href="deeplearning_paper6.html#10">Lichtenstein et al.</a></text>
<text top="552" left="65" width="73" height="9" font="font4" id="p9_t47" reading_order_no="46" segment_no="20" tag_type="text">In USENIX , 2016.<a href="deeplearning_paper6.html#10">(</a></text>
<text top="572" left="55" width="235" height="9" font="font4" id="p9_t48" reading_order_no="47" segment_no="22" tag_type="text">Amsaleg, L., Chelly, O., Furon, T., Girard, S., Houle, M. E.,<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="584" left="65" width="226" height="9" font="font4" id="p9_t49" reading_order_no="48" segment_no="22" tag_type="text">Kawarabayashi, K., and Nett, M. Estimating local intrin-<a href="deeplearning_paper6.html#10">) </a>on all settings. Extracting shared</text>
<text top="596" left="65" width="153" height="9" font="font4" id="p9_t50" reading_order_no="49" segment_no="22" tag_type="text">sic dimensionality. In SIGKDD , 2015.</text>
<text top="616" left="55" width="235" height="9" font="font4" id="p9_t51" reading_order_no="50" segment_no="24" tag_type="text">Amsaleg, L., Bailey, J., Barbe, D., Erfani, S. M., Houle,</text>
<text top="628" left="65" width="226" height="9" font="font4" id="p9_t52" reading_order_no="51" segment_no="24" tag_type="text">M. E., Nguyen, V., and Radovanovic, M. The vulnerabil-</text>
<text top="640" left="65" width="224" height="9" font="font4" id="p9_t53" reading_order_no="52" segment_no="24" tag_type="text">ity of learning to adversarial perturbation increases with</text>
<text top="652" left="65" width="227" height="9" font="font4" id="p9_t54" reading_order_no="53" segment_no="24" tag_type="text">intrinsic dimensionality. In IEEE Workshop on Informa-</text>
<text top="664" left="65" width="161" height="9" font="font5" id="p9_t55" reading_order_no="54" segment_no="24" tag_type="text">tion Forensics and Security WIFS , 2017.</text>
<text top="684" left="55" width="236" height="9" font="font4" id="p9_t56" reading_order_no="55" segment_no="26" tag_type="text">Ansuini, A., Laio, A., Macke, J. H., and Zoccolan, D. In-</text>
<text top="696" left="65" width="224" height="9" font="font4" id="p9_t57" reading_order_no="56" segment_no="26" tag_type="text">trinsic dimension of data representations in deep neural</text>
<text top="708" left="65" width="115" height="9" font="font4" id="p9_t58" reading_order_no="57" segment_no="26" tag_type="text">networks. In NeurIPS , 2019.</text>
<text top="70" left="307" width="236" height="9" font="font4" id="p9_t59" reading_order_no="58" segment_no="2" tag_type="text">Antoniou, A. and Storkey, A. J. Learning to learn by self-</text>
<text top="82" left="317" width="109" height="9" font="font4" id="p9_t60" reading_order_no="59" segment_no="2" tag_type="text">critique. In NeurIPS , 2019.</text>
<text top="102" left="307" width="235" height="9" font="font4" id="p9_t61" reading_order_no="60" segment_no="4" tag_type="text">Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio,</text>
<text top="114" left="317" width="225" height="9" font="font4" id="p9_t62" reading_order_no="61" segment_no="4" tag_type="text">E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville,</text>
<text top="126" left="317" width="224" height="9" font="font4" id="p9_t63" reading_order_no="62" segment_no="4" tag_type="text">A. C., Bengio, Y., and Lacoste-Julien, S. A closer look at</text>
<text top="138" left="317" width="194" height="9" font="font4" id="p9_t64" reading_order_no="63" segment_no="4" tag_type="text">memorization in deep networks. In ICML , 2017.</text>
<text top="158" left="307" width="235" height="9" font="font4" id="p9_t65" reading_order_no="64" segment_no="5" tag_type="text">Boudiaf, M., Ziko, I. M., Rony, J., Dolz, J., Piantanida, P.,</text>
<text top="170" left="317" width="224" height="9" font="font4" id="p9_t66" reading_order_no="65" segment_no="5" tag_type="text">and Ayed, I. B. Information maximization for few-shot</text>
<text top="182" left="317" width="111" height="9" font="font4" id="p9_t67" reading_order_no="66" segment_no="5" tag_type="text">learning. In NeurIPS , 2020.</text>
<text top="202" left="307" width="235" height="9" font="font4" id="p9_t68" reading_order_no="67" segment_no="7" tag_type="text">Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,</text>
<text top="214" left="317" width="224" height="9" font="font4" id="p9_t69" reading_order_no="68" segment_no="7" tag_type="text">and Joulin, A. Unsupervised learning of visual features</text>
<text top="226" left="317" width="216" height="9" font="font4" id="p9_t70" reading_order_no="69" segment_no="7" tag_type="text">by contrasting cluster assignments. In NeurIPS , 2020.</text>
<text top="246" left="307" width="234" height="9" font="font4" id="p9_t71" reading_order_no="70" segment_no="9" tag_type="text">Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and</text>
<text top="258" left="317" width="226" height="9" font="font4" id="p9_t72" reading_order_no="71" segment_no="9" tag_type="text">Hinton, G. Big self-supervised models are strong semi-</text>
<text top="270" left="317" width="155" height="9" font="font4" id="p9_t73" reading_order_no="72" segment_no="9" tag_type="text">supervised learners. In NeurIPS , 2020.</text>
<text top="290" left="307" width="234" height="9" font="font4" id="p9_t74" reading_order_no="73" segment_no="11" tag_type="text">Chen, W., Liu, Y., Kira, Z., Wang, Y. F., and Huang, J. A</text>
<text top="302" left="317" width="211" height="9" font="font4" id="p9_t75" reading_order_no="74" segment_no="11" tag_type="text">closer look at few-shot classification. In ICLR , 2019.</text>
<text top="322" left="307" width="235" height="9" font="font4" id="p9_t76" reading_order_no="75" segment_no="12" tag_type="text">Dhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto,</text>
<text top="333" left="317" width="225" height="10" font="font4" id="p9_t77" reading_order_no="76" segment_no="12" tag_type="text">S. A baseline for few-shot image classification. In ICLR ,</text>
<text top="345" left="317" width="22" height="9" font="font4" id="p9_t78" reading_order_no="77" segment_no="12" tag_type="text">2020.</text>
<text top="365" left="307" width="236" height="9" font="font4" id="p9_t79" reading_order_no="78" segment_no="13" tag_type="text">Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-</text>
<text top="377" left="317" width="226" height="9" font="font4" id="p9_t80" reading_order_no="79" segment_no="13" tag_type="text">learning for fast adaptation of deep networks. In ICML ,</text>
<text top="389" left="317" width="22" height="9" font="font4" id="p9_t81" reading_order_no="80" segment_no="13" tag_type="text">2017.</text>
<text top="409" left="307" width="234" height="9" font="font4" id="p9_t82" reading_order_no="81" segment_no="14" tag_type="text">Gidaris, S. and Komodakis, N. Generating classification</text>
<text top="421" left="317" width="224" height="9" font="font4" id="p9_t83" reading_order_no="82" segment_no="14" tag_type="text">weights with GNN denoising autoencoders for few-shot</text>
<text top="433" left="317" width="102" height="9" font="font4" id="p9_t84" reading_order_no="83" segment_no="14" tag_type="text">learning. In CVPR , 2019.</text>
<text top="453" left="307" width="234" height="9" font="font4" id="p9_t85" reading_order_no="84" segment_no="17" tag_type="text">Gidaris, S., Singh, P., and Komodakis, N. Unsupervised</text>
<text top="465" left="317" width="224" height="9" font="font4" id="p9_t86" reading_order_no="85" segment_no="17" tag_type="text">representation learning by predicting image rotations. In</text>
<text top="477" left="317" width="49" height="9" font="font5" id="p9_t87" reading_order_no="86" segment_no="17" tag_type="text">ICLR , 2018.</text>
<text top="497" left="307" width="234" height="9" font="font4" id="p9_t88" reading_order_no="87" segment_no="19" tag_type="text">Gidaris, S., Bursuc, A., Komodakis, N., Perez, P., and</text>
<text top="509" left="317" width="226" height="9" font="font4" id="p9_t89" reading_order_no="88" segment_no="19" tag_type="text">Cord, M. Boosting few-shot visual learning with self-</text>
<text top="521" left="317" width="113" height="9" font="font4" id="p9_t90" reading_order_no="89" segment_no="19" tag_type="text">supervision. In ICCV , 2019.</text>
<text top="541" left="307" width="234" height="9" font="font4" id="p9_t91" reading_order_no="90" segment_no="21" tag_type="text">Gong, S., Boddeti, V. N., and Jain, A. K. On the intrinsic</text>
<text top="553" left="317" width="226" height="9" font="font4" id="p9_t92" reading_order_no="91" segment_no="21" tag_type="text">dimensionality of image representations. In CVPR , 2019.</text>
<text top="573" left="307" width="235" height="9" font="font4" id="p9_t93" reading_order_no="92" segment_no="23" tag_type="text">Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,</text>
<text top="585" left="317" width="225" height="9" font="font4" id="p9_t94" reading_order_no="93" segment_no="23" tag_type="text">P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,</text>
<text top="597" left="317" width="224" height="9" font="font4" id="p9_t95" reading_order_no="94" segment_no="23" tag_type="text">Z. D., Azar, M. G., et al. Bootstrap your own latent: A</text>
<text top="609" left="317" width="226" height="9" font="font4" id="p9_t96" reading_order_no="95" segment_no="23" tag_type="text">new approach to self-supervised learning. In NeurIPS ,</text>
<text top="621" left="317" width="22" height="9" font="font4" id="p9_t97" reading_order_no="96" segment_no="23" tag_type="text">2020.</text>
<text top="641" left="307" width="234" height="9" font="font4" id="p9_t98" reading_order_no="97" segment_no="25" tag_type="text">Hendrycks, D., Lee, K., and Mazeika, M. Using pre-training</text>
<text top="652" left="317" width="225" height="10" font="font4" id="p9_t99" reading_order_no="98" segment_no="25" tag_type="text">can improve model robustness and uncertainty. In ICML ,</text>
<text top="664" left="317" width="22" height="9" font="font4" id="p9_t100" reading_order_no="99" segment_no="25" tag_type="text">2019.</text>
<text top="684" left="307" width="234" height="9" font="font4" id="p9_t101" reading_order_no="100" segment_no="27" tag_type="text">Hou, R., Chang, H., Ma, B., Shan, S., and Chen, X. Cross</text>
<text top="696" left="317" width="226" height="9" font="font4" id="p9_t102" reading_order_no="101" segment_no="27" tag_type="text">attention network for few-shot classification. NeurIPS ,</text>
<text top="708" left="317" width="22" height="9" font="font4" id="p9_t103" reading_order_no="102" segment_no="27" tag_type="text">2019.</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p10_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="70" left="55" width="236" height="9" font="font4" id="p10_t2" reading_order_no="1" segment_no="1" tag_type="text">Houle, M. E. Local intrinsic dimensionality I: an extreme-</text>
<text top="82" left="65" width="224" height="9" font="font4" id="p10_t3" reading_order_no="2" segment_no="1" tag_type="text">value-theoretic foundation for similarity applications. In</text>
<text top="94" left="65" width="58" height="9" font="font5" id="p10_t4" reading_order_no="3" segment_no="1" tag_type="text">SISAP , 2017a.</text>
<text top="113" left="55" width="234" height="9" font="font4" id="p10_t5" reading_order_no="4" segment_no="3" tag_type="text">Houle, M. E. Local intrinsic dimensionality II: multivariate</text>
<text top="125" left="65" width="212" height="9" font="font4" id="p10_t6" reading_order_no="5" segment_no="3" tag_type="text">analysis and distributional support. In SISAP , 2017b.</text>
<text top="144" left="55" width="235" height="9" font="font4" id="p10_t7" reading_order_no="6" segment_no="5" tag_type="text">Hu, S. X., Moreno, P. G., Xiao, Y., Shen, X., Obozinski, G.,</text>
<text top="156" left="65" width="224" height="9" font="font4" id="p10_t8" reading_order_no="7" segment_no="5" tag_type="text">Lawrence, N. D., and Damianou, A. C. Empirical bayes</text>
<text top="168" left="65" width="224" height="9" font="font4" id="p10_t9" reading_order_no="8" segment_no="5" tag_type="text">transductive meta-learning with synthetic gradients. In</text>
<text top="180" left="65" width="49" height="9" font="font5" id="p10_t10" reading_order_no="9" segment_no="5" tag_type="text">ICLR , 2020.</text>
<text top="199" left="55" width="236" height="9" font="font4" id="p10_t11" reading_order_no="10" segment_no="7" tag_type="text">Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.</text>
<text top="211" left="65" width="224" height="9" font="font4" id="p10_t12" reading_order_no="11" segment_no="7" tag_type="text">Mentornet: Learning data-driven curriculum for very deep</text>
<text top="223" left="65" width="211" height="9" font="font4" id="p10_t13" reading_order_no="12" segment_no="7" tag_type="text">neural networks on corrupted labels. In ICML , 2018.</text>
<text top="242" left="55" width="235" height="9" font="font4" id="p10_t14" reading_order_no="13" segment_no="9" tag_type="text">Kim, B., Choo, J., Kwon, Y.-D., Joe, S., Min, S., and Gwon,</text>
<text top="254" left="65" width="225" height="9" font="font4" id="p10_t15" reading_order_no="14" segment_no="9" tag_type="text">Y. Selfmatch: Combining contrastive self-supervision and</text>
<text top="266" left="65" width="224" height="9" font="font4" id="p10_t16" reading_order_no="15" segment_no="9" tag_type="text">consistency for semi-supervised learning. arXiv preprint</text>
<text top="278" left="65" width="100" height="9" font="font5" id="p10_t17" reading_order_no="16" segment_no="9" tag_type="text">arXiv:2101.06480 , 2021.</text>
<text top="297" left="55" width="234" height="9" font="font4" id="p10_t18" reading_order_no="17" segment_no="11" tag_type="text">Kim, J., Kim, T., Kim, S., and Yoo, C. D. Edge-labeling</text>
<text top="309" left="65" width="226" height="9" font="font4" id="p10_t19" reading_order_no="18" segment_no="11" tag_type="text">graph neural network for few-shot learning. In CVPR ,</text>
<text top="321" left="65" width="22" height="9" font="font4" id="p10_t20" reading_order_no="19" segment_no="11" tag_type="text">2019.</text>
<text top="340" left="55" width="234" height="9" font="font4" id="p10_t21" reading_order_no="20" segment_no="13" tag_type="text">Kingma, D. P. and Ba, J. Adam: A method for stochastic</text>
<text top="352" left="65" width="116" height="9" font="font4" id="p10_t22" reading_order_no="21" segment_no="13" tag_type="text">optimization. In ICLR , 2015.</text>
<text top="371" left="55" width="234" height="9" font="font4" id="p10_t23" reading_order_no="22" segment_no="15" tag_type="text">Lampinen, A. K. and Ganguli, S. An analytic theory of</text>
<text top="383" left="65" width="224" height="9" font="font4" id="p10_t24" reading_order_no="23" segment_no="15" tag_type="text">generalization dynamics and transfer learning in deep</text>
<text top="395" left="65" width="127" height="9" font="font4" id="p10_t25" reading_order_no="24" segment_no="15" tag_type="text">linear networks. In ICLR , 2019.</text>
<text top="414" left="55" width="236" height="9" font="font4" id="p10_t26" reading_order_no="25" segment_no="17" tag_type="text">Li, H., Eigen, D., Dodge, S., Zeiler, M., and Wang, X. Find-</text>
<text top="426" left="65" width="226" height="9" font="font4" id="p10_t27" reading_order_no="26" segment_no="17" tag_type="text">ing task-relevant features for few-shot learning by cate-</text>
<text top="438" left="65" width="124" height="9" font="font4" id="p10_t28" reading_order_no="27" segment_no="17" tag_type="text">gory traversal. In CVPR , 2019.</text>
<text top="457" left="55" width="234" height="9" font="font4" id="p10_t29" reading_order_no="28" segment_no="19" tag_type="text">Lichtenstein, M., Sattigeri, P., Feris, R., Giryes, R., and</text>
<text top="469" left="65" width="224" height="9" font="font4" id="p10_t30" reading_order_no="29" segment_no="19" tag_type="text">Karlinsky, L. TAFSSL: task-adaptive feature sub-space</text>
<text top="481" left="65" width="208" height="9" font="font4" id="p10_t31" reading_order_no="30" segment_no="19" tag_type="text">learning for few-shot classification. In ECCV , 2020.</text>
<text top="500" left="55" width="234" height="9" font="font4" id="p10_t32" reading_order_no="31" segment_no="21" tag_type="text">Liu, J., Song, L., and Qin, Y. Prototype rectification for</text>
<text top="512" left="65" width="139" height="9" font="font4" id="p10_t33" reading_order_no="32" segment_no="21" tag_type="text">few-shot learning. In ECCV , 2020.</text>
<text top="531" left="55" width="235" height="9" font="font4" id="p10_t34" reading_order_no="33" segment_no="23" tag_type="text">Liu, Y., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S. J.,</text>
<text top="543" left="65" width="224" height="9" font="font4" id="p10_t35" reading_order_no="34" segment_no="23" tag_type="text">and Yang, Y. Learning to propagate labels: Transductive</text>
<text top="555" left="65" width="226" height="9" font="font4" id="p10_t36" reading_order_no="35" segment_no="23" tag_type="text">propagation network for few-shot learning. In ICLR ,</text>
<text top="567" left="65" width="22" height="9" font="font4" id="p10_t37" reading_order_no="36" segment_no="23" tag_type="text">2019.</text>
<text top="586" left="55" width="236" height="9" font="font4" id="p10_t38" reading_order_no="37" segment_no="25" tag_type="text">Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S.</text>
<text top="598" left="65" width="224" height="9" font="font4" id="p10_t39" reading_order_no="38" segment_no="25" tag_type="text">N. R., Schoenebeck, G., Song, D., Houle, M. E., and</text>
<text top="610" left="65" width="224" height="9" font="font4" id="p10_t40" reading_order_no="39" segment_no="25" tag_type="text">Bailey, J. Characterizing adversarial subspaces using</text>
<text top="622" left="65" width="186" height="9" font="font4" id="p10_t41" reading_order_no="40" segment_no="25" tag_type="text">local intrinsic dimensionality. In ICLR , 2018a.</text>
<text top="641" left="55" width="235" height="9" font="font4" id="p10_t42" reading_order_no="41" segment_no="27" tag_type="text">Ma, X., Wang, Y., Houle, M. E., Zhou, S., Erfani, S. M., Xia,</text>
<text top="653" left="65" width="226" height="9" font="font4" id="p10_t43" reading_order_no="42" segment_no="27" tag_type="text">S., Wijewickrema, S. N. R., and Bailey, J. Dimensionality-</text>
<text top="665" left="65" width="203" height="9" font="font4" id="p10_t44" reading_order_no="43" segment_no="27" tag_type="text">driven learning with noisy labels. In ICML , 2018b.</text>
<text top="684" left="55" width="234" height="9" font="font4" id="p10_t45" reading_order_no="44" segment_no="29" tag_type="text">Miller, E. G., Matsakis, N. E., and Viola, P. A. Learning</text>
<text top="696" left="65" width="226" height="9" font="font4" id="p10_t46" reading_order_no="45" segment_no="29" tag_type="text">from one example through shared densities on transforms.</text>
<text top="708" left="65" width="63" height="9" font="font4" id="p10_t47" reading_order_no="46" segment_no="29" tag_type="text">In CVPR , 2000.</text>
<text top="70" left="307" width="234" height="9" font="font4" id="p10_t48" reading_order_no="47" segment_no="2" tag_type="text">Noroozi, M. and Favaro, P. Unsupervised learning of visual</text>
<text top="82" left="317" width="226" height="9" font="font4" id="p10_t49" reading_order_no="48" segment_no="2" tag_type="text">representations by solving jigsaw puzzles. In ECCV ,</text>
<text top="94" left="317" width="22" height="9" font="font4" id="p10_t50" reading_order_no="49" segment_no="2" tag_type="text">2016.</text>
<text top="116" left="307" width="235" height="9" font="font4" id="p10_t51" reading_order_no="50" segment_no="4" tag_type="text">Oreshkin, B. N., Rodriguez, P., and Lacoste, A. Tadam:</text>
<text top="128" left="317" width="224" height="9" font="font4" id="p10_t52" reading_order_no="51" segment_no="4" tag_type="text">Task dependent adaptive metric for improved few-shot</text>
<text top="140" left="317" width="100" height="9" font="font4" id="p10_t53" reading_order_no="52" segment_no="4" tag_type="text">learning. NeurIPS , 2018.</text>
<text top="161" left="307" width="236" height="9" font="font4" id="p10_t54" reading_order_no="53" segment_no="6" tag_type="text">Oymak, S., Fabian, Z., Li, M., and Soltanolkotabi, M. Gen-</text>
<text top="173" left="317" width="224" height="9" font="font4" id="p10_t55" reading_order_no="54" segment_no="6" tag_type="text">eralization guarantees for neural networks via harnessing</text>
<text top="185" left="317" width="225" height="9" font="font4" id="p10_t56" reading_order_no="55" segment_no="6" tag_type="text">the low-rank structure of the jacobian. arXiv preprint</text>
<text top="197" left="317" width="100" height="9" font="font5" id="p10_t57" reading_order_no="56" segment_no="6" tag_type="text">arXiv:1906.05392 , 2019.</text>
<text top="219" left="307" width="236" height="9" font="font4" id="p10_t58" reading_order_no="57" segment_no="8" tag_type="text">Qiao, L., Shi, Y., Li, J., Tian, Y., Huang, T., and Wang, Y.</text>
<text top="231" left="317" width="224" height="9" font="font4" id="p10_t59" reading_order_no="58" segment_no="8" tag_type="text">Transductive episodic-wise adaptive metric for few-shot</text>
<text top="243" left="317" width="100" height="9" font="font4" id="p10_t60" reading_order_no="59" segment_no="8" tag_type="text">learning. In ICCV , 2019.</text>
<text top="265" left="307" width="234" height="9" font="font4" id="p10_t61" reading_order_no="60" segment_no="10" tag_type="text">Ravi, S. and Larochelle, H. Optimization as a model for</text>
<text top="276" left="317" width="136" height="10" font="font4" id="p10_t62" reading_order_no="61" segment_no="10" tag_type="text">few-shot learning. In ICLR , 2017.</text>
<text top="298" left="307" width="235" height="9" font="font4" id="p10_t63" reading_order_no="62" segment_no="12" tag_type="text">Ren, M., Ravi, S., Triantafillou, E., Snell, J., Swersky, K.,</text>
<text top="310" left="317" width="226" height="9" font="font4" id="p10_t64" reading_order_no="63" segment_no="12" tag_type="text">Tenenbaum, J. B., Larochelle, H., and Zemel, R. S. Meta-</text>
<text top="322" left="317" width="224" height="9" font="font4" id="p10_t65" reading_order_no="64" segment_no="12" tag_type="text">learning for semi-supervised few-shot classification. In</text>
<text top="334" left="317" width="49" height="9" font="font5" id="p10_t66" reading_order_no="65" segment_no="12" tag_type="text">ICLR , 2018.</text>
<text top="356" left="307" width="236" height="9" font="font4" id="p10_t67" reading_order_no="66" segment_no="14" tag_type="text">Rodr´ıguez, P., Laradji, I., Drouin, A., and Lacoste, A. Em-</text>
<text top="368" left="317" width="224" height="9" font="font4" id="p10_t68" reading_order_no="67" segment_no="14" tag_type="text">bedding propagation: Smoother manifold for few-shot</text>
<text top="380" left="317" width="122" height="9" font="font4" id="p10_t69" reading_order_no="68" segment_no="14" tag_type="text">classification. In ECCV , 2020.</text>
<text top="401" left="307" width="235" height="9" font="font4" id="p10_t70" reading_order_no="69" segment_no="16" tag_type="text">Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu,</text>
<text top="413" left="317" width="224" height="9" font="font4" id="p10_t71" reading_order_no="70" segment_no="16" tag_type="text">R., Osindero, S., and Hadsell, R. Meta-learning with</text>
<text top="425" left="317" width="188" height="9" font="font4" id="p10_t72" reading_order_no="71" segment_no="16" tag_type="text">latent embedding optimization. In ICLR , 2019.</text>
<text top="447" left="307" width="236" height="9" font="font4" id="p10_t73" reading_order_no="72" segment_no="18" tag_type="text">Snell, J., Swersky, K., and Zemel, R. S. Prototypical net-</text>
<text top="459" left="317" width="189" height="9" font="font4" id="p10_t74" reading_order_no="73" segment_no="18" tag_type="text">works for few-shot learning. In NeurIPS , 2017.</text>
<text top="480" left="307" width="234" height="9" font="font4" id="p10_t75" reading_order_no="74" segment_no="20" tag_type="text">Song, H., Kim, M., Park, D., and Lee, J.-G. How does early</text>
<text top="492" left="317" width="225" height="9" font="font4" id="p10_t76" reading_order_no="75" segment_no="20" tag_type="text">stopping help generalization against label noise? arXiv</text>
<text top="504" left="317" width="135" height="9" font="font5" id="p10_t77" reading_order_no="76" segment_no="20" tag_type="text">preprint arXiv:1911.08059 , 2020.</text>
<text top="526" left="307" width="235" height="9" font="font4" id="p10_t78" reading_order_no="77" segment_no="22" tag_type="text">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,</text>
<text top="538" left="317" width="224" height="9" font="font4" id="p10_t79" reading_order_no="78" segment_no="22" tag_type="text">and Salakhutdinov, R. Dropout: A simple way to prevent</text>
<text top="550" left="317" width="224" height="9" font="font4" id="p10_t80" reading_order_no="79" segment_no="22" tag_type="text">neural networks from overfitting. Journal of Machine</text>
<text top="562" left="317" width="183" height="9" font="font5" id="p10_t81" reading_order_no="80" segment_no="22" tag_type="text">Learning Research , 15(56):1929–1958, 2014.</text>
<text top="584" left="307" width="235" height="9" font="font4" id="p10_t82" reading_order_no="81" segment_no="24" tag_type="text">Stephenson, C., suchismita padhy, Ganesh, A., Hui, Y., Tang,</text>
<text top="595" left="317" width="224" height="9" font="font4" id="p10_t83" reading_order_no="82" segment_no="24" tag_type="text">H., and Chung, S. On the geometry of generalization and</text>
<text top="607" left="317" width="219" height="10" font="font4" id="p10_t84" reading_order_no="83" segment_no="24" tag_type="text">memorization in deep neural networks. In ICLR , 2021.</text>
<text top="629" left="307" width="236" height="9" font="font4" id="p10_t85" reading_order_no="84" segment_no="26" tag_type="text">Su, J.-C., Maji, S., and Hariharan, B. When does self-</text>
<text top="641" left="317" width="225" height="9" font="font4" id="p10_t86" reading_order_no="85" segment_no="26" tag_type="text">supervision improve few-shot learning? In ECCV , 2020.</text>
<text top="663" left="307" width="234" height="9" font="font4" id="p10_t87" reading_order_no="86" segment_no="28" tag_type="text">Sugiyama, M. Co-teaching: Robust training of deep neural</text>
<text top="675" left="317" width="226" height="9" font="font4" id="p10_t88" reading_order_no="87" segment_no="28" tag_type="text">networks with extremely noisy labels. In NeurIPS , 2018.</text>
<text top="696" left="307" width="234" height="9" font="font4" id="p10_t89" reading_order_no="88" segment_no="30" tag_type="text">Thrun, S. and Pratt, L. Learning to learn . Springer Science</text>
<text top="708" left="317" width="101" height="9" font="font4" id="p10_t90" reading_order_no="89" segment_no="30" tag_type="text">&amp; Business Media, 2012.</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p11_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="70" left="55" width="234" height="9" font="font4" id="p11_t2" reading_order_no="1" segment_no="1" tag_type="text">Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J. B., and</text>
<text top="82" left="65" width="35" height="9" font="font4" id="p11_t3" reading_order_no="2" segment_no="1" tag_type="text">Isola, P.</text>
<text top="82" left="113" width="178" height="9" font="font4" id="p11_t4" reading_order_no="3" segment_no="1" tag_type="text">Rethinking few-shot image classification:</text>
<text top="94" left="65" width="154" height="9" font="font4" id="p11_t5" reading_order_no="4" segment_no="1" tag_type="text">a good embedding is all you need?</text>
<text top="94" left="229" width="60" height="9" font="font5" id="p11_t6" reading_order_no="5" segment_no="1" tag_type="text">arXiv preprint</text>
<text top="106" left="65" width="100" height="9" font="font5" id="p11_t7" reading_order_no="6" segment_no="1" tag_type="text">arXiv:2003.11539 , 2020.</text>
<text top="126" left="55" width="236" height="9" font="font4" id="p11_t8" reading_order_no="7" segment_no="2" tag_type="text">Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.</text>
<text top="138" left="65" width="226" height="9" font="font4" id="p11_t9" reading_order_no="8" segment_no="2" tag_type="text">Matching networks for one shot learning. NeurIPS , 2016.</text>
<text top="158" left="55" width="235" height="9" font="font4" id="p11_t10" reading_order_no="9" segment_no="3" tag_type="text">Wang, Y., Chao, W., Weinberger, K. Q., and van der Maaten,</text>
<text top="170" left="65" width="224" height="9" font="font4" id="p11_t11" reading_order_no="10" segment_no="3" tag_type="text">L. Simpleshot: Revisiting nearest-neighbor classification</text>
<text top="182" left="65" width="225" height="9" font="font4" id="p11_t12" reading_order_no="11" segment_no="3" tag_type="text">for few-shot learning. arXiv preprint arXiv:1911.04623 ,</text>
<text top="194" left="65" width="22" height="9" font="font4" id="p11_t13" reading_order_no="12" segment_no="3" tag_type="text">2019.</text>
<text top="214" left="55" width="235" height="9" font="font4" id="p11_t14" reading_order_no="13" segment_no="4" tag_type="text">Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F.,</text>
<text top="226" left="65" width="226" height="9" font="font4" id="p11_t15" reading_order_no="14" segment_no="4" tag_type="text">Belongie, S., and Perona, P. Caltech-ucsd birds 200.</text>
<text top="238" left="65" width="22" height="9" font="font4" id="p11_t16" reading_order_no="15" segment_no="4" tag_type="text">2010.</text>
<text top="258" left="55" width="234" height="9" font="font4" id="p11_t17" reading_order_no="16" segment_no="5" tag_type="text">Ye, H., Hu, H., Zhan, D., and Sha, F. Few-shot learning via</text>
<text top="270" left="65" width="225" height="9" font="font4" id="p11_t18" reading_order_no="17" segment_no="5" tag_type="text">embedding adaptation with set-to-set functions. In CVPR ,</text>
<text top="281" left="65" width="22" height="9" font="font4" id="p11_t19" reading_order_no="18" segment_no="5" tag_type="text">2020.</text>
<text top="301" left="55" width="235" height="9" font="font4" id="p11_t20" reading_order_no="19" segment_no="6" tag_type="text">Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama,</text>
<text top="313" left="65" width="224" height="9" font="font4" id="p11_t21" reading_order_no="20" segment_no="6" tag_type="text">M. How does disagreement help generalization against</text>
<text top="325" left="65" width="133" height="9" font="font4" id="p11_t22" reading_order_no="21" segment_no="6" tag_type="text">label corruption? In ICML , 2019.</text>
<text top="345" left="55" width="236" height="9" font="font4" id="p11_t23" reading_order_no="22" segment_no="7" tag_type="text">Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self-</text>
<text top="357" left="65" width="212" height="9" font="font4" id="p11_t24" reading_order_no="23" segment_no="7" tag_type="text">supervised semi-supervised learning. In ICCV , 2019.</text>
<text top="377" left="55" width="234" height="9" font="font4" id="p11_t25" reading_order_no="24" segment_no="8" tag_type="text">Ziko, I. M., Dolz, J., Granger, E., and Ayed, I. B. Laplacian</text>
<text top="389" left="65" width="185" height="9" font="font4" id="p11_t26" reading_order_no="25" segment_no="8" tag_type="text">regularized few-shot learning. In ICML , 2020.</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="792" width="612">
<text top="101" left="75" width="448" height="13" font="font0" id="p12_t1" reading_order_no="0" segment_no="0" tag_type="title">Appendix: Unsupervised Embedding Adaptation via Early-Stage Feature</text>
<text top="119" left="168" width="262" height="13" font="font0" id="p12_t2" reading_order_no="1" segment_no="0" tag_type="title">Reconstruction for Few-Shot Classification</text>
<text top="173" left="55" width="86" height="11" font="font3" id="p12_t3" reading_order_no="2" segment_no="1" tag_type="title">A. Preprocessing</text>
<text top="194" left="55" width="486" height="9" font="font4" id="p12_t4" reading_order_no="3" segment_no="2" tag_type="text">In this section, we describe the preprocessing including equations in our paper. Assume we are given the embedding support</text>
<text top="206" left="55" width="486" height="10" font="font4" id="p12_t5" reading_order_no="4" segment_no="2" tag_type="text">set S f and embedding query set Q f . We apply centering and l2-normalization to the embedding samples for reconstruction</text>
<text top="218" left="55" width="488" height="10" font="font4" id="p12_t6" reading_order_no="5" segment_no="2" tag_type="text">training as described in ( 10 ). Preprocessed embeddings z ∈ Z preprocess are used as an input to the reconstruction module g φ .</text>
<text top="230" left="55" width="486" height="9" font="font4" id="p12_t7" reading_order_no="6" segment_no="2" tag_type="text">The same preprocessing (centering and l2-normalization) is applied at the output of reconstruction module to compute the</text>
<text top="241" left="55" width="137" height="11" font="font4" id="p12_t8" reading_order_no="7" segment_no="2" tag_type="text">reconstruction loss L FR as in ( 13 ).</text>
<text top="268" left="99" width="15" height="9" font="font18" id="p12_t9" reading_order_no="8" segment_no="3" tag_type="formula">¯ =</text>
<text top="261" left="136" width="5" height="9" font="font18" id="p12_t10" reading_order_no="9" segment_no="3" tag_type="formula">1</text>
<text top="274" left="118" width="41" height="10" font="font6" id="p12_t11" reading_order_no="10" segment_no="3" tag_type="formula">| S f ∪ Q f |</text>
<text top="265" left="172" width="14" height="4" font="font24" id="p12_t12" reading_order_no="11" segment_no="3" tag_type="formula">X<a href="deeplearning_paper6.html#12">(</a></text>
<text top="281" left="162" width="33" height="7" font="font19" id="p12_t13" reading_order_no="12" segment_no="3" tag_type="formula">z ∈ S f ∪ Q f<a href="deeplearning_paper6.html#12">10</a></text>
<text top="268" left="199" width="5" height="9" font="font17" id="p12_t14" reading_order_no="13" segment_no="3" tag_type="formula">z<a href="deeplearning_paper6.html#12">). </a>Preprocessed embeddings</text>
<text top="268" left="530" width="12" height="9" font="font4" id="p12_t15" reading_order_no="14" segment_no="3" tag_type="text">(9)</text>
<text top="295" left="99" width="109" height="14" font="font17" id="p12_t16" reading_order_no="15" segment_no="4" tag_type="formula">S preprocessed = n ( z 0 , y ) | z 0 =</text>
<text top="292" left="219" width="23" height="10" font="font17" id="p12_t17" reading_order_no="16" segment_no="4" tag_type="formula">z − ¯ z</text>
<text top="306" left="212" width="36" height="11" font="font6" id="p12_t18" reading_order_no="17" segment_no="4" tag_type="formula">k z − ¯ z k 2</text>
<text top="299" left="250" width="49" height="10" font="font17" id="p12_t19" reading_order_no="18" segment_no="4" tag_type="formula">, ( z, y ) ∈ S f</text>
<text top="295" left="300" width="10" height="13" font="font24" id="p12_t20" reading_order_no="19" segment_no="4" tag_type="formula">o ,</text>
<text top="295" left="321" width="94" height="14" font="font17" id="p12_t21" reading_order_no="20" segment_no="4" tag_type="formula">Q preprocessed = n z 0 | z 0 =</text>
<text top="292" left="426" width="23" height="10" font="font17" id="p12_t22" reading_order_no="21" segment_no="4" tag_type="formula">z − ¯ z</text>
<text top="306" left="419" width="36" height="11" font="font6" id="p12_t23" reading_order_no="22" segment_no="4" tag_type="formula">k z − ¯ z k 2</text>
<text top="299" left="457" width="33" height="10" font="font17" id="p12_t24" reading_order_no="23" segment_no="4" tag_type="formula">, z ∈ Q f</text>
<text top="295" left="492" width="7" height="4" font="font24" id="p12_t25" reading_order_no="24" segment_no="4" tag_type="formula">o</text>
<text top="300" left="525" width="17" height="9" font="font4" id="p12_t26" reading_order_no="25" segment_no="4" tag_type="text">(10)</text>
<text top="321" left="99" width="155" height="10" font="font17" id="p12_t27" reading_order_no="26" segment_no="5" tag_type="formula">Z preprocessed = S preprocessed ∪ Q preprocessed<a href="deeplearning_paper6.html#12">(</a></text>
<text top="322" left="525" width="17" height="9" font="font4" id="p12_t28" reading_order_no="27" segment_no="5" tag_type="text">(11)<a href="deeplearning_paper6.html#12">13</a></text>
<text top="342" left="99" width="20" height="10" font="font18" id="p12_t29" reading_order_no="28" segment_no="6" tag_type="formula">¯ φ =<a href="deeplearning_paper6.html#12">).</a></text>
<text top="336" left="145" width="5" height="9" font="font18" id="p12_t30" reading_order_no="29" segment_no="6" tag_type="formula">1</text>
<text top="349" left="123" width="49" height="10" font="font6" id="p12_t31" reading_order_no="30" segment_no="6" tag_type="formula">| Z preprocessed |</text>
<text top="339" left="189" width="14" height="4" font="font24" id="p12_t32" reading_order_no="31" segment_no="6" tag_type="formula">X</text>
<text top="356" left="175" width="41" height="7" font="font19" id="p12_t33" reading_order_no="32" segment_no="6" tag_type="formula">z ∈ Z preprocessed</text>
<text top="342" left="218" width="34" height="11" font="font38" id="p12_t34" reading_order_no="33" segment_no="6" tag_type="formula">E µ [ g φ ( z</text>
<text top="342" left="264" width="13" height="9" font="font17" id="p12_t35" reading_order_no="34" segment_no="6" tag_type="formula">µ )]</text>
<text top="342" left="525" width="17" height="9" font="font4" id="p12_t36" reading_order_no="35" segment_no="6" tag_type="text">(12)</text>
<text top="375" left="99" width="50" height="10" font="font6" id="p12_t37" reading_order_no="36" segment_no="7" tag_type="formula">L FR ( φ ) = −</text>
<text top="369" left="173" width="5" height="9" font="font18" id="p12_t38" reading_order_no="37" segment_no="7" tag_type="formula">1</text>
<text top="382" left="150" width="50" height="10" font="font6" id="p12_t39" reading_order_no="38" segment_no="7" tag_type="formula">| Z preprocessed |</text>
<text top="372" left="216" width="14" height="4" font="font24" id="p12_t40" reading_order_no="39" segment_no="7" tag_type="formula">X</text>
<text top="389" left="203" width="40" height="7" font="font19" id="p12_t41" reading_order_no="40" segment_no="7" tag_type="formula">z ∈ Z preprocessed</text>
<text top="378" left="246" width="11" height="8" font="font38" id="p12_t42" reading_order_no="41" segment_no="7" tag_type="formula">E µ</text>
<text top="371" left="258" width="14" height="13" font="font24" id="p12_t43" reading_order_no="42" segment_no="7" tag_type="formula">h z T</text>
<text top="369" left="282" width="19" height="9" font="font17" id="p12_t44" reading_order_no="43" segment_no="7" tag_type="formula">g φ ( z</text>
<text top="368" left="313" width="32" height="10" font="font17" id="p12_t45" reading_order_no="44" segment_no="7" tag_type="formula">µ ) − ¯ z φ</text>
<text top="382" left="275" width="24" height="10" font="font6" id="p12_t46" reading_order_no="45" segment_no="7" tag_type="formula">k g φ ( z</text>
<text top="382" left="311" width="41" height="12" font="font17" id="p12_t47" reading_order_no="46" segment_no="7" tag_type="formula">µ ) − ¯ z φ k 2</text>
<text top="371" left="354" width="5" height="4" font="font24" id="p12_t48" reading_order_no="47" segment_no="7" tag_type="formula">i</text>
<text top="376" left="525" width="17" height="9" font="font4" id="p12_t49" reading_order_no="48" segment_no="7" tag_type="text">(13)</text>
<text top="414" left="55" width="487" height="9" font="font4" id="p12_t50" reading_order_no="49" segment_no="8" tag_type="text">As new embeddings for few-shot classification, we apply only l2-normalization since it performs the best. The new</text>
<text top="425" left="55" width="332" height="10" font="font4" id="p12_t51" reading_order_no="50" segment_no="8" tag_type="text">embedding sets S ESFR and Q ESFR for the few-shot classification task are as follows:</text>
<text top="451" left="169" width="103" height="16" font="font17" id="p12_t52" reading_order_no="51" segment_no="9" tag_type="formula">S ESFR = n ( z 0 , y ) | z 0 = 1</text>
<text top="464" left="264" width="12" height="10" font="font17" id="p12_t53" reading_order_no="52" segment_no="9" tag_type="formula">N e</text>
<text top="447" left="279" width="14" height="12" font="font24" id="p12_t54" reading_order_no="53" segment_no="9" tag_type="formula">N e X</text>
<text top="471" left="280" width="12" height="6" font="font19" id="p12_t55" reading_order_no="54" segment_no="9" tag_type="formula">i =1</text>
<text top="450" left="303" width="14" height="12" font="font17" id="p12_t56" reading_order_no="55" segment_no="9" tag_type="formula">g φ i ∗</text>
<text top="450" left="317" width="13" height="9" font="font18" id="p12_t57" reading_order_no="56" segment_no="9" tag_type="formula">( z )</text>
<text top="464" left="296" width="19" height="12" font="font6" id="p12_t58" reading_order_no="57" segment_no="9" tag_type="formula">k g φ i ∗</text>
<text top="464" left="315" width="18" height="9" font="font18" id="p12_t59" reading_order_no="58" segment_no="9" tag_type="formula">( z ) k</text>
<text top="471" left="333" width="4" height="6" font="font21" id="p12_t60" reading_order_no="59" segment_no="9" tag_type="formula">2</text>
<text top="457" left="339" width="81" height="10" font="font17" id="p12_t61" reading_order_no="60" segment_no="9" tag_type="formula">, ( z, y ) ∈ S preprocessed</text>
<text top="453" left="421" width="7" height="4" font="font24" id="p12_t62" reading_order_no="61" segment_no="9" tag_type="formula">o</text>
<text top="458" left="525" width="17" height="9" font="font4" id="p12_t63" reading_order_no="62" segment_no="9" tag_type="text">(14)</text>
<text top="486" left="169" width="87" height="16" font="font17" id="p12_t64" reading_order_no="63" segment_no="10" tag_type="formula">Q ESFR = n z 0 | z 0 = 1</text>
<text top="499" left="247" width="12" height="10" font="font17" id="p12_t65" reading_order_no="64" segment_no="10" tag_type="formula">N e</text>
<text top="482" left="263" width="14" height="12" font="font24" id="p12_t66" reading_order_no="65" segment_no="10" tag_type="formula">N e X</text>
<text top="506" left="263" width="13" height="6" font="font19" id="p12_t67" reading_order_no="66" segment_no="10" tag_type="formula">i =1</text>
<text top="485" left="287" width="14" height="12" font="font17" id="p12_t68" reading_order_no="67" segment_no="10" tag_type="formula">g φ i ∗</text>
<text top="485" left="301" width="13" height="9" font="font18" id="p12_t69" reading_order_no="68" segment_no="10" tag_type="formula">( z )</text>
<text top="499" left="280" width="18" height="12" font="font6" id="p12_t70" reading_order_no="69" segment_no="10" tag_type="formula">k g φ i ∗</text>
<text top="499" left="299" width="18" height="9" font="font18" id="p12_t71" reading_order_no="70" segment_no="10" tag_type="formula">( z ) k</text>
<text top="506" left="317" width="4" height="6" font="font21" id="p12_t72" reading_order_no="71" segment_no="10" tag_type="formula">2</text>
<text top="492" left="322" width="66" height="10" font="font17" id="p12_t73" reading_order_no="72" segment_no="10" tag_type="formula">, z ∈ Q preprocessed</text>
<text top="488" left="389" width="7" height="4" font="font24" id="p12_t74" reading_order_no="73" segment_no="10" tag_type="formula">o</text>
<text top="493" left="525" width="17" height="9" font="font4" id="p12_t75" reading_order_no="74" segment_no="10" tag_type="text">(15)</text>
<text top="529" left="55" width="487" height="9" font="font4" id="p12_t76" reading_order_no="75" segment_no="11" tag_type="text">For BD-CSPN ( Liu et al. , 2020 ) and our method used with BD-CSPN, additional shifting-term is added for query samples</text>
<text top="541" left="55" width="319" height="10" font="font4" id="p12_t77" reading_order_no="76" segment_no="11" tag_type="text">before preprocessing. In this case, we define S preprocess and Q preprocess as follows:</text>
<text top="561" left="91" width="89" height="16" font="font4" id="p12_t78" reading_order_no="77" segment_no="12" tag_type="formula">shifting-term: 4 = 1</text>
<text top="575" left="172" width="10" height="10" font="font17" id="p12_t79" reading_order_no="78" segment_no="12" tag_type="formula">S f</text>
<text top="565" left="190" width="14" height="4" font="font24" id="p12_t80" reading_order_no="79" segment_no="12" tag_type="formula">X</text>
<text top="582" left="186" width="21" height="7" font="font19" id="p12_t81" reading_order_no="80" segment_no="12" tag_type="formula">z s ∈ S f</text>
<text top="568" left="210" width="19" height="10" font="font17" id="p12_t82" reading_order_no="81" segment_no="12" tag_type="formula">z s −</text>
<text top="561" left="237" width="5" height="9" font="font18" id="p12_t83" reading_order_no="82" segment_no="12" tag_type="formula">1</text>
<text top="575" left="233" width="11" height="10" font="font17" id="p12_t84" reading_order_no="83" segment_no="12" tag_type="formula">Q f</text>
<text top="565" left="253" width="14" height="4" font="font24" id="p12_t85" reading_order_no="84" segment_no="12" tag_type="formula">X</text>
<text top="582" left="248" width="23" height="7" font="font19" id="p12_t86" reading_order_no="85" segment_no="12" tag_type="formula">z q ∈ Q f</text>
<text top="568" left="274" width="9" height="10" font="font17" id="p12_t87" reading_order_no="86" segment_no="12" tag_type="formula">z q</text>
<text top="568" left="525" width="17" height="9" font="font4" id="p12_t88" reading_order_no="87" segment_no="12" tag_type="text">(16)</text>
<text top="597" left="91" width="27" height="12" font="font17" id="p12_t89" reading_order_no="88" segment_no="13" tag_type="formula">Q shifted f</text>
<text top="594" left="122" width="108" height="14" font="font18" id="p12_t90" reading_order_no="89" segment_no="13" tag_type="formula">= n z 0 | z 0 = z + 4 , z ∈ Q f</text>
<text top="594" left="231" width="7" height="4" font="font24" id="p12_t91" reading_order_no="90" segment_no="13" tag_type="formula">o</text>
<text top="599" left="525" width="17" height="9" font="font4" id="p12_t92" reading_order_no="91" segment_no="13" tag_type="text">(17)</text>
<text top="622" left="91" width="16" height="9" font="font18" id="p12_t93" reading_order_no="92" segment_no="14" tag_type="formula">¯ z =</text>
<text top="615" left="136" width="5" height="9" font="font18" id="p12_t94" reading_order_no="93" segment_no="14" tag_type="formula">1</text>
<text top="628" left="111" width="52" height="12" font="font6" id="p12_t95" reading_order_no="94" segment_no="14" tag_type="formula">| S f ∪ Q shifted f</text>
<text top="629" left="163" width="3" height="9" font="font6" id="p12_t96" reading_order_no="95" segment_no="14" tag_type="formula">|</text>
<text top="619" left="184" width="14" height="4" font="font24" id="p12_t97" reading_order_no="96" segment_no="14" tag_type="formula">X</text>
<text top="636" left="169" width="45" height="9" font="font19" id="p12_t98" reading_order_no="97" segment_no="14" tag_type="formula">z ∈ S f ∪ Q shifted f</text>
<text top="622" left="215" width="5" height="9" font="font17" id="p12_t99" reading_order_no="98" segment_no="14" tag_type="formula">z</text>
<text top="622" left="525" width="17" height="9" font="font4" id="p12_t100" reading_order_no="99" segment_no="14" tag_type="text">(18)</text>
<text top="652" left="91" width="110" height="14" font="font17" id="p12_t101" reading_order_no="100" segment_no="15" tag_type="formula">S preprocessed = n ( z 0 , y ) | z 0 =</text>
<text top="649" left="212" width="23" height="9" font="font17" id="p12_t102" reading_order_no="101" segment_no="15" tag_type="formula">z − ¯ z</text>
<text top="662" left="205" width="36" height="12" font="font6" id="p12_t103" reading_order_no="102" segment_no="15" tag_type="formula">k z − ¯ z k 2</text>
<text top="656" left="243" width="49" height="10" font="font17" id="p12_t104" reading_order_no="103" segment_no="15" tag_type="formula">, ( z, y ) ∈ S f</text>
<text top="652" left="293" width="10" height="13" font="font24" id="p12_t105" reading_order_no="104" segment_no="15" tag_type="formula">o ,</text>
<text top="652" left="314" width="94" height="14" font="font17" id="p12_t106" reading_order_no="105" segment_no="15" tag_type="formula">Q preprocessed = n z 0 | z 0 =</text>
<text top="649" left="419" width="23" height="9" font="font17" id="p12_t107" reading_order_no="106" segment_no="15" tag_type="formula">z − ¯ z</text>
<text top="662" left="412" width="36" height="12" font="font6" id="p12_t108" reading_order_no="107" segment_no="15" tag_type="formula">k z − ¯ z k 2</text>
<text top="654" left="450" width="48" height="11" font="font17" id="p12_t109" reading_order_no="108" segment_no="15" tag_type="formula">, z ∈ Q shifted</text>
<text top="661" left="479" width="4" height="6" font="font19" id="p12_t110" reading_order_no="109" segment_no="15" tag_type="formula">f</text>
<text top="652" left="499" width="7" height="4" font="font24" id="p12_t111" reading_order_no="110" segment_no="15" tag_type="formula">o</text>
<text top="656" left="525" width="17" height="9" font="font4" id="p12_t112" reading_order_no="111" segment_no="15" tag_type="text">(19)</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p13_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="69" left="55" width="117" height="11" font="font3" id="p13_t2" reading_order_no="1" segment_no="1" tag_type="title">B. Comparison to TIM</text>
<text top="90" left="55" width="486" height="8" font="font34" id="p13_t3" reading_order_no="2" segment_no="2" tag_type="text">Table 5. Table describes the performance comparison with TIM ( Boudiaf et al. , 2020 ) of 5-way 1- and 5-shot accuracies (in %) on</text>
<text top="101" left="55" width="487" height="8" font="font12" id="p13_t4" reading_order_no="3" segment_no="2" tag_type="text">mini -ImageNet, tiered -ImageNet and CUB. The performance of TIM-GD is from the paper ( Boudiaf et al. , 2020 ). We use preprocessing<a href="deeplearning_paper6.html#9">(</a></text>
<text top="112" left="55" width="331" height="8" font="font10" id="p13_t5" reading_order_no="4" segment_no="2" tag_type="text">with shifting-term to acquire new embeddings for TIM-GD + ESFR since it performs better.<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="127" left="285" width="57" height="8" font="font42" id="p13_t6" reading_order_no="5" segment_no="3" tag_type="table">mini -ImageNet<a href="deeplearning_paper6.html#9">,</a></text>
<text top="127" left="355" width="61" height="8" font="font42" id="p13_t7" reading_order_no="6" segment_no="3" tag_type="table">tiered -ImageNet<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="127" left="448" width="19" height="8" font="font16" id="p13_t8" reading_order_no="7" segment_no="3" tag_type="table">CUB<a href="deeplearning_paper6.html#9">) </a>of 5-way 1- and 5-shot accuracies (in %) on</text>
<text top="137" left="110" width="30" height="8" font="font16" id="p13_t9" reading_order_no="8" segment_no="3" tag_type="table">Method</text>
<text top="137" left="234" width="38" height="8" font="font16" id="p13_t10" reading_order_no="9" segment_no="3" tag_type="table">Backbone</text>
<text top="137" left="284" width="23" height="8" font="font16" id="p13_t11" reading_order_no="10" segment_no="3" tag_type="table">1-shot</text>
<text top="137" left="319" width="23" height="8" font="font16" id="p13_t12" reading_order_no="11" segment_no="3" tag_type="table">5-shot<a href="deeplearning_paper6.html#9">(</a></text>
<text top="137" left="355" width="23" height="8" font="font16" id="p13_t13" reading_order_no="12" segment_no="3" tag_type="table">1-shot<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="137" left="391" width="23" height="8" font="font16" id="p13_t14" reading_order_no="13" segment_no="3" tag_type="table">5-shot<a href="deeplearning_paper6.html#9">,</a></text>
<text top="137" left="428" width="23" height="8" font="font16" id="p13_t15" reading_order_no="14" segment_no="3" tag_type="table">1-shot<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="137" left="463" width="23" height="8" font="font16" id="p13_t16" reading_order_no="15" segment_no="3" tag_type="table">5-shot<a href="deeplearning_paper6.html#9">). </a>We use preprocessing</text>
<text top="147" left="110" width="112" height="8" font="font10" id="p13_t17" reading_order_no="16" segment_no="3" tag_type="table">TIM-GD ( Boudiaf et al. , 2020 )</text>
<text top="147" left="234" width="38" height="8" font="font10" id="p13_t18" reading_order_no="17" segment_no="3" tag_type="table">ResNet-18</text>
<text top="147" left="288" width="16" height="8" font="font10" id="p13_t19" reading_order_no="18" segment_no="3" tag_type="table">73.9</text>
<text top="147" left="323" width="16" height="8" font="font16" id="p13_t20" reading_order_no="19" segment_no="3" tag_type="table">85.0</text>
<text top="147" left="359" width="16" height="8" font="font10" id="p13_t21" reading_order_no="20" segment_no="3" tag_type="table">79.9</text>
<text top="147" left="395" width="16" height="8" font="font16" id="p13_t22" reading_order_no="21" segment_no="3" tag_type="table">88.5</text>
<text top="147" left="432" width="16" height="8" font="font10" id="p13_t23" reading_order_no="22" segment_no="3" tag_type="table">82.2</text>
<text top="147" left="467" width="16" height="8" font="font16" id="p13_t24" reading_order_no="23" segment_no="3" tag_type="table">90.8</text>
<text top="157" left="110" width="63" height="8" font="font10" id="p13_t25" reading_order_no="24" segment_no="3" tag_type="table">TIM-GD + ESFR</text>
<text top="157" left="234" width="38" height="8" font="font10" id="p13_t26" reading_order_no="25" segment_no="3" tag_type="table">ResNet-18</text>
<text top="157" left="286" width="20" height="8" font="font16" id="p13_t27" reading_order_no="26" segment_no="3" tag_type="table">76.02</text>
<text top="157" left="321" width="20" height="8" font="font10" id="p13_t28" reading_order_no="27" segment_no="3" tag_type="table">84.42</text>
<text top="157" left="356" width="20" height="8" font="font16" id="p13_t29" reading_order_no="28" segment_no="3" tag_type="table">82.03</text>
<text top="157" left="393" width="20" height="8" font="font10" id="p13_t30" reading_order_no="29" segment_no="3" tag_type="table">88.11</text>
<text top="157" left="430" width="20" height="8" font="font16" id="p13_t31" reading_order_no="30" segment_no="3" tag_type="table">84.69<a href="deeplearning_paper6.html#9">(</a></text>
<text top="157" left="465" width="20" height="8" font="font10" id="p13_t32" reading_order_no="31" segment_no="3" tag_type="table">90.43<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="167" left="110" width="112" height="8" font="font10" id="p13_t33" reading_order_no="32" segment_no="3" tag_type="table">TIM-GD ( Boudiaf et al. , 2020 )<a href="deeplearning_paper6.html#9">,</a></text>
<text top="167" left="242" width="21" height="8" font="font10" id="p13_t34" reading_order_no="33" segment_no="3" tag_type="table">WRN<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="167" left="288" width="16" height="8" font="font10" id="p13_t35" reading_order_no="34" segment_no="3" tag_type="table">77.8<a href="deeplearning_paper6.html#9">)</a></text>
<text top="167" left="323" width="16" height="8" font="font16" id="p13_t36" reading_order_no="35" segment_no="3" tag_type="table">87.4</text>
<text top="167" left="359" width="16" height="8" font="font10" id="p13_t37" reading_order_no="36" segment_no="3" tag_type="table">82.1</text>
<text top="167" left="395" width="16" height="8" font="font16" id="p13_t38" reading_order_no="37" segment_no="3" tag_type="table">89.8</text>
<text top="167" left="438" width="3" height="8" font="font10" id="p13_t39" reading_order_no="38" segment_no="3" tag_type="table">-</text>
<text top="167" left="474" width="3" height="8" font="font10" id="p13_t40" reading_order_no="39" segment_no="3" tag_type="table">-</text>
<text top="177" left="110" width="63" height="8" font="font10" id="p13_t41" reading_order_no="40" segment_no="3" tag_type="table">TIM-GD + ESFR</text>
<text top="177" left="242" width="21" height="8" font="font10" id="p13_t42" reading_order_no="41" segment_no="3" tag_type="table">WRN</text>
<text top="177" left="286" width="20" height="8" font="font16" id="p13_t43" reading_order_no="42" segment_no="3" tag_type="table">79.25</text>
<text top="177" left="321" width="20" height="8" font="font10" id="p13_t44" reading_order_no="43" segment_no="3" tag_type="table">86.38</text>
<text top="177" left="356" width="20" height="8" font="font16" id="p13_t45" reading_order_no="44" segment_no="3" tag_type="table">83.58</text>
<text top="177" left="393" width="20" height="8" font="font10" id="p13_t46" reading_order_no="45" segment_no="3" tag_type="table">89.44</text>
<text top="177" left="438" width="3" height="8" font="font10" id="p13_t47" reading_order_no="46" segment_no="3" tag_type="table">-</text>
<text top="177" left="474" width="3" height="8" font="font10" id="p13_t48" reading_order_no="47" segment_no="3" tag_type="table">-</text>
<text top="201" left="55" width="487" height="9" font="font4" id="p13_t49" reading_order_no="48" segment_no="4" tag_type="text">We separately compare the performance of our method with TIM ( Boudiaf et al. , 2020 ) since we believe TIM uses a strong</text>
<text top="213" left="55" width="486" height="9" font="font4" id="p13_t50" reading_order_no="49" segment_no="4" tag_type="text">prior that query samples per class are balanced in the standard few-shot classification benchmarks (e.g., 15-query samples per</text>
<text top="225" left="55" width="487" height="9" font="font4" id="p13_t51" reading_order_no="50" segment_no="4" tag_type="text">class); while our method combined with the baseline methods (NN, Linear, BD-CSPN ( Liu et al. , 2020 )) does not utilize any<a href="deeplearning_paper6.html#9">(</a></text>
<text top="237" left="55" width="486" height="9" font="font4" id="p13_t52" reading_order_no="51" segment_no="4" tag_type="text">query statistics. We find that TIM’s proposed regularization term with conditional entropy and label-marginal entropy forces<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="249" left="55" width="486" height="9" font="font4" id="p13_t53" reading_order_no="52" segment_no="4" tag_type="text">balancing among the predicted number of query samples per class. To be specific, the conditional entropy minimization<a href="deeplearning_paper6.html#9">,</a></text>
<text top="260" left="55" width="486" height="9" font="font4" id="p13_t54" reading_order_no="53" segment_no="4" tag_type="text">term encourages the classification model to output confident prediction and the label-marginal entropy maximization term<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="272" left="55" width="486" height="9" font="font4" id="p13_t55" reading_order_no="54" segment_no="4" tag_type="text">encourages marginal predicted label distribution to be uniform. When both conditional and label-marginal entropy terms are<a href="deeplearning_paper6.html#9">)</a></text>
<text top="284" left="55" width="486" height="9" font="font4" id="p13_t56" reading_order_no="55" segment_no="4" tag_type="text">used simultaneously, the predicted labels close to one-hot from uniform class distribution, resulting in the same number of</text>
<text top="296" left="55" width="486" height="9" font="font4" id="p13_t57" reading_order_no="56" segment_no="4" tag_type="text">predicted samples for each class. This seems helpful in the balanced query class distribution setting where all query samples</text>
<text top="308" left="55" width="486" height="9" font="font4" id="p13_t58" reading_order_no="57" segment_no="4" tag_type="text">per class are the same, but its possible use case will be limited. We find that query class imbalance setting can easily ruin</text>
<text top="320" left="55" width="132" height="9" font="font4" id="p13_t59" reading_order_no="58" segment_no="4" tag_type="text">TIM’s performance in Section D .</text>
<text top="338" left="55" width="486" height="9" font="font4" id="p13_t60" reading_order_no="59" segment_no="5" tag_type="text">To investigate our method when using query statistics, we experiment with TIM-GD + ESFR. Table 5 shows the results on</text>
<text top="350" left="55" width="486" height="9" font="font4" id="p13_t61" reading_order_no="60" segment_no="5" tag_type="text">standard mini-ImageNet, tiered-ImageNet, and CUB with 5-way 1- and 5-shot settings. For 1-shot settings, our method</text>
<text top="361" left="55" width="488" height="10" font="font4" id="p13_t62" reading_order_no="61" segment_no="5" tag_type="text">improves the performance of TIM by 1.5% ∼ 2.5% across all datasets and backbones. As mentioned before (in Section 5.3),</text>
<text top="374" left="55" width="486" height="9" font="font4" id="p13_t63" reading_order_no="62" segment_no="5" tag_type="text">this indicates that our method can offer a complementary improvement to semi-supervised learning techniques such as TIM</text>
<text top="385" left="55" width="487" height="10" font="font4" id="p13_t64" reading_order_no="63" segment_no="5" tag_type="text">for 1-shot. For 5-shot settings, our method decreases the performance by 0.4% ∼ 1.0%. The decrease in 5-shot performance</text>
<text top="398" left="55" width="486" height="9" font="font4" id="p13_t65" reading_order_no="64" segment_no="5" tag_type="text">encourages further research about the simultaneous use of pseudo-label information and unsupervised information, which</text>
<text top="410" left="55" width="97" height="9" font="font4" id="p13_t66" reading_order_no="65" segment_no="5" tag_type="text">we leave as future work.</text>
<text top="437" left="55" width="175" height="11" font="font3" id="p13_t67" reading_order_no="66" segment_no="6" tag_type="title">C. Ablation: noise level of dropout</text>
<text top="462" left="55" width="487" height="8" font="font34" id="p13_t68" reading_order_no="67" segment_no="7" tag_type="text">Table 6. The table shows the influence of drop-rate applied to our method. We experimented with ResNet-18 backbone on mini -ImageNet</text>
<text top="473" left="55" width="77" height="8" font="font10" id="p13_t69" reading_order_no="68" segment_no="7" tag_type="text">and tiered -ImageNet.</text>
<text top="488" left="247" width="57" height="8" font="font42" id="p13_t70" reading_order_no="69" segment_no="8" tag_type="table">mini -ImageNet</text>
<text top="488" left="316" width="61" height="8" font="font42" id="p13_t71" reading_order_no="70" segment_no="8" tag_type="table">tiered -ImageNet<a href="deeplearning_paper6.html#9">(</a></text>
<text top="498" left="219" width="15" height="8" font="font16" id="p13_t72" reading_order_no="71" segment_no="8" tag_type="table">rate<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="498" left="247" width="20" height="8" font="font16" id="p13_t73" reading_order_no="72" segment_no="8" tag_type="table">1shot<a href="deeplearning_paper6.html#9">,</a></text>
<text top="498" left="281" width="20" height="8" font="font16" id="p13_t74" reading_order_no="73" segment_no="8" tag_type="table">5shot<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="498" left="316" width="20" height="8" font="font16" id="p13_t75" reading_order_no="74" segment_no="8" tag_type="table">1shot<a href="deeplearning_paper6.html#9">) </a>since we believe TIM uses a strong</text>
<text top="498" left="353" width="20" height="8" font="font16" id="p13_t76" reading_order_no="75" segment_no="8" tag_type="table">5shot</text>
<text top="508" left="224" width="7" height="8" font="font10" id="p13_t77" reading_order_no="76" segment_no="8" tag_type="table">0.<a href="deeplearning_paper6.html#10">(</a></text>
<text top="508" left="247" width="20" height="8" font="font10" id="p13_t78" reading_order_no="77" segment_no="8" tag_type="table">68.90<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="508" left="282" width="20" height="8" font="font10" id="p13_t79" reading_order_no="78" segment_no="8" tag_type="table">81.53<a href="deeplearning_paper6.html#10">,</a></text>
<text top="508" left="316" width="20" height="8" font="font10" id="p13_t80" reading_order_no="79" segment_no="8" tag_type="table">75.39<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="508" left="353" width="20" height="8" font="font10" id="p13_t81" reading_order_no="80" segment_no="8" tag_type="table">85.31<a href="deeplearning_paper6.html#10">)) </a>does not utilize any</text>
<text top="518" left="222" width="11" height="8" font="font10" id="p13_t82" reading_order_no="81" segment_no="8" tag_type="table">0.1</text>
<text top="518" left="247" width="20" height="8" font="font10" id="p13_t83" reading_order_no="82" segment_no="8" tag_type="table">69.41</text>
<text top="518" left="282" width="20" height="8" font="font10" id="p13_t84" reading_order_no="83" segment_no="8" tag_type="table">81.59</text>
<text top="518" left="316" width="20" height="8" font="font10" id="p13_t85" reading_order_no="84" segment_no="8" tag_type="table">75.90</text>
<text top="518" left="353" width="20" height="8" font="font10" id="p13_t86" reading_order_no="85" segment_no="8" tag_type="table">85.50</text>
<text top="528" left="222" width="11" height="8" font="font10" id="p13_t87" reading_order_no="86" segment_no="8" tag_type="table">0.2</text>
<text top="528" left="247" width="20" height="8" font="font10" id="p13_t88" reading_order_no="87" segment_no="8" tag_type="table">69.90</text>
<text top="528" left="282" width="20" height="8" font="font10" id="p13_t89" reading_order_no="88" segment_no="8" tag_type="table">81.70</text>
<text top="528" left="316" width="20" height="8" font="font10" id="p13_t90" reading_order_no="89" segment_no="8" tag_type="table">76.39<a href="deeplearning_paper6.html#5">D</a></text>
<text top="528" left="353" width="20" height="8" font="font10" id="p13_t91" reading_order_no="90" segment_no="8" tag_type="table">85.63<a href="deeplearning_paper6.html#5">.</a></text>
<text top="538" left="222" width="11" height="8" font="font10" id="p13_t92" reading_order_no="91" segment_no="8" tag_type="table">0.3</text>
<text top="538" left="247" width="20" height="8" font="font10" id="p13_t93" reading_order_no="92" segment_no="8" tag_type="table">70.39<a href="deeplearning_paper6.html#13">5</a></text>
<text top="538" left="282" width="20" height="8" font="font16" id="p13_t94" reading_order_no="93" segment_no="8" tag_type="table">81.71</text>
<text top="538" left="316" width="20" height="8" font="font10" id="p13_t95" reading_order_no="94" segment_no="8" tag_type="table">76.78</text>
<text top="538" left="353" width="20" height="8" font="font10" id="p13_t96" reading_order_no="95" segment_no="8" tag_type="table">85.71</text>
<text top="548" left="222" width="11" height="8" font="font10" id="p13_t97" reading_order_no="96" segment_no="8" tag_type="table">0.4</text>
<text top="548" left="247" width="20" height="8" font="font10" id="p13_t98" reading_order_no="97" segment_no="8" tag_type="table">70.63</text>
<text top="548" left="282" width="20" height="8" font="font16" id="p13_t99" reading_order_no="98" segment_no="8" tag_type="table">81.71</text>
<text top="548" left="316" width="20" height="8" font="font10" id="p13_t100" reading_order_no="99" segment_no="8" tag_type="table">77.23</text>
<text top="548" left="353" width="20" height="8" font="font10" id="p13_t101" reading_order_no="100" segment_no="8" tag_type="table">85.77</text>
<text top="558" left="222" width="11" height="8" font="font10" id="p13_t102" reading_order_no="101" segment_no="8" tag_type="table">0.5</text>
<text top="558" left="247" width="20" height="8" font="font16" id="p13_t103" reading_order_no="102" segment_no="8" tag_type="table">70.94</text>
<text top="558" left="282" width="20" height="8" font="font10" id="p13_t104" reading_order_no="103" segment_no="8" tag_type="table">81.61</text>
<text top="558" left="316" width="20" height="8" font="font16" id="p13_t105" reading_order_no="104" segment_no="8" tag_type="table">77.44</text>
<text top="558" left="353" width="20" height="8" font="font16" id="p13_t106" reading_order_no="105" segment_no="8" tag_type="table">85.84</text>
<text top="589" left="55" width="486" height="9" font="font4" id="p13_t107" reading_order_no="106" segment_no="9" tag_type="text">We further investigate the effect of the dropout noise level. In the main text, we argued that multiplicative noise by dropout</text>
<text top="601" left="55" width="487" height="9" font="font4" id="p13_t108" reading_order_no="107" segment_no="9" tag_type="text">seems well suited for our method. Experiments in Table 6 with various drop-rate show that the dropout can be used in our</text>
<text top="613" left="55" width="123" height="9" font="font4" id="p13_t109" reading_order_no="108" segment_no="9" tag_type="text">method without careful tuning.</text>
<text top="640" left="55" width="333" height="11" font="font3" id="p13_t110" reading_order_no="109" segment_no="10" tag_type="title">D. Few-shot classification with imbalance query class distribution</text>
<text top="660" left="55" width="487" height="9" font="font4" id="p13_t111" reading_order_no="110" segment_no="11" tag_type="text">To verify our method’s robustness on various query settings, we experiment with the setting when the numbers of query</text>
<text top="672" left="55" width="487" height="9" font="font4" id="p13_t112" reading_order_no="111" segment_no="11" tag_type="text">samples per class are imbalanced. We set the number of query samples per class as (11, 13, 15, 17, 19) and (7, 11, 15, 19,</text>
<text top="684" left="55" width="487" height="9" font="font4" id="p13_t113" reading_order_no="112" segment_no="11" tag_type="text">23). Table 7 shows that our method consistently improves the performance of few-shot classification regardless of the query</text>
<text top="696" left="55" width="487" height="9" font="font4" id="p13_t114" reading_order_no="113" segment_no="11" tag_type="text">imbalance setting. To be more specific, the improvement by our method in different query settings varies within &lt; 1 . 5% ;</text>
<text top="708" left="55" width="486" height="9" font="font4" id="p13_t115" reading_order_no="114" segment_no="11" tag_type="text">thus, our method is robust to different query settings. In contrast, TIM ( Boudiaf et al. , 2020 ) that uses the strong prior about</text>
</page>
<page number="14" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p14_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="77" left="55" width="486" height="8" font="font34" id="p14_t2" reading_order_no="1" segment_no="1" tag_type="text">Table 7. This table shows few-shot classification performance when the numbers of query samples per class are imbalanced. For standard</text>
<text top="88" left="55" width="486" height="8" font="font10" id="p14_t3" reading_order_no="2" segment_no="1" tag_type="text">settings, the number of query samples per class is equally 15, given as (15, 15, 15, 15, 15). For the imbalance case, we set the number of</text>
<text top="98" left="55" width="486" height="9" font="font10" id="p14_t4" reading_order_no="3" segment_no="1" tag_type="text">query samples per class as (11, 13, 15, 17, 19) and (7, 11, 15, 19, 23). The ± describes 95% confidence interval. For these results, we use</text>
<text top="110" left="55" width="487" height="8" font="font10" id="p14_t5" reading_order_no="4" segment_no="1" tag_type="text">our implementation version of TIM-GD ( Boudiaf et al. , 2020 ), which matches the original paper’s performance. For BD-CSPN ( Liu et al. ,</text>
<text top="121" left="55" width="396" height="8" font="font25" id="p14_t6" reading_order_no="5" segment_no="1" tag_type="text">2020 ) with an imbalance number of query samples, we do not use shift-term since it worsens the performance.</text>
<text top="136" left="270" width="57" height="8" font="font42" id="p14_t7" reading_order_no="6" segment_no="2" tag_type="table">mini -ImageNet</text>
<text top="151" left="93" width="30" height="8" font="font16" id="p14_t8" reading_order_no="7" segment_no="2" tag_type="table">Method<a href="deeplearning_paper6.html#9">(</a></text>
<text top="151" left="139" width="38" height="8" font="font16" id="p14_t9" reading_order_no="8" segment_no="2" tag_type="table">Backbone<a href="deeplearning_paper6.html#9">Boudiaf et al.</a></text>
<text top="146" left="204" width="69" height="8" font="font16" id="p14_t10" reading_order_no="9" segment_no="2" tag_type="table">(15, 15, 15, 15, 15)<a href="deeplearning_paper6.html#9">,</a></text>
<text top="146" left="314" width="69" height="8" font="font16" id="p14_t11" reading_order_no="10" segment_no="2" tag_type="table">(11, 13, 15, 17, 19)<a href="deeplearning_paper6.html#9">2020</a></text>
<text top="146" left="426" width="64" height="8" font="font16" id="p14_t12" reading_order_no="11" segment_no="2" tag_type="table">(7, 11, 15, 19, 23)<a href="deeplearning_paper6.html#9">), </a>which matches the original paper’s performance. For BD-CSPN <a href="deeplearning_paper6.html#10">(</a></text>
<text top="156" left="199" width="23" height="8" font="font16" id="p14_t13" reading_order_no="12" segment_no="2" tag_type="table">1-shot<a href="deeplearning_paper6.html#10">Liu et al.</a></text>
<text top="156" left="254" width="23" height="8" font="font16" id="p14_t14" reading_order_no="13" segment_no="2" tag_type="table">5-shot<a href="deeplearning_paper6.html#10">,</a></text>
<text top="156" left="309" width="23" height="8" font="font16" id="p14_t15" reading_order_no="14" segment_no="2" tag_type="table">1-shot<a href="deeplearning_paper6.html#10">2020</a></text>
<text top="156" left="364" width="23" height="8" font="font16" id="p14_t16" reading_order_no="15" segment_no="2" tag_type="table">5-shot<a href="deeplearning_paper6.html#10">) </a>with an imbalance number of query samples, we do not use shift-term since it worsens the performance.</text>
<text top="156" left="419" width="23" height="8" font="font16" id="p14_t17" reading_order_no="16" segment_no="2" tag_type="table">1-shot</text>
<text top="156" left="474" width="23" height="8" font="font16" id="p14_t18" reading_order_no="17" segment_no="2" tag_type="table">5-shot</text>
<text top="167" left="92" width="32" height="8" font="font10" id="p14_t19" reading_order_no="18" segment_no="2" tag_type="table">TIM-GD</text>
<text top="167" left="139" width="38" height="8" font="font10" id="p14_t20" reading_order_no="19" segment_no="2" tag_type="table">ResNet-18</text>
<text top="166" left="190" width="43" height="9" font="font10" id="p14_t21" reading_order_no="20" segment_no="2" tag_type="table">73.67 ± 0.33</text>
<text top="166" left="245" width="43" height="9" font="font10" id="p14_t22" reading_order_no="21" segment_no="2" tag_type="table">85.01 ± 0.19</text>
<text top="166" left="299" width="44" height="9" font="font10" id="p14_t23" reading_order_no="22" segment_no="2" tag_type="table">68.93 ± 0.30</text>
<text top="166" left="354" width="44" height="9" font="font10" id="p14_t24" reading_order_no="23" segment_no="2" tag_type="table">79.05 ± 0.17</text>
<text top="166" left="409" width="44" height="9" font="font10" id="p14_t25" reading_order_no="24" segment_no="2" tag_type="table">66.04 ± 0.28</text>
<text top="166" left="464" width="44" height="9" font="font10" id="p14_t26" reading_order_no="25" segment_no="2" tag_type="table">75.60 ± 0.16</text>
<text top="177" left="102" width="13" height="8" font="font10" id="p14_t27" reading_order_no="26" segment_no="2" tag_type="table">NN</text>
<text top="177" left="139" width="38" height="8" font="font10" id="p14_t28" reading_order_no="27" segment_no="2" tag_type="table">ResNet-18</text>
<text top="177" left="190" width="43" height="8" font="font10" id="p14_t29" reading_order_no="28" segment_no="2" tag_type="table">64.04 ± 0.44</text>
<text top="177" left="245" width="43" height="8" font="font10" id="p14_t30" reading_order_no="29" segment_no="2" tag_type="table">79.71 ± 0.32</text>
<text top="177" left="299" width="44" height="8" font="font10" id="p14_t31" reading_order_no="30" segment_no="2" tag_type="table">63.73 ± 0.46</text>
<text top="177" left="354" width="44" height="8" font="font10" id="p14_t32" reading_order_no="31" segment_no="2" tag_type="table">80.01 ± 0.33</text>
<text top="177" left="409" width="44" height="8" font="font10" id="p14_t33" reading_order_no="32" segment_no="2" tag_type="table">63.25 ± 0.47</text>
<text top="177" left="464" width="44" height="8" font="font10" id="p14_t34" reading_order_no="33" segment_no="2" tag_type="table">79.88 ± 0.33</text>
<text top="187" left="95" width="26" height="8" font="font10" id="p14_t35" reading_order_no="34" segment_no="2" tag_type="table">+ESFR</text>
<text top="187" left="139" width="38" height="8" font="font10" id="p14_t36" reading_order_no="35" segment_no="2" tag_type="table">ResNet-18</text>
<text top="187" left="190" width="43" height="8" font="font10" id="p14_t37" reading_order_no="36" segment_no="2" tag_type="table">70.94 ± 0.50</text>
<text top="187" left="245" width="43" height="8" font="font10" id="p14_t38" reading_order_no="37" segment_no="2" tag_type="table">81.61 ± 0.33</text>
<text top="187" left="299" width="44" height="8" font="font10" id="p14_t39" reading_order_no="38" segment_no="2" tag_type="table">70.32 ± 0.52</text>
<text top="187" left="354" width="44" height="8" font="font10" id="p14_t40" reading_order_no="39" segment_no="2" tag_type="table">81.35 ± 0.33</text>
<text top="187" left="409" width="44" height="8" font="font10" id="p14_t41" reading_order_no="40" segment_no="2" tag_type="table">69.74 ± 0.53</text>
<text top="187" left="464" width="44" height="8" font="font10" id="p14_t42" reading_order_no="41" segment_no="2" tag_type="table">81.12 ± 0.34</text>
<text top="198" left="89" width="38" height="8" font="font10" id="p14_t43" reading_order_no="42" segment_no="2" tag_type="table">BD-CSPN</text>
<text top="198" left="139" width="38" height="8" font="font10" id="p14_t44" reading_order_no="43" segment_no="2" tag_type="table">ResNet-18</text>
<text top="197" left="190" width="43" height="9" font="font10" id="p14_t45" reading_order_no="44" segment_no="2" tag_type="table">70.00 ± 0.51</text>
<text top="197" left="245" width="43" height="9" font="font10" id="p14_t46" reading_order_no="45" segment_no="2" tag_type="table">82.36 ± 0.32</text>
<text top="197" left="299" width="44" height="9" font="font10" id="p14_t47" reading_order_no="46" segment_no="2" tag_type="table">68.99 ± 0.51</text>
<text top="197" left="354" width="44" height="9" font="font10" id="p14_t48" reading_order_no="47" segment_no="2" tag_type="table">81.49 ± 0.34</text>
<text top="197" left="409" width="44" height="9" font="font10" id="p14_t49" reading_order_no="48" segment_no="2" tag_type="table">68.26 ± 0.52</text>
<text top="197" left="464" width="44" height="9" font="font10" id="p14_t50" reading_order_no="49" segment_no="2" tag_type="table">81.12 ± 0.34</text>
<text top="208" left="95" width="26" height="8" font="font10" id="p14_t51" reading_order_no="50" segment_no="2" tag_type="table">+ESFR</text>
<text top="208" left="139" width="38" height="8" font="font10" id="p14_t52" reading_order_no="51" segment_no="2" tag_type="table">ResNet-18</text>
<text top="207" left="190" width="43" height="9" font="font10" id="p14_t53" reading_order_no="52" segment_no="2" tag_type="table">73.98 ± 0.55</text>
<text top="207" left="245" width="43" height="9" font="font10" id="p14_t54" reading_order_no="53" segment_no="2" tag_type="table">82.32 ± 0.33</text>
<text top="207" left="299" width="44" height="9" font="font10" id="p14_t55" reading_order_no="54" segment_no="2" tag_type="table">72.39 ± 0.56</text>
<text top="207" left="354" width="44" height="9" font="font10" id="p14_t56" reading_order_no="55" segment_no="2" tag_type="table">81.51 ± 0.34</text>
<text top="207" left="409" width="44" height="9" font="font10" id="p14_t57" reading_order_no="56" segment_no="2" tag_type="table">71.74 ± 0.57</text>
<text top="207" left="464" width="44" height="9" font="font10" id="p14_t58" reading_order_no="57" segment_no="2" tag_type="table">81.17 ± 0.35</text>
<text top="218" left="100" width="16" height="8" font="font10" id="p14_t59" reading_order_no="58" segment_no="2" tag_type="table">TIM</text>
<text top="218" left="148" width="21" height="8" font="font10" id="p14_t60" reading_order_no="59" segment_no="2" tag_type="table">WRN</text>
<text top="217" left="190" width="43" height="9" font="font10" id="p14_t61" reading_order_no="60" segment_no="2" tag_type="table">77.60 ± 0.31</text>
<text top="217" left="245" width="43" height="9" font="font10" id="p14_t62" reading_order_no="61" segment_no="2" tag_type="table">87.31 ± 0.17</text>
<text top="217" left="299" width="44" height="9" font="font10" id="p14_t63" reading_order_no="62" segment_no="2" tag_type="table">72.03 ± 0.28</text>
<text top="217" left="354" width="44" height="9" font="font10" id="p14_t64" reading_order_no="63" segment_no="2" tag_type="table">80.91 ± 0.16</text>
<text top="217" left="409" width="44" height="9" font="font10" id="p14_t65" reading_order_no="64" segment_no="2" tag_type="table">68.86 ± 0.26</text>
<text top="217" left="464" width="44" height="9" font="font10" id="p14_t66" reading_order_no="65" segment_no="2" tag_type="table">77.28 ± 0.15</text>
<text top="228" left="102" width="13" height="8" font="font10" id="p14_t67" reading_order_no="66" segment_no="2" tag_type="table">NN</text>
<text top="228" left="148" width="21" height="8" font="font10" id="p14_t68" reading_order_no="67" segment_no="2" tag_type="table">WRN</text>
<text top="228" left="190" width="43" height="8" font="font10" id="p14_t69" reading_order_no="68" segment_no="2" tag_type="table">66.73 ± 0.44</text>
<text top="228" left="245" width="43" height="8" font="font10" id="p14_t70" reading_order_no="69" segment_no="2" tag_type="table">81.85 ± 0.31</text>
<text top="228" left="299" width="44" height="8" font="font10" id="p14_t71" reading_order_no="70" segment_no="2" tag_type="table">66.64 ± 0.46</text>
<text top="228" left="354" width="44" height="8" font="font10" id="p14_t72" reading_order_no="71" segment_no="2" tag_type="table">82.07 ± 0.31</text>
<text top="228" left="409" width="44" height="8" font="font10" id="p14_t73" reading_order_no="72" segment_no="2" tag_type="table">66.30 ± 0.47</text>
<text top="228" left="464" width="44" height="8" font="font10" id="p14_t74" reading_order_no="73" segment_no="2" tag_type="table">81.98 ± 0.32</text>
<text top="238" left="95" width="26" height="8" font="font10" id="p14_t75" reading_order_no="74" segment_no="2" tag_type="table">+ESFR</text>
<text top="238" left="148" width="21" height="8" font="font10" id="p14_t76" reading_order_no="75" segment_no="2" tag_type="table">WRN</text>
<text top="238" left="190" width="43" height="8" font="font10" id="p14_t77" reading_order_no="76" segment_no="2" tag_type="table">74.01 ± 0.51</text>
<text top="238" left="245" width="43" height="8" font="font10" id="p14_t78" reading_order_no="77" segment_no="2" tag_type="table">83.58 ± 0.31</text>
<text top="238" left="299" width="44" height="8" font="font10" id="p14_t79" reading_order_no="78" segment_no="2" tag_type="table">73.34 ± 0.51</text>
<text top="238" left="354" width="44" height="8" font="font10" id="p14_t80" reading_order_no="79" segment_no="2" tag_type="table">83.27 ± 0.32</text>
<text top="238" left="409" width="44" height="8" font="font10" id="p14_t81" reading_order_no="80" segment_no="2" tag_type="table">72.89 ± 0.52</text>
<text top="238" left="464" width="44" height="8" font="font10" id="p14_t82" reading_order_no="81" segment_no="2" tag_type="table">83.03 ± 0.33</text>
<text top="249" left="89" width="38" height="8" font="font10" id="p14_t83" reading_order_no="82" segment_no="2" tag_type="table">BD-CSPN</text>
<text top="249" left="148" width="21" height="8" font="font10" id="p14_t84" reading_order_no="83" segment_no="2" tag_type="table">WRN</text>
<text top="248" left="190" width="43" height="9" font="font10" id="p14_t85" reading_order_no="84" segment_no="2" tag_type="table">72.74 ± 0.49</text>
<text top="248" left="245" width="43" height="9" font="font10" id="p14_t86" reading_order_no="85" segment_no="2" tag_type="table">84.14 ± 0.30</text>
<text top="248" left="299" width="44" height="9" font="font10" id="p14_t87" reading_order_no="86" segment_no="2" tag_type="table">71.67 ± 0.51</text>
<text top="248" left="354" width="44" height="9" font="font10" id="p14_t88" reading_order_no="87" segment_no="2" tag_type="table">83.34 ± 0.32</text>
<text top="248" left="409" width="44" height="9" font="font10" id="p14_t89" reading_order_no="88" segment_no="2" tag_type="table">71.19 ± 0.51</text>
<text top="248" left="464" width="44" height="9" font="font10" id="p14_t90" reading_order_no="89" segment_no="2" tag_type="table">83.02 ± 0.33</text>
<text top="259" left="95" width="26" height="8" font="font10" id="p14_t91" reading_order_no="90" segment_no="2" tag_type="table">+ESFR</text>
<text top="259" left="148" width="21" height="8" font="font10" id="p14_t92" reading_order_no="91" segment_no="2" tag_type="table">WRN</text>
<text top="258" left="190" width="43" height="9" font="font10" id="p14_t93" reading_order_no="92" segment_no="2" tag_type="table">76.84 ± 0.54</text>
<text top="258" left="245" width="43" height="9" font="font10" id="p14_t94" reading_order_no="93" segment_no="2" tag_type="table">84.36 ± 0.32</text>
<text top="258" left="299" width="44" height="9" font="font10" id="p14_t95" reading_order_no="94" segment_no="2" tag_type="table">75.26 ± 0.55</text>
<text top="258" left="354" width="44" height="9" font="font10" id="p14_t96" reading_order_no="95" segment_no="2" tag_type="table">83.48 ± 0.33</text>
<text top="258" left="409" width="44" height="9" font="font10" id="p14_t97" reading_order_no="96" segment_no="2" tag_type="table">74.66 ± 0.55</text>
<text top="258" left="464" width="44" height="9" font="font10" id="p14_t98" reading_order_no="97" segment_no="2" tag_type="table">83.09 ± 0.34</text>
<text top="269" left="268" width="61" height="8" font="font42" id="p14_t99" reading_order_no="98" segment_no="2" tag_type="table">tiered -ImageNet</text>
<text top="284" left="93" width="30" height="8" font="font16" id="p14_t100" reading_order_no="99" segment_no="2" tag_type="table">Method</text>
<text top="284" left="139" width="38" height="8" font="font16" id="p14_t101" reading_order_no="100" segment_no="2" tag_type="table">Backbone</text>
<text top="279" left="204" width="69" height="8" font="font16" id="p14_t102" reading_order_no="101" segment_no="2" tag_type="table">(15, 15, 15, 15, 15)</text>
<text top="279" left="314" width="69" height="8" font="font16" id="p14_t103" reading_order_no="102" segment_no="2" tag_type="table">(11, 13, 15, 17, 19)</text>
<text top="279" left="426" width="64" height="8" font="font16" id="p14_t104" reading_order_no="103" segment_no="2" tag_type="table">(7, 11, 15, 19, 23)</text>
<text top="289" left="199" width="23" height="8" font="font16" id="p14_t105" reading_order_no="104" segment_no="2" tag_type="table">1-shot</text>
<text top="289" left="254" width="23" height="8" font="font16" id="p14_t106" reading_order_no="105" segment_no="2" tag_type="table">5-shot</text>
<text top="289" left="309" width="23" height="8" font="font16" id="p14_t107" reading_order_no="106" segment_no="2" tag_type="table">1-shot</text>
<text top="289" left="364" width="23" height="8" font="font16" id="p14_t108" reading_order_no="107" segment_no="2" tag_type="table">5-shot</text>
<text top="289" left="419" width="23" height="8" font="font16" id="p14_t109" reading_order_no="108" segment_no="2" tag_type="table">1-shot</text>
<text top="289" left="474" width="23" height="8" font="font16" id="p14_t110" reading_order_no="109" segment_no="2" tag_type="table">5-shot</text>
<text top="300" left="92" width="32" height="8" font="font10" id="p14_t111" reading_order_no="110" segment_no="2" tag_type="table">TIM-GD</text>
<text top="300" left="139" width="38" height="8" font="font10" id="p14_t112" reading_order_no="111" segment_no="2" tag_type="table">ResNet-18</text>
<text top="299" left="190" width="43" height="9" font="font10" id="p14_t113" reading_order_no="112" segment_no="2" tag_type="table">79.99 ± 0.33</text>
<text top="299" left="245" width="43" height="9" font="font10" id="p14_t114" reading_order_no="113" segment_no="2" tag_type="table">88.62 ± 0.20</text>
<text top="299" left="299" width="44" height="9" font="font10" id="p14_t115" reading_order_no="114" segment_no="2" tag_type="table">74.21 ± 0.29</text>
<text top="299" left="354" width="44" height="9" font="font10" id="p14_t116" reading_order_no="115" segment_no="2" tag_type="table">81.93 ± 0.18</text>
<text top="299" left="409" width="44" height="9" font="font10" id="p14_t117" reading_order_no="116" segment_no="2" tag_type="table">70.95 ± 0.28</text>
<text top="299" left="464" width="44" height="9" font="font10" id="p14_t118" reading_order_no="117" segment_no="2" tag_type="table">78.36 ± 0.17</text>
<text top="310" left="102" width="13" height="8" font="font10" id="p14_t119" reading_order_no="118" segment_no="2" tag_type="table">NN</text>
<text top="310" left="139" width="38" height="8" font="font10" id="p14_t120" reading_order_no="119" segment_no="2" tag_type="table">ResNet-18</text>
<text top="309" left="190" width="43" height="9" font="font10" id="p14_t121" reading_order_no="120" segment_no="2" tag_type="table">71.60 ± 0.49</text>
<text top="309" left="245" width="43" height="9" font="font10" id="p14_t122" reading_order_no="121" segment_no="2" tag_type="table">84.62 ± 0.36</text>
<text top="309" left="299" width="44" height="9" font="font10" id="p14_t123" reading_order_no="122" segment_no="2" tag_type="table">71.10 ± 0.49</text>
<text top="309" left="354" width="44" height="9" font="font10" id="p14_t124" reading_order_no="123" segment_no="2" tag_type="table">84.59 ± 0.35</text>
<text top="309" left="409" width="44" height="9" font="font10" id="p14_t125" reading_order_no="124" segment_no="2" tag_type="table">70.51 ± 0.49</text>
<text top="309" left="464" width="44" height="9" font="font10" id="p14_t126" reading_order_no="125" segment_no="2" tag_type="table">84.52 ± 0.35</text>
<text top="320" left="95" width="26" height="8" font="font10" id="p14_t127" reading_order_no="126" segment_no="2" tag_type="table">+ESFR</text>
<text top="320" left="139" width="38" height="8" font="font10" id="p14_t128" reading_order_no="127" segment_no="2" tag_type="table">ResNet-18</text>
<text top="319" left="190" width="43" height="9" font="font10" id="p14_t129" reading_order_no="128" segment_no="2" tag_type="table">77.44 ± 0.52</text>
<text top="319" left="245" width="43" height="9" font="font10" id="p14_t130" reading_order_no="129" segment_no="2" tag_type="table">85.84 ± 0.35</text>
<text top="319" left="299" width="44" height="9" font="font10" id="p14_t131" reading_order_no="130" segment_no="2" tag_type="table">76.77 ± 0.53</text>
<text top="319" left="354" width="44" height="9" font="font10" id="p14_t132" reading_order_no="131" segment_no="2" tag_type="table">85.64 ± 0.35</text>
<text top="319" left="409" width="44" height="9" font="font10" id="p14_t133" reading_order_no="132" segment_no="2" tag_type="table">76.21 ± 0.54</text>
<text top="319" left="464" width="44" height="9" font="font10" id="p14_t134" reading_order_no="133" segment_no="2" tag_type="table">85.42 ± 0.36</text>
<text top="330" left="89" width="38" height="8" font="font10" id="p14_t135" reading_order_no="134" segment_no="2" tag_type="table">BD-CSPN</text>
<text top="330" left="139" width="38" height="8" font="font10" id="p14_t136" reading_order_no="135" segment_no="2" tag_type="table">ResNet-18</text>
<text top="330" left="190" width="43" height="8" font="font10" id="p14_t137" reading_order_no="136" segment_no="2" tag_type="table">77.28 ± 0.52</text>
<text top="330" left="245" width="43" height="8" font="font10" id="p14_t138" reading_order_no="137" segment_no="2" tag_type="table">86.55 ± 0.34</text>
<text top="330" left="299" width="44" height="8" font="font10" id="p14_t139" reading_order_no="138" segment_no="2" tag_type="table">76.38 ± 0.52</text>
<text top="330" left="354" width="44" height="8" font="font10" id="p14_t140" reading_order_no="139" segment_no="2" tag_type="table">85.89 ± 0.35</text>
<text top="330" left="409" width="44" height="8" font="font10" id="p14_t141" reading_order_no="140" segment_no="2" tag_type="table">75.63 ± 0.53</text>
<text top="330" left="464" width="44" height="8" font="font10" id="p14_t142" reading_order_no="141" segment_no="2" tag_type="table">85.65 ± 0.36</text>
<text top="340" left="95" width="26" height="8" font="font10" id="p14_t143" reading_order_no="142" segment_no="2" tag_type="table">+ESFR</text>
<text top="340" left="139" width="38" height="8" font="font10" id="p14_t144" reading_order_no="143" segment_no="2" tag_type="table">ResNet-18</text>
<text top="340" left="190" width="43" height="8" font="font10" id="p14_t145" reading_order_no="144" segment_no="2" tag_type="table">80.13 ± 0.56</text>
<text top="340" left="245" width="43" height="8" font="font10" id="p14_t146" reading_order_no="145" segment_no="2" tag_type="table">86.34 ± 0.36</text>
<text top="340" left="299" width="44" height="8" font="font10" id="p14_t147" reading_order_no="146" segment_no="2" tag_type="table">78.72 ± 0.57</text>
<text top="340" left="354" width="44" height="8" font="font10" id="p14_t148" reading_order_no="147" segment_no="2" tag_type="table">85.76 ± 0.36</text>
<text top="340" left="409" width="44" height="8" font="font10" id="p14_t149" reading_order_no="148" segment_no="2" tag_type="table">78.12 ± 0.57</text>
<text top="340" left="464" width="44" height="8" font="font10" id="p14_t150" reading_order_no="149" segment_no="2" tag_type="table">85.50 ± 0.37</text>
<text top="351" left="100" width="16" height="8" font="font10" id="p14_t151" reading_order_no="150" segment_no="2" tag_type="table">TIM</text>
<text top="351" left="148" width="21" height="8" font="font10" id="p14_t152" reading_order_no="151" segment_no="2" tag_type="table">WRN</text>
<text top="350" left="190" width="43" height="9" font="font10" id="p14_t153" reading_order_no="152" segment_no="2" tag_type="table">82.18 ± 0.32</text>
<text top="350" left="245" width="43" height="9" font="font10" id="p14_t154" reading_order_no="153" segment_no="2" tag_type="table">89.87 ± 0.19</text>
<text top="350" left="299" width="44" height="9" font="font10" id="p14_t155" reading_order_no="154" segment_no="2" tag_type="table">76.11 ± 0.28</text>
<text top="350" left="354" width="44" height="9" font="font10" id="p14_t156" reading_order_no="155" segment_no="2" tag_type="table">83.18 ± 0.17</text>
<text top="350" left="409" width="44" height="9" font="font10" id="p14_t157" reading_order_no="156" segment_no="2" tag_type="table">72.72 ± 0.27</text>
<text top="350" left="464" width="44" height="9" font="font10" id="p14_t158" reading_order_no="157" segment_no="2" tag_type="table">79.55 ± 0.16</text>
<text top="361" left="102" width="13" height="8" font="font10" id="p14_t159" reading_order_no="158" segment_no="2" tag_type="table">NN</text>
<text top="361" left="148" width="21" height="8" font="font10" id="p14_t160" reading_order_no="159" segment_no="2" tag_type="table">WRN</text>
<text top="360" left="190" width="43" height="9" font="font10" id="p14_t161" reading_order_no="160" segment_no="2" tag_type="table">72.97 ± 0.49</text>
<text top="360" left="245" width="43" height="9" font="font10" id="p14_t162" reading_order_no="161" segment_no="2" tag_type="table">85.74 ± 0.34</text>
<text top="360" left="299" width="44" height="9" font="font10" id="p14_t163" reading_order_no="162" segment_no="2" tag_type="table">72.17 ± 0.48</text>
<text top="360" left="354" width="44" height="9" font="font10" id="p14_t164" reading_order_no="163" segment_no="2" tag_type="table">85.79 ± 0.34</text>
<text top="360" left="409" width="44" height="9" font="font10" id="p14_t165" reading_order_no="164" segment_no="2" tag_type="table">71.57 ± 0.49</text>
<text top="360" left="464" width="44" height="9" font="font10" id="p14_t166" reading_order_no="165" segment_no="2" tag_type="table">85.70 ± 0.34</text>
<text top="371" left="95" width="26" height="8" font="font10" id="p14_t167" reading_order_no="166" segment_no="2" tag_type="table">+ESFR</text>
<text top="371" left="148" width="21" height="8" font="font10" id="p14_t168" reading_order_no="167" segment_no="2" tag_type="table">WRN</text>
<text top="370" left="190" width="43" height="9" font="font10" id="p14_t169" reading_order_no="168" segment_no="2" tag_type="table">79.13 ± 0.52</text>
<text top="370" left="245" width="43" height="9" font="font10" id="p14_t170" reading_order_no="169" segment_no="2" tag_type="table">87.08 ± 0.34</text>
<text top="370" left="299" width="44" height="9" font="font10" id="p14_t171" reading_order_no="170" segment_no="2" tag_type="table">78.30 ± 0.53</text>
<text top="370" left="354" width="44" height="9" font="font10" id="p14_t172" reading_order_no="171" segment_no="2" tag_type="table">86.90 ± 0.34</text>
<text top="370" left="409" width="44" height="9" font="font10" id="p14_t173" reading_order_no="172" segment_no="2" tag_type="table">77.67 ± 0.53</text>
<text top="370" left="464" width="44" height="9" font="font10" id="p14_t174" reading_order_no="173" segment_no="2" tag_type="table">86.69 ± 0.34</text>
<text top="381" left="89" width="38" height="8" font="font10" id="p14_t175" reading_order_no="174" segment_no="2" tag_type="table">BD-CSPN</text>
<text top="381" left="148" width="21" height="8" font="font10" id="p14_t176" reading_order_no="175" segment_no="2" tag_type="table">WRN</text>
<text top="381" left="190" width="43" height="8" font="font10" id="p14_t177" reading_order_no="176" segment_no="2" tag_type="table">78.89 ± 0.52</text>
<text top="381" left="245" width="43" height="8" font="font10" id="p14_t178" reading_order_no="177" segment_no="2" tag_type="table">87.72 ± 0.32</text>
<text top="381" left="299" width="44" height="8" font="font10" id="p14_t179" reading_order_no="178" segment_no="2" tag_type="table">77.71 ± 0.52</text>
<text top="381" left="354" width="44" height="8" font="font10" id="p14_t180" reading_order_no="179" segment_no="2" tag_type="table">87.11 ± 0.34</text>
<text top="381" left="409" width="44" height="8" font="font10" id="p14_t181" reading_order_no="180" segment_no="2" tag_type="table">77.05 ± 0.53</text>
<text top="381" left="464" width="44" height="8" font="font10" id="p14_t182" reading_order_no="181" segment_no="2" tag_type="table">86.86 ± 0.35</text>
<text top="391" left="95" width="26" height="8" font="font10" id="p14_t183" reading_order_no="182" segment_no="2" tag_type="table">+ESFR</text>
<text top="391" left="148" width="21" height="8" font="font10" id="p14_t184" reading_order_no="183" segment_no="2" tag_type="table">WRN</text>
<text top="391" left="190" width="43" height="8" font="font10" id="p14_t185" reading_order_no="184" segment_no="2" tag_type="table">81.77 ± 0.55</text>
<text top="391" left="245" width="43" height="8" font="font10" id="p14_t186" reading_order_no="185" segment_no="2" tag_type="table">87.61 ± 0.34</text>
<text top="391" left="299" width="44" height="8" font="font10" id="p14_t187" reading_order_no="186" segment_no="2" tag_type="table">80.50 ± 0.55</text>
<text top="391" left="354" width="44" height="8" font="font10" id="p14_t188" reading_order_no="187" segment_no="2" tag_type="table">87.04 ± 0.35</text>
<text top="391" left="409" width="44" height="8" font="font10" id="p14_t189" reading_order_no="188" segment_no="2" tag_type="table">79.67 ± 0.56</text>
<text top="391" left="464" width="44" height="8" font="font10" id="p14_t190" reading_order_no="189" segment_no="2" tag_type="table">86.72 ± 0.35</text>
<text top="430" left="55" width="486" height="9" font="font4" id="p14_t191" reading_order_no="190" segment_no="3" tag_type="text">query statistics, suffers from the change in query setting. The performance of TIM on the 5-shot when the number of query</text>
<text top="441" left="55" width="459" height="10" font="font4" id="p14_t192" reading_order_no="191" segment_no="3" tag_type="text">samples per class is (7, 11, 15, 19, 23) shows − 6.2% ∼− 4.2% performance decrease compare to the baseline (NN).</text>
<text top="469" left="55" width="255" height="11" font="font3" id="p14_t193" reading_order_no="192" segment_no="4" tag_type="title">E. Naively applied unsupervised learning methods</text>
<text top="505" left="247" width="102" height="8" font="font34" id="p14_t194" reading_order_no="193" segment_no="5" tag_type="title">Table 8. Experiment settings</text>
<text top="514" left="327" width="40" height="8" font="font10" id="p14_t195" reading_order_no="194" segment_no="6" tag_type="table">Candidates</text>
<text top="525" left="187" width="48" height="8" font="font10" id="p14_t196" reading_order_no="195" segment_no="6" tag_type="table">Learning rate</text>
<text top="525" left="329" width="36" height="8" font="font10" id="p14_t197" reading_order_no="196" segment_no="6" tag_type="table">1e-3, 1e-4</text>
<text top="535" left="194" width="34" height="8" font="font10" id="p14_t198" reading_order_no="197" segment_no="6" tag_type="table">Classifier</text>
<text top="535" left="320" width="54" height="8" font="font10" id="p14_t199" reading_order_no="198" segment_no="6" tag_type="table">Linear, Cosine</text>
<text top="546" left="177" width="67" height="8" font="font10" id="p14_t200" reading_order_no="199" segment_no="6" tag_type="table">Additional module</text>
<text top="545" left="311" width="72" height="9" font="font16" id="p14_t201" reading_order_no="200" segment_no="6" tag_type="table">2-layer FCN , None</text>
<text top="556" left="184" width="54" height="8" font="font10" id="p14_t202" reading_order_no="201" segment_no="6" tag_type="table">update weights</text>
<text top="566" left="169" width="85" height="8" font="font10" id="p14_t203" reading_order_no="202" segment_no="6" tag_type="table">of embedding networks</text>
<text top="556" left="328" width="38" height="8" font="font10" id="p14_t204" reading_order_no="203" segment_no="6" tag_type="table">None, All,</text>
<text top="566" left="294" width="105" height="8" font="font16" id="p14_t205" reading_order_no="204" segment_no="6" tag_type="table">only-the-last-residual-block</text>
<text top="576" left="180" width="62" height="8" font="font10" id="p14_t206" reading_order_no="205" segment_no="6" tag_type="table">New embeddings</text>
<text top="576" left="266" width="162" height="8" font="font16" id="p14_t207" reading_order_no="206" segment_no="6" tag_type="table">Backbone output , Additional module output</text>
<text top="607" left="55" width="486" height="9" font="font4" id="p14_t208" reading_order_no="207" segment_no="7" tag_type="text">We experiment if naively applied unsupervised (or self-supervised) learning can improve few-shot classification in the</text>
<text top="619" left="55" width="487" height="9" font="font4" id="p14_t209" reading_order_no="208" segment_no="7" tag_type="text">standard settings. For a fair comparison, we use the pre-trained embeddings of ResNet-18 on mini -ImageNet. We test with</text>
<text top="631" left="55" width="487" height="9" font="font4" id="p14_t210" reading_order_no="209" segment_no="7" tag_type="text">pretext task-based self-supervised methods of rotation ( Gidaris et al. , 2018 ) and jigsaw ( Noroozi &amp; Favaro , 2016 ). For both</text>
<text top="643" left="55" width="487" height="9" font="font4" id="p14_t211" reading_order_no="210" segment_no="7" tag_type="text">methods, we use grid search to find the best performing settings; shown in Table 8 . An additional module is inserted between</text>
<text top="654" left="55" width="486" height="10" font="font4" id="p14_t212" reading_order_no="211" segment_no="7" tag_type="text">the embedding network and classifier and we use hidden dimensions from ? . For jigsaw tasks, we use 35-permutations from</text>
<text top="666" left="55" width="345" height="9" font="font1" id="p14_t213" reading_order_no="212" segment_no="7" tag_type="text">? . For both methods, the same setting with the bold font on Table 8 performs the best.</text>
<text top="684" left="55" width="486" height="9" font="font4" id="p14_t214" reading_order_no="213" segment_no="8" tag_type="text">Table- 9 shows the results with (1) new embeddings are provided when training becomes converged and (2) new embeddings</text>
<text top="696" left="55" width="487" height="9" font="font4" id="p14_t215" reading_order_no="214" segment_no="8" tag_type="text">are given via oracle early stopping at the best performing training iteration (for ≥ 1 ). The result with converged embeddings</text>
<text top="708" left="55" width="486" height="9" font="font4" id="p14_t216" reading_order_no="215" segment_no="8" tag_type="text">shows that the naively applied self-supervised learning fails to improve few-shot classification performance. Note that our</text>
</page>
<page number="15" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="149" width="298" height="8" font="font16" id="p15_t1" reading_order_no="0" segment_no="0" tag_type="title">Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction</text>
<text top="77" left="201" width="195" height="8" font="font34" id="p15_t2" reading_order_no="1" segment_no="1" tag_type="title">Table 9. Naively applied unsupervised learning results</text>
<text top="87" left="197" width="30" height="8" font="font16" id="p15_t3" reading_order_no="2" segment_no="2" tag_type="table">Method</text>
<text top="87" left="261" width="120" height="8" font="font42" id="p15_t4" reading_order_no="3" segment_no="2" tag_type="table">mini -ImageNet 1-shot accuracy</text>
<text top="97" left="205" width="13" height="8" font="font16" id="p15_t5" reading_order_no="4" segment_no="2" tag_type="table">NN</text>
<text top="97" left="311" width="20" height="8" font="font10" id="p15_t6" reading_order_no="5" segment_no="2" tag_type="table">64.04</text>
<text top="107" left="198" width="28" height="8" font="font16" id="p15_t7" reading_order_no="6" segment_no="2" tag_type="table">+ESFR</text>
<text top="107" left="302" width="38" height="8" font="font10" id="p15_t8" reading_order_no="7" segment_no="2" tag_type="table">70.94 +6.90</text>
<text top="118" left="240" width="54" height="8" font="font16" id="p15_t9" reading_order_no="8" segment_no="2" tag_type="table">(1) Converged</text>
<text top="118" left="306" width="95" height="8" font="font16" id="p15_t10" reading_order_no="9" segment_no="2" tag_type="table">(2) Oracle early stopping</text>
<text top="128" left="199" width="26" height="8" font="font16" id="p15_t11" reading_order_no="10" segment_no="2" tag_type="table">Jigsaw</text>
<text top="128" left="260" width="16" height="8" font="font10" id="p15_t12" reading_order_no="11" segment_no="2" tag_type="table">33.1</text>
<text top="128" left="335" width="38" height="8" font="font10" id="p15_t13" reading_order_no="12" segment_no="2" tag_type="table">67.22 +3.18</text>
<text top="138" left="195" width="33" height="8" font="font16" id="p15_t14" reading_order_no="13" segment_no="2" tag_type="table">Rotation</text>
<text top="138" left="260" width="16" height="8" font="font10" id="p15_t15" reading_order_no="14" segment_no="2" tag_type="table">32.2</text>
<text top="138" left="335" width="38" height="8" font="font10" id="p15_t16" reading_order_no="15" segment_no="2" tag_type="table">66.70 +2.66</text>
<text top="174" left="55" width="486" height="11" font="font4" id="p15_t17" reading_order_no="16" segment_no="3" tag_type="text">method achieves 70 . 94 on the same setting. Our method outperforms both the rotation- and jigsaw-based 15 unsupervised</text>
<text top="188" left="55" width="228" height="9" font="font4" id="p15_t18" reading_order_no="17" segment_no="3" tag_type="text">learning methods that even contain oracle early-stopping.</text>
<text top="215" left="55" width="138" height="11" font="font3" id="p15_t19" reading_order_no="18" segment_no="4" tag_type="title">F. Experiments with Conv4</text>
<text top="249" left="55" width="474" height="8" font="font34" id="p15_t20" reading_order_no="19" segment_no="5" tag_type="title">Table 10. The table shows the experimental results of our method with Conv4-64 backbone on mini -ImageNet and tiered -ImageNet.</text>
<text top="264" left="244" width="58" height="8" font="font16" id="p15_t21" reading_order_no="20" segment_no="6" tag_type="table">mini-ImageNet</text>
<text top="264" left="343" width="63" height="8" font="font16" id="p15_t22" reading_order_no="21" segment_no="6" tag_type="table">tiered-ImageNet</text>
<text top="274" left="182" width="30" height="8" font="font16" id="p15_t23" reading_order_no="22" segment_no="6" tag_type="table">Method</text>
<text top="274" left="236" width="23" height="8" font="font16" id="p15_t24" reading_order_no="23" segment_no="6" tag_type="table">1-shot</text>
<text top="274" left="286" width="23" height="8" font="font16" id="p15_t25" reading_order_no="24" segment_no="6" tag_type="table">5-shot</text>
<text top="274" left="337" width="23" height="8" font="font16" id="p15_t26" reading_order_no="25" segment_no="6" tag_type="table">1-shot<a href="deeplearning_paper6.html#15">w-based</a></text>
<text top="274" left="387" width="23" height="8" font="font16" id="p15_t27" reading_order_no="26" segment_no="6" tag_type="table">5-shot<a href="deeplearning_paper6.html#15">15</a></text>
<text top="284" left="191" width="13" height="8" font="font10" id="p15_t28" reading_order_no="27" segment_no="6" tag_type="table">NN</text>
<text top="284" left="237" width="20" height="8" font="font10" id="p15_t29" reading_order_no="28" segment_no="6" tag_type="table">50.72</text>
<text top="284" left="288" width="20" height="8" font="font10" id="p15_t30" reading_order_no="29" segment_no="6" tag_type="table">67.17</text>
<text top="284" left="339" width="20" height="8" font="font10" id="p15_t31" reading_order_no="30" segment_no="6" tag_type="table">52.18</text>
<text top="284" left="389" width="20" height="8" font="font10" id="p15_t32" reading_order_no="31" segment_no="6" tag_type="table">69.60</text>
<text top="294" left="182" width="30" height="8" font="font16" id="p15_t33" reading_order_no="32" segment_no="6" tag_type="table">+ ESFR</text>
<text top="294" left="228" width="39" height="8" font="font16" id="p15_t34" reading_order_no="33" segment_no="6" tag_type="table">54.63 +3.91</text>
<text top="294" left="279" width="38" height="8" font="font16" id="p15_t35" reading_order_no="34" segment_no="6" tag_type="table">68.32 +1.15</text>
<text top="294" left="329" width="39" height="8" font="font16" id="p15_t36" reading_order_no="35" segment_no="6" tag_type="table">57.56 +5.38</text>
<text top="294" left="380" width="38" height="8" font="font16" id="p15_t37" reading_order_no="36" segment_no="6" tag_type="table">71.46 +1.86</text>
<text top="305" left="178" width="38" height="8" font="font10" id="p15_t38" reading_order_no="37" segment_no="6" tag_type="table">BD-CSPN</text>
<text top="305" left="237" width="20" height="8" font="font10" id="p15_t39" reading_order_no="38" segment_no="6" tag_type="table">52.73</text>
<text top="305" left="290" width="16" height="8" font="font10" id="p15_t40" reading_order_no="39" segment_no="6" tag_type="table">68.5</text>
<text top="305" left="339" width="20" height="8" font="font10" id="p15_t41" reading_order_no="40" segment_no="6" tag_type="table">54.94</text>
<text top="305" left="389" width="20" height="8" font="font10" id="p15_t42" reading_order_no="41" segment_no="6" tag_type="table">71.53</text>
<text top="314" left="182" width="30" height="8" font="font16" id="p15_t43" reading_order_no="42" segment_no="6" tag_type="table">+ ESFR</text>
<text top="314" left="228" width="39" height="8" font="font16" id="p15_t44" reading_order_no="43" segment_no="6" tag_type="table">56.24 +3.51</text>
<text top="314" left="279" width="38" height="8" font="font16" id="p15_t45" reading_order_no="44" segment_no="6" tag_type="table">69.18 +0.68</text>
<text top="314" left="329" width="39" height="8" font="font16" id="p15_t46" reading_order_no="45" segment_no="6" tag_type="table">60.16 +5.22</text>
<text top="314" left="380" width="38" height="8" font="font16" id="p15_t47" reading_order_no="46" segment_no="6" tag_type="table">72.37 +0.84</text>
<text top="344" left="55" width="487" height="9" font="font4" id="p15_t48" reading_order_no="47" segment_no="7" tag_type="text">We use pre-trained Conv4-64 backbones following the settings of Wang et al. ( 2019 ). We applied the same preprocessing</text>
<text top="356" left="55" width="486" height="9" font="font4" id="p15_t49" reading_order_no="48" segment_no="7" tag_type="text">strategy as in the main text. For the reconstruction module, we find that the bottleneck structure (Section 3.1) is helpful for</text>
<text top="368" left="55" width="486" height="9" font="font4" id="p15_t50" reading_order_no="49" segment_no="7" tag_type="text">Conv4-64; while reconstructed embeddings still outperform the encoded ones. Thus, we use 800-400-800-1600 as hidden</text>
<text top="380" left="55" width="48" height="9" font="font4" id="p15_t51" reading_order_no="50" segment_no="7" tag_type="text">dimensions.</text>
<text top="398" left="55" width="487" height="9" font="font4" id="p15_t52" reading_order_no="51" segment_no="8" tag_type="text">Table- F shows experimental results with Conv4-64. As in the experimental results with ResNet and WideResNet, our method</text>
<text top="410" left="55" width="486" height="9" font="font4" id="p15_t53" reading_order_no="52" segment_no="8" tag_type="text">consistently improves the performance of the baseline methods: NN and BD-CSPN. ESFR also offers a complementary</text>
<text top="421" left="55" width="488" height="9" font="font4" id="p15_t54" reading_order_no="53" segment_no="8" tag_type="text">improvement to BD-CSPN ( Liu et al. , 2020 ) in 1-shot settings. Compare to prior state-of-the-art methods with Conv4-64,</text>
<text top="432" left="55" width="249" height="10" font="font4" id="p15_t55" reading_order_no="54" segment_no="8" tag_type="text">our method with BD-CSPN has slightly lower performance. 16</text>
<text top="696" left="65" width="477" height="10" font="font10" id="p15_t56" reading_order_no="55" segment_no="9" tag_type="footnote">15 We improved the performance with the jigsaw task after the rebuttal period. Originally performance with rotation task performs better.</text>
<text top="707" left="65" width="401" height="10" font="font25" id="p15_t57" reading_order_no="56" segment_no="10" tag_type="footnote">16 Hu et al. ( 2020 ) shows 58 . 0% and 70 . 7% accuracies on mini -ImageNet in 1- and 5-shot settings, respectively.</text>
</page>
</pdf2xml>
