<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="14" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font2" size="12" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="8" family="CMSY8" color="#000000"/>
	<fontspec id="font4" size="8" family="NimbusMonL-Regu" color="#000000"/>
	<fontspec id="font5" size="12" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font6" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font7" size="10" family="NimbusMonL-ReguObli" color="#ec008b"/>
	<fontspec id="font8" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font9" size="6" family="Arial,Bold" color="#000000"/>
	<fontspec id="font10" size="5" family="ArialMT" color="#000000"/>
	<fontspec id="font11" size="5" family="Arial,Bold" color="#000000"/>
	<fontspec id="font12" size="10" family="NimbusRomNo9L-Regu" color="#00ff00"/>
	<fontspec id="font13" size="20" family="Times" color="#7f7f7f"/>
<text top="39" left="60" width="475" height="8" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">Accepted to CVPR 2021 Workshop on 3D Vision and Robotics (3DVR). https://sites.google.com/view/cvpr2021-3d-vision-robotics/</text>
<text top="83" left="74" width="448" height="13" font="font1" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="title">SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for</text>
<text top="101" left="209" width="177" height="13" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="title">Day-Night Place Recognition</text>
<text top="141" left="214" width="60" height="11" font="font2" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="text">Sourav Garg</text>
<text top="141" left="302" width="79" height="11" font="font2" id="p1_t5" reading_order_no="5" segment_no="2" tag_type="text">Michael Milford</text>
<text top="155" left="143" width="309" height="11" font="font2" id="p1_t6" reading_order_no="6" segment_no="2" tag_type="text">QUT Centre for Robotics, Queensland University of Technology</text>
<text top="171" left="211" width="4" height="8" font="font3" id="p1_t7" reading_order_no="7" segment_no="3" tag_type="text">{</text>
<text top="172" left="215" width="110" height="6" font="font4" id="p1_t8" reading_order_no="8" segment_no="3" tag_type="text">s.garg, michael.milford</text>
<text top="171" left="325" width="4" height="8" font="font3" id="p1_t9" reading_order_no="9" segment_no="3" tag_type="text">}</text>
<text top="172" left="329" width="53" height="6" font="font4" id="p1_t10" reading_order_no="10" segment_no="3" tag_type="text">@qut.edu.au</text>
<text top="210" left="146" width="44" height="11" font="font5" id="p1_t11" reading_order_no="11" segment_no="4" tag_type="title">Abstract</text>
<text top="237" left="62" width="224" height="9" font="font6" id="p1_t12" reading_order_no="12" segment_no="5" tag_type="text">Place Recognition is a crucial capability for mobile</text>
<text top="249" left="50" width="236" height="9" font="font6" id="p1_t13" reading_order_no="13" segment_no="5" tag_type="text">robot localization and navigation. Image-based or Visual</text>
<text top="261" left="50" width="236" height="9" font="font6" id="p1_t14" reading_order_no="14" segment_no="5" tag_type="text">Place Recognition (VPR) is a challenging problem as scene</text>
<text top="272" left="50" width="236" height="9" font="font6" id="p1_t15" reading_order_no="15" segment_no="5" tag_type="text">appearance and camera viewpoint can change significantly</text>
<text top="284" left="50" width="236" height="9" font="font6" id="p1_t16" reading_order_no="16" segment_no="5" tag_type="text">when places are revisited. Recent VPR methods based on</text>
<text top="296" left="50" width="236" height="9" font="font6" id="p1_t17" reading_order_no="17" segment_no="5" tag_type="text">“sequential representations” have shown promising results</text>
<text top="308" left="50" width="236" height="9" font="font6" id="p1_t18" reading_order_no="18" segment_no="5" tag_type="text">as compared to traditional sequence score aggregation or</text>
<text top="320" left="50" width="236" height="9" font="font6" id="p1_t19" reading_order_no="19" segment_no="5" tag_type="text">single image based techniques. In parallel to these endeav-</text>
<text top="332" left="50" width="236" height="9" font="font6" id="p1_t20" reading_order_no="20" segment_no="5" tag_type="text">ors, 3D point clouds based place recognition is also be-</text>
<text top="344" left="50" width="236" height="9" font="font6" id="p1_t21" reading_order_no="21" segment_no="5" tag_type="text">ing explored following the advances in deep learning based</text>
<text top="356" left="50" width="236" height="9" font="font6" id="p1_t22" reading_order_no="22" segment_no="5" tag_type="text">point cloud processing. However, a key question remains: is</text>
<text top="368" left="50" width="236" height="9" font="font6" id="p1_t23" reading_order_no="23" segment_no="5" tag_type="text">an explicit 3D structure based place representation always</text>
<text top="380" left="50" width="236" height="9" font="font6" id="p1_t24" reading_order_no="24" segment_no="5" tag_type="text">superior to an implicit “spatial” representation based on</text>
<text top="392" left="50" width="236" height="9" font="font6" id="p1_t25" reading_order_no="25" segment_no="5" tag_type="text">sequence of RGB images which can inherently learn scene</text>
<text top="404" left="50" width="236" height="9" font="font6" id="p1_t26" reading_order_no="26" segment_no="5" tag_type="text">structure. In this extended abstract, we attempt to com-</text>
<text top="416" left="50" width="236" height="9" font="font6" id="p1_t27" reading_order_no="27" segment_no="5" tag_type="text">pare these two types of methods by considering a similar</text>
<text top="428" left="50" width="236" height="9" font="font6" id="p1_t28" reading_order_no="28" segment_no="5" tag_type="text">“metric span” to represent places. We compare a 3D point</text>
<text top="440" left="50" width="236" height="9" font="font6" id="p1_t29" reading_order_no="29" segment_no="5" tag_type="text">cloud based method (PointNetVLAD) with image sequence</text>
<text top="452" left="50" width="236" height="9" font="font6" id="p1_t30" reading_order_no="30" segment_no="5" tag_type="text">based methods (SeqNet and others) and showcase that im-</text>
<text top="464" left="50" width="236" height="9" font="font6" id="p1_t31" reading_order_no="31" segment_no="5" tag_type="text">age sequence based techniques approach, and can even sur-</text>
<text top="476" left="50" width="236" height="9" font="font6" id="p1_t32" reading_order_no="32" segment_no="5" tag_type="text">pass, the performance achieved by point cloud based meth-</text>
<text top="488" left="50" width="236" height="9" font="font6" id="p1_t33" reading_order_no="33" segment_no="5" tag_type="text">ods for a given metric span. These performance variations</text>
<text top="500" left="50" width="236" height="9" font="font6" id="p1_t34" reading_order_no="34" segment_no="5" tag_type="text">can be attributed to differences in data richness of input</text>
<text top="512" left="50" width="236" height="9" font="font6" id="p1_t35" reading_order_no="35" segment_no="5" tag_type="text">sensors as well as data accumulation strategies for a mo-</text>
<text top="524" left="50" width="236" height="9" font="font6" id="p1_t36" reading_order_no="36" segment_no="5" tag_type="text">bile robot. While a perfect apple-to-apple comparison may</text>
<text top="535" left="50" width="236" height="9" font="font6" id="p1_t37" reading_order_no="37" segment_no="5" tag_type="text">not be feasible for these two different modalities, the pre-</text>
<text top="547" left="50" width="236" height="9" font="font6" id="p1_t38" reading_order_no="38" segment_no="5" tag_type="text">sented comparison takes a step in the direction of answer-</text>
<text top="559" left="50" width="236" height="9" font="font6" id="p1_t39" reading_order_no="39" segment_no="5" tag_type="text">ing deeper questions regarding spatial representations, rel-</text>
<text top="571" left="50" width="236" height="9" font="font6" id="p1_t40" reading_order_no="40" segment_no="5" tag_type="text">evant to several applications like Autonomous Driving and</text>
<text top="583" left="50" width="236" height="9" font="font6" id="p1_t41" reading_order_no="41" segment_no="5" tag_type="text">Augmented/Virtual Reality. Source code available publicly:</text>
<text top="596" left="50" width="191" height="8" font="font7" id="p1_t42" reading_order_no="42" segment_no="5" tag_type="text"><a href="https://github.com/oravus/seqNet">https://github.com/oravus/seqNet</a></text>
<text top="595" left="242" width="2" height="9" font="font6" id="p1_t43" reading_order_no="43" segment_no="5" tag_type="text"><a href="https://github.com/oravus/seqNet">.</a></text>
<text top="634" left="50" width="77" height="11" font="font5" id="p1_t44" reading_order_no="44" segment_no="11" tag_type="title">1. Introduction</text>
<text top="656" left="62" width="224" height="9" font="font8" id="p1_t45" reading_order_no="45" segment_no="12" tag_type="text">Visual Place Recognition (VPR) is crucial for mobile</text>
<text top="668" left="50" width="236" height="9" font="font8" id="p1_t46" reading_order_no="46" segment_no="12" tag_type="text">robot localization and is typically challenging due to sig-</text>
<text top="680" left="50" width="236" height="9" font="font8" id="p1_t47" reading_order_no="47" segment_no="12" tag_type="text">nificant changes in scene appearance and camera view-</text>
<text top="346" left="408" width="49" height="6" font="font9" id="p1_t48" reading_order_no="55" segment_no="6" tag_type="figure"><b>Sequence Based </b></text>
<text top="353" left="415" width="33" height="6" font="font9" id="p1_t49" reading_order_no="56" segment_no="6" tag_type="figure"><b>Descriptors</b></text>
<text top="325" left="471" width="49" height="6" font="font9" id="p1_t50" reading_order_no="51" segment_no="6" tag_type="figure"><b>Sequential Score </b></text>
<text top="332" left="478" width="35" height="6" font="font9" id="p1_t51" reading_order_no="52" segment_no="6" tag_type="figure"><b>Aggregation</b></text>
<text top="353" left="318" width="24" height="4" font="font10" id="p1_t52" reading_order_no="53" segment_no="6" tag_type="figure">Candidates</text>
<text top="358" left="329" width="13" height="4" font="font10" id="p1_t53" reading_order_no="54" segment_no="6" tag_type="figure">Query</text>
<text top="353" left="488" width="45" height="4" font="font10" id="p1_t54" reading_order_no="57" segment_no="6" tag_type="figure">Sequential Descriptor</text>
<text top="359" left="488" width="36" height="4" font="font10" id="p1_t55" reading_order_no="58" segment_no="6" tag_type="figure">Single Descriptor</text>
<text top="311" left="434" width="19" height="5" font="font11" id="p1_t56" reading_order_no="50" segment_no="6" tag_type="figure"><b>SeqNet</b></text>
<text top="256" left="509" width="23" height="5" font="font10" id="p1_t57" reading_order_no="48" segment_no="6" tag_type="figure">Correctly </text>
<text top="262" left="508" width="23" height="5" font="font10" id="p1_t58" reading_order_no="49" segment_no="6" tag_type="figure">Localized</text>
<text top="369" left="309" width="236" height="8" font="font0" id="p1_t59" reading_order_no="59" segment_no="7" tag_type="text">Figure 1. Sequence-based hierarchical visual place recognition.</text>
<text top="380" left="309" width="236" height="8" font="font0" id="p1_t60" reading_order_no="60" segment_no="7" tag_type="text">SeqNet learns short sequential descriptors that generate high per-</text>
<text top="391" left="309" width="236" height="8" font="font0" id="p1_t61" reading_order_no="61" segment_no="7" tag_type="text">formance initial match candidates and enables selective control se-</text>
<text top="402" left="309" width="227" height="8" font="font0" id="p1_t62" reading_order_no="62" segment_no="7" tag_type="text">quence score aggregation using single image learnt descriptors.</text>
<text top="438" left="309" width="204" height="9" font="font8" id="p1_t63" reading_order_no="63" segment_no="8" tag_type="text">point during subsequent visits of known places <a href="deeplearning_paper8.html#4">[</a></text>
<text top="438" left="513" width="10" height="9" font="font12" id="p1_t64" reading_order_no="64" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">22</a></text>
<text top="438" left="523" width="2" height="9" font="font8" id="p1_t65" reading_order_no="65" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="438" left="529" width="10" height="9" font="font12" id="p1_t66" reading_order_no="66" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">14</a></text>
<text top="438" left="539" width="6" height="9" font="font8" id="p1_t67" reading_order_no="67" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">].</a></text>
<text top="450" left="309" width="236" height="9" font="font8" id="p1_t68" reading_order_no="68" segment_no="8" tag_type="text">Researchers have explored a variety of methods to deal</text>
<text top="462" left="309" width="236" height="9" font="font8" id="p1_t69" reading_order_no="69" segment_no="8" tag_type="text">with this problem ranging from traditional hand-crafted</text>
<text top="474" left="309" width="51" height="9" font="font8" id="p1_t70" reading_order_no="70" segment_no="8" tag_type="text">techniques <a href="deeplearning_paper8.html#4">[</a></text>
<text top="474" left="360" width="5" height="9" font="font12" id="p1_t71" reading_order_no="71" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">7</a></text>
<text top="474" left="365" width="2" height="9" font="font8" id="p1_t72" reading_order_no="72" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="474" left="372" width="10" height="9" font="font12" id="p1_t73" reading_order_no="73" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="474" left="382" width="163" height="9" font="font8" id="p1_t74" reading_order_no="74" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>to modern deep learning-based solu-</text>
<text top="486" left="309" width="27" height="9" font="font8" id="p1_t75" reading_order_no="75" segment_no="8" tag_type="text">tions <a href="deeplearning_paper8.html#3">[</a></text>
<text top="486" left="336" width="5" height="9" font="font12" id="p1_t76" reading_order_no="76" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">2</a></text>
<text top="486" left="341" width="2" height="9" font="font8" id="p1_t77" reading_order_no="77" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">,</a></text>
<text top="486" left="349" width="10" height="9" font="font12" id="p1_t78" reading_order_no="78" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">31</a></text>
<text top="486" left="359" width="2" height="9" font="font8" id="p1_t79" reading_order_no="79" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">,</a></text>
<text top="486" left="366" width="10" height="9" font="font12" id="p1_t80" reading_order_no="80" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">18</a></text>
<text top="486" left="376" width="6" height="9" font="font8" id="p1_t81" reading_order_no="81" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">].</a></text>
<text top="486" left="392" width="154" height="9" font="font8" id="p1_t82" reading_order_no="82" segment_no="8" tag_type="text">Many of these systems aim to push</text>
<text top="498" left="309" width="236" height="9" font="font8" id="p1_t83" reading_order_no="83" segment_no="8" tag_type="text">the performance of single image based place recognition</text>
<text top="510" left="309" width="236" height="9" font="font8" id="p1_t84" reading_order_no="84" segment_no="8" tag_type="text">by learning better image representations as global descrip-</text>
<text top="522" left="309" width="21" height="9" font="font8" id="p1_t85" reading_order_no="85" segment_no="8" tag_type="text">tors <a href="deeplearning_paper8.html#3">[</a></text>
<text top="522" left="330" width="5" height="9" font="font12" id="p1_t86" reading_order_no="86" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">2</a></text>
<text top="522" left="335" width="2" height="9" font="font8" id="p1_t87" reading_order_no="87" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">,</a></text>
<text top="522" left="340" width="5" height="9" font="font12" id="p1_t88" reading_order_no="88" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">6</a></text>
<text top="522" left="345" width="2" height="9" font="font8" id="p1_t89" reading_order_no="89" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="522" left="350" width="10" height="9" font="font12" id="p1_t90" reading_order_no="90" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">29</a></text>
<text top="522" left="360" width="2" height="9" font="font8" id="p1_t91" reading_order_no="91" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">,</a></text>
<text top="522" left="365" width="10" height="9" font="font12" id="p1_t92" reading_order_no="92" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">30</a></text>
<text top="522" left="375" width="89" height="9" font="font8" id="p1_t93" reading_order_no="93" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">] </a>or local descriptors <a href="deeplearning_paper8.html#4">[</a></text>
<text top="522" left="464" width="5" height="9" font="font12" id="p1_t94" reading_order_no="94" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">9</a></text>
<text top="522" left="469" width="2" height="9" font="font8" id="p1_t95" reading_order_no="95" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="522" left="474" width="10" height="9" font="font12" id="p1_t96" reading_order_no="96" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">11</a></text>
<text top="522" left="484" width="2" height="9" font="font8" id="p1_t97" reading_order_no="97" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="522" left="489" width="5" height="9" font="font12" id="p1_t98" reading_order_no="98" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">4</a></text>
<text top="522" left="494" width="51" height="9" font="font8" id="p1_t99" reading_order_no="99" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>and match-</text>
<text top="534" left="309" width="17" height="9" font="font8" id="p1_t100" reading_order_no="100" segment_no="8" tag_type="text">ers <a href="deeplearning_paper8.html#5">[</a></text>
<text top="534" left="326" width="10" height="9" font="font12" id="p1_t101" reading_order_no="101" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">32</a></text>
<text top="534" left="336" width="6" height="9" font="font8" id="p1_t102" reading_order_no="102" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">].</a></text>
<text top="549" left="321" width="224" height="9" font="font8" id="p1_t103" reading_order_no="103" segment_no="10" tag_type="text">To further improve the accuracy of such techniques, re-</text>
<text top="561" left="309" width="236" height="9" font="font8" id="p1_t104" reading_order_no="104" segment_no="10" tag_type="text">searchers have also explored the use of sequential infor-</text>
<text top="573" left="309" width="236" height="9" font="font8" id="p1_t105" reading_order_no="105" segment_no="10" tag_type="text">mation inherent within the problem of mobile robot lo-</text>
<text top="585" left="309" width="236" height="9" font="font8" id="p1_t106" reading_order_no="106" segment_no="10" tag_type="text">calization. However, most of these methods only focus</text>
<text top="597" left="309" width="97" height="9" font="font8" id="p1_t107" reading_order_no="107" segment_no="10" tag_type="text">on robustly aggregating</text>
<text top="597" left="410" width="52" height="9" font="font6" id="p1_t108" reading_order_no="108" segment_no="10" tag_type="text">single image</text>
<text top="597" left="466" width="79" height="9" font="font8" id="p1_t109" reading_order_no="109" segment_no="10" tag_type="text">match scores along</text>
<text top="609" left="309" width="54" height="9" font="font8" id="p1_t110" reading_order_no="110" segment_no="10" tag_type="text">a sequence <a href="deeplearning_paper8.html#4">[</a></text>
<text top="609" left="362" width="10" height="9" font="font12" id="p1_t111" reading_order_no="111" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="609" left="372" width="2" height="9" font="font8" id="p1_t112" reading_order_no="112" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="609" left="380" width="10" height="9" font="font12" id="p1_t113" reading_order_no="113" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">26</a></text>
<text top="609" left="390" width="2" height="9" font="font8" id="p1_t114" reading_order_no="114" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="609" left="397" width="10" height="9" font="font12" id="p1_t115" reading_order_no="115" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">23</a></text>
<text top="609" left="407" width="2" height="9" font="font8" id="p1_t116" reading_order_no="116" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="609" left="414" width="10" height="9" font="font12" id="p1_t117" reading_order_no="117" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#5">35</a></text>
<text top="609" left="424" width="121" height="9" font="font8" id="p1_t118" reading_order_no="118" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#5">], </a>where single image repre-</text>
<text top="621" left="309" width="236" height="9" font="font8" id="p1_t119" reading_order_no="119" segment_no="10" tag_type="text">sentations are agnostic to this post sequential processing.</text>
<text top="633" left="309" width="153" height="9" font="font8" id="p1_t120" reading_order_no="120" segment_no="10" tag_type="text">More recent methods have proposed</text>
<text top="633" left="467" width="78" height="9" font="font6" id="p1_t121" reading_order_no="121" segment_no="10" tag_type="text">sequential descrip-</text>
<text top="645" left="309" width="15" height="9" font="font6" id="p1_t122" reading_order_no="122" segment_no="10" tag_type="text">tors</text>
<text top="644" left="328" width="3" height="9" font="font8" id="p1_t123" reading_order_no="123" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">[</a></text>
<text top="644" left="331" width="10" height="9" font="font12" id="p1_t124" reading_order_no="124" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">15</a></text>
<text top="644" left="341" width="2" height="9" font="font8" id="p1_t125" reading_order_no="125" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="644" left="348" width="10" height="9" font="font12" id="p1_t126" reading_order_no="126" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">12</a></text>
<text top="644" left="358" width="2" height="9" font="font8" id="p1_t127" reading_order_no="127" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="644" left="364" width="10" height="9" font="font12" id="p1_t128" reading_order_no="128" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">27</a></text>
<text top="644" left="374" width="2" height="9" font="font8" id="p1_t129" reading_order_no="129" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="644" left="380" width="5" height="9" font="font12" id="p1_t130" reading_order_no="130" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="644" left="385" width="2" height="9" font="font8" id="p1_t131" reading_order_no="131" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#3">,</a></text>
<text top="644" left="392" width="5" height="9" font="font12" id="p1_t132" reading_order_no="132" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">5</a></text>
<text top="644" left="396" width="149" height="9" font="font8" id="p1_t133" reading_order_no="133" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>that generate place representations</text>
<text top="656" left="309" width="126" height="9" font="font8" id="p1_t134" reading_order_no="134" segment_no="10" tag_type="text">considering sequential imagery</text>
<text top="657" left="438" width="25" height="9" font="font6" id="p1_t135" reading_order_no="135" segment_no="10" tag_type="text">before</text>
<text top="656" left="467" width="78" height="9" font="font8" id="p1_t136" reading_order_no="136" segment_no="10" tag_type="text">any sequence score</text>
<text top="668" left="309" width="56" height="9" font="font8" id="p1_t137" reading_order_no="137" segment_no="10" tag_type="text">aggregation. <a href="deeplearning_paper8.html#4">[</a></text>
<text top="668" left="365" width="10" height="9" font="font12" id="p1_t138" reading_order_no="138" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="668" left="375" width="170" height="9" font="font8" id="p1_t139" reading_order_no="139" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>proposed SeqNet and a hierarchical VPR</text>
<text top="680" left="309" width="236" height="9" font="font8" id="p1_t140" reading_order_no="140" segment_no="10" tag_type="text">pipeline where sequential descriptors are used to select</text>
<text top="546" left="32" width="0" height="18" font="font13" id="p1_t141" reading_order_no="0" segment_no="9" tag_type="title">arXiv:2106.11481v1  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font14" size="9" family="NimbusRomNo9L-Regu" color="#00ff00"/>
	<fontspec id="font15" size="10" family="NimbusRomNo9L-Regu" color="#ff0000"/>
	<fontspec id="font16" size="11" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font17" size="7" family="NimbusRomNo9L-Regu" color="#ff0000"/>
	<fontspec id="font18" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font19" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font20" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font21" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font22" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font23" size="10" family="NimbusRomNo9L-Regu" color="#3f3f3f"/>
	<fontspec id="font24" size="10" family="CMMI10" color="#3f3f3f"/>
	<fontspec id="font25" size="7" family="CMR7" color="#3f3f3f"/>
	<fontspec id="font26" size="10" family="NimbusRomNo9L-Medi" color="#3f3f3f"/>
	<fontspec id="font27" size="10" family="NimbusRomNo9L-ReguItal" color="#3f3f3f"/>
	<fontspec id="font28" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font29" size="8" family="CMR8" color="#000000"/>
<text top="139" left="50" width="236" height="8" font="font0" id="p2_t1" reading_order_no="0" segment_no="2" tag_type="text">Figure 2. (Left) Train and test splits for the Oxford Robotcar</text>
<text top="150" left="50" width="72" height="8" font="font0" id="p2_t2" reading_order_no="1" segment_no="2" tag_type="text">dataset as used in <a href="deeplearning_paper8.html#3">[</a></text>
<text top="150" left="122" width="4" height="8" font="font14" id="p2_t3" reading_order_no="2" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="150" left="126" width="160" height="8" font="font0" id="p2_t4" reading_order_no="3" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#3">] </a>and (right) an updated train split instance</text>
<text top="161" left="50" width="209" height="8" font="font0" id="p2_t5" reading_order_no="4" segment_no="2" tag_type="text">ensuring no visual overlap between train and test imagery.</text>
<text top="193" left="50" width="236" height="9" font="font8" id="p2_t6" reading_order_no="5" segment_no="3" tag_type="text">match hypotheses for single image based sequence score</text>
<text top="205" left="50" width="127" height="9" font="font8" id="p2_t7" reading_order_no="6" segment_no="3" tag_type="text">aggregation, as shown in Figure</text>
<text top="205" left="180" width="5" height="9" font="font15" id="p2_t8" reading_order_no="7" segment_no="3" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="205" left="185" width="2" height="9" font="font8" id="p2_t9" reading_order_no="8" segment_no="3" tag_type="text"><a href="deeplearning_paper8.html#1">.</a></text>
<text top="217" left="62" width="224" height="9" font="font8" id="p2_t10" reading_order_no="9" segment_no="4" tag_type="text">A parallel line of research for place recognition ex-</text>
<text top="229" left="50" width="236" height="9" font="font8" id="p2_t11" reading_order_no="10" segment_no="4" tag_type="text">ists with regards to using 3D data in the form of point</text>
<text top="241" left="50" width="154" height="9" font="font8" id="p2_t12" reading_order_no="11" segment_no="4" tag_type="text">clouds, as done in PointNetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="241" left="204" width="5" height="9" font="font12" id="p2_t13" reading_order_no="12" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="241" left="209" width="45" height="9" font="font8" id="p2_t14" reading_order_no="13" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">], </a>DH3D <a href="deeplearning_paper8.html#4">[</a></text>
<text top="241" left="254" width="10" height="9" font="font12" id="p2_t15" reading_order_no="14" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">10</a></text>
<text top="241" left="264" width="22" height="9" font="font8" id="p2_t16" reading_order_no="15" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>and</text>
<text top="253" left="50" width="32" height="9" font="font8" id="p2_t17" reading_order_no="16" segment_no="4" tag_type="text">others <a href="deeplearning_paper8.html#4">[</a></text>
<text top="253" left="82" width="10" height="9" font="font12" id="p2_t18" reading_order_no="17" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">21</a></text>
<text top="253" left="92" width="2" height="9" font="font8" id="p2_t19" reading_order_no="18" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="253" left="99" width="10" height="9" font="font12" id="p2_t20" reading_order_no="19" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">19</a></text>
<text top="253" left="109" width="2" height="9" font="font8" id="p2_t21" reading_order_no="20" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="253" left="115" width="10" height="9" font="font12" id="p2_t22" reading_order_no="21" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#5">33</a></text>
<text top="253" left="125" width="2" height="9" font="font8" id="p2_t23" reading_order_no="22" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#5">,</a></text>
<text top="253" left="132" width="10" height="9" font="font12" id="p2_t24" reading_order_no="23" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#5">34</a></text>
<text top="253" left="142" width="2" height="9" font="font8" id="p2_t25" reading_order_no="24" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#5">,</a></text>
<text top="253" left="149" width="10" height="9" font="font12" id="p2_t26" reading_order_no="25" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">20</a></text>
<text top="253" left="159" width="128" height="9" font="font8" id="p2_t27" reading_order_no="26" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">]. </a>Instead of using single im-</text>
<text top="265" left="50" width="236" height="9" font="font8" id="p2_t28" reading_order_no="27" segment_no="4" tag_type="text">ages or image sequences, these methods rely on point cloud</text>
<text top="277" left="50" width="236" height="9" font="font8" id="p2_t29" reading_order_no="28" segment_no="4" tag_type="text">data, typically captured through a LiDAR sensor. Using</text>
<text top="289" left="50" width="236" height="9" font="font8" id="p2_t30" reading_order_no="29" segment_no="4" tag_type="text">3D information in this fashion has significant advantages</text>
<text top="301" left="50" width="236" height="9" font="font8" id="p2_t31" reading_order_no="30" segment_no="4" tag_type="text">when considering extreme appearance variations, for exam-</text>
<text top="313" left="50" width="236" height="9" font="font8" id="p2_t32" reading_order_no="31" segment_no="4" tag_type="text">ple, matching data across day vs night, where single image-</text>
<text top="325" left="50" width="155" height="9" font="font8" id="p2_t33" reading_order_no="32" segment_no="4" tag_type="text">based solutions fail catastrophically. <a href="deeplearning_paper8.html#3">[</a></text>
<text top="325" left="205" width="5" height="9" font="font12" id="p2_t34" reading_order_no="33" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="325" left="210" width="77" height="9" font="font8" id="p2_t35" reading_order_no="34" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">] </a>demonstrates this</text>
<text top="337" left="50" width="225" height="9" font="font8" id="p2_t36" reading_order_no="35" segment_no="4" tag_type="text">behavior by comparing their method against NetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="337" left="276" width="5" height="9" font="font12" id="p2_t37" reading_order_no="36" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">2</a></text>
<text top="337" left="281" width="6" height="9" font="font8" id="p2_t38" reading_order_no="37" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">].</a></text>
<text top="349" left="50" width="236" height="9" font="font8" id="p2_t39" reading_order_no="38" segment_no="4" tag_type="text">However, it is not known how well image-based methods</text>
<text top="361" left="50" width="236" height="9" font="font8" id="p2_t40" reading_order_no="39" segment_no="4" tag_type="text">compare against a 3D point cloud based technique when a</text>
<text top="373" left="50" width="236" height="9" font="font8" id="p2_t41" reading_order_no="40" segment_no="4" tag_type="text">sequence of images is considered. In this extended abstract,</text>
<text top="385" left="50" width="194" height="9" font="font8" id="p2_t42" reading_order_no="41" segment_no="4" tag_type="text">we conduct additional experiments with SeqNet <a href="deeplearning_paper8.html#4">[</a></text>
<text top="385" left="244" width="10" height="9" font="font12" id="p2_t43" reading_order_no="42" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="385" left="254" width="32" height="9" font="font8" id="p2_t44" reading_order_no="43" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">], </a>show-</text>
<text top="397" left="50" width="236" height="9" font="font8" id="p2_t45" reading_order_no="44" segment_no="4" tag_type="text">casing that image sequences can potentially outperform 3D</text>
<text top="409" left="50" width="236" height="9" font="font8" id="p2_t46" reading_order_no="45" segment_no="4" tag_type="text">point cloud based methods given a similar metric span and</text>
<text top="421" left="50" width="236" height="9" font="font8" id="p2_t47" reading_order_no="46" segment_no="4" tag_type="text">localization radius. As a preliminary investigation’s late-</text>
<text top="433" left="50" width="236" height="9" font="font8" id="p2_t48" reading_order_no="47" segment_no="4" tag_type="text">breaking result, we only consider a single dataset for this</text>
<text top="445" left="50" width="113" height="9" font="font8" id="p2_t49" reading_order_no="48" segment_no="4" tag_type="text">analysis: Oxford Robotcar <a href="deeplearning_paper8.html#4">[</a></text>
<text top="445" left="163" width="10" height="9" font="font12" id="p2_t50" reading_order_no="49" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">24</a></text>
<text top="445" left="173" width="114" height="9" font="font8" id="p2_t51" reading_order_no="50" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">], </a>which was originally used</text>
<text top="457" left="50" width="68" height="9" font="font8" id="p2_t52" reading_order_no="51" segment_no="4" tag_type="text">by both SeqNet <a href="deeplearning_paper8.html#4">[</a></text>
<text top="457" left="118" width="10" height="9" font="font12" id="p2_t53" reading_order_no="52" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="457" left="128" width="92" height="9" font="font8" id="p2_t54" reading_order_no="53" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>and PointNetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="457" left="219" width="5" height="9" font="font12" id="p2_t55" reading_order_no="54" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="457" left="224" width="6" height="9" font="font8" id="p2_t56" reading_order_no="55" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#3">].</a></text>
<text top="469" left="62" width="172" height="9" font="font8" id="p2_t57" reading_order_no="56" segment_no="6" tag_type="text">We refer the readers to the original works <a href="deeplearning_paper8.html#3">[</a></text>
<text top="469" left="234" width="5" height="9" font="font12" id="p2_t58" reading_order_no="57" segment_no="6" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="469" left="239" width="2" height="9" font="font8" id="p2_t59" reading_order_no="58" segment_no="6" tag_type="text"><a href="deeplearning_paper8.html#3">,</a></text>
<text top="469" left="244" width="10" height="9" font="font12" id="p2_t60" reading_order_no="59" segment_no="6" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="469" left="254" width="33" height="9" font="font8" id="p2_t61" reading_order_no="60" segment_no="6" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>for de-</text>
<text top="481" left="50" width="236" height="9" font="font8" id="p2_t62" reading_order_no="61" segment_no="6" tag_type="text">tailed methodology description. Here, we primarily focus</text>
<text top="493" left="50" width="160" height="9" font="font8" id="p2_t63" reading_order_no="62" segment_no="6" tag_type="text">on the experimental settings and results.</text>
<text top="516" left="50" width="125" height="11" font="font5" id="p2_t64" reading_order_no="63" segment_no="7" tag_type="title">2. Experimental Settings</text>
<text top="536" left="50" width="54" height="10" font="font16" id="p2_t65" reading_order_no="64" segment_no="8" tag_type="title">2.1. Dataset</text>
<text top="555" left="62" width="224" height="9" font="font8" id="p2_t66" reading_order_no="65" segment_no="10" tag_type="text">We use two traverses from the Oxford Robotcar dataset:</text>
<text top="567" left="50" width="236" height="9" font="font8" id="p2_t67" reading_order_no="66" segment_no="10" tag_type="text">one from day time (2015-03-17-11-08-44) and the other</text>
<text top="579" left="50" width="236" height="9" font="font8" id="p2_t68" reading_order_no="67" segment_no="10" tag_type="text">from night time (2014-12-16-18-44-24). Both these tra-</text>
<text top="591" left="50" width="236" height="9" font="font8" id="p2_t69" reading_order_no="68" segment_no="10" tag_type="text">verses were used in the original works, however, train and</text>
<text top="603" left="50" width="236" height="9" font="font8" id="p2_t70" reading_order_no="69" segment_no="10" tag_type="text">test splits differed. Hence, we use the splits defined by</text>
<text top="615" left="50" width="63" height="9" font="font8" id="p2_t71" reading_order_no="70" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#2">PointNetVLAD</a></text>
<text top="614" left="113" width="3" height="6" font="font17" id="p2_t72" reading_order_no="71" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#2">1</a></text>
<text top="615" left="121" width="98" height="9" font="font8" id="p2_t73" reading_order_no="72" segment_no="10" tag_type="text">to train and test <a href="deeplearning_paper8.html#2">SeqNet</a></text>
<text top="614" left="219" width="3" height="6" font="font17" id="p2_t74" reading_order_no="73" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="615" left="223" width="63" height="9" font="font8" id="p2_t75" reading_order_no="74" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#2">. </a>Note that the</text>
<text top="627" left="50" width="236" height="9" font="font8" id="p2_t76" reading_order_no="75" segment_no="10" tag_type="text">training and test splits are captured from geographically dis-</text>
<text top="639" left="50" width="236" height="9" font="font8" id="p2_t77" reading_order_no="76" segment_no="10" tag_type="text">parate locations and each split comprises its own reference</text>
<text top="651" left="50" width="132" height="9" font="font8" id="p2_t78" reading_order_no="77" segment_no="10" tag_type="text">(day) and query (night) database.</text>
<text top="670" left="61" width="3" height="5" font="font18" id="p2_t79" reading_order_no="230" segment_no="13" tag_type="footnote">1</text>
<text top="672" left="64" width="130" height="7" font="font19" id="p2_t80" reading_order_no="231" segment_no="13" tag_type="footnote">https://github.com/mikacuy/pointnetvlad</text>
<text top="680" left="61" width="3" height="5" font="font18" id="p2_t81" reading_order_no="232" segment_no="14" tag_type="footnote">2</text>
<text top="682" left="64" width="106" height="7" font="font19" id="p2_t82" reading_order_no="233" segment_no="14" tag_type="footnote">https://github.com/oravus/seqNet</text>
<text top="48" left="309" width="236" height="8" font="font0" id="p2_t83" reading_order_no="78" segment_no="0" tag_type="title">Table 1. Performance Comparison - Oxford (Day vs Night): Re-</text>
<text top="59" left="309" width="58" height="8" font="font0" id="p2_t84" reading_order_no="79" segment_no="0" tag_type="title">call@K (1,5,20)</text>
<text top="73" left="318" width="33" height="9" font="font20" id="p2_t85" reading_order_no="80" segment_no="1" tag_type="table">Method</text>
<text top="73" left="465" width="68" height="9" font="font8" id="p2_t86" reading_order_no="81" segment_no="1" tag_type="table">Oxford Robotcar</text>
<text top="90" left="318" width="110" height="9" font="font20" id="p2_t87" reading_order_no="82" segment_no="1" tag_type="table">Single Image Descriptors:</text>
<text top="102" left="318" width="48" height="9" font="font8" id="p2_t88" reading_order_no="83" segment_no="1" tag_type="table">NetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="102" left="366" width="5" height="9" font="font12" id="p2_t89" reading_order_no="84" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#3">2</a></text>
<text top="102" left="371" width="3" height="9" font="font8" id="p2_t90" reading_order_no="85" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#3">]</a></text>
<text top="102" left="465" width="58" height="9" font="font8" id="p2_t91" reading_order_no="86" segment_no="1" tag_type="table">0.54/0.74/0.89</text>
<text top="114" left="318" width="63" height="9" font="font8" id="p2_t92" reading_order_no="87" segment_no="1" tag_type="table">NetVLAD-FT (</text>
<text top="114" left="381" width="6" height="9" font="font21" id="p2_t93" reading_order_no="88" segment_no="1" tag_type="table">S</text>
<text top="117" left="387" width="4" height="6" font="font22" id="p2_t94" reading_order_no="89" segment_no="1" tag_type="table">1</text>
<text top="114" left="394" width="3" height="9" font="font8" id="p2_t95" reading_order_no="90" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">[</a></text>
<text top="114" left="398" width="10" height="9" font="font12" id="p2_t96" reading_order_no="91" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="114" left="408" width="7" height="9" font="font8" id="p2_t97" reading_order_no="92" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">])</a></text>
<text top="114" left="465" width="58" height="9" font="font8" id="p2_t98" reading_order_no="93" segment_no="1" tag_type="table">0.62/0.83/0.94</text>
<text top="126" left="318" width="68" height="9" font="font23" id="p2_t99" reading_order_no="94" segment_no="1" tag_type="table">NetVLAD-FT* (</text>
<text top="126" left="386" width="6" height="9" font="font24" id="p2_t100" reading_order_no="95" segment_no="1" tag_type="table">S</text>
<text top="129" left="392" width="4" height="6" font="font25" id="p2_t101" reading_order_no="96" segment_no="1" tag_type="table">1</text>
<text top="126" left="399" width="3" height="9" font="font23" id="p2_t102" reading_order_no="97" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">[</a></text>
<text top="126" left="403" width="10" height="9" font="font12" id="p2_t103" reading_order_no="98" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="126" left="413" width="7" height="9" font="font23" id="p2_t104" reading_order_no="99" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">])</a></text>
<text top="126" left="465" width="17" height="9" font="font23" id="p2_t105" reading_order_no="100" segment_no="1" tag_type="table">0.59</text>
<text top="126" left="483" width="3" height="9" font="font8" id="p2_t106" reading_order_no="101" segment_no="1" tag_type="table">/</text>
<text top="126" left="485" width="17" height="9" font="font23" id="p2_t107" reading_order_no="102" segment_no="1" tag_type="table">0.78</text>
<text top="126" left="503" width="3" height="9" font="font8" id="p2_t108" reading_order_no="103" segment_no="1" tag_type="table">/</text>
<text top="126" left="506" width="17" height="9" font="font23" id="p2_t109" reading_order_no="104" segment_no="1" tag_type="table">0.92</text>
<text top="143" left="318" width="110" height="9" font="font20" id="p2_t110" reading_order_no="105" segment_no="1" tag_type="table">Point Clouds Descriptors:</text>
<text top="155" left="318" width="97" height="9" font="font8" id="p2_t111" reading_order_no="106" segment_no="1" tag_type="table">PointNetVLAD (Base) <a href="deeplearning_paper8.html#3">[</a></text>
<text top="155" left="416" width="5" height="9" font="font12" id="p2_t112" reading_order_no="107" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="155" left="421" width="3" height="9" font="font8" id="p2_t113" reading_order_no="108" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#3">]</a></text>
<text top="155" left="465" width="58" height="9" font="font8" id="p2_t114" reading_order_no="109" segment_no="1" tag_type="table">0.77/0.92/0.96</text>
<text top="167" left="318" width="104" height="9" font="font8" id="p2_t115" reading_order_no="110" segment_no="1" tag_type="table">PointNetVLAD (Refine) <a href="deeplearning_paper8.html#3">[</a></text>
<text top="167" left="422" width="5" height="9" font="font12" id="p2_t116" reading_order_no="111" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="167" left="427" width="3" height="9" font="font8" id="p2_t117" reading_order_no="112" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#3">]</a></text>
<text top="167" left="465" width="58" height="9" font="font8" id="p2_t118" reading_order_no="113" segment_no="1" tag_type="table">0.76/0.90/0.94</text>
<text top="183" left="318" width="100" height="9" font="font20" id="p2_t119" reading_order_no="114" segment_no="1" tag_type="table">Sequential Descriptors:</text>
<text top="196" left="318" width="79" height="9" font="font8" id="p2_t120" reading_order_no="115" segment_no="1" tag_type="table">SmoothNetVLAD <a href="deeplearning_paper8.html#4">[</a></text>
<text top="196" left="397" width="10" height="9" font="font12" id="p2_t121" reading_order_no="116" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">15</a></text>
<text top="196" left="407" width="3" height="9" font="font8" id="p2_t122" reading_order_no="117" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="196" left="465" width="58" height="9" font="font8" id="p2_t123" reading_order_no="118" segment_no="1" tag_type="table">0.66/0.75/0.87</text>
<text top="208" left="318" width="69" height="9" font="font8" id="p2_t124" reading_order_no="119" segment_no="1" tag_type="table">DeltaNetVLAD <a href="deeplearning_paper8.html#4">[</a></text>
<text top="208" left="388" width="10" height="9" font="font12" id="p2_t125" reading_order_no="120" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">15</a></text>
<text top="208" left="398" width="3" height="9" font="font8" id="p2_t126" reading_order_no="121" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="208" left="465" width="58" height="9" font="font8" id="p2_t127" reading_order_no="122" segment_no="1" tag_type="table">0.41/0.64/0.84</text>
<text top="219" left="318" width="63" height="9" font="font8" id="p2_t128" reading_order_no="123" segment_no="1" tag_type="table">SeqNetVLAD (</text>
<text top="219" left="381" width="6" height="9" font="font21" id="p2_t129" reading_order_no="124" segment_no="1" tag_type="table">S</text>
<text top="223" left="387" width="4" height="6" font="font22" id="p2_t130" reading_order_no="125" segment_no="1" tag_type="table">5</text>
<text top="219" left="392" width="3" height="9" font="font8" id="p2_t131" reading_order_no="126" segment_no="1" tag_type="table">)</text>
<text top="220" left="465" width="17" height="9" font="font6" id="p2_t132" reading_order_no="127" segment_no="1" tag_type="table">0.87</text>
<text top="219" left="483" width="3" height="9" font="font8" id="p2_t133" reading_order_no="128" segment_no="1" tag_type="table">/</text>
<text top="220" left="486" width="17" height="9" font="font6" id="p2_t134" reading_order_no="129" segment_no="1" tag_type="table">0.94</text>
<text top="219" left="503" width="3" height="9" font="font8" id="p2_t135" reading_order_no="130" segment_no="1" tag_type="table">/</text>
<text top="219" left="506" width="17" height="9" font="font20" id="p2_t136" reading_order_no="131" segment_no="1" tag_type="table">0.99</text>
<text top="231" left="318" width="68" height="9" font="font23" id="p2_t137" reading_order_no="132" segment_no="1" tag_type="table">SeqNetVLAD* (</text>
<text top="231" left="386" width="6" height="9" font="font24" id="p2_t138" reading_order_no="133" segment_no="1" tag_type="table">S</text>
<text top="235" left="392" width="4" height="6" font="font25" id="p2_t139" reading_order_no="134" segment_no="1" tag_type="table">5</text>
<text top="231" left="397" width="3" height="9" font="font23" id="p2_t140" reading_order_no="135" segment_no="1" tag_type="table">)</text>
<text top="231" left="465" width="17" height="9" font="font26" id="p2_t141" reading_order_no="136" segment_no="1" tag_type="table">0.85</text>
<text top="231" left="483" width="3" height="9" font="font8" id="p2_t142" reading_order_no="137" segment_no="1" tag_type="table">/</text>
<text top="232" left="485" width="17" height="9" font="font27" id="p2_t143" reading_order_no="138" segment_no="1" tag_type="table">0.91</text>
<text top="231" left="503" width="3" height="9" font="font8" id="p2_t144" reading_order_no="139" segment_no="1" tag_type="table">/</text>
<text top="231" left="506" width="17" height="9" font="font26" id="p2_t145" reading_order_no="140" segment_no="1" tag_type="table">0.98</text>
<text top="248" left="318" width="129" height="9" font="font20" id="p2_t146" reading_order_no="141" segment_no="1" tag_type="table">Sequential Score Aggregation:</text>
<text top="260" left="318" width="92" height="9" font="font8" id="p2_t147" reading_order_no="142" segment_no="1" tag_type="table">SeqMatch-NetVLAD <a href="deeplearning_paper8.html#4">[</a></text>
<text top="260" left="410" width="10" height="9" font="font12" id="p2_t148" reading_order_no="143" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="260" left="420" width="3" height="9" font="font8" id="p2_t149" reading_order_no="144" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="260" left="465" width="58" height="9" font="font8" id="p2_t150" reading_order_no="145" segment_no="1" tag_type="table">0.67/0.78/0.89</text>
<text top="272" left="318" width="107" height="9" font="font8" id="p2_t151" reading_order_no="146" segment_no="1" tag_type="table">SeqMatch-NetVLAD-FT <a href="deeplearning_paper8.html#4">[</a></text>
<text top="272" left="425" width="10" height="9" font="font12" id="p2_t152" reading_order_no="147" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="272" left="435" width="3" height="9" font="font8" id="p2_t153" reading_order_no="148" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="272" left="465" width="58" height="9" font="font8" id="p2_t154" reading_order_no="149" segment_no="1" tag_type="table">0.84/0.92/0.98</text>
<text top="284" left="318" width="106" height="9" font="font23" id="p2_t155" reading_order_no="150" segment_no="1" tag_type="table">SeqMatch-NetVLAD-FT*</text>
<text top="284" left="427" width="3" height="9" font="font8" id="p2_t156" reading_order_no="151" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">[</a></text>
<text top="284" left="430" width="10" height="9" font="font12" id="p2_t157" reading_order_no="152" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="284" left="440" width="3" height="9" font="font8" id="p2_t158" reading_order_no="153" segment_no="1" tag_type="table"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="284" left="465" width="17" height="9" font="font23" id="p2_t159" reading_order_no="154" segment_no="1" tag_type="table">0.79</text>
<text top="284" left="483" width="3" height="9" font="font8" id="p2_t160" reading_order_no="155" segment_no="1" tag_type="table">/</text>
<text top="284" left="485" width="17" height="9" font="font23" id="p2_t161" reading_order_no="156" segment_no="1" tag_type="table">0.88</text>
<text top="284" left="503" width="3" height="9" font="font8" id="p2_t162" reading_order_no="157" segment_no="1" tag_type="table">/</text>
<text top="284" left="506" width="17" height="9" font="font23" id="p2_t163" reading_order_no="158" segment_no="1" tag_type="table">0.96</text>
<text top="296" left="318" width="93" height="9" font="font8" id="p2_t164" reading_order_no="159" segment_no="1" tag_type="table">SeqNetVLAD-HVPR (</text>
<text top="296" left="411" width="6" height="9" font="font21" id="p2_t165" reading_order_no="160" segment_no="1" tag_type="table">S</text>
<text top="300" left="417" width="4" height="6" font="font22" id="p2_t166" reading_order_no="161" segment_no="1" tag_type="table">5</text>
<text top="296" left="424" width="8" height="9" font="font8" id="p2_t167" reading_order_no="162" segment_no="1" tag_type="table">to</text>
<text top="296" left="434" width="6" height="9" font="font21" id="p2_t168" reading_order_no="163" segment_no="1" tag_type="table">S</text>
<text top="300" left="441" width="4" height="6" font="font22" id="p2_t169" reading_order_no="164" segment_no="1" tag_type="table">1</text>
<text top="296" left="445" width="3" height="9" font="font8" id="p2_t170" reading_order_no="165" segment_no="1" tag_type="table">)</text>
<text top="296" left="465" width="17" height="9" font="font20" id="p2_t171" reading_order_no="166" segment_no="1" tag_type="table">0.88</text>
<text top="296" left="483" width="3" height="9" font="font8" id="p2_t172" reading_order_no="167" segment_no="1" tag_type="table">/</text>
<text top="296" left="485" width="17" height="9" font="font20" id="p2_t173" reading_order_no="168" segment_no="1" tag_type="table">0.96</text>
<text top="296" left="503" width="3" height="9" font="font8" id="p2_t174" reading_order_no="169" segment_no="1" tag_type="table">/</text>
<text top="296" left="506" width="17" height="9" font="font20" id="p2_t175" reading_order_no="170" segment_no="1" tag_type="table">0.99</text>
<text top="308" left="318" width="98" height="9" font="font23" id="p2_t176" reading_order_no="171" segment_no="1" tag_type="table">SeqNetVLAD-HVPR* (</text>
<text top="308" left="416" width="6" height="9" font="font24" id="p2_t177" reading_order_no="172" segment_no="1" tag_type="table">S</text>
<text top="312" left="422" width="4" height="6" font="font25" id="p2_t178" reading_order_no="173" segment_no="1" tag_type="table">5</text>
<text top="308" left="429" width="8" height="9" font="font23" id="p2_t179" reading_order_no="174" segment_no="1" tag_type="table">to</text>
<text top="308" left="439" width="6" height="9" font="font24" id="p2_t180" reading_order_no="175" segment_no="1" tag_type="table">S</text>
<text top="312" left="446" width="4" height="6" font="font25" id="p2_t181" reading_order_no="176" segment_no="1" tag_type="table">1</text>
<text top="308" left="450" width="3" height="9" font="font23" id="p2_t182" reading_order_no="177" segment_no="1" tag_type="table">)</text>
<text top="308" left="465" width="17" height="9" font="font27" id="p2_t183" reading_order_no="178" segment_no="1" tag_type="table">0.83</text>
<text top="308" left="483" width="3" height="9" font="font8" id="p2_t184" reading_order_no="179" segment_no="1" tag_type="table">/</text>
<text top="308" left="485" width="17" height="9" font="font26" id="p2_t185" reading_order_no="180" segment_no="1" tag_type="table">0.93</text>
<text top="308" left="503" width="3" height="9" font="font8" id="p2_t186" reading_order_no="181" segment_no="1" tag_type="table">/</text>
<text top="308" left="506" width="17" height="9" font="font26" id="p2_t187" reading_order_no="182" segment_no="1" tag_type="table">0.98</text>
<text top="349" left="321" width="224" height="9" font="font8" id="p2_t188" reading_order_no="183" segment_no="5" tag_type="text">The LiDAR based 3D point cloud data in the Ox-</text>
<text top="361" left="309" width="228" height="9" font="font8" id="p2_t189" reading_order_no="184" segment_no="5" tag_type="text">ford dataset used for benchmarking PointNetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="361" left="537" width="5" height="9" font="font12" id="p2_t190" reading_order_no="185" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="361" left="542" width="3" height="9" font="font8" id="p2_t191" reading_order_no="186" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#3">]</a></text>
<text top="373" left="309" width="236" height="9" font="font8" id="p2_t192" reading_order_no="187" segment_no="5" tag_type="text">strictly captures the local surroundings. On the other hand,</text>
<text top="384" left="309" width="236" height="9" font="font8" id="p2_t193" reading_order_no="188" segment_no="5" tag_type="text">forward-facing RGB images can comprise information pro-</text>
<text top="396" left="309" width="236" height="9" font="font8" id="p2_t194" reading_order_no="189" segment_no="5" tag_type="text">jected from locations much farther away from the camera.</text>
<text top="408" left="309" width="146" height="9" font="font8" id="p2_t195" reading_order_no="190" segment_no="5" tag_type="text">Thus, the train-test split defined in <a href="deeplearning_paper8.html#3">[</a></text>
<text top="408" left="455" width="5" height="9" font="font12" id="p2_t196" reading_order_no="191" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="408" left="460" width="51" height="9" font="font8" id="p2_t197" reading_order_no="192" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#3">] </a>(see Figure</text>
<text top="408" left="514" width="5" height="9" font="font15" id="p2_t198" reading_order_no="193" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="408" left="522" width="23" height="9" font="font8" id="p2_t199" reading_order_no="194" segment_no="5" tag_type="text">(left))</text>
<text top="420" left="309" width="236" height="9" font="font8" id="p2_t200" reading_order_no="195" segment_no="5" tag_type="text">potentially leads to visual overlap between both the splits</text>
<text top="432" left="309" width="236" height="9" font="font8" id="p2_t201" reading_order_no="196" segment_no="5" tag_type="text">when using image data. Therefore, keeping the test split</text>
<text top="444" left="309" width="236" height="9" font="font8" id="p2_t202" reading_order_no="197" segment_no="5" tag_type="text">the same, we additionally created an updated instance of</text>
<text top="456" left="309" width="236" height="9" font="font8" id="p2_t203" reading_order_no="198" segment_no="5" tag_type="text">the train split avoiding such visual overlap between the two</text>
<text top="468" left="309" width="105" height="9" font="font8" id="p2_t204" reading_order_no="199" segment_no="5" tag_type="text">splits, as shown in Figure</text>
<text top="468" left="417" width="5" height="9" font="font15" id="p2_t205" reading_order_no="200" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="468" left="425" width="67" height="9" font="font8" id="p2_t206" reading_order_no="201" segment_no="5" tag_type="text">(right). In Table</text>
<text top="468" left="496" width="5" height="9" font="font15" id="p2_t207" reading_order_no="202" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">1</a></text>
<text top="468" left="501" width="44" height="9" font="font8" id="p2_t208" reading_order_no="203" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">, </a>* marked</text>
<text top="480" left="309" width="77" height="9" font="font8" id="p2_t209" reading_order_no="204" segment_no="5" tag_type="text">results presented in</text>
<text top="480" left="389" width="18" height="9" font="font23" id="p2_t210" reading_order_no="205" segment_no="5" tag_type="text">gray</text>
<text top="480" left="409" width="136" height="9" font="font8" id="p2_t211" reading_order_no="206" segment_no="5" tag_type="text">font color correspond to the usage</text>
<text top="492" left="309" width="236" height="9" font="font8" id="p2_t212" reading_order_no="207" segment_no="5" tag_type="text">of the revised train split. For both the split settings, the best</text>
<text top="504" left="309" width="236" height="9" font="font8" id="p2_t213" reading_order_no="208" segment_no="5" tag_type="text">(bold) and the second best (italics) results are formatted in-</text>
<text top="516" left="309" width="83" height="9" font="font8" id="p2_t214" reading_order_no="209" segment_no="5" tag_type="text">dependently in Table</text>
<text top="516" left="395" width="5" height="9" font="font15" id="p2_t215" reading_order_no="210" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">1</a></text>
<text top="516" left="400" width="2" height="9" font="font8" id="p2_t216" reading_order_no="211" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">.</a></text>
<text top="542" left="309" width="77" height="10" font="font16" id="p2_t217" reading_order_no="212" segment_no="9" tag_type="title">2.2. Metric Span</text>
<text top="563" left="321" width="224" height="9" font="font8" id="p2_t218" reading_order_no="213" segment_no="11" tag_type="text">For PointNetVLAD, we use the authors’ provided im-</text>
<text top="575" left="309" width="236" height="9" font="font8" id="p2_t219" reading_order_no="214" segment_no="11" tag_type="text">plementation for extracting point clouds and corresponding</text>
<text top="587" left="309" width="205" height="9" font="font8" id="p2_t220" reading_order_no="215" segment_no="11" tag_type="text">place representations, which have a metric span of</text>
<text top="586" left="517" width="10" height="9" font="font28" id="p2_t221" reading_order_no="216" segment_no="11" tag_type="text">20</text>
<text top="587" left="530" width="15" height="9" font="font8" id="p2_t222" reading_order_no="217" segment_no="11" tag_type="text">me-</text>
<text top="599" left="309" width="236" height="9" font="font8" id="p2_t223" reading_order_no="218" segment_no="11" tag_type="text">ters per place. For SeqNet and other sequence based meth-</text>
<text top="611" left="309" width="173" height="9" font="font8" id="p2_t224" reading_order_no="219" segment_no="11" tag_type="text">ods, we use an image sequence of length</text>
<text top="610" left="486" width="5" height="9" font="font28" id="p2_t225" reading_order_no="220" segment_no="11" tag_type="text">5</text>
<text top="611" left="495" width="50" height="9" font="font8" id="p2_t226" reading_order_no="221" segment_no="11" tag_type="text">with a fixed</text>
<text top="623" left="309" width="77" height="9" font="font8" id="p2_t227" reading_order_no="222" segment_no="11" tag_type="text">frame separation of</text>
<text top="622" left="387" width="5" height="9" font="font28" id="p2_t228" reading_order_no="223" segment_no="11" tag_type="text">2</text>
<text top="623" left="394" width="151" height="9" font="font8" id="p2_t229" reading_order_no="224" segment_no="11" tag_type="text">meters between adjacent frames, lead-</text>
<text top="634" left="309" width="89" height="9" font="font8" id="p2_t230" reading_order_no="225" segment_no="11" tag_type="text">ing to a metric span of</text>
<text top="634" left="401" width="10" height="9" font="font28" id="p2_t231" reading_order_no="226" segment_no="11" tag_type="text">10</text>
<text top="634" left="413" width="65" height="9" font="font8" id="p2_t232" reading_order_no="227" segment_no="11" tag_type="text">meters per <a href="deeplearning_paper8.html#2">place</a></text>
<text top="633" left="478" width="3" height="6" font="font17" id="p2_t233" reading_order_no="228" segment_no="11" tag_type="text"><a href="deeplearning_paper8.html#2">3</a></text>
<text top="634" left="482" width="2" height="9" font="font8" id="p2_t234" reading_order_no="229" segment_no="11" tag_type="text"><a href="deeplearning_paper8.html#2">.</a></text>
<text top="661" left="320" width="3" height="5" font="font18" id="p2_t235" reading_order_no="234" segment_no="12" tag_type="footnote">3</text>
<text top="663" left="323" width="134" height="7" font="font19" id="p2_t236" reading_order_no="235" segment_no="12" tag_type="footnote">Since SeqNet results with a metric span of</text>
<text top="663" left="459" width="8" height="7" font="font29" id="p2_t237" reading_order_no="236" segment_no="12" tag_type="footnote">10</text>
<text top="663" left="469" width="76" height="7" font="font19" id="p2_t238" reading_order_no="237" segment_no="12" tag_type="footnote">meters were found to be</text>
<text top="672" left="309" width="236" height="7" font="font19" id="p2_t239" reading_order_no="238" segment_no="12" tag_type="footnote">superior to PointNetVLAD, we did not conduct further experiments with a</text>
<text top="682" left="309" width="96" height="7" font="font19" id="p2_t240" reading_order_no="239" segment_no="12" tag_type="footnote">larger metric span for SeqNet.</text>
<text top="710" left="295" width="5" height="9" font="font8" id="p2_t241" reading_order_no="240" segment_no="15" tag_type="text">2</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font30" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font31" size="10" family="CMR10" color="#3f3f3f"/>
<text top="50" left="50" width="108" height="10" font="font16" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="title">2.3. Descriptors Details</text>
<text top="70" left="62" width="159" height="9" font="font8" id="p3_t2" reading_order_no="1" segment_no="2" tag_type="text">PointNetVLAD descriptors are of size</text>
<text top="69" left="226" width="15" height="9" font="font28" id="p3_t3" reading_order_no="2" segment_no="2" tag_type="text">256</text>
<text top="70" left="245" width="42" height="9" font="font8" id="p3_t4" reading_order_no="3" segment_no="2" tag_type="text">as the au-</text>
<text top="82" left="50" width="236" height="9" font="font8" id="p3_t5" reading_order_no="4" segment_no="2" tag_type="text">thors observed no notable performance gain with further</text>
<text top="94" left="50" width="145" height="9" font="font8" id="p3_t6" reading_order_no="5" segment_no="2" tag_type="text">doubling of descriptor dimensions <a href="deeplearning_paper8.html#3">[</a></text>
<text top="94" left="195" width="5" height="9" font="font12" id="p3_t7" reading_order_no="6" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="94" left="200" width="86" height="9" font="font8" id="p3_t8" reading_order_no="7" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#3">]. </a>For SeqNet, 4096-</text>
<text top="105" left="50" width="236" height="9" font="font8" id="p3_t9" reading_order_no="8" segment_no="2" tag_type="text">dimensional PCA’d NetVLAD descriptors were used as the</text>
<text top="117" left="50" width="236" height="9" font="font8" id="p3_t10" reading_order_no="9" segment_no="2" tag_type="text">underlying single image representations and output was a</text>
<text top="129" left="50" width="236" height="9" font="font8" id="p3_t11" reading_order_no="10" segment_no="2" tag_type="text">4096-dimensional sequential descriptor, as per the original</text>
<text top="141" left="50" width="32" height="9" font="font8" id="p3_t12" reading_order_no="11" segment_no="2" tag_type="text">setting <a href="deeplearning_paper8.html#4">[</a></text>
<text top="141" left="82" width="10" height="9" font="font12" id="p3_t13" reading_order_no="12" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="141" left="92" width="194" height="9" font="font8" id="p3_t14" reading_order_no="13" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#4">]. </a>From here on, we refer to this SeqNet descrip-</text>
<text top="153" left="50" width="104" height="9" font="font8" id="p3_t15" reading_order_no="14" segment_no="2" tag_type="text">tor based on NetVLAD as</text>
<text top="153" left="157" width="53" height="9" font="font6" id="p3_t16" reading_order_no="15" segment_no="2" tag_type="text">SeqNetVLAD</text>
<text top="153" left="210" width="2" height="9" font="font8" id="p3_t17" reading_order_no="16" segment_no="2" tag_type="text">.</text>
<text top="174" left="50" width="70" height="10" font="font16" id="p3_t18" reading_order_no="17" segment_no="3" tag_type="title">2.4. Evaluation</text>
<text top="193" left="62" width="68" height="9" font="font8" id="p3_t19" reading_order_no="18" segment_no="4" tag_type="text">We use Recall@</text>
<text top="193" left="130" width="8" height="9" font="font21" id="p3_t20" reading_order_no="19" segment_no="4" tag_type="text">K</text>
<text top="193" left="143" width="3" height="9" font="font8" id="p3_t21" reading_order_no="20" segment_no="4" tag_type="text">(</text>
<text top="193" left="146" width="8" height="9" font="font21" id="p3_t22" reading_order_no="21" segment_no="4" tag_type="text">K</text>
<text top="192" left="160" width="16" height="9" font="font30" id="p3_t23" reading_order_no="22" segment_no="4" tag_type="text">∈ {</text>
<text top="193" left="177" width="5" height="9" font="font28" id="p3_t24" reading_order_no="23" segment_no="4" tag_type="text">1</text>
<text top="193" left="182" width="3" height="9" font="font21" id="p3_t25" reading_order_no="24" segment_no="4" tag_type="text">,</text>
<text top="193" left="186" width="5" height="9" font="font28" id="p3_t26" reading_order_no="25" segment_no="4" tag_type="text">5</text>
<text top="193" left="191" width="3" height="9" font="font21" id="p3_t27" reading_order_no="26" segment_no="4" tag_type="text">,</text>
<text top="193" left="196" width="10" height="9" font="font28" id="p3_t28" reading_order_no="27" segment_no="4" tag_type="text">20</text>
<text top="192" left="206" width="5" height="9" font="font30" id="p3_t29" reading_order_no="28" segment_no="4" tag_type="text">}</text>
<text top="193" left="211" width="76" height="9" font="font8" id="p3_t30" reading_order_no="29" segment_no="4" tag_type="text">) as the evaluation</text>
<text top="205" left="50" width="236" height="9" font="font8" id="p3_t31" reading_order_no="30" segment_no="4" tag_type="text">metric as also used by both PointNetVLAD and SeqNet.</text>
<text top="217" left="50" width="236" height="9" font="font8" id="p3_t32" reading_order_no="31" segment_no="4" tag_type="text">Localization radius is set to be 25 meters as used by Point-</text>
<text top="229" left="50" width="45" height="9" font="font8" id="p3_t33" reading_order_no="32" segment_no="4" tag_type="text">NetVLAD.</text>
<text top="252" left="50" width="49" height="11" font="font5" id="p3_t34" reading_order_no="33" segment_no="5" tag_type="title">3. Results</text>
<text top="273" left="62" width="22" height="9" font="font8" id="p3_t35" reading_order_no="34" segment_no="7" tag_type="text">Table</text>
<text top="273" left="87" width="5" height="9" font="font15" id="p3_t36" reading_order_no="35" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#2">1</a></text>
<text top="273" left="95" width="192" height="9" font="font8" id="p3_t37" reading_order_no="36" segment_no="7" tag_type="text">shows performance comparison between differ-</text>
<text top="285" left="50" width="191" height="9" font="font8" id="p3_t38" reading_order_no="37" segment_no="7" tag_type="text">ent types of approaches to place recognition:</text>
<text top="285" left="248" width="8" height="9" font="font20" id="p3_t39" reading_order_no="38" segment_no="7" tag_type="text">1)</text>
<text top="285" left="261" width="25" height="9" font="font8" id="p3_t40" reading_order_no="39" segment_no="7" tag_type="text">Single</text>
<text top="297" left="50" width="169" height="9" font="font8" id="p3_t41" reading_order_no="40" segment_no="7" tag_type="text">Image Descriptors including NetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="297" left="219" width="5" height="9" font="font12" id="p3_t42" reading_order_no="41" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#3">2</a></text>
<text top="297" left="224" width="62" height="9" font="font8" id="p3_t43" reading_order_no="42" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#3">] </a>and its night-</text>
<text top="309" left="50" width="195" height="9" font="font8" id="p3_t44" reading_order_no="43" segment_no="7" tag_type="text">time fine-tuned version NetVLAD-FT (trained as</text>
<text top="309" left="248" width="6" height="9" font="font21" id="p3_t45" reading_order_no="44" segment_no="7" tag_type="text">S</text>
<text top="312" left="254" width="4" height="6" font="font22" id="p3_t46" reading_order_no="45" segment_no="7" tag_type="text">1</text>
<text top="309" left="258" width="28" height="9" font="font8" id="p3_t47" reading_order_no="46" segment_no="7" tag_type="text">, as de-</text>
<text top="321" left="50" width="44" height="9" font="font8" id="p3_t48" reading_order_no="47" segment_no="7" tag_type="text">scribed in <a href="deeplearning_paper8.html#4">[</a></text>
<text top="321" left="94" width="10" height="9" font="font12" id="p3_t49" reading_order_no="48" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="321" left="104" width="9" height="9" font="font8" id="p3_t50" reading_order_no="49" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">]);</a></text>
<text top="321" left="115" width="8" height="9" font="font20" id="p3_t51" reading_order_no="50" segment_no="7" tag_type="text">2)</text>
<text top="321" left="125" width="161" height="9" font="font8" id="p3_t52" reading_order_no="51" segment_no="7" tag_type="text">Point Cloud Descriptors including Point-</text>
<text top="333" left="50" width="98" height="9" font="font8" id="p3_t53" reading_order_no="52" segment_no="7" tag_type="text">NetVLAD with both its</text>
<text top="333" left="152" width="18" height="9" font="font6" id="p3_t54" reading_order_no="53" segment_no="7" tag_type="text">base</text>
<text top="333" left="174" width="112" height="9" font="font8" id="p3_t55" reading_order_no="54" segment_no="7" tag_type="text">version trained only on the</text>
<text top="345" left="50" width="113" height="9" font="font8" id="p3_t56" reading_order_no="55" segment_no="7" tag_type="text">Oxford Robotcar dataset and</text>
<text top="345" left="165" width="22" height="9" font="font6" id="p3_t57" reading_order_no="56" segment_no="7" tag_type="text">refine</text>
<text top="345" left="189" width="97" height="9" font="font8" id="p3_t58" reading_order_no="57" segment_no="7" tag_type="text">version trained on multi-</text>
<text top="357" left="50" width="49" height="9" font="font8" id="p3_t59" reading_order_no="58" segment_no="7" tag_type="text">ple datasets;</text>
<text top="357" left="102" width="8" height="9" font="font20" id="p3_t60" reading_order_no="59" segment_no="7" tag_type="text">3)</text>
<text top="357" left="113" width="174" height="9" font="font8" id="p3_t61" reading_order_no="60" segment_no="7" tag_type="text">Sequential Descriptors including Smoothed</text>
<text top="369" left="50" width="236" height="9" font="font8" id="p3_t62" reading_order_no="61" segment_no="7" tag_type="text">and Delta Descriptor defined using NetVLAD, as described</text>
<text top="381" left="50" width="13" height="9" font="font8" id="p3_t63" reading_order_no="62" segment_no="7" tag_type="text">in <a href="deeplearning_paper8.html#4">[</a></text>
<text top="381" left="63" width="10" height="9" font="font12" id="p3_t64" reading_order_no="63" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">15</a></text>
<text top="381" left="73" width="84" height="9" font="font8" id="p3_t65" reading_order_no="64" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>and SeqNetVLAD <a href="deeplearning_paper8.html#4">[</a></text>
<text top="381" left="157" width="10" height="9" font="font12" id="p3_t66" reading_order_no="65" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="381" left="167" width="23" height="9" font="font8" id="p3_t67" reading_order_no="66" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">]; </a>and</text>
<text top="381" left="192" width="8" height="9" font="font20" id="p3_t68" reading_order_no="67" segment_no="7" tag_type="text">4)</text>
<text top="381" left="202" width="84" height="9" font="font8" id="p3_t69" reading_order_no="68" segment_no="7" tag_type="text">Sequential Score Ag-</text>
<text top="393" left="50" width="154" height="9" font="font8" id="p3_t70" reading_order_no="69" segment_no="7" tag_type="text">gregation including SeqSLAM-based <a href="deeplearning_paper8.html#4">[</a></text>
<text top="393" left="204" width="10" height="9" font="font12" id="p3_t71" reading_order_no="70" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="393" left="214" width="72" height="9" font="font8" id="p3_t72" reading_order_no="71" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>sequence match-</text>
<text top="405" left="50" width="236" height="9" font="font8" id="p3_t73" reading_order_no="72" segment_no="7" tag_type="text">ing defined on single image descriptors using NetVLAD</text>
<text top="417" left="50" width="236" height="9" font="font8" id="p3_t74" reading_order_no="73" segment_no="7" tag_type="text">and NetVLAD-FT, referred to as SeqMatch-NetVLAD and</text>
<text top="428" left="50" width="236" height="9" font="font8" id="p3_t75" reading_order_no="74" segment_no="7" tag_type="text">SeqMatch-NetVLAD-FT respectively, and a hierarchical</text>
<text top="440" left="50" width="70" height="9" font="font8" id="p3_t76" reading_order_no="75" segment_no="7" tag_type="text">approach as per <a href="deeplearning_paper8.html#4">[</a></text>
<text top="440" left="121" width="10" height="9" font="font12" id="p3_t77" reading_order_no="76" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="440" left="131" width="156" height="9" font="font8" id="p3_t78" reading_order_no="77" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#4">], </a>referred to as SeqNetVLAD-HVPR,</text>
<text top="452" left="50" width="236" height="9" font="font8" id="p3_t79" reading_order_no="78" segment_no="7" tag_type="text">where SeqNetVLAD is used as a sequential descriptor to se-</text>
<text top="464" left="50" width="233" height="9" font="font8" id="p3_t80" reading_order_no="79" segment_no="7" tag_type="text">lect top matching candidates for SeqMatch-NetVLAD-FT.</text>
<text top="477" left="62" width="143" height="9" font="font8" id="p3_t81" reading_order_no="80" segment_no="10" tag_type="text">It can be observed from Table</text>
<text top="477" left="212" width="5" height="9" font="font15" id="p3_t82" reading_order_no="81" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#2">1</a></text>
<text top="477" left="224" width="62" height="9" font="font8" id="p3_t83" reading_order_no="82" segment_no="10" tag_type="text">that sequence-</text>
<text top="489" left="50" width="164" height="9" font="font8" id="p3_t84" reading_order_no="83" segment_no="10" tag_type="text">based methods like SmoothNetVLAD (</text>
<text top="488" left="215" width="5" height="9" font="font28" id="p3_t85" reading_order_no="84" segment_no="10" tag_type="text">0</text>
<text top="488" left="219" width="3" height="9" font="font21" id="p3_t86" reading_order_no="85" segment_no="10" tag_type="text">.</text>
<text top="488" left="222" width="10" height="9" font="font28" id="p3_t87" reading_order_no="86" segment_no="10" tag_type="text">66</text>
<text top="489" left="232" width="54" height="9" font="font8" id="p3_t88" reading_order_no="87" segment_no="10" tag_type="text">), SeqMatch-</text>
<text top="501" left="50" width="50" height="9" font="font8" id="p3_t89" reading_order_no="88" segment_no="10" tag_type="text">NetVLAD (</text>
<text top="500" left="100" width="5" height="9" font="font28" id="p3_t90" reading_order_no="89" segment_no="10" tag_type="text">0</text>
<text top="500" left="105" width="3" height="9" font="font21" id="p3_t91" reading_order_no="90" segment_no="10" tag_type="text">.</text>
<text top="500" left="108" width="10" height="9" font="font28" id="p3_t92" reading_order_no="91" segment_no="10" tag_type="text">67</text>
<text top="501" left="118" width="169" height="9" font="font8" id="p3_t93" reading_order_no="92" segment_no="10" tag_type="text">) improve performance on top of single</text>
<text top="513" left="50" width="147" height="9" font="font8" id="p3_t94" reading_order_no="93" segment_no="10" tag_type="text">image only techniques NetVLAD (</text>
<text top="512" left="197" width="5" height="9" font="font28" id="p3_t95" reading_order_no="94" segment_no="10" tag_type="text">0</text>
<text top="512" left="202" width="3" height="9" font="font21" id="p3_t96" reading_order_no="95" segment_no="10" tag_type="text">.</text>
<text top="512" left="205" width="10" height="9" font="font28" id="p3_t97" reading_order_no="96" segment_no="10" tag_type="text">54</text>
<text top="513" left="215" width="72" height="9" font="font8" id="p3_t98" reading_order_no="97" segment_no="10" tag_type="text">) and NetVLAD-</text>
<text top="525" left="50" width="18" height="9" font="font8" id="p3_t99" reading_order_no="98" segment_no="10" tag_type="text">FT (</text>
<text top="524" left="68" width="5" height="9" font="font31" id="p3_t100" reading_order_no="99" segment_no="10" tag_type="text">0</text>
<text top="524" left="73" width="3" height="9" font="font24" id="p3_t101" reading_order_no="100" segment_no="10" tag_type="text">.</text>
<text top="524" left="76" width="10" height="9" font="font31" id="p3_t102" reading_order_no="101" segment_no="10" tag_type="text">59</text>
<text top="525" left="86" width="3" height="9" font="font8" id="p3_t103" reading_order_no="102" segment_no="10" tag_type="text">/</text>
<text top="524" left="89" width="5" height="9" font="font28" id="p3_t104" reading_order_no="103" segment_no="10" tag_type="text">0</text>
<text top="524" left="94" width="3" height="9" font="font21" id="p3_t105" reading_order_no="104" segment_no="10" tag_type="text">.</text>
<text top="524" left="96" width="10" height="9" font="font28" id="p3_t106" reading_order_no="105" segment_no="10" tag_type="text">62</text>
<text top="525" left="106" width="180" height="9" font="font8" id="p3_t107" reading_order_no="106" segment_no="10" tag_type="text">) but do not approach performance of Point-</text>
<text top="536" left="50" width="52" height="9" font="font8" id="p3_t108" reading_order_no="107" segment_no="10" tag_type="text">NetVLAD (</text>
<text top="536" left="102" width="5" height="9" font="font28" id="p3_t109" reading_order_no="108" segment_no="10" tag_type="text">0</text>
<text top="536" left="107" width="3" height="9" font="font21" id="p3_t110" reading_order_no="109" segment_no="10" tag_type="text">.</text>
<text top="536" left="110" width="10" height="9" font="font28" id="p3_t111" reading_order_no="110" segment_no="10" tag_type="text">77</text>
<text top="536" left="120" width="6" height="9" font="font8" id="p3_t112" reading_order_no="111" segment_no="10" tag_type="text">).</text>
<text top="536" left="140" width="146" height="9" font="font8" id="p3_t113" reading_order_no="112" segment_no="10" tag_type="text">However, SeqMatch-NetVLAD-FT</text>
<text top="548" left="50" width="3" height="9" font="font8" id="p3_t114" reading_order_no="113" segment_no="10" tag_type="text">(</text>
<text top="548" left="53" width="5" height="9" font="font31" id="p3_t115" reading_order_no="114" segment_no="10" tag_type="text">0</text>
<text top="548" left="58" width="3" height="9" font="font24" id="p3_t116" reading_order_no="115" segment_no="10" tag_type="text">.</text>
<text top="548" left="61" width="10" height="9" font="font31" id="p3_t117" reading_order_no="116" segment_no="10" tag_type="text">79</text>
<text top="548" left="71" width="3" height="9" font="font8" id="p3_t118" reading_order_no="117" segment_no="10" tag_type="text">/</text>
<text top="548" left="74" width="5" height="9" font="font28" id="p3_t119" reading_order_no="118" segment_no="10" tag_type="text">0</text>
<text top="548" left="79" width="3" height="9" font="font21" id="p3_t120" reading_order_no="119" segment_no="10" tag_type="text">.</text>
<text top="548" left="82" width="10" height="9" font="font28" id="p3_t121" reading_order_no="120" segment_no="10" tag_type="text">84</text>
<text top="548" left="92" width="72" height="9" font="font8" id="p3_t122" reading_order_no="121" segment_no="10" tag_type="text">), SeqNetVLAD (</text>
<text top="548" left="164" width="5" height="9" font="font31" id="p3_t123" reading_order_no="122" segment_no="10" tag_type="text">0</text>
<text top="548" left="169" width="3" height="9" font="font24" id="p3_t124" reading_order_no="123" segment_no="10" tag_type="text">.</text>
<text top="548" left="172" width="10" height="9" font="font31" id="p3_t125" reading_order_no="124" segment_no="10" tag_type="text">83</text>
<text top="548" left="182" width="3" height="9" font="font8" id="p3_t126" reading_order_no="125" segment_no="10" tag_type="text">/</text>
<text top="548" left="185" width="5" height="9" font="font28" id="p3_t127" reading_order_no="126" segment_no="10" tag_type="text">0</text>
<text top="548" left="189" width="3" height="9" font="font21" id="p3_t128" reading_order_no="127" segment_no="10" tag_type="text">.</text>
<text top="548" left="192" width="10" height="9" font="font28" id="p3_t129" reading_order_no="128" segment_no="10" tag_type="text">87</text>
<text top="548" left="202" width="84" height="9" font="font8" id="p3_t130" reading_order_no="129" segment_no="10" tag_type="text">) and SeqNetVLAD-</text>
<text top="560" left="50" width="33" height="9" font="font8" id="p3_t131" reading_order_no="130" segment_no="10" tag_type="text">HVPR (</text>
<text top="560" left="83" width="5" height="9" font="font31" id="p3_t132" reading_order_no="131" segment_no="10" tag_type="text">0</text>
<text top="560" left="88" width="3" height="9" font="font24" id="p3_t133" reading_order_no="132" segment_no="10" tag_type="text">.</text>
<text top="560" left="91" width="10" height="9" font="font31" id="p3_t134" reading_order_no="133" segment_no="10" tag_type="text">85</text>
<text top="560" left="101" width="3" height="9" font="font8" id="p3_t135" reading_order_no="134" segment_no="10" tag_type="text">/</text>
<text top="560" left="104" width="5" height="9" font="font28" id="p3_t136" reading_order_no="135" segment_no="10" tag_type="text">0</text>
<text top="560" left="109" width="3" height="9" font="font21" id="p3_t137" reading_order_no="136" segment_no="10" tag_type="text">.</text>
<text top="560" left="111" width="10" height="9" font="font28" id="p3_t138" reading_order_no="137" segment_no="10" tag_type="text">88</text>
<text top="560" left="121" width="165" height="9" font="font8" id="p3_t139" reading_order_no="138" segment_no="10" tag_type="text">) surpass PointNetVLAD’s performance.</text>
<text top="572" left="50" width="236" height="9" font="font8" id="p3_t140" reading_order_no="139" segment_no="10" tag_type="text">This demonstrates that not only the sequential informa-</text>
<text top="584" left="50" width="236" height="9" font="font8" id="p3_t141" reading_order_no="140" segment_no="10" tag_type="text">tion is a strong cue for place recognition under challenging</text>
<text top="596" left="50" width="223" height="9" font="font8" id="p3_t142" reading_order_no="141" segment_no="10" tag_type="text">appearance conditions, trained sequential descriptors <a href="deeplearning_paper8.html#4">[</a></text>
<text top="596" left="273" width="10" height="9" font="font12" id="p3_t143" reading_order_no="142" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="596" left="283" width="3" height="9" font="font8" id="p3_t144" reading_order_no="143" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="608" left="50" width="236" height="9" font="font8" id="p3_t145" reading_order_no="144" segment_no="10" tag_type="text">might be learning the underlying 3D scene structure implic-</text>
<text top="620" left="50" width="236" height="9" font="font8" id="p3_t146" reading_order_no="145" segment_no="10" tag_type="text">itly, leading to better performance than what was achievable</text>
<text top="632" left="50" width="182" height="9" font="font8" id="p3_t147" reading_order_no="146" segment_no="10" tag_type="text">through traditional sequence-based methods <a href="deeplearning_paper8.html#4">[</a></text>
<text top="632" left="232" width="10" height="9" font="font12" id="p3_t148" reading_order_no="147" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="632" left="242" width="6" height="9" font="font8" id="p3_t149" reading_order_no="148" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#4">].</a></text>
<text top="644" left="62" width="224" height="9" font="font8" id="p3_t150" reading_order_no="149" segment_no="14" tag_type="text">The experiments conducted in this preliminary investiga-</text>
<text top="656" left="50" width="236" height="9" font="font8" id="p3_t151" reading_order_no="150" segment_no="14" tag_type="text">tion have their limitations as a perfect apple-to-apple com-</text>
<text top="668" left="50" width="236" height="9" font="font8" id="p3_t152" reading_order_no="151" segment_no="14" tag_type="text">parison between RGB image sequence and LiDAR point</text>
<text top="680" left="50" width="236" height="9" font="font8" id="p3_t153" reading_order_no="152" segment_no="14" tag_type="text">clouds is not trivial. Both the sensor modalities have com-</text>
<text top="51" left="309" width="236" height="9" font="font8" id="p3_t154" reading_order_no="153" segment_no="1" tag_type="text">plementary characteristics. As compared to RGB cameras,</text>
<text top="63" left="309" width="236" height="9" font="font8" id="p3_t155" reading_order_no="154" segment_no="1" tag_type="text">active range sensors like LiDARs are not drastically af-</text>
<text top="75" left="309" width="236" height="9" font="font8" id="p3_t156" reading_order_no="155" segment_no="1" tag_type="text">fected by variations in environmental conditions such as</text>
<text top="87" left="309" width="236" height="9" font="font8" id="p3_t157" reading_order_no="156" segment_no="1" tag_type="text">time of day and seasonal cycles. However, the inherent in-</text>
<text top="99" left="309" width="236" height="9" font="font8" id="p3_t158" reading_order_no="157" segment_no="1" tag_type="text">formation richness of RGB image sensors, compared to Li-</text>
<text top="111" left="309" width="236" height="9" font="font8" id="p3_t159" reading_order_no="158" segment_no="1" tag_type="text">DAR point clouds which are typically sparse, makes room</text>
<text top="123" left="309" width="236" height="9" font="font8" id="p3_t160" reading_order_no="159" segment_no="1" tag_type="text">for advanced image processing techniques, potentially lead-</text>
<text top="135" left="309" width="236" height="9" font="font8" id="p3_t161" reading_order_no="160" segment_no="1" tag_type="text">ing to improved performance even under challenging en-</text>
<text top="147" left="309" width="97" height="9" font="font8" id="p3_t162" reading_order_no="161" segment_no="1" tag_type="text">vironmental conditions.</text>
<text top="147" left="415" width="130" height="9" font="font8" id="p3_t163" reading_order_no="162" segment_no="1" tag_type="text">Furthermore, as a robot moves</text>
<text top="159" left="309" width="236" height="9" font="font8" id="p3_t164" reading_order_no="163" segment_no="1" tag_type="text">through an environment, the data accumulation strategy also</text>
<text top="171" left="309" width="236" height="9" font="font8" id="p3_t165" reading_order_no="164" segment_no="1" tag_type="text">plays a key role in determining the robustness of a place</text>
<text top="183" left="309" width="236" height="9" font="font8" id="p3_t166" reading_order_no="165" segment_no="1" tag_type="text">representation. For example, the strategy of feeding single</text>
<text top="195" left="309" width="193" height="9" font="font8" id="p3_t167" reading_order_no="166" segment_no="1" tag_type="text">images to sequence score aggregation methods <a href="deeplearning_paper8.html#4">[</a></text>
<text top="195" left="502" width="10" height="9" font="font12" id="p3_t168" reading_order_no="167" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">25</a></text>
<text top="195" left="512" width="2" height="9" font="font8" id="p3_t169" reading_order_no="168" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="195" left="517" width="10" height="9" font="font12" id="p3_t170" reading_order_no="169" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">26</a></text>
<text top="195" left="527" width="2" height="9" font="font8" id="p3_t171" reading_order_no="170" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="195" left="532" width="10" height="9" font="font12" id="p3_t172" reading_order_no="171" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">23</a></text>
<text top="195" left="542" width="3" height="9" font="font8" id="p3_t173" reading_order_no="172" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">]</a></text>
<text top="207" left="309" width="136" height="9" font="font8" id="p3_t174" reading_order_no="173" segment_no="1" tag_type="text">or sequential descriptor networks <a href="deeplearning_paper8.html#4">[</a></text>
<text top="207" left="445" width="10" height="9" font="font12" id="p3_t175" reading_order_no="174" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="207" left="455" width="2" height="9" font="font8" id="p3_t176" reading_order_no="175" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="207" left="460" width="10" height="9" font="font12" id="p3_t177" reading_order_no="176" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">12</a></text>
<text top="207" left="470" width="2" height="9" font="font8" id="p3_t178" reading_order_no="177" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="207" left="474" width="5" height="9" font="font12" id="p3_t179" reading_order_no="178" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">5</a></text>
<text top="207" left="479" width="66" height="9" font="font8" id="p3_t180" reading_order_no="179" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>can also be em-</text>
<text top="219" left="309" width="217" height="9" font="font8" id="p3_t181" reading_order_no="180" segment_no="1" tag_type="text">ulated for point cloud based techniques that currently</text>
<text top="219" left="529" width="16" height="9" font="font6" id="p3_t182" reading_order_no="181" segment_no="1" tag_type="text">pre-</text>
<text top="231" left="309" width="30" height="9" font="font6" id="p3_t183" reading_order_no="182" segment_no="1" tag_type="text">process</text>
<text top="231" left="342" width="203" height="9" font="font8" id="p3_t184" reading_order_no="183" segment_no="1" tag_type="text">individual point clouds to form a relatively larger</text>
<text top="243" left="309" width="20" height="9" font="font8" id="p3_t185" reading_order_no="184" segment_no="1" tag_type="text">one <a href="deeplearning_paper8.html#3">[</a></text>
<text top="243" left="329" width="5" height="9" font="font12" id="p3_t186" reading_order_no="185" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="243" left="334" width="2" height="9" font="font8" id="p3_t187" reading_order_no="186" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#3">,</a></text>
<text top="243" left="339" width="10" height="9" font="font12" id="p3_t188" reading_order_no="187" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">10</a></text>
<text top="243" left="349" width="172" height="9" font="font8" id="p3_t189" reading_order_no="188" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>before learning any place representations.</text>
<text top="271" left="309" width="69" height="11" font="font5" id="p3_t190" reading_order_no="189" segment_no="6" tag_type="title">4. Conclusion</text>
<text top="293" left="321" width="224" height="9" font="font8" id="p3_t191" reading_order_no="190" segment_no="8" tag_type="text">With recent advances in deep learning, several novel</text>
<text top="305" left="309" width="236" height="9" font="font8" id="p3_t192" reading_order_no="191" segment_no="8" tag_type="text">methods have been developed for place (spatial) representa-</text>
<text top="317" left="309" width="173" height="9" font="font8" id="p3_t193" reading_order_no="192" segment_no="8" tag_type="text">tions including both 3D point cloud based <a href="deeplearning_paper8.html#3">[</a></text>
<text top="317" left="482" width="5" height="9" font="font12" id="p3_t194" reading_order_no="193" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">1</a></text>
<text top="317" left="487" width="2" height="9" font="font8" id="p3_t195" reading_order_no="194" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">,</a></text>
<text top="317" left="492" width="10" height="9" font="font12" id="p3_t196" reading_order_no="195" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">10</a></text>
<text top="317" left="501" width="44" height="9" font="font8" id="p3_t197" reading_order_no="196" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">] </a>and those</text>
<text top="329" left="309" width="112" height="9" font="font8" id="p3_t198" reading_order_no="197" segment_no="8" tag_type="text">based on image sequences <a href="deeplearning_paper8.html#4">[</a></text>
<text top="329" left="421" width="10" height="9" font="font12" id="p3_t199" reading_order_no="198" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">16</a></text>
<text top="329" left="431" width="2" height="9" font="font8" id="p3_t200" reading_order_no="199" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="329" left="436" width="10" height="9" font="font12" id="p3_t201" reading_order_no="200" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">12</a></text>
<text top="329" left="446" width="2" height="9" font="font8" id="p3_t202" reading_order_no="201" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="329" left="451" width="10" height="9" font="font12" id="p3_t203" reading_order_no="202" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">15</a></text>
<text top="329" left="461" width="84" height="9" font="font8" id="p3_t204" reading_order_no="203" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">]. </a>Both these modal-</text>
<text top="341" left="309" width="236" height="9" font="font8" id="p3_t205" reading_order_no="204" segment_no="8" tag_type="text">ities have their own inherent characteristics and there re-</text>
<text top="353" left="309" width="236" height="9" font="font8" id="p3_t206" reading_order_no="205" segment_no="8" tag_type="text">main several questions unanswered in terms of what might</text>
<text top="365" left="309" width="236" height="9" font="font8" id="p3_t207" reading_order_no="206" segment_no="8" tag_type="text">constitute an ideal representation of the world perceived by</text>
<text top="377" left="309" width="67" height="9" font="font8" id="p3_t208" reading_order_no="207" segment_no="8" tag_type="text">a mobile robot <a href="deeplearning_paper8.html#4">[</a></text>
<text top="377" left="375" width="5" height="9" font="font12" id="p3_t209" reading_order_no="208" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">8</a></text>
<text top="377" left="380" width="2" height="9" font="font8" id="p3_t210" reading_order_no="209" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="377" left="386" width="10" height="9" font="font12" id="p3_t211" reading_order_no="210" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">17</a></text>
<text top="377" left="396" width="149" height="9" font="font8" id="p3_t212" reading_order_no="211" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">]. </a>The analysis presented in this ex-</text>
<text top="389" left="309" width="236" height="9" font="font8" id="p3_t213" reading_order_no="212" segment_no="8" tag_type="text">tended abstract takes an initial step towards answering such</text>
<text top="401" left="309" width="236" height="9" font="font8" id="p3_t214" reading_order_no="213" segment_no="8" tag_type="text">questions with preliminary investigations. Future work will</text>
<text top="413" left="309" width="236" height="9" font="font8" id="p3_t215" reading_order_no="214" segment_no="8" tag_type="text">investigate the scope of combining image sequences and</text>
<text top="425" left="309" width="236" height="9" font="font8" id="p3_t216" reading_order_no="215" segment_no="8" tag_type="text">3D information for an even further improved spatial under-</text>
<text top="437" left="309" width="151" height="9" font="font8" id="p3_t217" reading_order_no="216" segment_no="8" tag_type="text">standing as also explored recently in <a href="deeplearning_paper8.html#4">[</a></text>
<text top="437" left="460" width="10" height="9" font="font12" id="p3_t218" reading_order_no="217" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">13</a></text>
<text top="437" left="470" width="2" height="9" font="font8" id="p3_t219" reading_order_no="218" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#4">,</a></text>
<text top="437" left="475" width="10" height="9" font="font12" id="p3_t220" reading_order_no="219" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">28</a></text>
<text top="437" left="485" width="6" height="9" font="font8" id="p3_t221" reading_order_no="220" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#5">].</a></text>
<text top="465" left="309" width="56" height="11" font="font5" id="p3_t222" reading_order_no="221" segment_no="9" tag_type="title">References</text>
<text top="487" left="314" width="231" height="9" font="font8" id="p3_t223" reading_order_no="222" segment_no="11" tag_type="text">[1] Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad:</text>
<text top="499" left="330" width="215" height="9" font="font8" id="p3_t224" reading_order_no="223" segment_no="11" tag_type="text">Deep point cloud based retrieval for large-scale place</text>
<text top="511" left="330" width="62" height="9" font="font8" id="p3_t225" reading_order_no="224" segment_no="11" tag_type="text">recognition. In</text>
<text top="511" left="396" width="149" height="9" font="font6" id="p3_t226" reading_order_no="225" segment_no="11" tag_type="text">Proceedings of the IEEE Conference</text>
<text top="523" left="330" width="185" height="9" font="font6" id="p3_t227" reading_order_no="226" segment_no="11" tag_type="text">on Computer Vision and Pattern Recognition</text>
<text top="523" left="516" width="29" height="9" font="font8" id="p3_t228" reading_order_no="227" segment_no="11" tag_type="text">, pages</text>
<text top="535" left="330" width="72" height="9" font="font8" id="p3_t229" reading_order_no="228" segment_no="11" tag_type="text">4470–4479, 2018.</text>
<text top="535" left="406" width="5" height="9" font="font15" id="p3_t230" reading_order_no="229" segment_no="11" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="535" left="411" width="2" height="9" font="font8" id="p3_t231" reading_order_no="230" segment_no="11" tag_type="text"><a href="deeplearning_paper8.html#2">,</a></text>
<text top="535" left="416" width="5" height="9" font="font15" id="p3_t232" reading_order_no="231" segment_no="11" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="554" left="314" width="231" height="9" font="font8" id="p3_t233" reading_order_no="232" segment_no="12" tag_type="text">[2] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas</text>
<text top="566" left="330" width="215" height="9" font="font8" id="p3_t234" reading_order_no="233" segment_no="12" tag_type="text">Pajdla, and Josef Sivic. Netvlad: Cnn architecture for</text>
<text top="578" left="330" width="162" height="9" font="font8" id="p3_t235" reading_order_no="234" segment_no="12" tag_type="text">weakly supervised place recognition. In</text>
<text top="578" left="496" width="49" height="9" font="font6" id="p3_t236" reading_order_no="235" segment_no="12" tag_type="text">Proceedings</text>
<text top="590" left="330" width="215" height="9" font="font6" id="p3_t237" reading_order_no="236" segment_no="12" tag_type="text">of the IEEE Conference on Computer Vision and Pat-</text>
<text top="602" left="330" width="67" height="9" font="font6" id="p3_t238" reading_order_no="237" segment_no="12" tag_type="text">tern Recognition</text>
<text top="602" left="397" width="102" height="9" font="font8" id="p3_t239" reading_order_no="238" segment_no="12" tag_type="text">, pages 5297–5307, 2016.</text>
<text top="602" left="503" width="5" height="9" font="font15" id="p3_t240" reading_order_no="239" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="602" left="508" width="2" height="9" font="font8" id="p3_t241" reading_order_no="240" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="602" left="513" width="5" height="9" font="font15" id="p3_t242" reading_order_no="241" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="602" left="518" width="2" height="9" font="font8" id="p3_t243" reading_order_no="242" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#2">,</a></text>
<text top="602" left="523" width="5" height="9" font="font15" id="p3_t244" reading_order_no="243" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="621" left="314" width="231" height="9" font="font8" id="p3_t245" reading_order_no="244" segment_no="13" tag_type="text">[3] Roberto Arroyo, Pablo F Alcantarilla, Luis M</text>
<text top="633" left="330" width="215" height="9" font="font8" id="p3_t246" reading_order_no="245" segment_no="13" tag_type="text">Bergasa, and Eduardo Romera. Towards life-long vi-</text>
<text top="644" left="330" width="215" height="9" font="font8" id="p3_t247" reading_order_no="246" segment_no="13" tag_type="text">sual localization using an efficient matching of binary</text>
<text top="656" left="330" width="111" height="9" font="font8" id="p3_t248" reading_order_no="247" segment_no="13" tag_type="text">sequences from images. In</text>
<text top="657" left="445" width="100" height="9" font="font6" id="p3_t249" reading_order_no="248" segment_no="13" tag_type="text">2015 IEEE international</text>
<text top="668" left="330" width="187" height="9" font="font6" id="p3_t250" reading_order_no="249" segment_no="13" tag_type="text">conference on robotics and automation (ICRA)</text>
<text top="668" left="517" width="28" height="9" font="font8" id="p3_t251" reading_order_no="250" segment_no="13" tag_type="text">, pages</text>
<text top="680" left="330" width="99" height="9" font="font8" id="p3_t252" reading_order_no="251" segment_no="13" tag_type="text">6328–6335. IEEE, 2015.</text>
<text top="680" left="433" width="5" height="9" font="font15" id="p3_t253" reading_order_no="252" segment_no="13" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="710" left="295" width="5" height="9" font="font8" id="p3_t254" reading_order_no="253" segment_no="15" tag_type="text">3</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
<text top="51" left="55" width="231" height="9" font="font8" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">[4] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying</text>
<text top="63" left="72" width="215" height="9" font="font8" id="p4_t2" reading_order_no="1" segment_no="0" tag_type="text">deep local and global features for image search. In</text>
<text top="75" left="72" width="94" height="9" font="font6" id="p4_t3" reading_order_no="2" segment_no="0" tag_type="text">Eur. Conf. Comput. Vis.</text>
<text top="75" left="166" width="92" height="9" font="font8" id="p4_t4" reading_order_no="3" segment_no="0" tag_type="text">, pages 726–743, 2020.</text>
<text top="75" left="262" width="5" height="9" font="font15" id="p4_t5" reading_order_no="4" segment_no="0" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="91" left="55" width="180" height="9" font="font8" id="p4_t6" reading_order_no="5" segment_no="2" tag_type="text">[5] Marvin Chanc´an and Michael Milford.</text>
<text top="91" left="245" width="41" height="9" font="font8" id="p4_t7" reading_order_no="6" segment_no="2" tag_type="text">Deepseqs-</text>
<text top="103" left="72" width="215" height="9" font="font8" id="p4_t8" reading_order_no="7" segment_no="2" tag_type="text">lam: A trainable cnn+ rnn for joint global description</text>
<text top="115" left="72" width="154" height="9" font="font8" id="p4_t9" reading_order_no="8" segment_no="2" tag_type="text">and sequence-based place recognition.</text>
<text top="115" left="229" width="57" height="9" font="font6" id="p4_t10" reading_order_no="9" segment_no="2" tag_type="text">arXiv preprint</text>
<text top="127" left="72" width="73" height="9" font="font6" id="p4_t11" reading_order_no="10" segment_no="2" tag_type="text">arXiv:2011.08518</text>
<text top="127" left="144" width="27" height="9" font="font8" id="p4_t12" reading_order_no="11" segment_no="2" tag_type="text">, 2020.</text>
<text top="127" left="175" width="5" height="9" font="font15" id="p4_t13" reading_order_no="12" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="127" left="180" width="2" height="9" font="font8" id="p4_t14" reading_order_no="13" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="127" left="185" width="5" height="9" font="font15" id="p4_t15" reading_order_no="14" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="143" left="55" width="231" height="9" font="font8" id="p4_t16" reading_order_no="15" segment_no="4" tag_type="text">[6] Zetao Chen, Adam Jacobson, Niko S¨underhauf, Ben</text>
<text top="155" left="72" width="215" height="9" font="font8" id="p4_t17" reading_order_no="16" segment_no="4" tag_type="text">Upcroft, Lingqiao Liu, Chunhua Shen, Ian Reid, and</text>
<text top="167" left="72" width="215" height="9" font="font8" id="p4_t18" reading_order_no="17" segment_no="4" tag_type="text">Michael Milford. Deep learning features at scale for</text>
<text top="179" left="72" width="116" height="9" font="font8" id="p4_t19" reading_order_no="18" segment_no="4" tag_type="text">visual place recognition. In</text>
<text top="179" left="192" width="95" height="9" font="font6" id="p4_t20" reading_order_no="19" segment_no="4" tag_type="text">IEEE Int. Conf. Robot.</text>
<text top="191" left="72" width="28" height="9" font="font6" id="p4_t21" reading_order_no="20" segment_no="4" tag_type="text">Autom.</text>
<text top="191" left="100" width="102" height="9" font="font8" id="p4_t22" reading_order_no="21" segment_no="4" tag_type="text">, pages 3223–3230, 2017.</text>
<text top="191" left="206" width="5" height="9" font="font15" id="p4_t23" reading_order_no="22" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="207" left="55" width="231" height="9" font="font8" id="p4_t24" reading_order_no="23" segment_no="6" tag_type="text">[7] Mark Cummins and Paul Newman. Fab-map: Prob-</text>
<text top="219" left="72" width="215" height="9" font="font8" id="p4_t25" reading_order_no="24" segment_no="6" tag_type="text">abilistic localization and mapping in the space of ap-</text>
<text top="230" left="72" width="38" height="9" font="font8" id="p4_t26" reading_order_no="25" segment_no="6" tag_type="text">pearance.</text>
<text top="231" left="117" width="75" height="9" font="font6" id="p4_t27" reading_order_no="26" segment_no="6" tag_type="text">Int. J. Robot. Res.</text>
<text top="230" left="192" width="95" height="9" font="font8" id="p4_t28" reading_order_no="27" segment_no="6" tag_type="text">, 27(6):647–665, 2008.</text>
<text top="242" left="72" width="5" height="9" font="font15" id="p4_t29" reading_order_no="28" segment_no="6" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="258" left="55" width="231" height="9" font="font8" id="p4_t30" reading_order_no="29" segment_no="8" tag_type="text">[8] Andrew J Davison. Futuremapping: The computa-</text>
<text top="270" left="72" width="150" height="9" font="font8" id="p4_t31" reading_order_no="30" segment_no="8" tag_type="text">tional structure of spatial ai systems.</text>
<text top="270" left="229" width="58" height="9" font="font6" id="p4_t32" reading_order_no="31" segment_no="8" tag_type="text">arXiv preprint</text>
<text top="282" left="72" width="73" height="9" font="font6" id="p4_t33" reading_order_no="32" segment_no="8" tag_type="text">arXiv:1803.11288</text>
<text top="282" left="144" width="27" height="9" font="font8" id="p4_t34" reading_order_no="33" segment_no="8" tag_type="text">, 2018.</text>
<text top="282" left="175" width="5" height="9" font="font15" id="p4_t35" reading_order_no="34" segment_no="8" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="298" left="55" width="231" height="9" font="font8" id="p4_t36" reading_order_no="35" segment_no="10" tag_type="text">[9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Ra-</text>
<text top="310" left="72" width="215" height="9" font="font8" id="p4_t37" reading_order_no="36" segment_no="10" tag_type="text">binovich. Superpoint: Self-supervised interest point</text>
<text top="322" left="72" width="114" height="9" font="font8" id="p4_t38" reading_order_no="37" segment_no="10" tag_type="text">detection and description. In</text>
<text top="322" left="188" width="99" height="9" font="font6" id="p4_t39" reading_order_no="38" segment_no="10" tag_type="text">IEEE Conf. Comput. Vis.</text>
<text top="334" left="72" width="93" height="9" font="font6" id="p4_t40" reading_order_no="39" segment_no="10" tag_type="text">Pattern Recog. Worksh.</text>
<text top="334" left="165" width="92" height="9" font="font8" id="p4_t41" reading_order_no="40" segment_no="10" tag_type="text">, pages 224–236, 2018.</text>
<text top="334" left="261" width="5" height="9" font="font15" id="p4_t42" reading_order_no="41" segment_no="10" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="350" left="50" width="236" height="9" font="font8" id="p4_t43" reading_order_no="42" segment_no="12" tag_type="text">[10] Juan Du, Rui Wang, and Daniel Cremers. Dh3d: Deep</text>
<text top="362" left="72" width="215" height="9" font="font8" id="p4_t44" reading_order_no="43" segment_no="12" tag_type="text">hierarchical 3d descriptors for robust large-scale 6dof</text>
<text top="374" left="72" width="69" height="9" font="font8" id="p4_t45" reading_order_no="44" segment_no="12" tag_type="text">relocalization. In</text>
<text top="374" left="144" width="142" height="9" font="font6" id="p4_t46" reading_order_no="45" segment_no="12" tag_type="text">European Conference on Computer</text>
<text top="386" left="72" width="25" height="9" font="font6" id="p4_t47" reading_order_no="46" segment_no="12" tag_type="text">Vision</text>
<text top="386" left="96" width="131" height="9" font="font8" id="p4_t48" reading_order_no="47" segment_no="12" tag_type="text">, pages 744–762. Springer, 2020.</text>
<text top="386" left="231" width="5" height="9" font="font15" id="p4_t49" reading_order_no="48" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="386" left="236" width="2" height="9" font="font8" id="p4_t50" reading_order_no="49" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#2">,</a></text>
<text top="386" left="241" width="5" height="9" font="font15" id="p4_t51" reading_order_no="50" segment_no="12" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="402" left="50" width="236" height="9" font="font8" id="p4_t52" reading_order_no="51" segment_no="14" tag_type="text">[11] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc</text>
<text top="414" left="72" width="215" height="9" font="font8" id="p4_t53" reading_order_no="52" segment_no="14" tag_type="text">Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sat-</text>
<text top="426" left="72" width="215" height="9" font="font8" id="p4_t54" reading_order_no="53" segment_no="14" tag_type="text">tler. D2-net: A trainable CNN for joint description and</text>
<text top="437" left="72" width="115" height="9" font="font8" id="p4_t55" reading_order_no="54" segment_no="14" tag_type="text">detection of local features. In</text>
<text top="438" left="189" width="98" height="9" font="font6" id="p4_t56" reading_order_no="55" segment_no="14" tag_type="text">IEEE Conf. Comput. Vis.</text>
<text top="450" left="72" width="59" height="9" font="font6" id="p4_t57" reading_order_no="56" segment_no="14" tag_type="text">Pattern Recog.</text>
<text top="449" left="130" width="102" height="9" font="font8" id="p4_t58" reading_order_no="57" segment_no="14" tag_type="text">, pages 8092–8101, 2019.</text>
<text top="449" left="236" width="5" height="9" font="font15" id="p4_t59" reading_order_no="58" segment_no="14" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="465" left="50" width="236" height="9" font="font8" id="p4_t60" reading_order_no="59" segment_no="16" tag_type="text">[12] Jose M Facil, Daniel Olid, Luis Montesano, and Javier</text>
<text top="477" left="72" width="215" height="9" font="font8" id="p4_t61" reading_order_no="60" segment_no="16" tag_type="text">Civera. Condition-invariant multi-view place recogni-</text>
<text top="489" left="72" width="18" height="9" font="font8" id="p4_t62" reading_order_no="61" segment_no="16" tag_type="text">tion.</text>
<text top="489" left="93" width="132" height="9" font="font6" id="p4_t63" reading_order_no="62" segment_no="16" tag_type="text">arXiv preprint arXiv:1902.09516</text>
<text top="489" left="225" width="27" height="9" font="font8" id="p4_t64" reading_order_no="63" segment_no="16" tag_type="text">, 2019.</text>
<text top="489" left="256" width="5" height="9" font="font15" id="p4_t65" reading_order_no="64" segment_no="16" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="489" left="261" width="2" height="9" font="font8" id="p4_t66" reading_order_no="65" segment_no="16" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="489" left="266" width="5" height="9" font="font15" id="p4_t67" reading_order_no="66" segment_no="16" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="505" left="50" width="236" height="9" font="font8" id="p4_t68" reading_order_no="67" segment_no="18" tag_type="text">[13] Sourav Garg, V Babu, Thanuja Dharmasiri, Stephen</text>
<text top="517" left="72" width="215" height="9" font="font8" id="p4_t69" reading_order_no="68" segment_no="18" tag_type="text">Hausler, Niko Suenderhauf, Swagat Kumar, Tom</text>
<text top="529" left="72" width="215" height="9" font="font8" id="p4_t70" reading_order_no="69" segment_no="18" tag_type="text">Drummond, and Michael Milford. Look no deeper:</text>
<text top="541" left="72" width="215" height="9" font="font8" id="p4_t71" reading_order_no="70" segment_no="18" tag_type="text">Recognizing places from opposing viewpoints under</text>
<text top="553" left="72" width="215" height="9" font="font8" id="p4_t72" reading_order_no="71" segment_no="18" tag_type="text">varying scene appearance using single-view depth es-</text>
<text top="565" left="72" width="53" height="9" font="font8" id="p4_t73" reading_order_no="72" segment_no="18" tag_type="text">timation. In</text>
<text top="565" left="129" width="128" height="9" font="font6" id="p4_t74" reading_order_no="73" segment_no="18" tag_type="text">IEEE Int. Conf. Robot. Autom.</text>
<text top="565" left="257" width="30" height="9" font="font8" id="p4_t75" reading_order_no="74" segment_no="18" tag_type="text">, pages</text>
<text top="577" left="72" width="72" height="9" font="font8" id="p4_t76" reading_order_no="75" segment_no="18" tag_type="text">4916–4923, 2019.</text>
<text top="577" left="148" width="5" height="9" font="font15" id="p4_t77" reading_order_no="76" segment_no="18" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="593" left="50" width="236" height="9" font="font8" id="p4_t78" reading_order_no="77" segment_no="20" tag_type="text">[14] Sourav Garg, Tobias Fischer, and Michael Milford.</text>
<text top="605" left="72" width="201" height="9" font="font8" id="p4_t79" reading_order_no="78" segment_no="20" tag_type="text">Where is your place, visual place recognition? In</text>
<text top="605" left="275" width="11" height="9" font="font6" id="p4_t80" reading_order_no="79" segment_no="20" tag_type="text">IJ-</text>
<text top="617" left="72" width="16" height="9" font="font6" id="p4_t81" reading_order_no="80" segment_no="20" tag_type="text">CAI</text>
<text top="617" left="88" width="27" height="9" font="font8" id="p4_t82" reading_order_no="81" segment_no="20" tag_type="text">, 2021.</text>
<text top="617" left="119" width="5" height="9" font="font15" id="p4_t83" reading_order_no="82" segment_no="20" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="633" left="50" width="236" height="9" font="font8" id="p4_t84" reading_order_no="83" segment_no="22" tag_type="text">[15] Sourav Garg, Ben Harwood, Gaurangi Anand, and</text>
<text top="644" left="72" width="70" height="9" font="font8" id="p4_t85" reading_order_no="84" segment_no="22" tag_type="text">Michael Milford.</text>
<text top="644" left="151" width="135" height="9" font="font8" id="p4_t86" reading_order_no="85" segment_no="22" tag_type="text">Delta descriptors: Change-based</text>
<text top="656" left="72" width="215" height="9" font="font8" id="p4_t87" reading_order_no="86" segment_no="22" tag_type="text">place representation for robust visual localization.</text>
<text top="668" left="72" width="104" height="9" font="font6" id="p4_t88" reading_order_no="87" segment_no="22" tag_type="text">IEEE Robot. Autom. Lett.</text>
<text top="668" left="175" width="98" height="9" font="font8" id="p4_t89" reading_order_no="88" segment_no="22" tag_type="text">, 5(4):5120–5127, 2020.</text>
<text top="668" left="279" width="5" height="9" font="font15" id="p4_t90" reading_order_no="89" segment_no="22" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="668" left="284" width="2" height="9" font="font8" id="p4_t91" reading_order_no="90" segment_no="22" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="680" left="72" width="5" height="9" font="font15" id="p4_t92" reading_order_no="91" segment_no="22" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="680" left="77" width="2" height="9" font="font8" id="p4_t93" reading_order_no="92" segment_no="22" tag_type="text"><a href="deeplearning_paper8.html#2">,</a></text>
<text top="680" left="82" width="5" height="9" font="font15" id="p4_t94" reading_order_no="93" segment_no="22" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="51" left="309" width="236" height="9" font="font8" id="p4_t95" reading_order_no="94" segment_no="1" tag_type="text">[16] Sourav Garg and Michael Milford. SeqNet: Learn-</text>
<text top="63" left="330" width="215" height="9" font="font8" id="p4_t96" reading_order_no="95" segment_no="1" tag_type="text">ing descriptors for sequence-based hierarchical place</text>
<text top="75" left="330" width="48" height="9" font="font8" id="p4_t97" reading_order_no="96" segment_no="1" tag_type="text">recognition.</text>
<text top="75" left="382" width="102" height="9" font="font6" id="p4_t98" reading_order_no="97" segment_no="1" tag_type="text">IEEE Robot. Autom. Lett.</text>
<text top="75" left="484" width="27" height="9" font="font8" id="p4_t99" reading_order_no="98" segment_no="1" tag_type="text">, 2021.</text>
<text top="75" left="515" width="5" height="9" font="font15" id="p4_t100" reading_order_no="99" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="75" left="520" width="2" height="9" font="font8" id="p4_t101" reading_order_no="100" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="75" left="524" width="5" height="9" font="font15" id="p4_t102" reading_order_no="101" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="75" left="529" width="2" height="9" font="font8" id="p4_t103" reading_order_no="102" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#2">,</a></text>
<text top="75" left="534" width="5" height="9" font="font15" id="p4_t104" reading_order_no="103" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="91" left="309" width="236" height="9" font="font8" id="p4_t105" reading_order_no="104" segment_no="3" tag_type="text">[17] Sourav Garg, Niko S¨underhauf, Feras Dayoub, Dou-</text>
<text top="103" left="330" width="215" height="9" font="font8" id="p4_t106" reading_order_no="105" segment_no="3" tag_type="text">glas Morrison, Akansel Cosgun, Gustavo Carneiro, Qi</text>
<text top="115" left="330" width="215" height="9" font="font8" id="p4_t107" reading_order_no="106" segment_no="3" tag_type="text">Wu, Tat-Jun Chin, Ian Reid, Stephen Gould, et al. Se-</text>
<text top="127" left="330" width="215" height="9" font="font8" id="p4_t108" reading_order_no="107" segment_no="3" tag_type="text">mantics for robotic mapping, perception and interac-</text>
<text top="139" left="330" width="59" height="9" font="font8" id="p4_t109" reading_order_no="108" segment_no="3" tag_type="text">tion: A survey.</text>
<text top="139" left="392" width="150" height="9" font="font6" id="p4_t110" reading_order_no="109" segment_no="3" tag_type="text">Foundations and Trends® in Robotics</text>
<text top="139" left="543" width="2" height="9" font="font8" id="p4_t111" reading_order_no="110" segment_no="3" tag_type="text">,</text>
<text top="151" left="330" width="82" height="9" font="font8" id="p4_t112" reading_order_no="111" segment_no="3" tag_type="text">8(1–2):1–224, 2020.</text>
<text top="151" left="416" width="5" height="9" font="font15" id="p4_t113" reading_order_no="112" segment_no="3" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="167" left="309" width="236" height="9" font="font8" id="p4_t114" reading_order_no="113" segment_no="5" tag_type="text">[18] Sourav Garg, Niko S¨underhauf, and Michael Milford.</text>
<text top="179" left="330" width="215" height="9" font="font8" id="p4_t115" reading_order_no="114" segment_no="5" tag_type="text">Lost? appearance-invariant place recognition for op-</text>
<text top="191" left="330" width="164" height="9" font="font8" id="p4_t116" reading_order_no="115" segment_no="5" tag_type="text">posite viewpoints using visual semantics.</text>
<text top="191" left="498" width="47" height="9" font="font6" id="p4_t117" reading_order_no="116" segment_no="5" tag_type="text">Robot.: Sci.</text>
<text top="203" left="330" width="19" height="9" font="font6" id="p4_t118" reading_order_no="117" segment_no="5" tag_type="text">Syst.</text>
<text top="203" left="349" width="27" height="9" font="font8" id="p4_t119" reading_order_no="118" segment_no="5" tag_type="text">, 2018.</text>
<text top="203" left="380" width="5" height="9" font="font15" id="p4_t120" reading_order_no="119" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="219" left="309" width="236" height="9" font="font8" id="p4_t121" reading_order_no="120" segment_no="7" tag_type="text">[19] Le Hui, Mingmei Cheng, Jin Xie, and Jian Yang. Ef-</text>
<text top="230" left="330" width="215" height="9" font="font8" id="p4_t122" reading_order_no="121" segment_no="7" tag_type="text">ficient 3d point cloud feature learning for large-scale</text>
<text top="242" left="330" width="72" height="9" font="font8" id="p4_t123" reading_order_no="122" segment_no="7" tag_type="text">place recognition.</text>
<text top="243" left="409" width="134" height="9" font="font6" id="p4_t124" reading_order_no="123" segment_no="7" tag_type="text">arXiv preprint arXiv:2101.02374</text>
<text top="242" left="543" width="2" height="9" font="font8" id="p4_t125" reading_order_no="124" segment_no="7" tag_type="text">,</text>
<text top="254" left="330" width="22" height="9" font="font8" id="p4_t126" reading_order_no="125" segment_no="7" tag_type="text">2021.</text>
<text top="254" left="356" width="5" height="9" font="font15" id="p4_t127" reading_order_no="126" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="270" left="309" width="236" height="9" font="font8" id="p4_t128" reading_order_no="127" segment_no="9" tag_type="text">[20] Giseop Kim and Ayoung Kim. Scan context: Egocen-</text>
<text top="282" left="330" width="215" height="9" font="font8" id="p4_t129" reading_order_no="128" segment_no="9" tag_type="text">tric spatial descriptor for place recognition within 3d</text>
<text top="294" left="330" width="78" height="9" font="font8" id="p4_t130" reading_order_no="129" segment_no="9" tag_type="text">point cloud map. In</text>
<text top="294" left="411" width="134" height="9" font="font6" id="p4_t131" reading_order_no="130" segment_no="9" tag_type="text">IEEE/RSJ Int. Conf. Intell. Robot.</text>
<text top="306" left="330" width="19" height="9" font="font6" id="p4_t132" reading_order_no="131" segment_no="9" tag_type="text">Syst.</text>
<text top="306" left="349" width="129" height="9" font="font8" id="p4_t133" reading_order_no="132" segment_no="9" tag_type="text">, pages 4802–4809. IEEE, 2018.</text>
<text top="306" left="482" width="5" height="9" font="font15" id="p4_t134" reading_order_no="133" segment_no="9" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="322" left="309" width="236" height="9" font="font8" id="p4_t135" reading_order_no="134" segment_no="11" tag_type="text">[21] Zhe Liu, Shunbo Zhou, Chuanzhe Suo, Peng Yin, Wen</text>
<text top="334" left="330" width="215" height="9" font="font8" id="p4_t136" reading_order_no="135" segment_no="11" tag_type="text">Chen, Hesheng Wang, Haoang Li, and Yun-Hui Liu.</text>
<text top="346" left="330" width="215" height="9" font="font8" id="p4_t137" reading_order_no="136" segment_no="11" tag_type="text">Lpd-net: 3d point cloud learning for large-scale place</text>
<text top="358" left="330" width="163" height="9" font="font8" id="p4_t138" reading_order_no="137" segment_no="11" tag_type="text">recognition and environment analysis. In</text>
<text top="358" left="496" width="49" height="9" font="font6" id="p4_t139" reading_order_no="138" segment_no="11" tag_type="text">Proceedings</text>
<text top="370" left="330" width="215" height="9" font="font6" id="p4_t140" reading_order_no="139" segment_no="11" tag_type="text">of the IEEE/CVF International Conference on Com-</text>
<text top="382" left="330" width="48" height="9" font="font6" id="p4_t141" reading_order_no="140" segment_no="11" tag_type="text">puter Vision</text>
<text top="382" left="379" width="102" height="9" font="font8" id="p4_t142" reading_order_no="141" segment_no="11" tag_type="text">, pages 2831–2840, 2019.</text>
<text top="382" left="485" width="5" height="9" font="font15" id="p4_t143" reading_order_no="142" segment_no="11" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="398" left="309" width="236" height="9" font="font8" id="p4_t144" reading_order_no="143" segment_no="13" tag_type="text">[22] Stephanie Lowry, Niko S¨underhauf, Paul Newman,</text>
<text top="410" left="330" width="215" height="9" font="font8" id="p4_t145" reading_order_no="144" segment_no="13" tag_type="text">John J Leonard, David Cox, Peter Corke, and</text>
<text top="422" left="330" width="215" height="9" font="font8" id="p4_t146" reading_order_no="145" segment_no="13" tag_type="text">Michael J Milford. Visual place recognition: A survey.</text>
<text top="434" left="330" width="125" height="9" font="font6" id="p4_t147" reading_order_no="146" segment_no="13" tag_type="text">IEEE Transactions on Robotics</text>
<text top="434" left="456" width="77" height="9" font="font8" id="p4_t148" reading_order_no="147" segment_no="13" tag_type="text">, 32(1):1–19, 2016.</text>
<text top="434" left="536" width="5" height="9" font="font15" id="p4_t149" reading_order_no="148" segment_no="13" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="449" left="309" width="236" height="9" font="font8" id="p4_t150" reading_order_no="149" segment_no="15" tag_type="text">[23] Simon Lynen, Michael Bosse, Paul Timothy Furgale,</text>
<text top="461" left="330" width="215" height="9" font="font8" id="p4_t151" reading_order_no="150" segment_no="15" tag_type="text">and Roland Siegwart. Placeless place-recognition. In</text>
<text top="473" left="330" width="18" height="9" font="font6" id="p4_t152" reading_order_no="151" segment_no="15" tag_type="text">3DV</text>
<text top="473" left="348" width="92" height="9" font="font8" id="p4_t153" reading_order_no="152" segment_no="15" tag_type="text">, pages 303–310, 2014.</text>
<text top="473" left="444" width="5" height="9" font="font15" id="p4_t154" reading_order_no="153" segment_no="15" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="473" left="449" width="2" height="9" font="font8" id="p4_t155" reading_order_no="154" segment_no="15" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="473" left="454" width="5" height="9" font="font15" id="p4_t156" reading_order_no="155" segment_no="15" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="489" left="309" width="236" height="9" font="font8" id="p4_t157" reading_order_no="156" segment_no="17" tag_type="text">[24] Will Maddern, Geoffrey Pascoe, Chris Linegar, and</text>
<text top="501" left="330" width="215" height="9" font="font8" id="p4_t158" reading_order_no="157" segment_no="17" tag_type="text">Paul Newman. 1 year, 1000 km: The oxford robotcar</text>
<text top="513" left="330" width="30" height="9" font="font8" id="p4_t159" reading_order_no="158" segment_no="17" tag_type="text">dataset.</text>
<text top="513" left="364" width="64" height="9" font="font6" id="p4_t160" reading_order_no="159" segment_no="17" tag_type="text">IJ Robotics Res.</text>
<text top="513" left="429" width="77" height="9" font="font8" id="p4_t161" reading_order_no="160" segment_no="17" tag_type="text">, 36(1):3–15, 2017.</text>
<text top="513" left="509" width="5" height="9" font="font15" id="p4_t162" reading_order_no="161" segment_no="17" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="529" left="309" width="236" height="9" font="font8" id="p4_t163" reading_order_no="162" segment_no="19" tag_type="text">[25] Michael J Milford and Gordon F Wyeth. Seqslam:</text>
<text top="541" left="330" width="215" height="9" font="font8" id="p4_t164" reading_order_no="163" segment_no="19" tag_type="text">Visual route-based navigation for sunny summer days</text>
<text top="553" left="330" width="120" height="9" font="font8" id="p4_t165" reading_order_no="164" segment_no="19" tag_type="text">and stormy winter nights. In</text>
<text top="553" left="454" width="91" height="9" font="font6" id="p4_t166" reading_order_no="165" segment_no="19" tag_type="text">Robotics and Automa-</text>
<text top="565" left="330" width="212" height="9" font="font6" id="p4_t167" reading_order_no="166" segment_no="19" tag_type="text">tion (ICRA), 2012 IEEE International Conference on</text>
<text top="565" left="543" width="2" height="9" font="font8" id="p4_t168" reading_order_no="167" segment_no="19" tag_type="text">,</text>
<text top="577" left="330" width="124" height="9" font="font8" id="p4_t169" reading_order_no="168" segment_no="19" tag_type="text">pages 1643–1649. IEEE, 2012.</text>
<text top="577" left="458" width="5" height="9" font="font15" id="p4_t170" reading_order_no="169" segment_no="19" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="577" left="463" width="2" height="9" font="font8" id="p4_t171" reading_order_no="170" segment_no="19" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="577" left="468" width="5" height="9" font="font15" id="p4_t172" reading_order_no="171" segment_no="19" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="577" left="473" width="2" height="9" font="font8" id="p4_t173" reading_order_no="172" segment_no="19" tag_type="text"><a href="deeplearning_paper8.html#2">,</a></text>
<text top="577" left="478" width="5" height="9" font="font15" id="p4_t174" reading_order_no="173" segment_no="19" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="593" left="309" width="236" height="9" font="font8" id="p4_t175" reading_order_no="174" segment_no="21" tag_type="text">[26] Tayyab Naseer, Luciano Spinello, Wolfram Burgard,</text>
<text top="605" left="330" width="215" height="9" font="font8" id="p4_t176" reading_order_no="175" segment_no="21" tag_type="text">and Cyrill Stachniss. Robust visual robot localization</text>
<text top="617" left="330" width="154" height="9" font="font8" id="p4_t177" reading_order_no="176" segment_no="21" tag_type="text">across seasons using network flows. In</text>
<text top="617" left="487" width="58" height="9" font="font6" id="p4_t178" reading_order_no="177" segment_no="21" tag_type="text">Twenty-Eighth</text>
<text top="629" left="330" width="169" height="9" font="font6" id="p4_t179" reading_order_no="178" segment_no="21" tag_type="text">AAAI Conference on Artificial Intelligence</text>
<text top="629" left="500" width="27" height="9" font="font8" id="p4_t180" reading_order_no="179" segment_no="21" tag_type="text">, 2014.</text>
<text top="629" left="530" width="5" height="9" font="font15" id="p4_t181" reading_order_no="180" segment_no="21" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="629" left="535" width="2" height="9" font="font8" id="p4_t182" reading_order_no="181" segment_no="21" tag_type="text"><a href="deeplearning_paper8.html#1">,</a></text>
<text top="629" left="540" width="5" height="9" font="font15" id="p4_t183" reading_order_no="182" segment_no="21" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="644" left="309" width="236" height="9" font="font8" id="p4_t184" reading_order_no="183" segment_no="23" tag_type="text">[27] Peer Neubert, Stefan Schubert, and Peter Protzel. A</text>
<text top="656" left="330" width="215" height="9" font="font8" id="p4_t185" reading_order_no="184" segment_no="23" tag_type="text">neurologically inspired sequence processing model</text>
<text top="668" left="330" width="137" height="9" font="font8" id="p4_t186" reading_order_no="185" segment_no="23" tag_type="text">for mobile robot place recognition.</text>
<text top="668" left="470" width="75" height="9" font="font6" id="p4_t187" reading_order_no="186" segment_no="23" tag_type="text">IEEE Robotics and</text>
<text top="680" left="330" width="76" height="9" font="font6" id="p4_t188" reading_order_no="187" segment_no="23" tag_type="text">Automation Letters</text>
<text top="680" left="407" width="97" height="9" font="font8" id="p4_t189" reading_order_no="188" segment_no="23" tag_type="text">, 4(4):3200–3207, 2019.</text>
<text top="680" left="507" width="5" height="9" font="font15" id="p4_t190" reading_order_no="189" segment_no="23" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="710" left="295" width="5" height="9" font="font8" id="p4_t191" reading_order_no="190" segment_no="24" tag_type="text">4</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
<text top="51" left="50" width="236" height="9" font="font8" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">[28] Amadeus Oertel, Titus Cieslewski, and Davide Scara-</text>
<text top="63" left="72" width="28" height="9" font="font8" id="p5_t2" reading_order_no="1" segment_no="0" tag_type="text">muzza.</text>
<text top="63" left="110" width="176" height="9" font="font8" id="p5_t3" reading_order_no="2" segment_no="0" tag_type="text">Augmenting visual place recognition with</text>
<text top="75" left="72" width="60" height="9" font="font8" id="p5_t4" reading_order_no="3" segment_no="0" tag_type="text">structural cues.</text>
<text top="75" left="136" width="102" height="9" font="font6" id="p5_t5" reading_order_no="4" segment_no="0" tag_type="text">IEEE Robot. Autom. Lett.</text>
<text top="75" left="237" width="49" height="9" font="font8" id="p5_t6" reading_order_no="5" segment_no="0" tag_type="text">, 5(4):5534–</text>
<text top="87" left="72" width="47" height="9" font="font8" id="p5_t7" reading_order_no="6" segment_no="0" tag_type="text">5541, 2020.</text>
<text top="87" left="123" width="5" height="9" font="font15" id="p5_t8" reading_order_no="7" segment_no="0" tag_type="text"><a href="deeplearning_paper8.html#3">3</a></text>
<text top="102" left="50" width="236" height="9" font="font8" id="p5_t9" reading_order_no="8" segment_no="3" tag_type="text">[29] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum.</text>
<text top="114" left="72" width="215" height="9" font="font8" id="p5_t10" reading_order_no="9" segment_no="3" tag_type="text">Fine-tuning cnn image retrieval with no human an-</text>
<text top="126" left="72" width="35" height="9" font="font8" id="p5_t11" reading_order_no="10" segment_no="3" tag_type="text">notation.</text>
<text top="126" left="116" width="167" height="9" font="font6" id="p5_t12" reading_order_no="11" segment_no="3" tag_type="text">IEEE Trans. Pattern Anal. Mach. Intell.</text>
<text top="126" left="284" width="2" height="9" font="font8" id="p5_t13" reading_order_no="12" segment_no="3" tag_type="text">,</text>
<text top="138" left="72" width="97" height="9" font="font8" id="p5_t14" reading_order_no="13" segment_no="3" tag_type="text">41(7):1655–1668, 2018.</text>
<text top="138" left="172" width="5" height="9" font="font15" id="p5_t15" reading_order_no="14" segment_no="3" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="153" left="50" width="236" height="9" font="font8" id="p5_t16" reading_order_no="15" segment_no="4" tag_type="text">[30] Jerome Revaud, Jon Almaz´an, Rafael S Rezende, and</text>
<text top="165" left="72" width="215" height="9" font="font8" id="p5_t17" reading_order_no="16" segment_no="4" tag_type="text">Cesar Roberto de Souza. Learning with average preci-</text>
<text top="177" left="72" width="215" height="9" font="font8" id="p5_t18" reading_order_no="17" segment_no="4" tag_type="text">sion: Training image retrieval with a listwise loss. In</text>
<text top="189" left="72" width="91" height="9" font="font6" id="p5_t19" reading_order_no="18" segment_no="4" tag_type="text">Int. Conf. Comput. Vis.</text>
<text top="189" left="163" width="102" height="9" font="font8" id="p5_t20" reading_order_no="19" segment_no="4" tag_type="text">, pages 5107–5116, 2019.</text>
<text top="189" left="269" width="5" height="9" font="font15" id="p5_t21" reading_order_no="20" segment_no="4" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="204" left="50" width="236" height="9" font="font8" id="p5_t22" reading_order_no="21" segment_no="6" tag_type="text">[31] Paul-Edouard Sarlin, Frederic Debraine, Marcin</text>
<text top="216" left="72" width="215" height="9" font="font8" id="p5_t23" reading_order_no="22" segment_no="6" tag_type="text">Dymczyk, and Roland Siegwart. Leveraging deep vi-</text>
<text top="228" left="72" width="215" height="9" font="font8" id="p5_t24" reading_order_no="23" segment_no="6" tag_type="text">sual descriptors for hierarchical efficient localization.</text>
<text top="240" left="72" width="8" height="9" font="font8" id="p5_t25" reading_order_no="24" segment_no="6" tag_type="text">In</text>
<text top="240" left="85" width="130" height="9" font="font6" id="p5_t26" reading_order_no="25" segment_no="6" tag_type="text">Conference on Robot Learning</text>
<text top="240" left="214" width="72" height="9" font="font8" id="p5_t27" reading_order_no="26" segment_no="6" tag_type="text">, pages 456–465,</text>
<text top="252" left="72" width="22" height="9" font="font8" id="p5_t28" reading_order_no="27" segment_no="6" tag_type="text">2018.</text>
<text top="252" left="98" width="5" height="9" font="font15" id="p5_t29" reading_order_no="28" segment_no="6" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="266" left="50" width="236" height="9" font="font8" id="p5_t30" reading_order_no="29" segment_no="8" tag_type="text">[32] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Mal-</text>
<text top="278" left="72" width="215" height="9" font="font8" id="p5_t31" reading_order_no="30" segment_no="8" tag_type="text">isiewicz, and Andrew Rabinovich. Superglue: Learn-</text>
<text top="290" left="72" width="215" height="9" font="font8" id="p5_t32" reading_order_no="31" segment_no="8" tag_type="text">ing feature matching with graph neural networks. In</text>
<text top="51" left="330" width="215" height="9" font="font6" id="p5_t33" reading_order_no="32" segment_no="1" tag_type="text">Proceedings of the IEEE/CVF Conference on Com-</text>
<text top="63" left="330" width="155" height="9" font="font6" id="p5_t34" reading_order_no="33" segment_no="1" tag_type="text">puter Vision and Pattern Recognition</text>
<text top="63" left="486" width="59" height="9" font="font8" id="p5_t35" reading_order_no="34" segment_no="1" tag_type="text">, pages 4938–</text>
<text top="75" left="330" width="47" height="9" font="font8" id="p5_t36" reading_order_no="35" segment_no="1" tag_type="text">4947, 2020.</text>
<text top="75" left="381" width="5" height="9" font="font15" id="p5_t37" reading_order_no="36" segment_no="1" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="92" left="309" width="236" height="9" font="font8" id="p5_t38" reading_order_no="37" segment_no="2" tag_type="text">[33] Qi Sun, Hongyan Liu, Jun He, Zhaoxin Fan, and Xi-</text>
<text top="104" left="330" width="49" height="9" font="font8" id="p5_t39" reading_order_no="38" segment_no="2" tag_type="text">aoyong Du.</text>
<text top="104" left="389" width="156" height="9" font="font8" id="p5_t40" reading_order_no="39" segment_no="2" tag_type="text">Dagc: Employing dual attention and</text>
<text top="116" left="330" width="215" height="9" font="font8" id="p5_t41" reading_order_no="40" segment_no="2" tag_type="text">graph convolution for point cloud based place recogni-</text>
<text top="128" left="330" width="29" height="9" font="font8" id="p5_t42" reading_order_no="41" segment_no="2" tag_type="text">tion. In</text>
<text top="128" left="362" width="183" height="9" font="font6" id="p5_t43" reading_order_no="42" segment_no="2" tag_type="text">Proceedings of the 2020 International Confer-</text>
<text top="140" left="330" width="120" height="9" font="font6" id="p5_t44" reading_order_no="43" segment_no="2" tag_type="text">ence on Multimedia Retrieval</text>
<text top="140" left="450" width="95" height="9" font="font8" id="p5_t45" reading_order_no="44" segment_no="2" tag_type="text">, pages 224–232, 2020.</text>
<text top="152" left="330" width="5" height="9" font="font15" id="p5_t46" reading_order_no="45" segment_no="2" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="168" left="309" width="236" height="9" font="font8" id="p5_t47" reading_order_no="46" segment_no="5" tag_type="text">[34] Kavisha Vidanapathirana, Peyman Moghadam, Ben</text>
<text top="180" left="330" width="215" height="9" font="font8" id="p5_t48" reading_order_no="47" segment_no="5" tag_type="text">Harwood, Muming Zhao, Sridha Sridharan, and Clin-</text>
<text top="192" left="330" width="215" height="9" font="font8" id="p5_t49" reading_order_no="48" segment_no="5" tag_type="text">ton Fookes. Locus: Lidar-based place recognition us-</text>
<text top="204" left="330" width="174" height="9" font="font8" id="p5_t50" reading_order_no="49" segment_no="5" tag_type="text">ing spatiotemporal higher-order pooling. In</text>
<text top="204" left="507" width="38" height="9" font="font6" id="p5_t51" reading_order_no="50" segment_no="5" tag_type="text">IEEE Int.</text>
<text top="216" left="330" width="81" height="9" font="font6" id="p5_t52" reading_order_no="51" segment_no="5" tag_type="text">Conf. Robot. Autom.</text>
<text top="216" left="412" width="27" height="9" font="font8" id="p5_t53" reading_order_no="52" segment_no="5" tag_type="text">, 2021.</text>
<text top="216" left="443" width="5" height="9" font="font15" id="p5_t54" reading_order_no="53" segment_no="5" tag_type="text"><a href="deeplearning_paper8.html#2">2</a></text>
<text top="233" left="309" width="236" height="9" font="font8" id="p5_t55" reading_order_no="54" segment_no="7" tag_type="text">[35] Olga Vysotska, Tayyab Naseer, Luciano Spinello,</text>
<text top="245" left="330" width="215" height="9" font="font8" id="p5_t56" reading_order_no="55" segment_no="7" tag_type="text">Wolfram Burgard, and Cyrill Stachniss. Efficient and</text>
<text top="257" left="330" width="215" height="9" font="font8" id="p5_t57" reading_order_no="56" segment_no="7" tag_type="text">effective matching of image sequences under substan-</text>
<text top="269" left="330" width="191" height="9" font="font8" id="p5_t58" reading_order_no="57" segment_no="7" tag_type="text">tial appearance changes exploiting gps priors. In</text>
<text top="269" left="524" width="22" height="9" font="font6" id="p5_t59" reading_order_no="58" segment_no="7" tag_type="text">IEEE</text>
<text top="281" left="330" width="104" height="9" font="font6" id="p5_t60" reading_order_no="59" segment_no="7" tag_type="text">Int. Conf. Robot. Autom.</text>
<text top="280" left="434" width="111" height="9" font="font8" id="p5_t61" reading_order_no="60" segment_no="7" tag_type="text">, pages 2774–2779. IEEE,</text>
<text top="292" left="330" width="22" height="9" font="font8" id="p5_t62" reading_order_no="61" segment_no="7" tag_type="text">2015.</text>
<text top="292" left="356" width="5" height="9" font="font15" id="p5_t63" reading_order_no="62" segment_no="7" tag_type="text"><a href="deeplearning_paper8.html#1">1</a></text>
<text top="710" left="295" width="5" height="9" font="font8" id="p5_t64" reading_order_no="63" segment_no="9" tag_type="text">5</text>
</page>
</pdf2xml>
