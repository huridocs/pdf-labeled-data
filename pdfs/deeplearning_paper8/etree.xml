<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="14" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font2" size="12" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="8" family="CMSY8" color="#000000"/>
	<fontspec id="font4" size="8" family="NimbusMonL-Regu" color="#000000"/>
	<fontspec id="font5" size="12" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font6" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font7" size="10" family="NimbusMonL-ReguObli" color="#ec008b"/>
	<fontspec id="font8" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font9" size="6" family="Arial,Bold" color="#000000"/>
	<fontspec id="font10" size="5" family="ArialMT" color="#000000"/>
	<fontspec id="font11" size="5" family="Arial,Bold" color="#000000"/>
	<fontspec id="font12" size="10" family="NimbusRomNo9L-Regu" color="#00ff00"/>
	<fontspec id="font13" size="20" family="Times" color="#7f7f7f"/>
<text top="39" left="60" width="475" height="8" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">Accepted to CVPR 2021 Workshop on 3D Vision and Robotics (3DVR). https://sites.google.com/view/cvpr2021-3d-vision-robotics/</text>
<text top="83" left="74" width="448" height="13" font="font1" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="title">SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for</text>
<text top="101" left="209" width="177" height="13" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="title">Day-Night Place Recognition</text>
<text top="141" left="214" width="60" height="11" font="font2" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="text">Sourav Garg</text>
<text top="141" left="302" width="79" height="11" font="font2" id="p1_t5" reading_order_no="5" segment_no="2" tag_type="text">Michael Milford</text>
<text top="155" left="143" width="309" height="11" font="font2" id="p1_t6" reading_order_no="6" segment_no="2" tag_type="text">QUT Centre for Robotics, Queensland University of Technology</text>
<text top="171" left="211" width="171" height="8" font="font3" id="p1_t7" reading_order_no="7" segment_no="3" tag_type="text">{ s.garg, michael.milford } @qut.edu.au</text>
<text top="210" left="146" width="44" height="11" font="font5" id="p1_t8" reading_order_no="8" segment_no="4" tag_type="title">Abstract</text>
<text top="237" left="62" width="224" height="9" font="font6" id="p1_t9" reading_order_no="9" segment_no="5" tag_type="text">Place Recognition is a crucial capability for mobile</text>
<text top="249" left="50" width="236" height="9" font="font6" id="p1_t10" reading_order_no="10" segment_no="5" tag_type="text">robot localization and navigation. Image-based or Visual</text>
<text top="261" left="50" width="236" height="9" font="font6" id="p1_t11" reading_order_no="11" segment_no="5" tag_type="text">Place Recognition (VPR) is a challenging problem as scene</text>
<text top="272" left="50" width="236" height="9" font="font6" id="p1_t12" reading_order_no="12" segment_no="5" tag_type="text">appearance and camera viewpoint can change significantly</text>
<text top="284" left="50" width="236" height="9" font="font6" id="p1_t13" reading_order_no="13" segment_no="5" tag_type="text">when places are revisited. Recent VPR methods based on</text>
<text top="296" left="50" width="236" height="9" font="font6" id="p1_t14" reading_order_no="14" segment_no="5" tag_type="text">“sequential representations” have shown promising results</text>
<text top="308" left="50" width="236" height="9" font="font6" id="p1_t15" reading_order_no="15" segment_no="5" tag_type="text">as compared to traditional sequence score aggregation or</text>
<text top="320" left="50" width="236" height="9" font="font6" id="p1_t16" reading_order_no="16" segment_no="5" tag_type="text">single image based techniques. In parallel to these endeav-</text>
<text top="332" left="50" width="236" height="9" font="font6" id="p1_t17" reading_order_no="17" segment_no="5" tag_type="text">ors, 3D point clouds based place recognition is also be-</text>
<text top="344" left="50" width="236" height="9" font="font6" id="p1_t18" reading_order_no="18" segment_no="5" tag_type="text">ing explored following the advances in deep learning based</text>
<text top="356" left="50" width="236" height="9" font="font6" id="p1_t19" reading_order_no="19" segment_no="5" tag_type="text">point cloud processing. However, a key question remains: is</text>
<text top="368" left="50" width="236" height="9" font="font6" id="p1_t20" reading_order_no="20" segment_no="5" tag_type="text">an explicit 3D structure based place representation always</text>
<text top="380" left="50" width="236" height="9" font="font6" id="p1_t21" reading_order_no="21" segment_no="5" tag_type="text">superior to an implicit “spatial” representation based on</text>
<text top="392" left="50" width="236" height="9" font="font6" id="p1_t22" reading_order_no="22" segment_no="5" tag_type="text">sequence of RGB images which can inherently learn scene</text>
<text top="404" left="50" width="236" height="9" font="font6" id="p1_t23" reading_order_no="23" segment_no="5" tag_type="text">structure. In this extended abstract, we attempt to com-</text>
<text top="416" left="50" width="236" height="9" font="font6" id="p1_t24" reading_order_no="24" segment_no="5" tag_type="text">pare these two types of methods by considering a similar</text>
<text top="428" left="50" width="236" height="9" font="font6" id="p1_t25" reading_order_no="25" segment_no="5" tag_type="text">“metric span” to represent places. We compare a 3D point</text>
<text top="440" left="50" width="236" height="9" font="font6" id="p1_t26" reading_order_no="26" segment_no="5" tag_type="text">cloud based method (PointNetVLAD) with image sequence</text>
<text top="452" left="50" width="236" height="9" font="font6" id="p1_t27" reading_order_no="27" segment_no="5" tag_type="text">based methods (SeqNet and others) and showcase that im-</text>
<text top="464" left="50" width="236" height="9" font="font6" id="p1_t28" reading_order_no="28" segment_no="5" tag_type="text">age sequence based techniques approach, and can even sur-</text>
<text top="476" left="50" width="236" height="9" font="font6" id="p1_t29" reading_order_no="29" segment_no="5" tag_type="text">pass, the performance achieved by point cloud based meth-</text>
<text top="488" left="50" width="236" height="9" font="font6" id="p1_t30" reading_order_no="30" segment_no="5" tag_type="text">ods for a given metric span. These performance variations</text>
<text top="500" left="50" width="236" height="9" font="font6" id="p1_t31" reading_order_no="31" segment_no="5" tag_type="text">can be attributed to differences in data richness of input</text>
<text top="512" left="50" width="236" height="9" font="font6" id="p1_t32" reading_order_no="32" segment_no="5" tag_type="text">sensors as well as data accumulation strategies for a mo-</text>
<text top="524" left="50" width="236" height="9" font="font6" id="p1_t33" reading_order_no="33" segment_no="5" tag_type="text">bile robot. While a perfect apple-to-apple comparison may</text>
<text top="535" left="50" width="236" height="9" font="font6" id="p1_t34" reading_order_no="34" segment_no="5" tag_type="text">not be feasible for these two different modalities, the pre-</text>
<text top="547" left="50" width="236" height="9" font="font6" id="p1_t35" reading_order_no="35" segment_no="5" tag_type="text">sented comparison takes a step in the direction of answer-</text>
<text top="559" left="50" width="236" height="9" font="font6" id="p1_t36" reading_order_no="36" segment_no="5" tag_type="text">ing deeper questions regarding spatial representations, rel-</text>
<text top="571" left="50" width="236" height="9" font="font6" id="p1_t37" reading_order_no="37" segment_no="5" tag_type="text">evant to several applications like Autonomous Driving and</text>
<text top="583" left="50" width="236" height="9" font="font6" id="p1_t38" reading_order_no="38" segment_no="5" tag_type="text">Augmented/Virtual Reality. Source code available publicly:</text>
<text top="595" left="50" width="194" height="9" font="font7" id="p1_t39" reading_order_no="39" segment_no="5" tag_type="text">https://github.com/oravus/seqNet .</text>
<text top="634" left="50" width="77" height="11" font="font5" id="p1_t40" reading_order_no="40" segment_no="11" tag_type="title">1. Introduction</text>
<text top="656" left="62" width="224" height="9" font="font8" id="p1_t41" reading_order_no="41" segment_no="12" tag_type="text">Visual Place Recognition (VPR) is crucial for mobile</text>
<text top="668" left="50" width="236" height="9" font="font8" id="p1_t42" reading_order_no="42" segment_no="12" tag_type="text">robot localization and is typically challenging due to sig-<a href="https://github.com/oravus/seqNet">https://github.com/oravus/seqNet</a></text>
<text top="680" left="50" width="236" height="9" font="font8" id="p1_t43" reading_order_no="43" segment_no="12" tag_type="text">nificant changes in scene appearance and camera view-<a href="https://github.com/oravus/seqNet">.</a></text>
<text top="346" left="408" width="49" height="6" font="font9" id="p1_t44" reading_order_no="51" segment_no="6" tag_type="figure">Sequence Based</text>
<text top="353" left="415" width="33" height="6" font="font9" id="p1_t45" reading_order_no="52" segment_no="6" tag_type="figure">Descriptors</text>
<text top="325" left="471" width="49" height="6" font="font9" id="p1_t46" reading_order_no="47" segment_no="6" tag_type="figure">Sequential Score</text>
<text top="332" left="478" width="35" height="6" font="font9" id="p1_t47" reading_order_no="48" segment_no="6" tag_type="figure">Aggregation</text>
<text top="353" left="318" width="24" height="4" font="font10" id="p1_t48" reading_order_no="49" segment_no="6" tag_type="figure">Candidates<b>Sequence Based </b></text>
<text top="358" left="329" width="13" height="4" font="font10" id="p1_t49" reading_order_no="50" segment_no="6" tag_type="figure">Query<b>Descriptors</b></text>
<text top="353" left="488" width="45" height="4" font="font10" id="p1_t50" reading_order_no="53" segment_no="6" tag_type="figure">Sequential Descriptor<b>Sequential Score </b></text>
<text top="359" left="488" width="36" height="4" font="font10" id="p1_t51" reading_order_no="54" segment_no="6" tag_type="figure">Single Descriptor<b>Aggregation</b></text>
<text top="311" left="434" width="19" height="5" font="font11" id="p1_t52" reading_order_no="46" segment_no="6" tag_type="figure">SeqNet</text>
<text top="256" left="509" width="23" height="5" font="font10" id="p1_t53" reading_order_no="44" segment_no="6" tag_type="figure">Correctly</text>
<text top="262" left="508" width="23" height="5" font="font10" id="p1_t54" reading_order_no="45" segment_no="6" tag_type="figure">Localized</text>
<text top="369" left="309" width="236" height="8" font="font0" id="p1_t55" reading_order_no="55" segment_no="7" tag_type="text">Figure 1. Sequence-based hierarchical visual place recognition.</text>
<text top="380" left="309" width="236" height="8" font="font0" id="p1_t56" reading_order_no="56" segment_no="7" tag_type="text">SeqNet learns short sequential descriptors that generate high per-<b>SeqNet</b></text>
<text top="391" left="309" width="236" height="8" font="font0" id="p1_t57" reading_order_no="57" segment_no="7" tag_type="text">formance initial match candidates and enables selective control se-</text>
<text top="402" left="309" width="227" height="8" font="font0" id="p1_t58" reading_order_no="58" segment_no="7" tag_type="text">quence score aggregation using single image learnt descriptors.</text>
<text top="438" left="309" width="236" height="9" font="font8" id="p1_t59" reading_order_no="59" segment_no="8" tag_type="text">point during subsequent visits of known places [ 22 , 14 ].</text>
<text top="450" left="309" width="236" height="9" font="font8" id="p1_t60" reading_order_no="60" segment_no="8" tag_type="text">Researchers have explored a variety of methods to deal</text>
<text top="462" left="309" width="236" height="9" font="font8" id="p1_t61" reading_order_no="61" segment_no="8" tag_type="text">with this problem ranging from traditional hand-crafted</text>
<text top="474" left="309" width="236" height="9" font="font8" id="p1_t62" reading_order_no="62" segment_no="8" tag_type="text">techniques [ 7 , 25 ] to modern deep learning-based solu-</text>
<text top="486" left="309" width="73" height="9" font="font8" id="p1_t63" reading_order_no="63" segment_no="8" tag_type="text">tions [ 2 , 31 , 18 ].<a href="deeplearning_paper8.html#4">[</a></text>
<text top="486" left="392" width="154" height="9" font="font8" id="p1_t64" reading_order_no="64" segment_no="8" tag_type="text">Many of these systems aim to push<a href="deeplearning_paper8.html#4">22</a></text>
<text top="498" left="309" width="236" height="9" font="font8" id="p1_t65" reading_order_no="65" segment_no="8" tag_type="text">the performance of single image based place recognition<a href="deeplearning_paper8.html#4">,</a></text>
<text top="510" left="309" width="236" height="9" font="font8" id="p1_t66" reading_order_no="66" segment_no="8" tag_type="text">by learning better image representations as global descrip-<a href="deeplearning_paper8.html#4">14</a></text>
<text top="522" left="309" width="236" height="9" font="font8" id="p1_t67" reading_order_no="67" segment_no="8" tag_type="text">tors [ 2 , 6 , 29 , 30 ] or local descriptors [ 9 , 11 , 4 ] and match-<a href="deeplearning_paper8.html#4">].</a></text>
<text top="534" left="309" width="33" height="9" font="font8" id="p1_t68" reading_order_no="68" segment_no="8" tag_type="text">ers [ 32 ].</text>
<text top="549" left="321" width="224" height="9" font="font8" id="p1_t69" reading_order_no="69" segment_no="10" tag_type="text">To further improve the accuracy of such techniques, re-</text>
<text top="561" left="309" width="236" height="9" font="font8" id="p1_t70" reading_order_no="70" segment_no="10" tag_type="text">searchers have also explored the use of sequential infor-<a href="deeplearning_paper8.html#4">[</a></text>
<text top="573" left="309" width="236" height="9" font="font8" id="p1_t71" reading_order_no="71" segment_no="10" tag_type="text">mation inherent within the problem of mobile robot lo-<a href="deeplearning_paper8.html#4">7</a></text>
<text top="585" left="309" width="236" height="9" font="font8" id="p1_t72" reading_order_no="72" segment_no="10" tag_type="text">calization. However, most of these methods only focus<a href="deeplearning_paper8.html#4">,</a></text>
<text top="597" left="309" width="236" height="9" font="font8" id="p1_t73" reading_order_no="73" segment_no="10" tag_type="text">on robustly aggregating single image match scores along<a href="deeplearning_paper8.html#4">25</a></text>
<text top="609" left="309" width="236" height="9" font="font8" id="p1_t74" reading_order_no="74" segment_no="10" tag_type="text">a sequence [ 25 , 26 , 23 , 35 ], where single image repre-<a href="deeplearning_paper8.html#4">] </a>to modern deep learning-based solu-</text>
<text top="621" left="309" width="236" height="9" font="font8" id="p1_t75" reading_order_no="75" segment_no="10" tag_type="text">sentations are agnostic to this post sequential processing.<a href="deeplearning_paper8.html#3">[</a></text>
<text top="633" left="309" width="236" height="9" font="font8" id="p1_t76" reading_order_no="76" segment_no="10" tag_type="text">More recent methods have proposed sequential descrip-<a href="deeplearning_paper8.html#3">2</a></text>
<text top="644" left="309" width="236" height="10" font="font6" id="p1_t77" reading_order_no="77" segment_no="10" tag_type="text">tors [ 15 , 12 , 27 , 3 , 5 ] that generate place representations<a href="deeplearning_paper8.html#3">,</a></text>
<text top="656" left="309" width="236" height="10" font="font8" id="p1_t78" reading_order_no="78" segment_no="10" tag_type="text">considering sequential imagery before any sequence score<a href="deeplearning_paper8.html#5">31</a></text>
<text top="668" left="309" width="236" height="9" font="font8" id="p1_t79" reading_order_no="79" segment_no="10" tag_type="text">aggregation. [ 16 ] proposed SeqNet and a hierarchical VPR<a href="deeplearning_paper8.html#5">,</a></text>
<text top="680" left="309" width="236" height="9" font="font8" id="p1_t80" reading_order_no="80" segment_no="10" tag_type="text">pipeline where sequential descriptors are used to select<a href="deeplearning_paper8.html#4">18</a></text>
<text top="546" left="32" width="0" height="18" font="font13" id="p1_t81" reading_order_no="0" segment_no="9" tag_type="title">arXiv:2106.11481v1  [cs.CV]  22 Jun 2021<a href="deeplearning_paper8.html#4">].</a></text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font14" size="9" family="NimbusRomNo9L-Regu" color="#00ff00"/>
	<fontspec id="font15" size="10" family="NimbusRomNo9L-Regu" color="#ff0000"/>
	<fontspec id="font16" size="11" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font17" size="7" family="NimbusRomNo9L-Regu" color="#ff0000"/>
	<fontspec id="font18" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font19" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font20" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font21" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font22" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font23" size="10" family="NimbusRomNo9L-Regu" color="#3f3f3f"/>
	<fontspec id="font24" size="10" family="CMMI10" color="#3f3f3f"/>
	<fontspec id="font25" size="7" family="CMR7" color="#3f3f3f"/>
	<fontspec id="font26" size="10" family="NimbusRomNo9L-Medi" color="#3f3f3f"/>
	<fontspec id="font27" size="10" family="NimbusRomNo9L-ReguItal" color="#3f3f3f"/>
	<fontspec id="font28" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font29" size="8" family="CMR8" color="#000000"/>
<text top="139" left="50" width="236" height="8" font="font0" id="p2_t1" reading_order_no="0" segment_no="2" tag_type="text">Figure 2. (Left) Train and test splits for the Oxford Robotcar</text>
<text top="150" left="50" width="236" height="8" font="font0" id="p2_t2" reading_order_no="1" segment_no="2" tag_type="text">dataset as used in [ 1 ] and (right) an updated train split instance<a href="deeplearning_paper8.html#3">[</a></text>
<text top="161" left="50" width="209" height="8" font="font0" id="p2_t3" reading_order_no="2" segment_no="2" tag_type="text">ensuring no visual overlap between train and test imagery.<a href="deeplearning_paper8.html#3">1</a></text>
<text top="193" left="50" width="236" height="9" font="font8" id="p2_t4" reading_order_no="3" segment_no="3" tag_type="text">match hypotheses for single image based sequence score<a href="deeplearning_paper8.html#3">] </a>and (right) an updated train split instance</text>
<text top="205" left="50" width="137" height="9" font="font8" id="p2_t5" reading_order_no="4" segment_no="3" tag_type="text">aggregation, as shown in Figure 1 .</text>
<text top="217" left="62" width="224" height="9" font="font8" id="p2_t6" reading_order_no="5" segment_no="4" tag_type="text">A parallel line of research for place recognition ex-</text>
<text top="229" left="50" width="236" height="9" font="font8" id="p2_t7" reading_order_no="6" segment_no="4" tag_type="text">ists with regards to using 3D data in the form of point</text>
<text top="241" left="50" width="236" height="9" font="font8" id="p2_t8" reading_order_no="7" segment_no="4" tag_type="text">clouds, as done in PointNetVLAD [ 1 ], DH3D [ 10 ] and<a href="deeplearning_paper8.html#1">1</a></text>
<text top="253" left="50" width="237" height="9" font="font8" id="p2_t9" reading_order_no="8" segment_no="4" tag_type="text">others [ 21 , 19 , 33 , 34 , 20 ]. Instead of using single im-<a href="deeplearning_paper8.html#1">.</a></text>
<text top="265" left="50" width="236" height="9" font="font8" id="p2_t10" reading_order_no="9" segment_no="4" tag_type="text">ages or image sequences, these methods rely on point cloud</text>
<text top="277" left="50" width="236" height="9" font="font8" id="p2_t11" reading_order_no="10" segment_no="4" tag_type="text">data, typically captured through a LiDAR sensor. Using</text>
<text top="289" left="50" width="236" height="9" font="font8" id="p2_t12" reading_order_no="11" segment_no="4" tag_type="text">3D information in this fashion has significant advantages<a href="deeplearning_paper8.html#3">[</a></text>
<text top="301" left="50" width="236" height="9" font="font8" id="p2_t13" reading_order_no="12" segment_no="4" tag_type="text">when considering extreme appearance variations, for exam-<a href="deeplearning_paper8.html#3">1</a></text>
<text top="313" left="50" width="236" height="9" font="font8" id="p2_t14" reading_order_no="13" segment_no="4" tag_type="text">ple, matching data across day vs night, where single image-<a href="deeplearning_paper8.html#3">], </a>DH3D <a href="deeplearning_paper8.html#4">[</a></text>
<text top="325" left="50" width="237" height="9" font="font8" id="p2_t15" reading_order_no="14" segment_no="4" tag_type="text">based solutions fail catastrophically. [ 1 ] demonstrates this<a href="deeplearning_paper8.html#4">10</a></text>
<text top="337" left="50" width="237" height="9" font="font8" id="p2_t16" reading_order_no="15" segment_no="4" tag_type="text">behavior by comparing their method against NetVLAD [ 2 ].<a href="deeplearning_paper8.html#4">] </a>and</text>
<text top="349" left="50" width="236" height="9" font="font8" id="p2_t17" reading_order_no="16" segment_no="4" tag_type="text">However, it is not known how well image-based methods<a href="deeplearning_paper8.html#4">[</a></text>
<text top="361" left="50" width="236" height="9" font="font8" id="p2_t18" reading_order_no="17" segment_no="4" tag_type="text">compare against a 3D point cloud based technique when a<a href="deeplearning_paper8.html#4">21</a></text>
<text top="373" left="50" width="236" height="9" font="font8" id="p2_t19" reading_order_no="18" segment_no="4" tag_type="text">sequence of images is considered. In this extended abstract,<a href="deeplearning_paper8.html#4">,</a></text>
<text top="385" left="50" width="236" height="9" font="font8" id="p2_t20" reading_order_no="19" segment_no="4" tag_type="text">we conduct additional experiments with SeqNet [ 16 ], show-<a href="deeplearning_paper8.html#4">19</a></text>
<text top="397" left="50" width="236" height="9" font="font8" id="p2_t21" reading_order_no="20" segment_no="4" tag_type="text">casing that image sequences can potentially outperform 3D<a href="deeplearning_paper8.html#4">,</a></text>
<text top="409" left="50" width="236" height="9" font="font8" id="p2_t22" reading_order_no="21" segment_no="4" tag_type="text">point cloud based methods given a similar metric span and<a href="deeplearning_paper8.html#5">33</a></text>
<text top="421" left="50" width="236" height="9" font="font8" id="p2_t23" reading_order_no="22" segment_no="4" tag_type="text">localization radius. As a preliminary investigation’s late-<a href="deeplearning_paper8.html#5">,</a></text>
<text top="433" left="50" width="236" height="9" font="font8" id="p2_t24" reading_order_no="23" segment_no="4" tag_type="text">breaking result, we only consider a single dataset for this<a href="deeplearning_paper8.html#5">34</a></text>
<text top="445" left="50" width="237" height="9" font="font8" id="p2_t25" reading_order_no="24" segment_no="4" tag_type="text">analysis: Oxford Robotcar [ 24 ], which was originally used<a href="deeplearning_paper8.html#5">,</a></text>
<text top="457" left="50" width="180" height="9" font="font8" id="p2_t26" reading_order_no="25" segment_no="4" tag_type="text">by both SeqNet [ 16 ] and PointNetVLAD [ 1 ].<a href="deeplearning_paper8.html#4">20</a></text>
<text top="469" left="62" width="225" height="9" font="font8" id="p2_t27" reading_order_no="26" segment_no="6" tag_type="text">We refer the readers to the original works [ 1 , 16 ] for de-<a href="deeplearning_paper8.html#4">]. </a>Instead of using single im-</text>
<text top="481" left="50" width="236" height="9" font="font8" id="p2_t28" reading_order_no="27" segment_no="6" tag_type="text">tailed methodology description. Here, we primarily focus</text>
<text top="493" left="50" width="160" height="9" font="font8" id="p2_t29" reading_order_no="28" segment_no="6" tag_type="text">on the experimental settings and results.</text>
<text top="516" left="50" width="125" height="11" font="font5" id="p2_t30" reading_order_no="29" segment_no="7" tag_type="title">2. Experimental Settings</text>
<text top="536" left="50" width="54" height="10" font="font16" id="p2_t31" reading_order_no="30" segment_no="8" tag_type="title">2.1. Dataset</text>
<text top="555" left="62" width="224" height="9" font="font8" id="p2_t32" reading_order_no="31" segment_no="10" tag_type="text">We use two traverses from the Oxford Robotcar dataset:</text>
<text top="567" left="50" width="236" height="9" font="font8" id="p2_t33" reading_order_no="32" segment_no="10" tag_type="text">one from day time (2015-03-17-11-08-44) and the other<a href="deeplearning_paper8.html#3">[</a></text>
<text top="579" left="50" width="236" height="9" font="font8" id="p2_t34" reading_order_no="33" segment_no="10" tag_type="text">from night time (2014-12-16-18-44-24). Both these tra-<a href="deeplearning_paper8.html#3">1</a></text>
<text top="591" left="50" width="236" height="9" font="font8" id="p2_t35" reading_order_no="34" segment_no="10" tag_type="text">verses were used in the original works, however, train and<a href="deeplearning_paper8.html#3">] </a>demonstrates this</text>
<text top="603" left="50" width="236" height="9" font="font8" id="p2_t36" reading_order_no="35" segment_no="10" tag_type="text">test splits differed. Hence, we use the splits defined by<a href="deeplearning_paper8.html#3">[</a></text>
<text top="614" left="50" width="236" height="10" font="font8" id="p2_t37" reading_order_no="36" segment_no="10" tag_type="text">PointNetVLAD 1 to train and test SeqNet 2 . Note that the<a href="deeplearning_paper8.html#3">2</a></text>
<text top="627" left="50" width="236" height="9" font="font8" id="p2_t38" reading_order_no="37" segment_no="10" tag_type="text">training and test splits are captured from geographically dis-<a href="deeplearning_paper8.html#3">].</a></text>
<text top="639" left="50" width="236" height="9" font="font8" id="p2_t39" reading_order_no="38" segment_no="10" tag_type="text">parate locations and each split comprises its own reference</text>
<text top="651" left="50" width="132" height="9" font="font8" id="p2_t40" reading_order_no="39" segment_no="10" tag_type="text">(day) and query (night) database.</text>
<text top="670" left="61" width="133" height="9" font="font19" id="p2_t41" reading_order_no="99" segment_no="13" tag_type="footnote">1 https://github.com/mikacuy/pointnetvlad</text>
<text top="680" left="61" width="109" height="9" font="font19" id="p2_t42" reading_order_no="100" segment_no="14" tag_type="footnote">2 https://github.com/oravus/seqNet<a href="deeplearning_paper8.html#4">[</a></text>
<text top="48" left="309" width="236" height="8" font="font0" id="p2_t43" reading_order_no="40" segment_no="0" tag_type="title">Table 1. Performance Comparison - Oxford (Day vs Night): Re-<a href="deeplearning_paper8.html#4">16</a></text>
<text top="59" left="309" width="58" height="8" font="font0" id="p2_t44" reading_order_no="41" segment_no="0" tag_type="title">call@K (1,5,20)<a href="deeplearning_paper8.html#4">], </a>show-</text>
<text top="73" left="318" width="33" height="9" font="font20" id="p2_t45" reading_order_no="42" segment_no="1" tag_type="table">Method</text>
<text top="73" left="465" width="68" height="9" font="font8" id="p2_t46" reading_order_no="43" segment_no="1" tag_type="table">Oxford Robotcar</text>
<text top="90" left="318" width="110" height="9" font="font20" id="p2_t47" reading_order_no="44" segment_no="1" tag_type="table">Single Image Descriptors:</text>
<text top="102" left="318" width="56" height="9" font="font8" id="p2_t48" reading_order_no="45" segment_no="1" tag_type="table">NetVLAD [ 2 ]</text>
<text top="102" left="465" width="58" height="9" font="font8" id="p2_t49" reading_order_no="46" segment_no="1" tag_type="table">0.54/0.74/0.89<a href="deeplearning_paper8.html#4">[</a></text>
<text top="114" left="318" width="97" height="9" font="font8" id="p2_t50" reading_order_no="47" segment_no="1" tag_type="table">NetVLAD-FT ( S 1 [ 16 ])<a href="deeplearning_paper8.html#4">24</a></text>
<text top="114" left="465" width="58" height="9" font="font8" id="p2_t51" reading_order_no="48" segment_no="1" tag_type="table">0.62/0.83/0.94<a href="deeplearning_paper8.html#4">], </a>which was originally used</text>
<text top="126" left="318" width="102" height="9" font="font23" id="p2_t52" reading_order_no="49" segment_no="1" tag_type="table">NetVLAD-FT* ( S 1 [ 16 ])<a href="deeplearning_paper8.html#4">[</a></text>
<text top="126" left="465" width="58" height="9" font="font23" id="p2_t53" reading_order_no="50" segment_no="1" tag_type="table">0.59 / 0.78 / 0.92<a href="deeplearning_paper8.html#4">16</a></text>
<text top="143" left="318" width="110" height="9" font="font20" id="p2_t54" reading_order_no="51" segment_no="1" tag_type="table">Point Clouds Descriptors:<a href="deeplearning_paper8.html#4">] </a>and PointNetVLAD <a href="deeplearning_paper8.html#3">[</a></text>
<text top="155" left="318" width="106" height="9" font="font8" id="p2_t55" reading_order_no="52" segment_no="1" tag_type="table">PointNetVLAD (Base) [ 1 ]<a href="deeplearning_paper8.html#3">1</a></text>
<text top="155" left="465" width="58" height="9" font="font8" id="p2_t56" reading_order_no="53" segment_no="1" tag_type="table">0.77/0.92/0.96<a href="deeplearning_paper8.html#3">].</a></text>
<text top="167" left="318" width="112" height="9" font="font8" id="p2_t57" reading_order_no="54" segment_no="1" tag_type="table">PointNetVLAD (Refine) [ 1 ]<a href="deeplearning_paper8.html#3">[</a></text>
<text top="167" left="465" width="58" height="9" font="font8" id="p2_t58" reading_order_no="55" segment_no="1" tag_type="table">0.76/0.90/0.94<a href="deeplearning_paper8.html#3">1</a></text>
<text top="183" left="318" width="100" height="9" font="font20" id="p2_t59" reading_order_no="56" segment_no="1" tag_type="table">Sequential Descriptors:<a href="deeplearning_paper8.html#3">,</a></text>
<text top="196" left="318" width="92" height="9" font="font8" id="p2_t60" reading_order_no="57" segment_no="1" tag_type="table">SmoothNetVLAD [ 15 ]<a href="deeplearning_paper8.html#4">16</a></text>
<text top="196" left="465" width="58" height="9" font="font8" id="p2_t61" reading_order_no="58" segment_no="1" tag_type="table">0.66/0.75/0.87<a href="deeplearning_paper8.html#4">] </a>for de-</text>
<text top="208" left="318" width="83" height="9" font="font8" id="p2_t62" reading_order_no="59" segment_no="1" tag_type="table">DeltaNetVLAD [ 15 ]</text>
<text top="208" left="465" width="58" height="9" font="font8" id="p2_t63" reading_order_no="60" segment_no="1" tag_type="table">0.41/0.64/0.84</text>
<text top="219" left="318" width="77" height="10" font="font8" id="p2_t64" reading_order_no="61" segment_no="1" tag_type="table">SeqNetVLAD ( S 5 )</text>
<text top="219" left="465" width="58" height="10" font="font6" id="p2_t65" reading_order_no="62" segment_no="1" tag_type="table">0.87 / 0.94 / 0.99</text>
<text top="231" left="318" width="82" height="10" font="font23" id="p2_t66" reading_order_no="63" segment_no="1" tag_type="table">SeqNetVLAD* ( S 5 )</text>
<text top="231" left="465" width="58" height="10" font="font26" id="p2_t67" reading_order_no="64" segment_no="1" tag_type="table">0.85 / 0.91 / 0.98</text>
<text top="248" left="318" width="129" height="9" font="font20" id="p2_t68" reading_order_no="65" segment_no="1" tag_type="table">Sequential Score Aggregation:</text>
<text top="260" left="318" width="105" height="9" font="font8" id="p2_t69" reading_order_no="66" segment_no="1" tag_type="table">SeqMatch-NetVLAD [ 25 ]</text>
<text top="260" left="465" width="58" height="9" font="font8" id="p2_t70" reading_order_no="67" segment_no="1" tag_type="table">0.67/0.78/0.89</text>
<text top="272" left="318" width="120" height="9" font="font8" id="p2_t71" reading_order_no="68" segment_no="1" tag_type="table">SeqMatch-NetVLAD-FT [ 25 ]<a href="deeplearning_paper8.html#2">PointNetVLAD</a></text>
<text top="272" left="465" width="58" height="9" font="font8" id="p2_t72" reading_order_no="69" segment_no="1" tag_type="table">0.84/0.92/0.98<a href="deeplearning_paper8.html#2">1</a></text>
<text top="284" left="318" width="125" height="9" font="font23" id="p2_t73" reading_order_no="70" segment_no="1" tag_type="table">SeqMatch-NetVLAD-FT* [ 25 ]<a href="deeplearning_paper8.html#2">SeqNet</a></text>
<text top="284" left="465" width="58" height="9" font="font23" id="p2_t74" reading_order_no="71" segment_no="1" tag_type="table">0.79 / 0.88 / 0.96<a href="deeplearning_paper8.html#2">2</a></text>
<text top="296" left="318" width="130" height="10" font="font8" id="p2_t75" reading_order_no="72" segment_no="1" tag_type="table">SeqNetVLAD-HVPR ( S 5 to S 1 )<a href="deeplearning_paper8.html#2">. </a>Note that the</text>
<text top="296" left="465" width="58" height="9" font="font20" id="p2_t76" reading_order_no="73" segment_no="1" tag_type="table">0.88 / 0.96 / 0.99</text>
<text top="308" left="318" width="135" height="10" font="font23" id="p2_t77" reading_order_no="74" segment_no="1" tag_type="table">SeqNetVLAD-HVPR* ( S 5 to S 1 )</text>
<text top="308" left="465" width="58" height="9" font="font27" id="p2_t78" reading_order_no="75" segment_no="1" tag_type="table">0.83 / 0.93 / 0.98</text>
<text top="349" left="321" width="224" height="9" font="font8" id="p2_t79" reading_order_no="76" segment_no="5" tag_type="text">The LiDAR based 3D point cloud data in the Ox-</text>
<text top="361" left="309" width="236" height="9" font="font8" id="p2_t80" reading_order_no="77" segment_no="5" tag_type="text">ford dataset used for benchmarking PointNetVLAD [ 1 ]</text>
<text top="373" left="309" width="236" height="9" font="font8" id="p2_t81" reading_order_no="78" segment_no="5" tag_type="text">strictly captures the local surroundings. On the other hand,</text>
<text top="384" left="309" width="236" height="9" font="font8" id="p2_t82" reading_order_no="79" segment_no="5" tag_type="text">forward-facing RGB images can comprise information pro-</text>
<text top="396" left="309" width="236" height="9" font="font8" id="p2_t83" reading_order_no="80" segment_no="5" tag_type="text">jected from locations much farther away from the camera.</text>
<text top="408" left="309" width="236" height="9" font="font8" id="p2_t84" reading_order_no="81" segment_no="5" tag_type="text">Thus, the train-test split defined in [ 1 ] (see Figure 2 (left))</text>
<text top="420" left="309" width="236" height="9" font="font8" id="p2_t85" reading_order_no="82" segment_no="5" tag_type="text">potentially leads to visual overlap between both the splits</text>
<text top="432" left="309" width="236" height="9" font="font8" id="p2_t86" reading_order_no="83" segment_no="5" tag_type="text">when using image data. Therefore, keeping the test split</text>
<text top="444" left="309" width="236" height="9" font="font8" id="p2_t87" reading_order_no="84" segment_no="5" tag_type="text">the same, we additionally created an updated instance of</text>
<text top="456" left="309" width="236" height="9" font="font8" id="p2_t88" reading_order_no="85" segment_no="5" tag_type="text">the train split avoiding such visual overlap between the two<a href="deeplearning_paper8.html#3">[</a></text>
<text top="468" left="309" width="236" height="9" font="font8" id="p2_t89" reading_order_no="86" segment_no="5" tag_type="text">splits, as shown in Figure 2 (right). In Table 1 , * marked<a href="deeplearning_paper8.html#3">2</a></text>
<text top="480" left="309" width="236" height="9" font="font8" id="p2_t90" reading_order_no="87" segment_no="5" tag_type="text">results presented in gray font color correspond to the usage<a href="deeplearning_paper8.html#3">]</a></text>
<text top="492" left="309" width="236" height="9" font="font8" id="p2_t91" reading_order_no="88" segment_no="5" tag_type="text">of the revised train split. For both the split settings, the best</text>
<text top="504" left="309" width="236" height="9" font="font8" id="p2_t92" reading_order_no="89" segment_no="5" tag_type="text">(bold) and the second best (italics) results are formatted in-</text>
<text top="516" left="309" width="93" height="9" font="font8" id="p2_t93" reading_order_no="90" segment_no="5" tag_type="text">dependently in Table 1 .</text>
<text top="542" left="309" width="77" height="10" font="font16" id="p2_t94" reading_order_no="91" segment_no="9" tag_type="title">2.2. Metric Span</text>
<text top="563" left="321" width="224" height="9" font="font8" id="p2_t95" reading_order_no="92" segment_no="11" tag_type="text">For PointNetVLAD, we use the authors’ provided im-<a href="deeplearning_paper8.html#4">[</a></text>
<text top="575" left="309" width="236" height="9" font="font8" id="p2_t96" reading_order_no="93" segment_no="11" tag_type="text">plementation for extracting point clouds and corresponding<a href="deeplearning_paper8.html#4">16</a></text>
<text top="586" left="309" width="236" height="10" font="font8" id="p2_t97" reading_order_no="94" segment_no="11" tag_type="text">place representations, which have a metric span of 20 me-<a href="deeplearning_paper8.html#4">])</a></text>
<text top="599" left="309" width="236" height="9" font="font8" id="p2_t98" reading_order_no="95" segment_no="11" tag_type="text">ters per place. For SeqNet and other sequence based meth-</text>
<text top="610" left="309" width="236" height="10" font="font8" id="p2_t99" reading_order_no="96" segment_no="11" tag_type="text">ods, we use an image sequence of length 5 with a fixed</text>
<text top="622" left="309" width="236" height="10" font="font8" id="p2_t100" reading_order_no="97" segment_no="11" tag_type="text">frame separation of 2 meters between adjacent frames, lead-</text>
<text top="633" left="309" width="175" height="10" font="font8" id="p2_t101" reading_order_no="98" segment_no="11" tag_type="text">ing to a metric span of 10 meters per place 3 .</text>
<text top="661" left="320" width="225" height="9" font="font19" id="p2_t102" reading_order_no="101" segment_no="12" tag_type="footnote">3 Since SeqNet results with a metric span of 10 meters were found to be<a href="deeplearning_paper8.html#4">[</a></text>
<text top="672" left="309" width="236" height="7" font="font19" id="p2_t103" reading_order_no="102" segment_no="12" tag_type="footnote">superior to PointNetVLAD, we did not conduct further experiments with a<a href="deeplearning_paper8.html#4">16</a></text>
<text top="682" left="309" width="96" height="7" font="font19" id="p2_t104" reading_order_no="103" segment_no="12" tag_type="footnote">larger metric span for SeqNet.<a href="deeplearning_paper8.html#4">])</a></text>
<text top="710" left="295" width="5" height="9" font="font8" id="p2_t105" reading_order_no="104" segment_no="15" tag_type="text">2</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font30" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font31" size="10" family="CMR10" color="#3f3f3f"/>
<text top="50" left="50" width="108" height="10" font="font16" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="title">2.3. Descriptors Details</text>
<text top="69" left="62" width="225" height="10" font="font8" id="p3_t2" reading_order_no="1" segment_no="2" tag_type="text">PointNetVLAD descriptors are of size 256 as the au-</text>
<text top="82" left="50" width="236" height="9" font="font8" id="p3_t3" reading_order_no="2" segment_no="2" tag_type="text">thors observed no notable performance gain with further</text>
<text top="94" left="50" width="236" height="9" font="font8" id="p3_t4" reading_order_no="3" segment_no="2" tag_type="text">doubling of descriptor dimensions [ 1 ]. For SeqNet, 4096-</text>
<text top="105" left="50" width="236" height="9" font="font8" id="p3_t5" reading_order_no="4" segment_no="2" tag_type="text">dimensional PCA’d NetVLAD descriptors were used as the</text>
<text top="117" left="50" width="236" height="9" font="font8" id="p3_t6" reading_order_no="5" segment_no="2" tag_type="text">underlying single image representations and output was a<a href="deeplearning_paper8.html#3">[</a></text>
<text top="129" left="50" width="236" height="9" font="font8" id="p3_t7" reading_order_no="6" segment_no="2" tag_type="text">4096-dimensional sequential descriptor, as per the original<a href="deeplearning_paper8.html#3">1</a></text>
<text top="141" left="50" width="236" height="9" font="font8" id="p3_t8" reading_order_no="7" segment_no="2" tag_type="text">setting [ 16 ]. From here on, we refer to this SeqNet descrip-<a href="deeplearning_paper8.html#3">]. </a>For SeqNet, 4096-</text>
<text top="153" left="50" width="162" height="9" font="font8" id="p3_t9" reading_order_no="8" segment_no="2" tag_type="text">tor based on NetVLAD as SeqNetVLAD .</text>
<text top="174" left="50" width="70" height="10" font="font16" id="p3_t10" reading_order_no="9" segment_no="3" tag_type="title">2.4. Evaluation</text>
<text top="192" left="62" width="225" height="10" font="font8" id="p3_t11" reading_order_no="10" segment_no="4" tag_type="text">We use Recall@ K ( K ∈ { 1 , 5 , 20 } ) as the evaluation</text>
<text top="205" left="50" width="236" height="9" font="font8" id="p3_t12" reading_order_no="11" segment_no="4" tag_type="text">metric as also used by both PointNetVLAD and SeqNet.<a href="deeplearning_paper8.html#4">[</a></text>
<text top="217" left="50" width="236" height="9" font="font8" id="p3_t13" reading_order_no="12" segment_no="4" tag_type="text">Localization radius is set to be 25 meters as used by Point-<a href="deeplearning_paper8.html#4">16</a></text>
<text top="229" left="50" width="45" height="9" font="font8" id="p3_t14" reading_order_no="13" segment_no="4" tag_type="text">NetVLAD.<a href="deeplearning_paper8.html#4">]. </a>From here on, we refer to this SeqNet descrip-</text>
<text top="252" left="50" width="49" height="11" font="font5" id="p3_t15" reading_order_no="14" segment_no="5" tag_type="title">3. Results</text>
<text top="273" left="62" width="225" height="9" font="font8" id="p3_t16" reading_order_no="15" segment_no="7" tag_type="text">Table 1 shows performance comparison between differ-</text>
<text top="285" left="50" width="236" height="9" font="font8" id="p3_t17" reading_order_no="16" segment_no="7" tag_type="text">ent types of approaches to place recognition: 1) Single</text>
<text top="297" left="50" width="236" height="9" font="font8" id="p3_t18" reading_order_no="17" segment_no="7" tag_type="text">Image Descriptors including NetVLAD [ 2 ] and its night-</text>
<text top="309" left="50" width="236" height="9" font="font8" id="p3_t19" reading_order_no="18" segment_no="7" tag_type="text">time fine-tuned version NetVLAD-FT (trained as S 1 , as de-</text>
<text top="321" left="50" width="236" height="9" font="font8" id="p3_t20" reading_order_no="19" segment_no="7" tag_type="text">scribed in [ 16 ]); 2) Point Cloud Descriptors including Point-</text>
<text top="333" left="50" width="236" height="9" font="font8" id="p3_t21" reading_order_no="20" segment_no="7" tag_type="text">NetVLAD with both its base version trained only on the</text>
<text top="345" left="50" width="236" height="9" font="font8" id="p3_t22" reading_order_no="21" segment_no="7" tag_type="text">Oxford Robotcar dataset and refine version trained on multi-</text>
<text top="357" left="50" width="237" height="9" font="font8" id="p3_t23" reading_order_no="22" segment_no="7" tag_type="text">ple datasets; 3) Sequential Descriptors including Smoothed</text>
<text top="369" left="50" width="236" height="9" font="font8" id="p3_t24" reading_order_no="23" segment_no="7" tag_type="text">and Delta Descriptor defined using NetVLAD, as described</text>
<text top="381" left="50" width="236" height="9" font="font8" id="p3_t25" reading_order_no="24" segment_no="7" tag_type="text">in [ 15 ] and SeqNetVLAD [ 16 ]; and 4) Sequential Score Ag-</text>
<text top="393" left="50" width="236" height="9" font="font8" id="p3_t26" reading_order_no="25" segment_no="7" tag_type="text">gregation including SeqSLAM-based [ 25 ] sequence match-</text>
<text top="405" left="50" width="236" height="9" font="font8" id="p3_t27" reading_order_no="26" segment_no="7" tag_type="text">ing defined on single image descriptors using NetVLAD</text>
<text top="417" left="50" width="236" height="9" font="font8" id="p3_t28" reading_order_no="27" segment_no="7" tag_type="text">and NetVLAD-FT, referred to as SeqMatch-NetVLAD and</text>
<text top="428" left="50" width="236" height="9" font="font8" id="p3_t29" reading_order_no="28" segment_no="7" tag_type="text">SeqMatch-NetVLAD-FT respectively, and a hierarchical</text>
<text top="440" left="50" width="237" height="9" font="font8" id="p3_t30" reading_order_no="29" segment_no="7" tag_type="text">approach as per [ 16 ], referred to as SeqNetVLAD-HVPR,</text>
<text top="452" left="50" width="236" height="9" font="font8" id="p3_t31" reading_order_no="30" segment_no="7" tag_type="text">where SeqNetVLAD is used as a sequential descriptor to se-</text>
<text top="464" left="50" width="233" height="9" font="font8" id="p3_t32" reading_order_no="31" segment_no="7" tag_type="text">lect top matching candidates for SeqMatch-NetVLAD-FT.</text>
<text top="477" left="62" width="224" height="9" font="font8" id="p3_t33" reading_order_no="32" segment_no="10" tag_type="text">It can be observed from Table 1 that sequence-</text>
<text top="488" left="50" width="236" height="10" font="font8" id="p3_t34" reading_order_no="33" segment_no="10" tag_type="text">based methods like SmoothNetVLAD ( 0 . 66 ), SeqMatch-</text>
<text top="500" left="50" width="237" height="10" font="font8" id="p3_t35" reading_order_no="34" segment_no="10" tag_type="text">NetVLAD ( 0 . 67 ) improve performance on top of single</text>
<text top="512" left="50" width="237" height="10" font="font8" id="p3_t36" reading_order_no="35" segment_no="10" tag_type="text">image only techniques NetVLAD ( 0 . 54 ) and NetVLAD-<a href="deeplearning_paper8.html#2">1</a></text>
<text top="524" left="50" width="236" height="10" font="font8" id="p3_t37" reading_order_no="36" segment_no="10" tag_type="text">FT ( 0 . 59 / 0 . 62 ) but do not approach performance of Point-</text>
<text top="536" left="50" width="76" height="9" font="font8" id="p3_t38" reading_order_no="37" segment_no="10" tag_type="text">NetVLAD ( 0 . 77 ).</text>
<text top="536" left="140" width="146" height="9" font="font8" id="p3_t39" reading_order_no="38" segment_no="10" tag_type="text">However, SeqMatch-NetVLAD-FT</text>
<text top="548" left="50" width="236" height="9" font="font8" id="p3_t40" reading_order_no="39" segment_no="10" tag_type="text">( 0 . 79 / 0 . 84 ), SeqNetVLAD ( 0 . 83 / 0 . 87 ) and SeqNetVLAD-</text>
<text top="560" left="50" width="236" height="9" font="font8" id="p3_t41" reading_order_no="40" segment_no="10" tag_type="text">HVPR ( 0 . 85 / 0 . 88 ) surpass PointNetVLAD’s performance.<a href="deeplearning_paper8.html#3">[</a></text>
<text top="572" left="50" width="236" height="9" font="font8" id="p3_t42" reading_order_no="41" segment_no="10" tag_type="text">This demonstrates that not only the sequential informa-<a href="deeplearning_paper8.html#3">2</a></text>
<text top="584" left="50" width="236" height="9" font="font8" id="p3_t43" reading_order_no="42" segment_no="10" tag_type="text">tion is a strong cue for place recognition under challenging<a href="deeplearning_paper8.html#3">] </a>and its night-</text>
<text top="596" left="50" width="236" height="9" font="font8" id="p3_t44" reading_order_no="43" segment_no="10" tag_type="text">appearance conditions, trained sequential descriptors [ 16 ]</text>
<text top="608" left="50" width="236" height="9" font="font8" id="p3_t45" reading_order_no="44" segment_no="10" tag_type="text">might be learning the underlying 3D scene structure implic-</text>
<text top="620" left="50" width="236" height="9" font="font8" id="p3_t46" reading_order_no="45" segment_no="10" tag_type="text">itly, leading to better performance than what was achievable</text>
<text top="632" left="50" width="198" height="9" font="font8" id="p3_t47" reading_order_no="46" segment_no="10" tag_type="text">through traditional sequence-based methods [ 25 ].</text>
<text top="644" left="62" width="224" height="9" font="font8" id="p3_t48" reading_order_no="47" segment_no="14" tag_type="text">The experiments conducted in this preliminary investiga-<a href="deeplearning_paper8.html#4">[</a></text>
<text top="656" left="50" width="236" height="9" font="font8" id="p3_t49" reading_order_no="48" segment_no="14" tag_type="text">tion have their limitations as a perfect apple-to-apple com-<a href="deeplearning_paper8.html#4">16</a></text>
<text top="668" left="50" width="236" height="9" font="font8" id="p3_t50" reading_order_no="49" segment_no="14" tag_type="text">parison between RGB image sequence and LiDAR point<a href="deeplearning_paper8.html#4">]);</a></text>
<text top="680" left="50" width="236" height="9" font="font8" id="p3_t51" reading_order_no="50" segment_no="14" tag_type="text">clouds is not trivial. Both the sensor modalities have com-</text>
<text top="51" left="309" width="236" height="9" font="font8" id="p3_t52" reading_order_no="51" segment_no="1" tag_type="text">plementary characteristics. As compared to RGB cameras,</text>
<text top="63" left="309" width="236" height="9" font="font8" id="p3_t53" reading_order_no="52" segment_no="1" tag_type="text">active range sensors like LiDARs are not drastically af-</text>
<text top="75" left="309" width="236" height="9" font="font8" id="p3_t54" reading_order_no="53" segment_no="1" tag_type="text">fected by variations in environmental conditions such as</text>
<text top="87" left="309" width="236" height="9" font="font8" id="p3_t55" reading_order_no="54" segment_no="1" tag_type="text">time of day and seasonal cycles. However, the inherent in-</text>
<text top="99" left="309" width="236" height="9" font="font8" id="p3_t56" reading_order_no="55" segment_no="1" tag_type="text">formation richness of RGB image sensors, compared to Li-</text>
<text top="111" left="309" width="236" height="9" font="font8" id="p3_t57" reading_order_no="56" segment_no="1" tag_type="text">DAR point clouds which are typically sparse, makes room</text>
<text top="123" left="309" width="236" height="9" font="font8" id="p3_t58" reading_order_no="57" segment_no="1" tag_type="text">for advanced image processing techniques, potentially lead-</text>
<text top="135" left="309" width="236" height="9" font="font8" id="p3_t59" reading_order_no="58" segment_no="1" tag_type="text">ing to improved performance even under challenging en-</text>
<text top="147" left="309" width="97" height="9" font="font8" id="p3_t60" reading_order_no="59" segment_no="1" tag_type="text">vironmental conditions.</text>
<text top="147" left="415" width="130" height="9" font="font8" id="p3_t61" reading_order_no="60" segment_no="1" tag_type="text">Furthermore, as a robot moves</text>
<text top="159" left="309" width="236" height="9" font="font8" id="p3_t62" reading_order_no="61" segment_no="1" tag_type="text">through an environment, the data accumulation strategy also</text>
<text top="171" left="309" width="236" height="9" font="font8" id="p3_t63" reading_order_no="62" segment_no="1" tag_type="text">plays a key role in determining the robustness of a place<a href="deeplearning_paper8.html#4">[</a></text>
<text top="183" left="309" width="236" height="9" font="font8" id="p3_t64" reading_order_no="63" segment_no="1" tag_type="text">representation. For example, the strategy of feeding single<a href="deeplearning_paper8.html#4">15</a></text>
<text top="195" left="309" width="236" height="9" font="font8" id="p3_t65" reading_order_no="64" segment_no="1" tag_type="text">images to sequence score aggregation methods [ 25 , 26 , 23 ]<a href="deeplearning_paper8.html#4">] </a>and SeqNetVLAD <a href="deeplearning_paper8.html#4">[</a></text>
<text top="207" left="309" width="236" height="9" font="font8" id="p3_t66" reading_order_no="65" segment_no="1" tag_type="text">or sequential descriptor networks [ 16 , 12 , 5 ] can also be em-<a href="deeplearning_paper8.html#4">16</a></text>
<text top="219" left="309" width="236" height="9" font="font8" id="p3_t67" reading_order_no="66" segment_no="1" tag_type="text">ulated for point cloud based techniques that currently pre-<a href="deeplearning_paper8.html#4">]; </a>and</text>
<text top="231" left="309" width="236" height="9" font="font6" id="p3_t68" reading_order_no="67" segment_no="1" tag_type="text">process individual point clouds to form a relatively larger</text>
<text top="243" left="309" width="212" height="9" font="font8" id="p3_t69" reading_order_no="68" segment_no="1" tag_type="text">one [ 1 , 10 ] before learning any place representations.</text>
<text top="271" left="309" width="69" height="11" font="font5" id="p3_t70" reading_order_no="69" segment_no="6" tag_type="title">4. Conclusion<a href="deeplearning_paper8.html#4">[</a></text>
<text top="293" left="321" width="224" height="9" font="font8" id="p3_t71" reading_order_no="70" segment_no="8" tag_type="text">With recent advances in deep learning, several novel<a href="deeplearning_paper8.html#4">25</a></text>
<text top="305" left="309" width="236" height="9" font="font8" id="p3_t72" reading_order_no="71" segment_no="8" tag_type="text">methods have been developed for place (spatial) representa-<a href="deeplearning_paper8.html#4">] </a>sequence match-</text>
<text top="317" left="309" width="236" height="9" font="font8" id="p3_t73" reading_order_no="72" segment_no="8" tag_type="text">tions including both 3D point cloud based [ 1 , 10 ] and those</text>
<text top="329" left="309" width="236" height="9" font="font8" id="p3_t74" reading_order_no="73" segment_no="8" tag_type="text">based on image sequences [ 16 , 12 , 15 ]. Both these modal-</text>
<text top="341" left="309" width="236" height="9" font="font8" id="p3_t75" reading_order_no="74" segment_no="8" tag_type="text">ities have their own inherent characteristics and there re-</text>
<text top="353" left="309" width="236" height="9" font="font8" id="p3_t76" reading_order_no="75" segment_no="8" tag_type="text">main several questions unanswered in terms of what might<a href="deeplearning_paper8.html#4">[</a></text>
<text top="365" left="309" width="236" height="9" font="font8" id="p3_t77" reading_order_no="76" segment_no="8" tag_type="text">constitute an ideal representation of the world perceived by<a href="deeplearning_paper8.html#4">16</a></text>
<text top="377" left="309" width="236" height="9" font="font8" id="p3_t78" reading_order_no="77" segment_no="8" tag_type="text">a mobile robot [ 8 , 17 ]. The analysis presented in this ex-<a href="deeplearning_paper8.html#4">], </a>referred to as SeqNetVLAD-HVPR,</text>
<text top="389" left="309" width="236" height="9" font="font8" id="p3_t79" reading_order_no="78" segment_no="8" tag_type="text">tended abstract takes an initial step towards answering such</text>
<text top="401" left="309" width="236" height="9" font="font8" id="p3_t80" reading_order_no="79" segment_no="8" tag_type="text">questions with preliminary investigations. Future work will</text>
<text top="413" left="309" width="236" height="9" font="font8" id="p3_t81" reading_order_no="80" segment_no="8" tag_type="text">investigate the scope of combining image sequences and</text>
<text top="425" left="309" width="236" height="9" font="font8" id="p3_t82" reading_order_no="81" segment_no="8" tag_type="text">3D information for an even further improved spatial under-<a href="deeplearning_paper8.html#2">1</a></text>
<text top="437" left="309" width="182" height="9" font="font8" id="p3_t83" reading_order_no="82" segment_no="8" tag_type="text">standing as also explored recently in [ 13 , 28 ].</text>
<text top="465" left="309" width="56" height="11" font="font5" id="p3_t84" reading_order_no="83" segment_no="9" tag_type="title">References</text>
<text top="487" left="314" width="231" height="9" font="font8" id="p3_t85" reading_order_no="84" segment_no="11" tag_type="text">[1] Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad:</text>
<text top="499" left="330" width="215" height="9" font="font8" id="p3_t86" reading_order_no="85" segment_no="11" tag_type="text">Deep point cloud based retrieval for large-scale place</text>
<text top="511" left="330" width="215" height="9" font="font8" id="p3_t87" reading_order_no="86" segment_no="11" tag_type="text">recognition. In Proceedings of the IEEE Conference</text>
<text top="523" left="330" width="215" height="9" font="font6" id="p3_t88" reading_order_no="87" segment_no="11" tag_type="text">on Computer Vision and Pattern Recognition , pages</text>
<text top="535" left="330" width="91" height="9" font="font8" id="p3_t89" reading_order_no="88" segment_no="11" tag_type="text">4470–4479, 2018. 2 , 3</text>
<text top="554" left="314" width="231" height="9" font="font8" id="p3_t90" reading_order_no="89" segment_no="12" tag_type="text">[2] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas</text>
<text top="566" left="330" width="215" height="9" font="font8" id="p3_t91" reading_order_no="90" segment_no="12" tag_type="text">Pajdla, and Josef Sivic. Netvlad: Cnn architecture for</text>
<text top="578" left="330" width="215" height="9" font="font8" id="p3_t92" reading_order_no="91" segment_no="12" tag_type="text">weakly supervised place recognition. In Proceedings</text>
<text top="590" left="330" width="215" height="9" font="font6" id="p3_t93" reading_order_no="92" segment_no="12" tag_type="text">of the IEEE Conference on Computer Vision and Pat-</text>
<text top="602" left="330" width="198" height="9" font="font6" id="p3_t94" reading_order_no="93" segment_no="12" tag_type="text">tern Recognition , pages 5297–5307, 2016. 1 , 2 , 3</text>
<text top="621" left="314" width="231" height="9" font="font8" id="p3_t95" reading_order_no="94" segment_no="13" tag_type="text">[3] Roberto Arroyo, Pablo F Alcantarilla, Luis M</text>
<text top="633" left="330" width="215" height="9" font="font8" id="p3_t96" reading_order_no="95" segment_no="13" tag_type="text">Bergasa, and Eduardo Romera. Towards life-long vi-</text>
<text top="644" left="330" width="215" height="9" font="font8" id="p3_t97" reading_order_no="96" segment_no="13" tag_type="text">sual localization using an efficient matching of binary</text>
<text top="656" left="330" width="215" height="10" font="font8" id="p3_t98" reading_order_no="97" segment_no="13" tag_type="text">sequences from images. In 2015 IEEE international</text>
<text top="668" left="330" width="215" height="9" font="font6" id="p3_t99" reading_order_no="98" segment_no="13" tag_type="text">conference on robotics and automation (ICRA) , pages</text>
<text top="680" left="330" width="108" height="9" font="font8" id="p3_t100" reading_order_no="99" segment_no="13" tag_type="text">6328–6335. IEEE, 2015. 1</text>
<text top="710" left="295" width="5" height="9" font="font8" id="p3_t101" reading_order_no="100" segment_no="15" tag_type="text">3</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
<text top="51" left="55" width="231" height="9" font="font8" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">[4] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying</text>
<text top="63" left="72" width="215" height="9" font="font8" id="p4_t2" reading_order_no="1" segment_no="0" tag_type="text">deep local and global features for image search. In</text>
<text top="75" left="72" width="195" height="9" font="font6" id="p4_t3" reading_order_no="2" segment_no="0" tag_type="text">Eur. Conf. Comput. Vis. , pages 726–743, 2020. 1</text>
<text top="91" left="55" width="180" height="9" font="font8" id="p4_t4" reading_order_no="3" segment_no="2" tag_type="text">[5] Marvin Chanc´an and Michael Milford.</text>
<text top="91" left="245" width="41" height="9" font="font8" id="p4_t5" reading_order_no="4" segment_no="2" tag_type="text">Deepseqs-<a href="deeplearning_paper8.html#1">1</a></text>
<text top="103" left="72" width="215" height="9" font="font8" id="p4_t6" reading_order_no="5" segment_no="2" tag_type="text">lam: A trainable cnn+ rnn for joint global description</text>
<text top="115" left="72" width="214" height="9" font="font8" id="p4_t7" reading_order_no="6" segment_no="2" tag_type="text">and sequence-based place recognition. arXiv preprint</text>
<text top="127" left="72" width="118" height="9" font="font6" id="p4_t8" reading_order_no="7" segment_no="2" tag_type="text">arXiv:2011.08518 , 2020. 1 , 3</text>
<text top="143" left="55" width="231" height="9" font="font8" id="p4_t9" reading_order_no="8" segment_no="4" tag_type="text">[6] Zetao Chen, Adam Jacobson, Niko S¨underhauf, Ben</text>
<text top="155" left="72" width="215" height="9" font="font8" id="p4_t10" reading_order_no="9" segment_no="4" tag_type="text">Upcroft, Lingqiao Liu, Chunhua Shen, Ian Reid, and</text>
<text top="167" left="72" width="215" height="9" font="font8" id="p4_t11" reading_order_no="10" segment_no="4" tag_type="text">Michael Milford. Deep learning features at scale for</text>
<text top="179" left="72" width="215" height="9" font="font8" id="p4_t12" reading_order_no="11" segment_no="4" tag_type="text">visual place recognition. In IEEE Int. Conf. Robot.</text>
<text top="191" left="72" width="139" height="9" font="font6" id="p4_t13" reading_order_no="12" segment_no="4" tag_type="text">Autom. , pages 3223–3230, 2017. 1<a href="deeplearning_paper8.html#1">1</a></text>
<text top="207" left="55" width="231" height="9" font="font8" id="p4_t14" reading_order_no="13" segment_no="6" tag_type="text">[7] Mark Cummins and Paul Newman. Fab-map: Prob-<a href="deeplearning_paper8.html#1">,</a></text>
<text top="219" left="72" width="215" height="9" font="font8" id="p4_t15" reading_order_no="14" segment_no="6" tag_type="text">abilistic localization and mapping in the space of ap-<a href="deeplearning_paper8.html#3">3</a></text>
<text top="230" left="72" width="215" height="10" font="font8" id="p4_t16" reading_order_no="15" segment_no="6" tag_type="text">pearance. Int. J. Robot. Res. , 27(6):647–665, 2008.</text>
<text top="242" left="72" width="5" height="9" font="font15" id="p4_t17" reading_order_no="16" segment_no="6" tag_type="text">1</text>
<text top="258" left="55" width="231" height="9" font="font8" id="p4_t18" reading_order_no="17" segment_no="8" tag_type="text">[8] Andrew J Davison. Futuremapping: The computa-</text>
<text top="270" left="72" width="215" height="9" font="font8" id="p4_t19" reading_order_no="18" segment_no="8" tag_type="text">tional structure of spatial ai systems. arXiv preprint</text>
<text top="282" left="72" width="108" height="9" font="font6" id="p4_t20" reading_order_no="19" segment_no="8" tag_type="text">arXiv:1803.11288 , 2018. 3</text>
<text top="298" left="55" width="231" height="9" font="font8" id="p4_t21" reading_order_no="20" segment_no="10" tag_type="text">[9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Ra-</text>
<text top="310" left="72" width="215" height="9" font="font8" id="p4_t22" reading_order_no="21" segment_no="10" tag_type="text">binovich. Superpoint: Self-supervised interest point</text>
<text top="322" left="72" width="215" height="9" font="font8" id="p4_t23" reading_order_no="22" segment_no="10" tag_type="text">detection and description. In IEEE Conf. Comput. Vis.<a href="deeplearning_paper8.html#1">1</a></text>
<text top="334" left="72" width="194" height="9" font="font6" id="p4_t24" reading_order_no="23" segment_no="10" tag_type="text">Pattern Recog. Worksh. , pages 224–236, 2018. 1</text>
<text top="350" left="50" width="236" height="9" font="font8" id="p4_t25" reading_order_no="24" segment_no="12" tag_type="text">[10] Juan Du, Rui Wang, and Daniel Cremers. Dh3d: Deep</text>
<text top="362" left="72" width="215" height="9" font="font8" id="p4_t26" reading_order_no="25" segment_no="12" tag_type="text">hierarchical 3d descriptors for robust large-scale 6dof</text>
<text top="374" left="72" width="214" height="9" font="font8" id="p4_t27" reading_order_no="26" segment_no="12" tag_type="text">relocalization. In European Conference on Computer</text>
<text top="386" left="72" width="174" height="9" font="font6" id="p4_t28" reading_order_no="27" segment_no="12" tag_type="text">Vision , pages 744–762. Springer, 2020. 2 , 3</text>
<text top="402" left="50" width="236" height="9" font="font8" id="p4_t29" reading_order_no="28" segment_no="14" tag_type="text">[11] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc<a href="deeplearning_paper8.html#1">1</a></text>
<text top="414" left="72" width="215" height="9" font="font8" id="p4_t30" reading_order_no="29" segment_no="14" tag_type="text">Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sat-</text>
<text top="426" left="72" width="215" height="9" font="font8" id="p4_t31" reading_order_no="30" segment_no="14" tag_type="text">tler. D2-net: A trainable CNN for joint description and</text>
<text top="437" left="72" width="215" height="10" font="font8" id="p4_t32" reading_order_no="31" segment_no="14" tag_type="text">detection of local features. In IEEE Conf. Comput. Vis.</text>
<text top="449" left="72" width="169" height="10" font="font6" id="p4_t33" reading_order_no="32" segment_no="14" tag_type="text">Pattern Recog. , pages 8092–8101, 2019. 1</text>
<text top="465" left="50" width="236" height="9" font="font8" id="p4_t34" reading_order_no="33" segment_no="16" tag_type="text">[12] Jose M Facil, Daniel Olid, Luis Montesano, and Javier</text>
<text top="477" left="72" width="215" height="9" font="font8" id="p4_t35" reading_order_no="34" segment_no="16" tag_type="text">Civera. Condition-invariant multi-view place recogni-<a href="deeplearning_paper8.html#3">3</a></text>
<text top="489" left="72" width="199" height="9" font="font8" id="p4_t36" reading_order_no="35" segment_no="16" tag_type="text">tion. arXiv preprint arXiv:1902.09516 , 2019. 1 , 3</text>
<text top="505" left="50" width="236" height="9" font="font8" id="p4_t37" reading_order_no="36" segment_no="18" tag_type="text">[13] Sourav Garg, V Babu, Thanuja Dharmasiri, Stephen</text>
<text top="517" left="72" width="215" height="9" font="font8" id="p4_t38" reading_order_no="37" segment_no="18" tag_type="text">Hausler, Niko Suenderhauf, Swagat Kumar, Tom</text>
<text top="529" left="72" width="215" height="9" font="font8" id="p4_t39" reading_order_no="38" segment_no="18" tag_type="text">Drummond, and Michael Milford. Look no deeper:</text>
<text top="541" left="72" width="215" height="9" font="font8" id="p4_t40" reading_order_no="39" segment_no="18" tag_type="text">Recognizing places from opposing viewpoints under</text>
<text top="553" left="72" width="215" height="9" font="font8" id="p4_t41" reading_order_no="40" segment_no="18" tag_type="text">varying scene appearance using single-view depth es-</text>
<text top="565" left="72" width="215" height="9" font="font8" id="p4_t42" reading_order_no="41" segment_no="18" tag_type="text">timation. In IEEE Int. Conf. Robot. Autom. , pages<a href="deeplearning_paper8.html#1">1</a></text>
<text top="577" left="72" width="81" height="9" font="font8" id="p4_t43" reading_order_no="42" segment_no="18" tag_type="text">4916–4923, 2019. 3</text>
<text top="593" left="50" width="236" height="9" font="font8" id="p4_t44" reading_order_no="43" segment_no="20" tag_type="text">[14] Sourav Garg, Tobias Fischer, and Michael Milford.</text>
<text top="605" left="72" width="214" height="9" font="font8" id="p4_t45" reading_order_no="44" segment_no="20" tag_type="text">Where is your place, visual place recognition? In IJ-</text>
<text top="617" left="72" width="52" height="9" font="font6" id="p4_t46" reading_order_no="45" segment_no="20" tag_type="text">CAI , 2021. 1</text>
<text top="633" left="50" width="236" height="9" font="font8" id="p4_t47" reading_order_no="46" segment_no="22" tag_type="text">[15] Sourav Garg, Ben Harwood, Gaurangi Anand, and</text>
<text top="644" left="72" width="70" height="9" font="font8" id="p4_t48" reading_order_no="47" segment_no="22" tag_type="text">Michael Milford.</text>
<text top="644" left="151" width="135" height="9" font="font8" id="p4_t49" reading_order_no="48" segment_no="22" tag_type="text">Delta descriptors: Change-based<a href="deeplearning_paper8.html#2">2</a></text>
<text top="656" left="72" width="215" height="9" font="font8" id="p4_t50" reading_order_no="49" segment_no="22" tag_type="text">place representation for robust visual localization.<a href="deeplearning_paper8.html#2">,</a></text>
<text top="668" left="72" width="214" height="9" font="font6" id="p4_t51" reading_order_no="50" segment_no="22" tag_type="text">IEEE Robot. Autom. Lett. , 5(4):5120–5127, 2020. 1 ,<a href="deeplearning_paper8.html#3">3</a></text>
<text top="680" left="72" width="15" height="9" font="font15" id="p4_t52" reading_order_no="51" segment_no="22" tag_type="text">2 , 3</text>
<text top="51" left="309" width="236" height="9" font="font8" id="p4_t53" reading_order_no="52" segment_no="1" tag_type="text">[16] Sourav Garg and Michael Milford. SeqNet: Learn-</text>
<text top="63" left="330" width="215" height="9" font="font8" id="p4_t54" reading_order_no="53" segment_no="1" tag_type="text">ing descriptors for sequence-based hierarchical place</text>
<text top="75" left="330" width="209" height="9" font="font8" id="p4_t55" reading_order_no="54" segment_no="1" tag_type="text">recognition. IEEE Robot. Autom. Lett. , 2021. 1 , 2 , 3</text>
<text top="91" left="309" width="236" height="9" font="font8" id="p4_t56" reading_order_no="55" segment_no="3" tag_type="text">[17] Sourav Garg, Niko S¨underhauf, Feras Dayoub, Dou-</text>
<text top="103" left="330" width="215" height="9" font="font8" id="p4_t57" reading_order_no="56" segment_no="3" tag_type="text">glas Morrison, Akansel Cosgun, Gustavo Carneiro, Qi</text>
<text top="115" left="330" width="215" height="9" font="font8" id="p4_t58" reading_order_no="57" segment_no="3" tag_type="text">Wu, Tat-Jun Chin, Ian Reid, Stephen Gould, et al. Se-</text>
<text top="127" left="330" width="215" height="9" font="font8" id="p4_t59" reading_order_no="58" segment_no="3" tag_type="text">mantics for robotic mapping, perception and interac-<a href="deeplearning_paper8.html#1">1</a></text>
<text top="139" left="330" width="215" height="9" font="font8" id="p4_t60" reading_order_no="59" segment_no="3" tag_type="text">tion: A survey. Foundations and Trends® in Robotics ,</text>
<text top="151" left="330" width="91" height="9" font="font8" id="p4_t61" reading_order_no="60" segment_no="3" tag_type="text">8(1–2):1–224, 2020. 3</text>
<text top="167" left="309" width="236" height="9" font="font8" id="p4_t62" reading_order_no="61" segment_no="5" tag_type="text">[18] Sourav Garg, Niko S¨underhauf, and Michael Milford.</text>
<text top="179" left="330" width="215" height="9" font="font8" id="p4_t63" reading_order_no="62" segment_no="5" tag_type="text">Lost? appearance-invariant place recognition for op-</text>
<text top="191" left="330" width="215" height="9" font="font8" id="p4_t64" reading_order_no="63" segment_no="5" tag_type="text">posite viewpoints using visual semantics. Robot.: Sci.</text>
<text top="203" left="330" width="55" height="9" font="font6" id="p4_t65" reading_order_no="64" segment_no="5" tag_type="text">Syst. , 2018. 1<a href="deeplearning_paper8.html#1">1</a></text>
<text top="219" left="309" width="236" height="9" font="font8" id="p4_t66" reading_order_no="65" segment_no="7" tag_type="text">[19] Le Hui, Mingmei Cheng, Jin Xie, and Jian Yang. Ef-<a href="deeplearning_paper8.html#1">,</a></text>
<text top="230" left="330" width="215" height="9" font="font8" id="p4_t67" reading_order_no="66" segment_no="7" tag_type="text">ficient 3d point cloud feature learning for large-scale<a href="deeplearning_paper8.html#3">3</a></text>
<text top="242" left="330" width="215" height="10" font="font8" id="p4_t68" reading_order_no="67" segment_no="7" tag_type="text">place recognition. arXiv preprint arXiv:2101.02374 ,</text>
<text top="254" left="330" width="31" height="9" font="font8" id="p4_t69" reading_order_no="68" segment_no="7" tag_type="text">2021. 2</text>
<text top="270" left="309" width="236" height="9" font="font8" id="p4_t70" reading_order_no="69" segment_no="9" tag_type="text">[20] Giseop Kim and Ayoung Kim. Scan context: Egocen-</text>
<text top="282" left="330" width="215" height="9" font="font8" id="p4_t71" reading_order_no="70" segment_no="9" tag_type="text">tric spatial descriptor for place recognition within 3d</text>
<text top="294" left="330" width="215" height="9" font="font8" id="p4_t72" reading_order_no="71" segment_no="9" tag_type="text">point cloud map. In IEEE/RSJ Int. Conf. Intell. Robot.</text>
<text top="306" left="330" width="157" height="9" font="font6" id="p4_t73" reading_order_no="72" segment_no="9" tag_type="text">Syst. , pages 4802–4809. IEEE, 2018. 2</text>
<text top="322" left="309" width="236" height="9" font="font8" id="p4_t74" reading_order_no="73" segment_no="11" tag_type="text">[21] Zhe Liu, Shunbo Zhou, Chuanzhe Suo, Peng Yin, Wen</text>
<text top="334" left="330" width="215" height="9" font="font8" id="p4_t75" reading_order_no="74" segment_no="11" tag_type="text">Chen, Hesheng Wang, Haoang Li, and Yun-Hui Liu.</text>
<text top="346" left="330" width="215" height="9" font="font8" id="p4_t76" reading_order_no="75" segment_no="11" tag_type="text">Lpd-net: 3d point cloud learning for large-scale place</text>
<text top="358" left="330" width="215" height="9" font="font8" id="p4_t77" reading_order_no="76" segment_no="11" tag_type="text">recognition and environment analysis. In Proceedings<a href="deeplearning_paper8.html#3">3</a></text>
<text top="370" left="330" width="215" height="9" font="font6" id="p4_t78" reading_order_no="77" segment_no="11" tag_type="text">of the IEEE/CVF International Conference on Com-</text>
<text top="382" left="330" width="160" height="9" font="font6" id="p4_t79" reading_order_no="78" segment_no="11" tag_type="text">puter Vision , pages 2831–2840, 2019. 2</text>
<text top="398" left="309" width="236" height="9" font="font8" id="p4_t80" reading_order_no="79" segment_no="13" tag_type="text">[22] Stephanie Lowry, Niko S¨underhauf, Paul Newman,</text>
<text top="410" left="330" width="215" height="9" font="font8" id="p4_t81" reading_order_no="80" segment_no="13" tag_type="text">John J Leonard, David Cox, Peter Corke, and</text>
<text top="422" left="330" width="215" height="9" font="font8" id="p4_t82" reading_order_no="81" segment_no="13" tag_type="text">Michael J Milford. Visual place recognition: A survey.</text>
<text top="434" left="330" width="211" height="9" font="font6" id="p4_t83" reading_order_no="82" segment_no="13" tag_type="text">IEEE Transactions on Robotics , 32(1):1–19, 2016. 1<a href="deeplearning_paper8.html#1">1</a></text>
<text top="449" left="309" width="236" height="9" font="font8" id="p4_t84" reading_order_no="83" segment_no="15" tag_type="text">[23] Simon Lynen, Michael Bosse, Paul Timothy Furgale,</text>
<text top="461" left="330" width="215" height="9" font="font8" id="p4_t85" reading_order_no="84" segment_no="15" tag_type="text">and Roland Siegwart. Placeless place-recognition. In</text>
<text top="473" left="330" width="129" height="9" font="font6" id="p4_t86" reading_order_no="85" segment_no="15" tag_type="text">3DV , pages 303–310, 2014. 1 , 3</text>
<text top="489" left="309" width="236" height="9" font="font8" id="p4_t87" reading_order_no="86" segment_no="17" tag_type="text">[24] Will Maddern, Geoffrey Pascoe, Chris Linegar, and</text>
<text top="501" left="330" width="215" height="9" font="font8" id="p4_t88" reading_order_no="87" segment_no="17" tag_type="text">Paul Newman. 1 year, 1000 km: The oxford robotcar</text>
<text top="513" left="330" width="184" height="9" font="font8" id="p4_t89" reading_order_no="88" segment_no="17" tag_type="text">dataset. IJ Robotics Res. , 36(1):3–15, 2017. 2</text>
<text top="529" left="309" width="236" height="9" font="font8" id="p4_t90" reading_order_no="89" segment_no="19" tag_type="text">[25] Michael J Milford and Gordon F Wyeth. Seqslam:<a href="deeplearning_paper8.html#1">1</a></text>
<text top="541" left="330" width="215" height="9" font="font8" id="p4_t91" reading_order_no="90" segment_no="19" tag_type="text">Visual route-based navigation for sunny summer days<a href="deeplearning_paper8.html#1">,</a></text>
<text top="553" left="330" width="215" height="9" font="font8" id="p4_t92" reading_order_no="91" segment_no="19" tag_type="text">and stormy winter nights. In Robotics and Automa-<a href="deeplearning_paper8.html#2">2</a></text>
<text top="565" left="330" width="215" height="9" font="font6" id="p4_t93" reading_order_no="92" segment_no="19" tag_type="text">tion (ICRA), 2012 IEEE International Conference on ,<a href="deeplearning_paper8.html#2">,</a></text>
<text top="577" left="330" width="153" height="9" font="font8" id="p4_t94" reading_order_no="93" segment_no="19" tag_type="text">pages 1643–1649. IEEE, 2012. 1 , 2 , 3<a href="deeplearning_paper8.html#3">3</a></text>
<text top="593" left="309" width="236" height="9" font="font8" id="p4_t95" reading_order_no="94" segment_no="21" tag_type="text">[26] Tayyab Naseer, Luciano Spinello, Wolfram Burgard,</text>
<text top="605" left="330" width="215" height="9" font="font8" id="p4_t96" reading_order_no="95" segment_no="21" tag_type="text">and Cyrill Stachniss. Robust visual robot localization</text>
<text top="617" left="330" width="215" height="9" font="font8" id="p4_t97" reading_order_no="96" segment_no="21" tag_type="text">across seasons using network flows. In Twenty-Eighth</text>
<text top="629" left="330" width="215" height="9" font="font6" id="p4_t98" reading_order_no="97" segment_no="21" tag_type="text">AAAI Conference on Artificial Intelligence , 2014. 1 , 3</text>
<text top="644" left="309" width="236" height="9" font="font8" id="p4_t99" reading_order_no="98" segment_no="23" tag_type="text">[27] Peer Neubert, Stefan Schubert, and Peter Protzel. A</text>
<text top="656" left="330" width="215" height="9" font="font8" id="p4_t100" reading_order_no="99" segment_no="23" tag_type="text">neurologically inspired sequence processing model<a href="deeplearning_paper8.html#1">1</a></text>
<text top="668" left="330" width="215" height="9" font="font8" id="p4_t101" reading_order_no="100" segment_no="23" tag_type="text">for mobile robot place recognition. IEEE Robotics and<a href="deeplearning_paper8.html#1">,</a></text>
<text top="680" left="330" width="182" height="9" font="font6" id="p4_t102" reading_order_no="101" segment_no="23" tag_type="text">Automation Letters , 4(4):3200–3207, 2019. 1<a href="deeplearning_paper8.html#2">2</a></text>
<text top="710" left="295" width="5" height="9" font="font8" id="p4_t103" reading_order_no="102" segment_no="24" tag_type="text">4<a href="deeplearning_paper8.html#2">,</a></text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
<text top="51" left="50" width="236" height="9" font="font8" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">[28] Amadeus Oertel, Titus Cieslewski, and Davide Scara-</text>
<text top="63" left="72" width="28" height="9" font="font8" id="p5_t2" reading_order_no="1" segment_no="0" tag_type="text">muzza.</text>
<text top="63" left="110" width="176" height="9" font="font8" id="p5_t3" reading_order_no="2" segment_no="0" tag_type="text">Augmenting visual place recognition with</text>
<text top="75" left="72" width="214" height="9" font="font8" id="p5_t4" reading_order_no="3" segment_no="0" tag_type="text">structural cues. IEEE Robot. Autom. Lett. , 5(4):5534–</text>
<text top="87" left="72" width="56" height="9" font="font8" id="p5_t5" reading_order_no="4" segment_no="0" tag_type="text">5541, 2020. 3</text>
<text top="102" left="50" width="236" height="9" font="font8" id="p5_t6" reading_order_no="5" segment_no="3" tag_type="text">[29] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum.</text>
<text top="114" left="72" width="215" height="9" font="font8" id="p5_t7" reading_order_no="6" segment_no="3" tag_type="text">Fine-tuning cnn image retrieval with no human an-</text>
<text top="126" left="72" width="35" height="9" font="font8" id="p5_t8" reading_order_no="7" segment_no="3" tag_type="text">notation.<a href="deeplearning_paper8.html#3">3</a></text>
<text top="126" left="116" width="170" height="9" font="font6" id="p5_t9" reading_order_no="8" segment_no="3" tag_type="text">IEEE Trans. Pattern Anal. Mach. Intell. ,</text>
<text top="138" left="72" width="105" height="9" font="font8" id="p5_t10" reading_order_no="9" segment_no="3" tag_type="text">41(7):1655–1668, 2018. 1</text>
<text top="153" left="50" width="236" height="9" font="font8" id="p5_t11" reading_order_no="10" segment_no="4" tag_type="text">[30] Jerome Revaud, Jon Almaz´an, Rafael S Rezende, and</text>
<text top="165" left="72" width="215" height="9" font="font8" id="p5_t12" reading_order_no="11" segment_no="4" tag_type="text">Cesar Roberto de Souza. Learning with average preci-</text>
<text top="177" left="72" width="215" height="9" font="font8" id="p5_t13" reading_order_no="12" segment_no="4" tag_type="text">sion: Training image retrieval with a listwise loss. In</text>
<text top="189" left="72" width="202" height="9" font="font6" id="p5_t14" reading_order_no="13" segment_no="4" tag_type="text">Int. Conf. Comput. Vis. , pages 5107–5116, 2019. 1</text>
<text top="204" left="50" width="236" height="9" font="font8" id="p5_t15" reading_order_no="14" segment_no="6" tag_type="text">[31] Paul-Edouard Sarlin, Frederic Debraine, Marcin<a href="deeplearning_paper8.html#1">1</a></text>
<text top="216" left="72" width="215" height="9" font="font8" id="p5_t16" reading_order_no="15" segment_no="6" tag_type="text">Dymczyk, and Roland Siegwart. Leveraging deep vi-</text>
<text top="228" left="72" width="215" height="9" font="font8" id="p5_t17" reading_order_no="16" segment_no="6" tag_type="text">sual descriptors for hierarchical efficient localization.</text>
<text top="240" left="72" width="214" height="9" font="font8" id="p5_t18" reading_order_no="17" segment_no="6" tag_type="text">In Conference on Robot Learning , pages 456–465,</text>
<text top="252" left="72" width="31" height="9" font="font8" id="p5_t19" reading_order_no="18" segment_no="6" tag_type="text">2018. 1</text>
<text top="266" left="50" width="236" height="9" font="font8" id="p5_t20" reading_order_no="19" segment_no="8" tag_type="text">[32] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Mal-</text>
<text top="278" left="72" width="215" height="9" font="font8" id="p5_t21" reading_order_no="20" segment_no="8" tag_type="text">isiewicz, and Andrew Rabinovich. Superglue: Learn-<a href="deeplearning_paper8.html#1">1</a></text>
<text top="290" left="72" width="215" height="9" font="font8" id="p5_t22" reading_order_no="21" segment_no="8" tag_type="text">ing feature matching with graph neural networks. In</text>
<text top="51" left="330" width="215" height="9" font="font6" id="p5_t23" reading_order_no="22" segment_no="1" tag_type="text">Proceedings of the IEEE/CVF Conference on Com-</text>
<text top="63" left="330" width="215" height="9" font="font6" id="p5_t24" reading_order_no="23" segment_no="1" tag_type="text">puter Vision and Pattern Recognition , pages 4938–</text>
<text top="75" left="330" width="56" height="9" font="font8" id="p5_t25" reading_order_no="24" segment_no="1" tag_type="text">4947, 2020. 1</text>
<text top="92" left="309" width="236" height="9" font="font8" id="p5_t26" reading_order_no="25" segment_no="2" tag_type="text">[33] Qi Sun, Hongyan Liu, Jun He, Zhaoxin Fan, and Xi-</text>
<text top="104" left="330" width="49" height="9" font="font8" id="p5_t27" reading_order_no="26" segment_no="2" tag_type="text">aoyong Du.</text>
<text top="104" left="389" width="156" height="9" font="font8" id="p5_t28" reading_order_no="27" segment_no="2" tag_type="text">Dagc: Employing dual attention and</text>
<text top="116" left="330" width="215" height="9" font="font8" id="p5_t29" reading_order_no="28" segment_no="2" tag_type="text">graph convolution for point cloud based place recogni-<a href="deeplearning_paper8.html#1">1</a></text>
<text top="128" left="330" width="215" height="9" font="font8" id="p5_t30" reading_order_no="29" segment_no="2" tag_type="text">tion. In Proceedings of the 2020 International Confer-</text>
<text top="140" left="330" width="215" height="9" font="font6" id="p5_t31" reading_order_no="30" segment_no="2" tag_type="text">ence on Multimedia Retrieval , pages 224–232, 2020.</text>
<text top="152" left="330" width="5" height="9" font="font15" id="p5_t32" reading_order_no="31" segment_no="2" tag_type="text">2</text>
<text top="168" left="309" width="236" height="9" font="font8" id="p5_t33" reading_order_no="32" segment_no="5" tag_type="text">[34] Kavisha Vidanapathirana, Peyman Moghadam, Ben</text>
<text top="180" left="330" width="215" height="9" font="font8" id="p5_t34" reading_order_no="33" segment_no="5" tag_type="text">Harwood, Muming Zhao, Sridha Sridharan, and Clin-</text>
<text top="192" left="330" width="215" height="9" font="font8" id="p5_t35" reading_order_no="34" segment_no="5" tag_type="text">ton Fookes. Locus: Lidar-based place recognition us-</text>
<text top="204" left="330" width="215" height="9" font="font8" id="p5_t36" reading_order_no="35" segment_no="5" tag_type="text">ing spatiotemporal higher-order pooling. In IEEE Int.</text>
<text top="216" left="330" width="118" height="9" font="font6" id="p5_t37" reading_order_no="36" segment_no="5" tag_type="text">Conf. Robot. Autom. , 2021. 2<a href="deeplearning_paper8.html#1">1</a></text>
<text top="233" left="309" width="236" height="9" font="font8" id="p5_t38" reading_order_no="37" segment_no="7" tag_type="text">[35] Olga Vysotska, Tayyab Naseer, Luciano Spinello,</text>
<text top="245" left="330" width="215" height="9" font="font8" id="p5_t39" reading_order_no="38" segment_no="7" tag_type="text">Wolfram Burgard, and Cyrill Stachniss. Efficient and</text>
<text top="257" left="330" width="215" height="9" font="font8" id="p5_t40" reading_order_no="39" segment_no="7" tag_type="text">effective matching of image sequences under substan-</text>
<text top="269" left="330" width="216" height="9" font="font8" id="p5_t41" reading_order_no="40" segment_no="7" tag_type="text">tial appearance changes exploiting gps priors. In IEEE</text>
<text top="280" left="330" width="215" height="10" font="font6" id="p5_t42" reading_order_no="41" segment_no="7" tag_type="text">Int. Conf. Robot. Autom. , pages 2774–2779. IEEE,</text>
<text top="292" left="330" width="31" height="9" font="font8" id="p5_t43" reading_order_no="42" segment_no="7" tag_type="text">2015. 1</text>
<text top="710" left="295" width="5" height="9" font="font8" id="p5_t44" reading_order_no="43" segment_no="9" tag_type="text">5</text>
</page>
</pdf2xml>
