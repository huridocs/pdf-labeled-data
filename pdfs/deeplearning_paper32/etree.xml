<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="14" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font1" size="12" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font2" size="12" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font3" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font4" size="10" family="NimbusMonL-ReguObli" color="#ec008b"/>
	<fontspec id="font5" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font6" size="10" family="NimbusRomNo9L-Regu" color="#00ff00"/>
	<fontspec id="font7" size="10" family="NimbusRomNo9L-Regu" color="#ff0000"/>
	<fontspec id="font8" size="20" family="Times" color="#7f7f7f"/>
<text top="107" left="91" width="414" height="13" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">Enhance Multimodal Model Performance with Data Augmentation:</text>
<text top="125" left="163" width="270" height="13" font="font0" id="p1_t2" reading_order_no="2" segment_no="0" tag_type="title">Facebook Hateful Meme Challenge Solution</text>
<text top="175" left="192" width="211" height="11" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="text">Yang Li , Zinc Zhang , and Hutchin Huang</text>
<text top="198" left="217" width="161" height="11" font="font2" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="text">Georgia Institute of Technology</text>
<text top="222" left="190" width="216" height="11" font="font1" id="p1_t5" reading_order_no="5" segment_no="3" tag_type="text">yli3297, zzhang889, hhuang400@gatech.edu</text>
<text top="272" left="146" width="44" height="11" font="font2" id="p1_t6" reading_order_no="6" segment_no="4" tag_type="title">Abstract</text>
<text top="298" left="62" width="224" height="9" font="font3" id="p1_t7" reading_order_no="7" segment_no="5" tag_type="text">Hateful content detection is one of the areas where deep</text>
<text top="310" left="50" width="236" height="9" font="font3" id="p1_t8" reading_order_no="8" segment_no="5" tag_type="text">learning can and should make a significant difference. The</text>
<text top="322" left="50" width="236" height="9" font="font3" id="p1_t9" reading_order_no="9" segment_no="5" tag_type="text">Hateful Memes Challenge from Facebook helps fulfill such</text>
<text top="334" left="50" width="236" height="9" font="font3" id="p1_t10" reading_order_no="10" segment_no="5" tag_type="text">potential by challenging the contestants to detect hateful</text>
<text top="346" left="50" width="236" height="9" font="font3" id="p1_t11" reading_order_no="11" segment_no="5" tag_type="text">speech in multi-modal memes using deep learning algo-</text>
<text top="358" left="50" width="236" height="9" font="font3" id="p1_t12" reading_order_no="12" segment_no="5" tag_type="text">rithms. In this paper, we utilize multi-modal, pre-trained</text>
<text top="370" left="50" width="236" height="9" font="font3" id="p1_t13" reading_order_no="13" segment_no="5" tag_type="text">models VilBERT and Visual BERT. We improved models’</text>
<text top="382" left="50" width="236" height="9" font="font3" id="p1_t14" reading_order_no="14" segment_no="5" tag_type="text">performance by adding training datasets generated from</text>
<text top="394" left="50" width="236" height="9" font="font3" id="p1_t15" reading_order_no="15" segment_no="5" tag_type="text">data augmentation. Enlarging the training data set helped</text>
<text top="406" left="50" width="236" height="9" font="font3" id="p1_t16" reading_order_no="16" segment_no="5" tag_type="text">us get a more than 2% boost in terms of AUROC with the</text>
<text top="418" left="50" width="236" height="9" font="font3" id="p1_t17" reading_order_no="17" segment_no="5" tag_type="text">Visual BERT model. Our approach achieved 0.7439 AU-</text>
<text top="430" left="50" width="236" height="9" font="font3" id="p1_t18" reading_order_no="18" segment_no="5" tag_type="text">ROC along with an accuracy of 0.7037 on the challenge’s</text>
<text top="442" left="50" width="236" height="9" font="font3" id="p1_t19" reading_order_no="19" segment_no="5" tag_type="text">test set, which revealed remarkable progress. Our code</text>
<text top="454" left="50" width="236" height="9" font="font3" id="p1_t20" reading_order_no="20" segment_no="5" tag_type="text">is available at: https://github.com/yangland/</text>
<text top="466" left="50" width="96" height="8" font="font4" id="p1_t21" reading_order_no="21" segment_no="5" tag_type="text">hatefulchallenge<a href="https://github.com/yangland/hatefulchallenge">https://github.com/yangland/</a></text>
<text top="503" left="50" width="77" height="11" font="font2" id="p1_t22" reading_order_no="22" segment_no="8" tag_type="title">1. Introduction<a href="https://github.com/yangland/hatefulchallenge">hatefulchallenge</a></text>
<text top="524" left="62" width="225" height="9" font="font5" id="p1_t23" reading_order_no="23" segment_no="9" tag_type="text">The Hateful Memes Challenges[ 3 ] from Facebook in-</text>
<text top="536" left="50" width="236" height="9" font="font5" id="p1_t24" reading_order_no="24" segment_no="9" tag_type="text">troduces a data set where messages are made from both<a href="deeplearning_paper32.html#6">Challenges[</a></text>
<text top="548" left="50" width="236" height="9" font="font5" id="p1_t25" reading_order_no="25" segment_no="9" tag_type="text">text and image. The traditional NLP and image process-<a href="deeplearning_paper32.html#6">3</a></text>
<text top="560" left="50" width="236" height="9" font="font5" id="p1_t26" reading_order_no="26" segment_no="9" tag_type="text">ing methods have a hard time analyzing this type of memes<a href="deeplearning_paper32.html#6">] </a>from Facebook in-</text>
<text top="572" left="50" width="236" height="9" font="font5" id="p1_t27" reading_order_no="27" segment_no="9" tag_type="text">due to their uni-modal nature. In this work, we investigate</text>
<text top="584" left="50" width="236" height="9" font="font5" id="p1_t28" reading_order_no="28" segment_no="9" tag_type="text">solutions for this challenge under Facebook’s MMF frame-</text>
<text top="596" left="50" width="236" height="9" font="font5" id="p1_t29" reading_order_no="29" segment_no="9" tag_type="text">work, evaluate the performances of the multi-modal models</text>
<text top="608" left="50" width="236" height="9" font="font5" id="p1_t30" reading_order_no="30" segment_no="9" tag_type="text">and propose modal turning options. Example memes can be</text>
<text top="620" left="50" width="72" height="9" font="font5" id="p1_t31" reading_order_no="31" segment_no="9" tag_type="text">found in Figure 1 .</text>
<text top="633" left="62" width="224" height="9" font="font5" id="p1_t32" reading_order_no="32" segment_no="12" tag_type="text">Despite the recent development for multi-modal learn-</text>
<text top="644" left="50" width="237" height="9" font="font5" id="p1_t33" reading_order_no="33" segment_no="12" tag_type="text">ing from models such as ViLBERT[ 7 ] and VisualBERT[ 4 ],</text>
<text top="656" left="50" width="236" height="9" font="font5" id="p1_t34" reading_order_no="34" segment_no="12" tag_type="text">the best model’s performance is still far behind from av-</text>
<text top="668" left="50" width="237" height="9" font="font5" id="p1_t35" reading_order_no="35" segment_no="12" tag_type="text">erage human performance[ 6 ]. The difficulty of the hateful<a href="deeplearning_paper32.html#1">1</a></text>
<text top="680" left="50" width="236" height="9" font="font5" id="p1_t36" reading_order_no="36" segment_no="12" tag_type="text">meme challenge is the knowledge for correct classification<a href="deeplearning_paper32.html#1">.</a></text>
<text top="692" left="50" width="236" height="9" font="font5" id="p1_t37" reading_order_no="37" segment_no="12" tag_type="text">needs to be learned from another dataset. The hateful meme</text>
<text top="704" left="50" width="236" height="9" font="font5" id="p1_t38" reading_order_no="38" segment_no="12" tag_type="text">dataset is very small, with 10,000 meme records. It can only<a href="deeplearning_paper32.html#6">T[</a></text>
<text top="389" left="359" width="136" height="9" font="font5" id="p1_t39" reading_order_no="39" segment_no="6" tag_type="text">Figure 1: Hateful meme examples<a href="deeplearning_paper32.html#6">7</a></text>
<text top="426" left="309" width="236" height="9" font="font5" id="p1_t40" reading_order_no="40" segment_no="7" tag_type="text">be used as examples and for validations. We can not use the<a href="deeplearning_paper32.html#6">] </a>and VisualBER<a href="deeplearning_paper32.html#6">T[</a></text>
<text top="438" left="309" width="236" height="9" font="font5" id="p1_t41" reading_order_no="41" segment_no="7" tag_type="text">hateful meme dataset as the only source for training. The<a href="deeplearning_paper32.html#6">4</a></text>
<text top="450" left="309" width="236" height="9" font="font5" id="p1_t42" reading_order_no="42" segment_no="7" tag_type="text">commonsense and reasoning would require training from<a href="deeplearning_paper32.html#6">],</a></text>
<text top="462" left="309" width="236" height="9" font="font5" id="p1_t43" reading_order_no="43" segment_no="7" tag_type="text">another more comprehensive dataset, which needs to have</text>
<text top="474" left="309" width="236" height="9" font="font5" id="p1_t44" reading_order_no="44" segment_no="7" tag_type="text">a broad coverage over characteristics that commonly been<a href="deeplearning_paper32.html#6">performance[</a></text>
<text top="486" left="309" width="236" height="9" font="font5" id="p1_t45" reading_order_no="45" segment_no="7" tag_type="text">targeted in hateful posts such as sex, race, and religion. The<a href="deeplearning_paper32.html#6">6</a></text>
<text top="498" left="309" width="236" height="9" font="font5" id="p1_t46" reading_order_no="46" segment_no="7" tag_type="text">multimodal learning model also needs to connect both im-<a href="deeplearning_paper32.html#6">]. </a>The difficulty of the hateful</text>
<text top="510" left="309" width="236" height="9" font="font5" id="p1_t47" reading_order_no="47" segment_no="7" tag_type="text">ages, natural language, and the attentions in-between to pro-</text>
<text top="522" left="309" width="236" height="9" font="font5" id="p1_t48" reading_order_no="48" segment_no="7" tag_type="text">vide the support for classification to be successful. Since</text>
<text top="534" left="309" width="236" height="9" font="font5" id="p1_t49" reading_order_no="49" segment_no="7" tag_type="text">the representation model is task-agnostic, the model also</text>
<text top="546" left="309" width="236" height="9" font="font5" id="p1_t50" reading_order_no="50" segment_no="7" tag_type="text">requires further turning for the specific task in identifying</text>
<text top="557" left="309" width="64" height="9" font="font5" id="p1_t51" reading_order_no="51" segment_no="7" tag_type="text">Hateful Memes.</text>
<text top="571" left="321" width="224" height="9" font="font5" id="p1_t52" reading_order_no="52" segment_no="11" tag_type="text">Visual Question Answering (VQA) and Visual Com-</text>
<text top="583" left="309" width="236" height="9" font="font5" id="p1_t53" reading_order_no="53" segment_no="11" tag_type="text">monsense Reasoning (VCR) are tasks tested in the Visu-</text>
<text top="595" left="309" width="236" height="9" font="font5" id="p1_t54" reading_order_no="54" segment_no="11" tag_type="text">alBERT Research[ 4 ]. They are related to the multimodal</text>
<text top="607" left="309" width="236" height="9" font="font5" id="p1_t55" reading_order_no="55" segment_no="11" tag_type="text">classification task for hateful meme challenges. These two</text>
<text top="619" left="309" width="236" height="9" font="font5" id="p1_t56" reading_order_no="56" segment_no="11" tag_type="text">tasks require cross-attention between text and image, and a</text>
<text top="631" left="309" width="236" height="9" font="font5" id="p1_t57" reading_order_no="57" segment_no="11" tag_type="text">background knowledge base that does not exist in the infor-</text>
<text top="643" left="309" width="133" height="9" font="font5" id="p1_t58" reading_order_no="58" segment_no="11" tag_type="text">mation from the current example.</text>
<text top="656" left="321" width="224" height="9" font="font5" id="p1_t59" reading_order_no="59" segment_no="13" tag_type="text">Directly, our research could help social media companies</text>
<text top="668" left="309" width="236" height="9" font="font5" id="p1_t60" reading_order_no="60" segment_no="13" tag_type="text">such as Facebook and Twitter. A better automatic process</text>
<text top="680" left="309" width="236" height="9" font="font5" id="p1_t61" reading_order_no="61" segment_no="13" tag_type="text">could save the high cost of hiring human reviewers to iden-</text>
<text top="692" left="309" width="236" height="9" font="font5" id="p1_t62" reading_order_no="62" segment_no="13" tag_type="text">tify inappropriate posts. On the other hand, a successful</text>
<text top="704" left="309" width="236" height="9" font="font5" id="p1_t63" reading_order_no="63" segment_no="13" tag_type="text">model could also guide publishers, media, and individual</text>
<text top="734" left="295" width="5" height="9" font="font5" id="p1_t64" reading_order_no="64" segment_no="14" tag_type="text">1</text>
<text top="546" left="32" width="0" height="18" font="font8" id="p1_t65" reading_order_no="0" segment_no="10" tag_type="title">arXiv:2105.13132v2  [cs.LG]  22 Jun 2021<a href="deeplearning_paper32.html#6">Research[</a></text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font9" size="10" family="NimbusRomNo9L-Regu" color="#ec008b"/>
	<fontspec id="font10" size="10" family="NimbusMonL-Regu" color="#ec008b"/>
<text top="75" left="50" width="236" height="9" font="font5" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="text">users, help them identifying potential inappropriate content</text>
<text top="87" left="50" width="236" height="9" font="font5" id="p2_t2" reading_order_no="1" segment_no="0" tag_type="text">before publishing. Broadly speaking, a better multi-modal</text>
<text top="99" left="50" width="236" height="9" font="font5" id="p2_t3" reading_order_no="2" segment_no="0" tag_type="text">model could sense more downstream task-related informa-</text>
<text top="111" left="50" width="236" height="9" font="font5" id="p2_t4" reading_order_no="3" segment_no="0" tag_type="text">tion, support better reasoning and apply better common-</text>
<text top="123" left="50" width="114" height="9" font="font5" id="p2_t5" reading_order_no="4" segment_no="0" tag_type="text">sense to visual-text contents.</text>
<text top="135" left="62" width="224" height="9" font="font5" id="p2_t6" reading_order_no="5" segment_no="1" tag_type="text">We use the Hateful Memes dataset comes from the offi-</text>
<text top="147" left="50" width="236" height="9" font="font5" id="p2_t7" reading_order_no="6" segment_no="1" tag_type="text">cial Facebook Challenge website here . It contains in total</text>
<text top="159" left="50" width="236" height="9" font="font5" id="p2_t8" reading_order_no="7" segment_no="1" tag_type="text">5 separate sets. One training set (train), two development<a href="https://www.drivendata.org/competitions/64/hateful-memes/page/206/">here</a></text>
<text top="171" left="50" width="236" height="9" font="font5" id="p2_t9" reading_order_no="8" segment_no="1" tag_type="text">sets (dev seen and dev unseen) and two test sets(test seen<a href="https://www.drivendata.org/competitions/64/hateful-memes/page/206/">. </a>It contains in total</text>
<text top="183" left="50" width="236" height="9" font="font5" id="p2_t10" reading_order_no="9" segment_no="1" tag_type="text">and test unseen). The training set has 8500 labeled sam-</text>
<text top="195" left="50" width="236" height="9" font="font5" id="p2_t11" reading_order_no="10" segment_no="1" tag_type="text">ples. Amongst two development sets, dev seen has 500 la-</text>
<text top="207" left="50" width="236" height="9" font="font5" id="p2_t12" reading_order_no="11" segment_no="1" tag_type="text">beled samples and dev unseen has 540 labeled samples. As</text>
<text top="219" left="50" width="236" height="9" font="font5" id="p2_t13" reading_order_no="12" segment_no="1" tag_type="text">for test sets, test seen and test unseen have 1000 and 2000</text>
<text top="231" left="50" width="125" height="9" font="font5" id="p2_t14" reading_order_no="13" segment_no="1" tag_type="text">unlabeled samples respectively.</text>
<text top="243" left="62" width="224" height="9" font="font5" id="p2_t15" reading_order_no="14" segment_no="2" tag_type="text">The data sets that we use as per-trained models are from</text>
<text top="255" left="50" width="177" height="9" font="font5" id="p2_t16" reading_order_no="15" segment_no="2" tag_type="text">the Facebook’s MMF framework, including:</text>
<text top="272" left="62" width="47" height="9" font="font5" id="p2_t17" reading_order_no="16" segment_no="3" tag_type="list">• COCO[ 1 ]</text>
<text top="291" left="62" width="46" height="9" font="font5" id="p2_t18" reading_order_no="17" segment_no="4" tag_type="list">• COCO17</text>
<text top="309" left="62" width="103" height="9" font="font5" id="p2_t19" reading_order_no="18" segment_no="5" tag_type="list">• Conceptual Captions[ 9 ]<a href="deeplearning_paper32.html#6">COCO[</a></text>
<text top="328" left="62" width="35" height="9" font="font5" id="p2_t20" reading_order_no="19" segment_no="6" tag_type="list">• VQA2<a href="deeplearning_paper32.html#6">1</a></text>
<text top="349" left="50" width="52" height="11" font="font2" id="p2_t21" reading_order_no="20" segment_no="7" tag_type="title">2. Method<a href="deeplearning_paper32.html#6">]</a></text>
<text top="370" left="62" width="224" height="9" font="font5" id="p2_t22" reading_order_no="21" segment_no="8" tag_type="text">Our approach can be divided into two parts: model</text>
<text top="381" left="50" width="103" height="9" font="font5" id="p2_t23" reading_order_no="22" segment_no="8" tag_type="text">screening and fine tuning.<a href="deeplearning_paper32.html#6">Captions[</a></text>
<text top="393" left="62" width="130" height="9" font="font5" id="p2_t24" reading_order_no="23" segment_no="9" tag_type="text">In part 1 of our approach,<a href="deeplearning_paper32.html#6">9</a></text>
<text top="393" left="202" width="84" height="9" font="font5" id="p2_t25" reading_order_no="24" segment_no="9" tag_type="text">we implemented a<a href="deeplearning_paper32.html#6">]</a></text>
<text top="405" left="50" width="236" height="9" font="font5" id="p2_t26" reading_order_no="25" segment_no="9" tag_type="text">quick survey over the seven models applicable to</text>
<text top="417" left="50" width="236" height="9" font="font5" id="p2_t27" reading_order_no="26" segment_no="9" tag_type="text">the Hateful Memes problem provided by the MMF</text>
<text top="429" left="50" width="237" height="9" font="font5" id="p2_t28" reading_order_no="27" segment_no="9" tag_type="text">framework’s “model zoo” ( https://mmf.sh/docs/</text>
<text top="441" left="50" width="237" height="9" font="font10" id="p2_t29" reading_order_no="28" segment_no="9" tag_type="text">notes/’model_zoo’ ), namely MMBT (“MMBT”),</text>
<text top="453" left="50" width="236" height="9" font="font5" id="p2_t30" reading_order_no="29" segment_no="9" tag_type="text">VilBERT (“ViLBERT”), Visual BERT(“Visual BERT”),</text>
<text top="465" left="50" width="236" height="9" font="font5" id="p2_t31" reading_order_no="30" segment_no="9" tag_type="text">MMF Transformer(“mmf transformer”), FUSIONS (“Con-</text>
<text top="477" left="50" width="236" height="9" font="font5" id="p2_t32" reading_order_no="31" segment_no="9" tag_type="text">cat BERT”, “Late Fusion”), and MMF BERT. To be spe-</text>
<text top="489" left="50" width="236" height="9" font="font5" id="p2_t33" reading_order_no="32" segment_no="9" tag_type="text">cific, we decided to utilize these pre-trained models by con-</text>
<text top="501" left="50" width="236" height="9" font="font5" id="p2_t34" reading_order_no="33" segment_no="9" tag_type="text">ducting the same settings of training routine with a fixed<a href="https://mmf.sh/docs/notes/'model_zoo'">(</a></text>
<text top="513" left="50" width="236" height="9" font="font5" id="p2_t35" reading_order_no="34" segment_no="9" tag_type="text">epoch of 3,000 on the Hateful Memes dataset, and then<a href="https://mmf.sh/docs/notes/'model_zoo'">https://mmf.sh/docs/</a></text>
<text top="525" left="50" width="236" height="9" font="font5" id="p2_t36" reading_order_no="35" segment_no="9" tag_type="text">compare these models with one another, qualitatively and<a href="https://mmf.sh/docs/notes/'model_zoo'">notes/’model_zoo’</a></text>
<text top="537" left="50" width="236" height="9" font="font5" id="p2_t37" reading_order_no="36" segment_no="9" tag_type="text">quantitatively analyze their performance using both accu-<a href="https://mmf.sh/docs/notes/'model_zoo'">), </a>namely MMBT (“MMBT”),</text>
<text top="549" left="50" width="236" height="9" font="font5" id="p2_t38" reading_order_no="37" segment_no="9" tag_type="text">racy and AUROC. The workflow of model screening shows</text>
<text top="561" left="50" width="46" height="9" font="font5" id="p2_t39" reading_order_no="38" segment_no="9" tag_type="text">in Figure 2 .</text>
<text top="573" left="62" width="224" height="9" font="font5" id="p2_t40" reading_order_no="39" segment_no="13" tag_type="text">Part 2 of our approach is to conduct a series of experi-</text>
<text top="585" left="50" width="236" height="9" font="font5" id="p2_t41" reading_order_no="40" segment_no="13" tag_type="text">ments to fine-tune the selected pre-trained models from the</text>
<text top="597" left="50" width="236" height="9" font="font5" id="p2_t42" reading_order_no="41" segment_no="13" tag_type="text">previous part. We would like to fine-tune the models from</text>
<text top="609" left="50" width="107" height="9" font="font5" id="p2_t43" reading_order_no="42" segment_no="13" tag_type="text">the following perspectives.</text>
<text top="621" left="62" width="224" height="9" font="font5" id="p2_t44" reading_order_no="43" segment_no="14" tag_type="text">First, since the hateful meme itself is relatively small in</text>
<text top="633" left="50" width="236" height="9" font="font5" id="p2_t45" reading_order_no="44" segment_no="14" tag_type="text">observation numbers, we want to prevent the models from</text>
<text top="644" left="50" width="236" height="9" font="font5" id="p2_t46" reading_order_no="45" segment_no="14" tag_type="text">overfitting this data set. Hence, we decided to exploit the</text>
<text top="656" left="50" width="236" height="9" font="font5" id="p2_t47" reading_order_no="46" segment_no="14" tag_type="text">“early stop” option provided by the MMF framework. We</text>
<text top="668" left="50" width="236" height="9" font="font5" id="p2_t48" reading_order_no="47" segment_no="14" tag_type="text">plan to stop training immediately when the accuracy rate<a href="deeplearning_paper32.html#2">2</a></text>
<text top="680" left="50" width="126" height="9" font="font5" id="p2_t49" reading_order_no="48" segment_no="14" tag_type="text">gets lower on the validation set.<a href="deeplearning_paper32.html#2">.</a></text>
<text top="692" left="62" width="224" height="9" font="font5" id="p2_t50" reading_order_no="49" segment_no="16" tag_type="text">Second, we want to try different loss functions other than</text>
<text top="704" left="50" width="236" height="9" font="font5" id="p2_t51" reading_order_no="50" segment_no="16" tag_type="text">Cross Entropy loss, such as focal loss [ 5 ], to reweight the</text>
<text top="399" left="354" width="146" height="9" font="font5" id="p2_t52" reading_order_no="51" segment_no="10" tag_type="text">Figure 2: Model screening workflow</text>
<text top="433" left="309" width="236" height="9" font="font5" id="p2_t53" reading_order_no="52" segment_no="11" tag_type="text">observations in the dataset that our models have difficulty</text>
<text top="445" left="309" width="236" height="9" font="font5" id="p2_t54" reading_order_no="53" segment_no="11" tag_type="text">in classifying correctly. We learned from Goswami et al [ 3 ]</text>
<text top="457" left="309" width="236" height="9" font="font5" id="p2_t55" reading_order_no="54" segment_no="11" tag_type="text">that “difficult examples (‘benign confounders’) are added to</text>
<text top="469" left="309" width="236" height="9" font="font5" id="p2_t56" reading_order_no="55" segment_no="11" tag_type="text">the dataset to make it hard to rely on unimodal signals”. We</text>
<text top="481" left="309" width="236" height="9" font="font5" id="p2_t57" reading_order_no="56" segment_no="11" tag_type="text">believe changing cross entropy loss to focal loss might help</text>
<text top="493" left="309" width="174" height="9" font="font5" id="p2_t58" reading_order_no="57" segment_no="11" tag_type="text">the model handle such “difficult examples”.</text>
<text top="505" left="321" width="224" height="9" font="font5" id="p2_t59" reading_order_no="58" segment_no="12" tag_type="text">Even the MMF models have been per-trained, the</text>
<text top="517" left="309" width="236" height="9" font="font5" id="p2_t60" reading_order_no="59" segment_no="12" tag_type="text">process of loading and running the train and validation</text>
<text top="529" left="309" width="236" height="9" font="font5" id="p2_t61" reading_order_no="60" segment_no="12" tag_type="text">on the hateful meme data set still takes a significant<a href="deeplearning_paper32.html#6">[</a></text>
<text top="541" left="309" width="155" height="9" font="font5" id="p2_t62" reading_order_no="61" segment_no="12" tag_type="text">amount of GPU and RAM resource.<a href="deeplearning_paper32.html#6">5</a></text>
<text top="541" left="474" width="71" height="9" font="font5" id="p2_t63" reading_order_no="62" segment_no="12" tag_type="text">To overcome the<a href="deeplearning_paper32.html#6">], </a>to reweight the</text>
<text top="553" left="309" width="236" height="9" font="font5" id="p2_t64" reading_order_no="63" segment_no="12" tag_type="text">physical challenge, our group uses a workstation equipped</text>
<text top="565" left="309" width="236" height="9" font="font5" id="p2_t65" reading_order_no="64" segment_no="12" tag_type="text">with a NVIDIA GeForce RTX 3090 GPU. Each of the</text>
<text top="577" left="309" width="236" height="9" font="font5" id="p2_t66" reading_order_no="65" segment_no="12" tag_type="text">experiments takes about 50 - 150 mins runtime under<a href="deeplearning_paper32.html#6">[</a></text>
<text top="589" left="309" width="100" height="9" font="font5" id="p2_t67" reading_order_no="66" segment_no="12" tag_type="text">the setup of early stop.<a href="deeplearning_paper32.html#6">3</a></text>
<text top="589" left="420" width="125" height="9" font="font5" id="p2_t68" reading_order_no="67" segment_no="12" tag_type="text">To save time, we freeze sev-<a href="deeplearning_paper32.html#6">]</a></text>
<text top="600" left="309" width="150" height="9" font="font5" id="p2_t69" reading_order_no="68" segment_no="12" tag_type="text">eral front-end parts of the models.</text>
<text top="600" left="471" width="74" height="9" font="font5" id="p2_t70" reading_order_no="69" segment_no="12" tag_type="text">We consulted the</text>
<text top="612" left="309" width="236" height="9" font="font5" id="p2_t71" reading_order_no="70" segment_no="12" tag_type="text">MMF starting code from facebook’s repository: https:</text>
<text top="625" left="309" width="209" height="8" font="font10" id="p2_t72" reading_order_no="71" segment_no="12" tag_type="text">//colab.research.google.com/github/</text>
<text top="637" left="309" width="215" height="8" font="font10" id="p2_t73" reading_order_no="72" segment_no="12" tag_type="text">facebookresearch/mmf/blob/notebooks/</text>
<text top="649" left="309" width="179" height="8" font="font10" id="p2_t74" reading_order_no="73" segment_no="12" tag_type="text">notebooks/mmf_hm_example.ipynb</text>
<text top="672" left="309" width="139" height="11" font="font2" id="p2_t75" reading_order_no="74" segment_no="15" tag_type="title">3. Experiments and Results</text>
<text top="692" left="321" width="224" height="9" font="font5" id="p2_t76" reading_order_no="75" segment_no="17" tag_type="text">As we cannot access the labeled test dataset, and the</text>
<text top="704" left="309" width="236" height="9" font="font5" id="p2_t77" reading_order_no="76" segment_no="17" tag_type="text">competition has a daily submission limit, all measurements</text>
<text top="734" left="295" width="5" height="9" font="font5" id="p2_t78" reading_order_no="77" segment_no="18" tag_type="text">2</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font11" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font12" size="10" family="CMEX10" color="#000000"/>
	<fontspec id="font13" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font14" size="7" family="CMMI7" color="#000000"/>
	<fontspec id="font15" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font16" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font17" size="11" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font18" size="10" family="CMSY10" color="#000000"/>
<text top="75" left="50" width="236" height="9" font="font5" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="text">are done locally and only done on the development set. We</text>
<text top="87" left="50" width="236" height="9" font="font5" id="p3_t2" reading_order_no="1" segment_no="0" tag_type="text">chose dev unseen as the validation set. The model that</text>
<text top="99" left="50" width="236" height="9" font="font5" id="p3_t3" reading_order_no="2" segment_no="0" tag_type="text">achieves a higher “score” on the validation set is considered</text>
<text top="111" left="50" width="236" height="9" font="font5" id="p3_t4" reading_order_no="3" segment_no="0" tag_type="text">better than other models. The “score”, under our configura-</text>
<text top="123" left="50" width="218" height="9" font="font5" id="p3_t5" reading_order_no="4" segment_no="0" tag_type="text">tion, can be validation accuracy or validation AUROC.</text>
<text top="135" left="62" width="224" height="9" font="font5" id="p3_t6" reading_order_no="5" segment_no="2" tag_type="text">We select AUROC, the area under the receiver operating</text>
<text top="147" left="50" width="236" height="9" font="font5" id="p3_t7" reading_order_no="6" segment_no="2" tag_type="text">characteristic curve, as the measure of binary classification</text>
<text top="159" left="50" width="137" height="9" font="font5" id="p3_t8" reading_order_no="7" segment_no="2" tag_type="text">task’s performance. Its formula is:</text>
<text top="186" left="87" width="47" height="9" font="font11" id="p3_t9" reading_order_no="8" segment_no="3" tag_type="formula">AUROC =</text>
<text top="177" left="137" width="14" height="6" font="font12" id="p3_t10" reading_order_no="9" segment_no="3" tag_type="formula">Z 1</text>
<text top="198" left="143" width="14" height="6" font="font14" id="p3_t11" reading_order_no="10" segment_no="3" tag_type="formula">x =0</text>
<text top="184" left="159" width="91" height="11" font="font11" id="p3_t12" reading_order_no="11" segment_no="3" tag_type="formula">TPR FPR − 1 ( x ) d x</text>
<text top="187" left="275" width="12" height="9" font="font5" id="p3_t13" reading_order_no="12" segment_no="3" tag_type="text">(1)</text>
<text top="213" left="50" width="194" height="9" font="font3" id="p3_t14" reading_order_no="13" segment_no="4" tag_type="text">TPR: true positive rate, FPR: false positive rate</text>
<text top="231" left="50" width="98" height="10" font="font17" id="p3_t15" reading_order_no="14" segment_no="5" tag_type="title">3.1. Model Screening</text>
<text top="250" left="62" width="224" height="9" font="font5" id="p3_t16" reading_order_no="15" segment_no="6" tag_type="text">In the first round of screening, we tested seven pre-</text>
<text top="262" left="50" width="236" height="9" font="font5" id="p3_t17" reading_order_no="16" segment_no="6" tag_type="text">trained models, and their learning curves are present as</text>
<text top="274" left="50" width="237" height="9" font="font5" id="p3_t18" reading_order_no="17" segment_no="6" tag_type="text">three groups in Figure 3 for a clear drawing. We imple-</text>
<text top="286" left="50" width="236" height="9" font="font5" id="p3_t19" reading_order_no="18" segment_no="6" tag_type="text">mented an early stop function so the steps for experiments</text>
<text top="298" left="50" width="236" height="9" font="font5" id="p3_t20" reading_order_no="19" segment_no="6" tag_type="text">are different, but they all converged. The scores, including</text>
<text top="310" left="50" width="236" height="9" font="font5" id="p3_t21" reading_order_no="20" segment_no="6" tag_type="text">validation accuracy and validation AUROC, are listed in Ta-</text>
<text top="322" left="50" width="236" height="9" font="font5" id="p3_t22" reading_order_no="21" segment_no="6" tag_type="text">ble 1 . We can see ViLBERT and Visual BERT performed</text>
<text top="333" left="50" width="236" height="10" font="font5" id="p3_t23" reading_order_no="22" segment_no="6" tag_type="text">best( ∼ 0.72 AUROC), MMBT family the second ( ∼ 0.7</text>
<text top="345" left="50" width="235" height="10" font="font5" id="p3_t24" reading_order_no="23" segment_no="6" tag_type="text">AUROC), and the unimodal family worst ( ∼ 0.6 AUROC).</text>
<text top="358" left="62" width="224" height="9" font="font5" id="p3_t25" reading_order_no="24" segment_no="8" tag_type="text">We can see that the unimodal family did not perform</text>
<text top="370" left="50" width="236" height="9" font="font5" id="p3_t26" reading_order_no="25" segment_no="8" tag_type="text">as well as the rest of the models. Unimodal-image is an</text>
<text top="381" left="50" width="236" height="9" font="font5" id="p3_t27" reading_order_no="26" segment_no="8" tag_type="text">image-only modal based on ResNet152, and unimodal-bert</text>
<text top="393" left="50" width="236" height="9" font="font5" id="p3_t28" reading_order_no="27" segment_no="8" tag_type="text">is a text-only modal based on text transformer. The meme is<a href="deeplearning_paper32.html#4">3</a></text>
<text top="405" left="50" width="236" height="9" font="font5" id="p3_t29" reading_order_no="28" segment_no="8" tag_type="text">made from both text and image, it is easy to understand why</text>
<text top="417" left="50" width="236" height="9" font="font5" id="p3_t30" reading_order_no="29" segment_no="8" tag_type="text">the unimodal models which only focusing on either text or</text>
<text top="429" left="50" width="236" height="9" font="font5" id="p3_t31" reading_order_no="30" segment_no="8" tag_type="text">image couldn’t achieve better results. Observing the sam-</text>
<text top="441" left="50" width="236" height="9" font="font5" id="p3_t32" reading_order_no="31" segment_no="8" tag_type="text">ples from the Hateful Memes dataset, we can find that many</text>
<text top="453" left="50" width="236" height="9" font="font5" id="p3_t33" reading_order_no="32" segment_no="8" tag_type="text">“hateful” meanings are expressed through antonyms, satire,</text>
<text top="465" left="50" width="236" height="9" font="font5" id="p3_t34" reading_order_no="33" segment_no="8" tag_type="text">or puns between the meaning of the text and the semantics<a href="deeplearning_paper32.html#3">1</a></text>
<text top="477" left="50" width="236" height="9" font="font5" id="p3_t35" reading_order_no="34" segment_no="8" tag_type="text">of the image. Unluckily, the unimodal models are lacking<a href="deeplearning_paper32.html#3">. </a>We can see ViLBERT and Visual BERT performed</text>
<text top="489" left="50" width="236" height="9" font="font5" id="p3_t36" reading_order_no="35" segment_no="8" tag_type="text">the ability to incorporate the information in the other modal</text>
<text top="501" left="50" width="219" height="9" font="font5" id="p3_t37" reading_order_no="36" segment_no="8" tag_type="text">and create a cross-attention relationship between them.</text>
<text top="513" left="62" width="224" height="9" font="font5" id="p3_t38" reading_order_no="37" segment_no="11" tag_type="text">MMBT(Multimodal Bitransformers)[ 2 ] made a signifi-</text>
<text top="525" left="50" width="236" height="9" font="font5" id="p3_t39" reading_order_no="38" segment_no="11" tag_type="text">cant improvement over the performance compare with the</text>
<text top="537" left="50" width="236" height="9" font="font5" id="p3_t40" reading_order_no="39" segment_no="11" tag_type="text">unimodal thanks to its BERT-like multimodal architectures.</text>
<text top="549" left="50" width="236" height="9" font="font5" id="p3_t41" reading_order_no="40" segment_no="11" tag_type="text">They leveled up AUROC by about 0.1, and accuracy by</text>
<text top="561" left="50" width="236" height="9" font="font5" id="p3_t42" reading_order_no="41" segment_no="11" tag_type="text">about 0.05. The advantage of these structures is to em-</text>
<text top="573" left="50" width="236" height="9" font="font5" id="p3_t43" reading_order_no="42" segment_no="11" tag_type="text">ploy self-attention over both text and image modalities si-</text>
<text top="585" left="50" width="236" height="9" font="font5" id="p3_t44" reading_order_no="43" segment_no="11" tag_type="text">multaneously. The result is an easier and more fine-grained</text>
<text top="597" left="50" width="236" height="9" font="font5" id="p3_t45" reading_order_no="44" segment_no="11" tag_type="text">multi-modal fusion. MMBT’s architecture components are</text>
<text top="609" left="50" width="236" height="9" font="font5" id="p3_t46" reading_order_no="45" segment_no="11" tag_type="text">pre-trained individually as a unimodal task, while its inter-</text>
<text top="621" left="50" width="236" height="9" font="font5" id="p3_t47" reading_order_no="46" segment_no="11" tag_type="text">nal structure allows information from different modalities to</text>
<text top="633" left="50" width="236" height="9" font="font5" id="p3_t48" reading_order_no="47" segment_no="11" tag_type="text">interact at multiple different levels via self-attention instead</text>
<text top="644" left="50" width="236" height="9" font="font5" id="p3_t49" reading_order_no="48" segment_no="11" tag_type="text">of just the final layer. Compare with ViLBERT, MMBT is</text>
<text top="656" left="50" width="236" height="9" font="font5" id="p3_t50" reading_order_no="49" segment_no="11" tag_type="text">simpler, but the ViLBERT’s BERT architecture is trained on</text>
<text top="668" left="50" width="236" height="9" font="font5" id="p3_t51" reading_order_no="50" segment_no="11" tag_type="text">the multi-model dataset. This could explain why in the final</text>
<text top="680" left="50" width="203" height="9" font="font5" id="p3_t52" reading_order_no="51" segment_no="11" tag_type="text">result ViLBERT is still slightly better than MMBT.</text>
<text top="692" left="62" width="224" height="9" font="font5" id="p3_t53" reading_order_no="52" segment_no="15" tag_type="text">Our best candidates are ViLBERT and Visual BERT,</text>
<text top="704" left="50" width="236" height="9" font="font5" id="p3_t54" reading_order_no="53" segment_no="15" tag_type="text">who achieved AUROC of 0.7187 and 0.7319 respectively.</text>
<text top="75" left="309" width="236" height="9" font="font5" id="p3_t55" reading_order_no="54" segment_no="1" tag_type="text">ViLBERT is a BERT architecture that extends to a multi-</text>
<text top="87" left="309" width="236" height="9" font="font5" id="p3_t56" reading_order_no="55" segment_no="1" tag_type="text">modal two-steam model. ViLBERT’s structure is designed</text>
<text top="99" left="309" width="236" height="9" font="font5" id="p3_t57" reading_order_no="56" segment_no="1" tag_type="text">for image content and natural language. Per-trained with<a href="deeplearning_paper32.html#6">Bitransformers)[</a></text>
<text top="111" left="309" width="236" height="9" font="font5" id="p3_t58" reading_order_no="57" segment_no="1" tag_type="text">a large dataset like Conceptual Captions, ViLBERT can<a href="deeplearning_paper32.html#6">2</a></text>
<text top="123" left="309" width="236" height="9" font="font5" id="p3_t59" reading_order_no="58" segment_no="1" tag_type="text">perform well on vision-and-language tasks including Vi-<a href="deeplearning_paper32.html#6">] </a>made a signifi-</text>
<text top="135" left="309" width="236" height="9" font="font5" id="p3_t60" reading_order_no="59" segment_no="1" tag_type="text">sual Question Answering (VQA) and Visual Commonsense</text>
<text top="147" left="309" width="236" height="9" font="font5" id="p3_t61" reading_order_no="60" segment_no="1" tag_type="text">Reasoning (VCR). The knowledge and commonsense were</text>
<text top="159" left="309" width="236" height="9" font="font5" id="p3_t62" reading_order_no="61" segment_no="1" tag_type="text">learned as a semantically meaningful alignment between vi-</text>
<text top="171" left="309" width="236" height="9" font="font5" id="p3_t63" reading_order_no="62" segment_no="1" tag_type="text">sion and language during pretraining. Another approach,</text>
<text top="183" left="309" width="236" height="9" font="font5" id="p3_t64" reading_order_no="63" segment_no="1" tag_type="text">VisualBERT incorporates BERT with pre-trained object</text>
<text top="195" left="309" width="236" height="9" font="font5" id="p3_t65" reading_order_no="64" segment_no="1" tag_type="text">proposals systems such as Faster-RCNN[ 4 ]. Similar to ViL-</text>
<text top="207" left="309" width="236" height="9" font="font5" id="p3_t66" reading_order_no="65" segment_no="1" tag_type="text">BERT, VisualBERT also has multiple Transformer layers</text>
<text top="219" left="309" width="236" height="9" font="font5" id="p3_t67" reading_order_no="66" segment_no="1" tag_type="text">that jointly process the text and image inputs. Different</text>
<text top="231" left="309" width="236" height="9" font="font5" id="p3_t68" reading_order_no="67" segment_no="1" tag_type="text">from ViLBERT, VisualBERT uses sets of visual embed-</text>
<text top="243" left="309" width="236" height="9" font="font5" id="p3_t69" reading_order_no="68" segment_no="1" tag_type="text">dings to pass object information from image input to multi-</text>
<text top="255" left="309" width="236" height="9" font="font5" id="p3_t70" reading_order_no="69" segment_no="1" tag_type="text">layer Transformer along with the original set of text embed-</text>
<text top="266" left="309" width="236" height="9" font="font5" id="p3_t71" reading_order_no="70" segment_no="1" tag_type="text">dings. Pre-trained with large data set such as COCO, Vi-</text>
<text top="278" left="309" width="236" height="9" font="font5" id="p3_t72" reading_order_no="71" segment_no="1" tag_type="text">sualBERT performance well on VQA, VCR, NLVR 2, and</text>
<text top="290" left="309" width="70" height="9" font="font5" id="p3_t73" reading_order_no="72" segment_no="1" tag_type="text">Flicker30K tasks.</text>
<text top="302" left="321" width="224" height="9" font="font5" id="p3_t74" reading_order_no="73" segment_no="7" tag_type="text">The reasons that ViLBERT and Visual BERT performed</text>
<text top="314" left="309" width="236" height="9" font="font5" id="p3_t75" reading_order_no="74" segment_no="7" tag_type="text">better in the Hateful Memes’ classification task could be 1)</text>
<text top="326" left="309" width="236" height="9" font="font5" id="p3_t76" reading_order_no="75" segment_no="7" tag_type="text">Their multimodal per-training. 2) Their ability to use mul-</text>
<text top="338" left="309" width="236" height="9" font="font5" id="p3_t77" reading_order_no="76" segment_no="7" tag_type="text">tiple transformers layers to closely connect image and text</text>
<text top="350" left="309" width="236" height="9" font="font5" id="p3_t78" reading_order_no="77" segment_no="7" tag_type="text">inputs. For both ViLBERT and VisualBERT, increasing the</text>
<text top="362" left="309" width="236" height="9" font="font5" id="p3_t79" reading_order_no="78" segment_no="7" tag_type="text">size of the pretraining data set or incorporate more data sets</text>
<text top="374" left="309" width="236" height="9" font="font5" id="p3_t80" reading_order_no="79" segment_no="7" tag_type="text">that are related to the downstream application could poten-</text>
<text top="386" left="309" width="126" height="9" font="font5" id="p3_t81" reading_order_no="80" segment_no="7" tag_type="text">tially enhance the performance.</text>
<text top="407" left="344" width="166" height="9" font="font5" id="p3_t82" reading_order_no="81" segment_no="9" tag_type="title">Table 1: Scores in Model Selection Phase</text>
<text top="433" left="429" width="38" height="9" font="font5" id="p3_t83" reading_order_no="82" segment_no="10" tag_type="table">Accuracy</text>
<text top="433" left="479" width="34" height="9" font="font5" id="p3_t84" reading_order_no="83" segment_no="10" tag_type="table">AUROC</text>
<text top="450" left="341" width="75" height="9" font="font5" id="p3_t85" reading_order_no="84" segment_no="10" tag_type="table">MMF Transformer</text>
<text top="450" left="434" width="27" height="9" font="font5" id="p3_t86" reading_order_no="85" segment_no="10" tag_type="table">0.6389<a href="deeplearning_paper32.html#6">-RCNN[</a></text>
<text top="450" left="482" width="27" height="9" font="font5" id="p3_t87" reading_order_no="86" segment_no="10" tag_type="table">0.6377<a href="deeplearning_paper32.html#6">4</a></text>
<text top="462" left="350" width="58" height="9" font="font5" id="p3_t88" reading_order_no="87" segment_no="10" tag_type="table">MMBT Image<a href="deeplearning_paper32.html#6">]. </a>Similar to ViL-</text>
<text top="462" left="434" width="27" height="9" font="font5" id="p3_t89" reading_order_no="88" segment_no="10" tag_type="table">0.6722</text>
<text top="462" left="482" width="27" height="9" font="font5" id="p3_t90" reading_order_no="89" segment_no="10" tag_type="table">0.7078</text>
<text top="474" left="346" width="67" height="9" font="font5" id="p3_t91" reading_order_no="90" segment_no="10" tag_type="table">MMBT Features</text>
<text top="474" left="434" width="27" height="9" font="font5" id="p3_t92" reading_order_no="91" segment_no="10" tag_type="table">0.6630</text>
<text top="474" left="482" width="27" height="9" font="font5" id="p3_t93" reading_order_no="92" segment_no="10" tag_type="table">0.6791</text>
<text top="486" left="345" width="67" height="9" font="font5" id="p3_t94" reading_order_no="93" segment_no="10" tag_type="table">Unimodal Image</text>
<text top="486" left="434" width="27" height="9" font="font5" id="p3_t95" reading_order_no="94" segment_no="10" tag_type="table">0.5796</text>
<text top="486" left="482" width="27" height="9" font="font5" id="p3_t96" reading_order_no="95" segment_no="10" tag_type="table">0.5539</text>
<text top="498" left="349" width="59" height="9" font="font5" id="p3_t97" reading_order_no="96" segment_no="10" tag_type="table">Unimodal Bert</text>
<text top="498" left="434" width="27" height="9" font="font5" id="p3_t98" reading_order_no="97" segment_no="10" tag_type="table">0.6241</text>
<text top="498" left="482" width="27" height="9" font="font5" id="p3_t99" reading_order_no="98" segment_no="10" tag_type="table">0.6135</text>
<text top="510" left="359" width="40" height="9" font="font5" id="p3_t100" reading_order_no="99" segment_no="10" tag_type="table">ViLBERT</text>
<text top="510" left="434" width="27" height="9" font="font5" id="p3_t101" reading_order_no="100" segment_no="10" tag_type="table">0.7056</text>
<text top="510" left="482" width="27" height="9" font="font5" id="p3_t102" reading_order_no="101" segment_no="10" tag_type="table">0.7187</text>
<text top="521" left="353" width="53" height="9" font="font5" id="p3_t103" reading_order_no="102" segment_no="10" tag_type="table">Visual BERT</text>
<text top="521" left="434" width="27" height="9" font="font5" id="p3_t104" reading_order_no="103" segment_no="10" tag_type="table">0.7056</text>
<text top="521" left="482" width="27" height="9" font="font5" id="p3_t105" reading_order_no="104" segment_no="10" tag_type="table">0.7319</text>
<text top="554" left="309" width="76" height="10" font="font17" id="p3_t106" reading_order_no="105" segment_no="12" tag_type="title">3.2. Fine Tuning</text>
<text top="573" left="321" width="224" height="9" font="font5" id="p3_t107" reading_order_no="106" segment_no="13" tag_type="text">Fine tuning phase was conducted on the models that</text>
<text top="585" left="309" width="236" height="9" font="font5" id="p3_t108" reading_order_no="107" segment_no="13" tag_type="text">were passed in model screen phase, which were ViLBERT</text>
<text top="597" left="309" width="236" height="9" font="font5" id="p3_t109" reading_order_no="108" segment_no="13" tag_type="text">and Visual BERT as mentioned in 3.1 . We implemented</text>
<text top="609" left="309" width="236" height="9" font="font5" id="p3_t110" reading_order_no="109" segment_no="13" tag_type="text">two ways to tune each of two models: 1) Use Focal loss</text>
<text top="621" left="309" width="236" height="9" font="font5" id="p3_t111" reading_order_no="110" segment_no="13" tag_type="text">rather than naive binary cross entropy loss; 2) Enlarge the</text>
<text top="633" left="309" width="236" height="9" font="font5" id="p3_t112" reading_order_no="111" segment_no="13" tag_type="text">train set by adding new labeled samples. These two ways</text>
<text top="644" left="309" width="236" height="9" font="font5" id="p3_t113" reading_order_no="112" segment_no="13" tag_type="text">are conducted separately, but both of them are used along</text>
<text top="656" left="309" width="95" height="9" font="font5" id="p3_t114" reading_order_no="113" segment_no="13" tag_type="text">with early stop enabled.</text>
<text top="668" left="321" width="224" height="9" font="font5" id="p3_t115" reading_order_no="114" segment_no="14" tag_type="text">Focal loss need two predefined hyper parameters: α and</text>
<text top="680" left="309" width="236" height="9" font="font16" id="p3_t116" reading_order_no="115" segment_no="14" tag_type="text">γ . α controls the weight that balances the ratio between</text>
<text top="692" left="309" width="236" height="9" font="font5" id="p3_t117" reading_order_no="116" segment_no="14" tag_type="text">positive and negative samples. When the amount of nega-</text>
<text top="704" left="309" width="236" height="9" font="font5" id="p3_t118" reading_order_no="117" segment_no="14" tag_type="text">tive samples is dominant, a smaller α is preferred to lower</text>
<text top="734" left="295" width="5" height="9" font="font5" id="p3_t119" reading_order_no="118" segment_no="16" tag_type="text">3</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font19" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font20" size="9" family="NimbusRomNo9L-Regu" color="#0c7dbe"/>
	<fontspec id="font21" size="9" family="NimbusRomNo9L-Regu" color="#cc3311"/>
	<fontspec id="font22" size="9" family="NimbusRomNo9L-Regu" color="#33bbee"/>
	<fontspec id="font23" size="9" family="NimbusRomNo9L-Regu" color="#bbbbbb"/>
	<fontspec id="font24" size="9" family="NimbusRomNo9L-Regu" color="#ff7043"/>
	<fontspec id="font25" size="9" family="NimbusRomNo9L-Regu" color="#009988"/>
	<fontspec id="font26" size="9" family="NimbusRomNo9L-Regu" color="#f06799"/>
<text top="174" left="115" width="44" height="8" font="font19" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="figure">(a) accuracy</text>
<text top="185" left="85" width="103" height="8" font="font19" id="p4_t2" reading_order_no="1" segment_no="0" tag_type="figure">( ViLBERT vs Visual BERT )</text>
<text top="174" left="225" width="44" height="8" font="font19" id="p4_t3" reading_order_no="2" segment_no="0" tag_type="figure">(b) accuracy</text>
<text top="185" left="225" width="166" height="8" font="font19" id="p4_t4" reading_order_no="3" segment_no="0" tag_type="figure">( MMF transformer vs MMBT image features )</text>
<text top="174" left="465" width="44" height="8" font="font19" id="p4_t5" reading_order_no="4" segment_no="0" tag_type="figure">(c) accuracy</text>
<text top="185" left="442" width="90" height="8" font="font19" id="p4_t6" reading_order_no="5" segment_no="0" tag_type="figure">(unimodal bert vs image )</text>
<text top="297" left="115" width="43" height="8" font="font19" id="p4_t7" reading_order_no="6" segment_no="0" tag_type="figure">(d) AUROC</text>
<text top="308" left="85" width="103" height="8" font="font19" id="p4_t8" reading_order_no="7" segment_no="0" tag_type="figure">( ViLBERT vs Visual BERT )</text>
<text top="297" left="225" width="43" height="8" font="font19" id="p4_t9" reading_order_no="8" segment_no="0" tag_type="figure">(e) AUROC</text>
<text top="308" left="225" width="166" height="8" font="font19" id="p4_t10" reading_order_no="9" segment_no="0" tag_type="figure">( MMF transformer vs MMBT image features )</text>
<text top="297" left="466" width="42" height="8" font="font19" id="p4_t11" reading_order_no="10" segment_no="0" tag_type="figure">(f) AUROC</text>
<text top="308" left="442" width="90" height="8" font="font19" id="p4_t12" reading_order_no="11" segment_no="0" tag_type="figure">(unimodal bert vs image )</text>
<text top="329" left="183" width="230" height="9" font="font5" id="p4_t13" reading_order_no="12" segment_no="1" tag_type="text">Figure 3: Accuracy and AUROC for 7 pre-trained models</text>
<text top="363" left="50" width="236" height="9" font="font5" id="p4_t14" reading_order_no="13" segment_no="2" tag_type="text">the impact by negative samples. γ is the focusing parameter,</text>
<text top="375" left="50" width="236" height="9" font="font5" id="p4_t15" reading_order_no="14" segment_no="2" tag_type="text">a weight to adjust the training strength on hard-to-classify</text>
<text top="387" left="50" width="237" height="9" font="font5" id="p4_t16" reading_order_no="15" segment_no="2" tag_type="text">samples. In our configuration, γ is set to 2.0 according to</text>
<text top="399" left="50" width="236" height="9" font="font5" id="p4_t17" reading_order_no="16" segment_no="2" tag_type="text">[ 5 ]. As for α , we found that the positive samples account</text>
<text top="411" left="50" width="236" height="9" font="font5" id="p4_t18" reading_order_no="17" segment_no="2" tag_type="text">for about 30% of the dataset, which should not be consid-</text>
<text top="422" left="50" width="237" height="10" font="font5" id="p4_t19" reading_order_no="18" segment_no="2" tag_type="text">ered an imbalance of samples, therefore we simply set α to</text>
<text top="435" left="50" width="62" height="9" font="font5" id="p4_t20" reading_order_no="19" segment_no="2" tag_type="text">1.0 empirically.</text>
<text top="450" left="62" width="225" height="9" font="font5" id="p4_t21" reading_order_no="20" segment_no="4" tag_type="text">One thing worthing to note is that in Figure 3 , all seven</text>
<text top="462" left="50" width="236" height="9" font="font5" id="p4_t22" reading_order_no="21" segment_no="4" tag_type="text">models reached their performance peak within just one or</text>
<text top="474" left="50" width="236" height="9" font="font5" id="p4_t23" reading_order_no="22" segment_no="4" tag_type="text">two thousand iterations, and then gradually corrupted after-</text>
<text top="486" left="50" width="236" height="9" font="font5" id="p4_t24" reading_order_no="23" segment_no="4" tag_type="text">ward. It suggests that the training set is somehow small.</text>
<text top="498" left="50" width="195" height="9" font="font5" id="p4_t25" reading_order_no="24" segment_no="4" tag_type="text">Regular fine-tuning on them will fast saturate.</text>
<text top="498" left="254" width="32" height="9" font="font5" id="p4_t26" reading_order_no="25" segment_no="4" tag_type="text">Without</text>
<text top="510" left="50" width="236" height="9" font="font5" id="p4_t27" reading_order_no="26" segment_no="4" tag_type="text">merged with the data sets that the models pre-trained on,</text>
<text top="522" left="50" width="236" height="9" font="font5" id="p4_t28" reading_order_no="27" segment_no="4" tag_type="text">they cannot contain the knowledge they already have for</text>
<text top="534" left="50" width="236" height="9" font="font5" id="p4_t29" reading_order_no="28" segment_no="4" tag_type="text">a long time and soon lose this knowledge. To relieve this</text>
<text top="545" left="50" width="236" height="9" font="font5" id="p4_t30" reading_order_no="29" segment_no="4" tag_type="text">problem, using an augmented training set is a practical way</text>
<text top="557" left="50" width="82" height="9" font="font5" id="p4_t31" reading_order_no="30" segment_no="4" tag_type="text">in fine-tuning phase.</text>
<text top="573" left="62" width="224" height="9" font="font5" id="p4_t32" reading_order_no="31" segment_no="7" tag_type="text">However, enlarging the training set is simple but a lit-</text>
<text top="585" left="50" width="236" height="9" font="font5" id="p4_t33" reading_order_no="32" segment_no="7" tag_type="text">tle tricky. The safest way to obtain new data is to collect a</text>
<text top="597" left="50" width="236" height="9" font="font5" id="p4_t34" reading_order_no="33" segment_no="7" tag_type="text">large amount of memes with or without text from the Inter-</text>
<text top="609" left="50" width="236" height="9" font="font5" id="p4_t35" reading_order_no="34" segment_no="7" tag_type="text">net, and then manually label them one by one. This method</text>
<text top="621" left="50" width="236" height="9" font="font5" id="p4_t36" reading_order_no="35" segment_no="7" tag_type="text">should be the most reliable, but it will take a long time.</text>
<text top="633" left="50" width="236" height="9" font="font5" id="p4_t37" reading_order_no="36" segment_no="7" tag_type="text">Therefore, we adopted an alternative scheme to increase the</text>
<text top="644" left="50" width="236" height="9" font="font5" id="p4_t38" reading_order_no="37" segment_no="7" tag_type="text">data set. First, since we only do validation on dev unseen,</text>
<text top="656" left="50" width="236" height="9" font="font5" id="p4_t39" reading_order_no="38" segment_no="7" tag_type="text">and dev seen contains additional annotation data, we merge</text>
<text top="668" left="50" width="236" height="9" font="font5" id="p4_t40" reading_order_no="39" segment_no="7" tag_type="text">the data in dev seen that does not appear in dev unseen into</text>
<text top="680" left="50" width="236" height="9" font="font5" id="p4_t41" reading_order_no="40" segment_no="7" tag_type="text">the training set, a total of 100 samples. Second, we used</text>
<text top="692" left="50" width="236" height="9" font="font5" id="p4_t42" reading_order_no="41" segment_no="7" tag_type="text">keywords such as “sarcasm memes”, “irony memes”, and</text>
<text top="704" left="50" width="236" height="9" font="font5" id="p4_t43" reading_order_no="42" segment_no="7" tag_type="text">“scorn memes” to collect 1500 additional pictures with the</text>
<text top="363" left="309" width="236" height="9" font="font5" id="p4_t44" reading_order_no="43" segment_no="3" tag_type="text">text through Google Image and Pinterest, and read the text</text>
<text top="375" left="309" width="236" height="9" font="font5" id="p4_t45" reading_order_no="44" segment_no="3" tag_type="text">on memes through Huawei Cloud OCR. It makes additional</text>
<text top="387" left="309" width="236" height="9" font="font5" id="p4_t46" reading_order_no="45" segment_no="3" tag_type="text">data consistent with the format of the hateful meme. To</text>
<text top="399" left="309" width="236" height="9" font="font5" id="p4_t47" reading_order_no="46" segment_no="3" tag_type="text">let the new data be compatible with our models, we dis-<a href="deeplearning_paper32.html#6">[</a></text>
<text top="411" left="309" width="236" height="9" font="font5" id="p4_t48" reading_order_no="47" segment_no="3" tag_type="text">carded the images with any non-alphanumeric characters.<a href="deeplearning_paper32.html#6">5</a></text>
<text top="423" left="309" width="236" height="9" font="font5" id="p4_t49" reading_order_no="48" segment_no="3" tag_type="text">After this, 1147 images were kept. Although without thor-<a href="deeplearning_paper32.html#6">]. </a>As for</text>
<text top="435" left="309" width="236" height="9" font="font5" id="p4_t50" reading_order_no="49" segment_no="3" tag_type="text">ough manual cleaning, we believe that among the memes</text>
<text top="447" left="309" width="236" height="9" font="font5" id="p4_t51" reading_order_no="50" segment_no="3" tag_type="text">obtained through the keywords mentioned above, those that</text>
<text top="458" left="309" width="236" height="9" font="font5" id="p4_t52" reading_order_no="51" segment_no="3" tag_type="text">were detected with texts by OCR can be marked as “hate-</text>
<text top="470" left="309" width="236" height="9" font="font5" id="p4_t53" reading_order_no="52" segment_no="3" tag_type="text">ful”, and those that were detected without any text are all</text>
<text top="482" left="309" width="236" height="9" font="font5" id="p4_t54" reading_order_no="53" segment_no="3" tag_type="text">labeled as “not hateful”. In the end, we got an expanded</text>
<text top="494" left="309" width="236" height="9" font="font5" id="p4_t55" reading_order_no="54" segment_no="3" tag_type="text">training data set with 8500+100+1147=9747 labeled sam-</text>
<text top="506" left="309" width="19" height="9" font="font5" id="p4_t56" reading_order_no="55" segment_no="3" tag_type="text">ples.</text>
<text top="531" left="353" width="149" height="9" font="font5" id="p4_t57" reading_order_no="56" segment_no="5" tag_type="title">Table 2: Scores in Fine Tuning Phase</text>
<text top="556" left="447" width="38" height="9" font="font5" id="p4_t58" reading_order_no="57" segment_no="6" tag_type="table">Accuracy<a href="deeplearning_paper32.html#4">3</a></text>
<text top="556" left="496" width="34" height="9" font="font5" id="p4_t59" reading_order_no="58" segment_no="6" tag_type="table">AUROC<a href="deeplearning_paper32.html#4">, </a>all seven</text>
<text top="573" left="359" width="40" height="9" font="font5" id="p4_t60" reading_order_no="59" segment_no="6" tag_type="table">ViLBERT</text>
<text top="573" left="452" width="27" height="9" font="font5" id="p4_t61" reading_order_no="60" segment_no="6" tag_type="table">0.7056</text>
<text top="573" left="500" width="27" height="9" font="font5" id="p4_t62" reading_order_no="61" segment_no="6" tag_type="table">0.7187</text>
<text top="585" left="330" width="99" height="9" font="font5" id="p4_t63" reading_order_no="62" segment_no="6" tag_type="table">ViLBERT w/ Focal Loss</text>
<text top="585" left="452" width="27" height="9" font="font5" id="p4_t64" reading_order_no="63" segment_no="6" tag_type="table">0.6852</text>
<text top="585" left="500" width="27" height="9" font="font5" id="p4_t65" reading_order_no="64" segment_no="6" tag_type="table">0.7188</text>
<text top="597" left="332" width="95" height="9" font="font5" id="p4_t66" reading_order_no="65" segment_no="6" tag_type="table">ViLBERT w/ more data</text>
<text top="597" left="452" width="27" height="9" font="font5" id="p4_t67" reading_order_no="66" segment_no="6" tag_type="table">0.7019</text>
<text top="597" left="500" width="27" height="9" font="font5" id="p4_t68" reading_order_no="67" segment_no="6" tag_type="table">0.7227</text>
<text top="614" left="353" width="53" height="9" font="font5" id="p4_t69" reading_order_no="68" segment_no="6" tag_type="table">Visual BERT</text>
<text top="614" left="452" width="27" height="9" font="font5" id="p4_t70" reading_order_no="69" segment_no="6" tag_type="table">0.7056</text>
<text top="614" left="500" width="27" height="9" font="font5" id="p4_t71" reading_order_no="70" segment_no="6" tag_type="table">0.7319</text>
<text top="626" left="324" width="111" height="9" font="font5" id="p4_t72" reading_order_no="71" segment_no="6" tag_type="table">Visual BERT w/ Focal Loss</text>
<text top="626" left="452" width="27" height="9" font="font5" id="p4_t73" reading_order_no="72" segment_no="6" tag_type="table">0.7056</text>
<text top="626" left="500" width="27" height="9" font="font5" id="p4_t74" reading_order_no="73" segment_no="6" tag_type="table">0.7070</text>
<text top="638" left="325" width="107" height="9" font="font5" id="p4_t75" reading_order_no="74" segment_no="6" tag_type="table">Visual BERT w/ more data</text>
<text top="638" left="452" width="27" height="9" font="font5" id="p4_t76" reading_order_no="75" segment_no="6" tag_type="table">0.7037</text>
<text top="638" left="500" width="27" height="9" font="font5" id="p4_t77" reading_order_no="76" segment_no="6" tag_type="table">0.7439</text>
<text top="667" left="321" width="224" height="9" font="font5" id="p4_t78" reading_order_no="77" segment_no="8" tag_type="text">The results of fine tuning phase are listed in Table 2 .</text>
<text top="679" left="309" width="174" height="9" font="font5" id="p4_t79" reading_order_no="78" segment_no="8" tag_type="text">Their learning curves are shown in Figure 4</text>
<text top="692" left="321" width="224" height="9" font="font5" id="p4_t80" reading_order_no="79" segment_no="9" tag_type="text">Using Focal Loss, unfortunately, failed to improve Vil-</text>
<text top="704" left="309" width="236" height="9" font="font5" id="p4_t81" reading_order_no="80" segment_no="9" tag_type="text">BERT and Visual BERT. Although ViLBERT achieved a</text>
<text top="734" left="295" width="5" height="9" font="font5" id="p4_t82" reading_order_no="81" segment_no="10" tag_type="text">4</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
<text top="188" left="110" width="128" height="8" font="font19" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">(a) Vilbert extral , focalloss , original</text>
<text top="188" left="347" width="153" height="8" font="font19" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="text">(b) Visual BERT extral , focalloss , original</text>
<text top="209" left="180" width="236" height="9" font="font5" id="p5_t3" reading_order_no="2" segment_no="2" tag_type="text">Figure 4: AUROC for Vilbert(left) and Visual BERT(right)</text>
<text top="243" left="50" width="236" height="9" font="font5" id="p5_t4" reading_order_no="3" segment_no="3" tag_type="text">0.0001 improvement in AUROC with the application of Fo-</text>
<text top="255" left="50" width="236" height="9" font="font5" id="p5_t5" reading_order_no="4" segment_no="3" tag_type="text">cal Loss, this degree of improvement can be considered as</text>
<text top="267" left="50" width="236" height="9" font="font5" id="p5_t6" reading_order_no="5" segment_no="3" tag_type="text">random error. This shows that the application of Focal Loss</text>
<text top="279" left="50" width="236" height="9" font="font5" id="p5_t7" reading_order_no="6" segment_no="3" tag_type="text">may not be suitable for the Hateful Memes data set. Fo-</text>
<text top="290" left="50" width="236" height="9" font="font5" id="p5_t8" reading_order_no="7" segment_no="3" tag_type="text">cal Loss was originally used for object detection tasks. The</text>
<text top="302" left="50" width="236" height="9" font="font5" id="p5_t9" reading_order_no="8" segment_no="3" tag_type="text">characteristic of object detection is that the number of neg-</text>
<text top="314" left="50" width="236" height="9" font="font5" id="p5_t10" reading_order_no="9" segment_no="3" tag_type="text">ative samples is much greater than the number of positive</text>
<text top="326" left="50" width="236" height="9" font="font5" id="p5_t11" reading_order_no="10" segment_no="3" tag_type="text">samples, so it should be effective to reweight them for diffi-</text>
<text top="338" left="50" width="236" height="9" font="font5" id="p5_t12" reading_order_no="11" segment_no="3" tag_type="text">cult samples. In the Hateful Memes data set, as mentioned</text>
<text top="350" left="50" width="236" height="9" font="font5" id="p5_t13" reading_order_no="12" segment_no="3" tag_type="text">above, the number of positive samples is more than 30%.</text>
<text top="362" left="50" width="236" height="9" font="font5" id="p5_t14" reading_order_no="13" segment_no="3" tag_type="text">In this case, Focal Loss makes the model get an inappro-</text>
<text top="374" left="50" width="236" height="9" font="font5" id="p5_t15" reading_order_no="14" segment_no="3" tag_type="text">priate reweighted gradient, making its performance on the</text>
<text top="386" left="50" width="82" height="9" font="font5" id="p5_t16" reading_order_no="15" segment_no="3" tag_type="text">validation set worse.</text>
<text top="398" left="62" width="224" height="9" font="font5" id="p5_t17" reading_order_no="16" segment_no="7" tag_type="text">The use of the expanded data sets improves the perfor-</text>
<text top="410" left="50" width="236" height="9" font="font5" id="p5_t18" reading_order_no="17" segment_no="7" tag_type="text">mance of both ViLBERT and Visual BERT. Visual BERT’s</text>
<text top="422" left="50" width="236" height="9" font="font5" id="p5_t19" reading_order_no="18" segment_no="7" tag_type="text">AUROC has increased by more than two percentage points.</text>
<text top="434" left="50" width="236" height="9" font="font5" id="p5_t20" reading_order_no="19" segment_no="7" tag_type="text">ViLBERT’s AUROC has increased by nearly half a percent-</text>
<text top="446" left="50" width="236" height="9" font="font5" id="p5_t21" reading_order_no="20" segment_no="7" tag_type="text">age point. However, the accuracy of both has a slight de-</text>
<text top="458" left="50" width="236" height="9" font="font5" id="p5_t22" reading_order_no="21" segment_no="7" tag_type="text">crease. Overall, the fine tuning strategy using augmented</text>
<text top="470" left="50" width="236" height="9" font="font5" id="p5_t23" reading_order_no="22" segment_no="7" tag_type="text">data sets is effective. This also confirms our previous anal-</text>
<text top="482" left="50" width="236" height="9" font="font5" id="p5_t24" reading_order_no="23" segment_no="7" tag_type="text">ysis on the learning curve. Using a larger data set can make</text>
<text top="494" left="50" width="236" height="9" font="font5" id="p5_t25" reading_order_no="24" segment_no="7" tag_type="text">the fine tune model more robust. Though from Figure 4 ,</text>
<text top="506" left="50" width="236" height="9" font="font5" id="p5_t26" reading_order_no="25" segment_no="7" tag_type="text">we can see that overfitting still exists, however is slightly</text>
<text top="518" left="50" width="153" height="9" font="font5" id="p5_t27" reading_order_no="26" segment_no="7" tag_type="text">delayed compared to those in Figure 3</text>
<text top="643" left="50" width="236" height="9" font="font5" id="p5_t28" reading_order_no="27" segment_no="10" tag_type="text">Figure 5: A new-added sample with pure black background</text>
<text top="668" left="62" width="224" height="9" font="font5" id="p5_t29" reading_order_no="28" segment_no="11" tag_type="text">As for why the accuracy of both has decreased, we think</text>
<text top="680" left="50" width="236" height="9" font="font5" id="p5_t30" reading_order_no="29" segment_no="11" tag_type="text">that there are some of the new-added samples, as shown</text>
<text top="692" left="50" width="236" height="9" font="font5" id="p5_t31" reading_order_no="30" segment_no="11" tag_type="text">in Figure 5 , are pure black backgrounds with some texts.</text>
<text top="704" left="50" width="236" height="9" font="font5" id="p5_t32" reading_order_no="31" segment_no="11" tag_type="text">Compared with the original training set and development</text>
<text top="243" left="309" width="236" height="9" font="font5" id="p5_t33" reading_order_no="32" segment_no="4" tag_type="text">set, this sample is very much different. Such samples may</text>
<text top="255" left="309" width="143" height="9" font="font5" id="p5_t34" reading_order_no="33" segment_no="4" tag_type="text">harm the performance of the model.</text>
<text top="277" left="309" width="161" height="11" font="font2" id="p5_t35" reading_order_no="34" segment_no="5" tag_type="title">4. Conclusion and Future Work</text>
<text top="297" left="321" width="224" height="9" font="font5" id="p5_t36" reading_order_no="35" segment_no="6" tag_type="text">We proposed an approach detecting Hateful Memes in<a href="deeplearning_paper32.html#5">4</a></text>
<text top="309" left="309" width="236" height="9" font="font5" id="p5_t37" reading_order_no="36" segment_no="6" tag_type="text">a small multimodal dataset from Facebook. We set up a<a href="deeplearning_paper32.html#5">,</a></text>
<text top="321" left="309" width="236" height="9" font="font5" id="p5_t38" reading_order_no="37" segment_no="6" tag_type="text">common workflow to exam and measure the performance of</text>
<text top="333" left="309" width="236" height="9" font="font5" id="p5_t39" reading_order_no="38" segment_no="6" tag_type="text">different models. We conclude that Visual BERT and ViL-</text>
<text top="345" left="309" width="236" height="9" font="font5" id="p5_t40" reading_order_no="39" segment_no="6" tag_type="text">BERT are better models for this challenge. We adjusted our<a href="deeplearning_paper32.html#4">3</a></text>
<text top="357" left="309" width="236" height="9" font="font5" id="p5_t41" reading_order_no="40" segment_no="6" tag_type="text">fine-turn strategy and proved data augmentation is an effec-</text>
<text top="369" left="309" width="236" height="9" font="font5" id="p5_t42" reading_order_no="41" segment_no="6" tag_type="text">tive solution for both Visual BERT and ViLBERT. Our ap-</text>
<text top="381" left="309" width="236" height="9" font="font5" id="p5_t43" reading_order_no="42" segment_no="6" tag_type="text">proach achieves 0.7439 AUROC with an accuracy of 0.7037</text>
<text top="393" left="309" width="236" height="9" font="font5" id="p5_t44" reading_order_no="43" segment_no="6" tag_type="text">on the challenge test set. We hope that these observations</text>
<text top="405" left="309" width="183" height="9" font="font5" id="p5_t45" reading_order_no="44" segment_no="6" tag_type="text">can further inform future research in the field.<a href="deeplearning_paper32.html#5">5</a></text>
<text top="417" left="321" width="224" height="9" font="font5" id="p5_t46" reading_order_no="45" segment_no="8" tag_type="text">The combination of text and images is a huge sam-<a href="deeplearning_paper32.html#5">, </a>are pure black backgrounds with some texts.</text>
<text top="429" left="309" width="236" height="9" font="font5" id="p5_t47" reading_order_no="46" segment_no="8" tag_type="text">ple space, so a large number of samples are essential.</text>
<text top="441" left="309" width="236" height="9" font="font5" id="p5_t48" reading_order_no="47" segment_no="8" tag_type="text">ImageNet[ 8 ] is a well-known image classification pre-</text>
<text top="453" left="309" width="236" height="9" font="font5" id="p5_t49" reading_order_no="48" segment_no="8" tag_type="text">training data set with over one million samples. The corpus</text>
<text top="465" left="309" width="236" height="9" font="font5" id="p5_t50" reading_order_no="49" segment_no="8" tag_type="text">used by BERT has reached the magnitude of ten million. In</text>
<text top="477" left="309" width="236" height="9" font="font5" id="p5_t51" reading_order_no="50" segment_no="8" tag_type="text">contrast, the amount of data contained in Hateful Memes is</text>
<text top="489" left="309" width="236" height="9" font="font5" id="p5_t52" reading_order_no="51" segment_no="8" tag_type="text">too small. Future work can start from the amount of data,</text>
<text top="501" left="309" width="236" height="9" font="font5" id="p5_t53" reading_order_no="52" segment_no="8" tag_type="text">take into account samples with multiple characteristics, and</text>
<text top="513" left="309" width="236" height="9" font="font5" id="p5_t54" reading_order_no="53" segment_no="8" tag_type="text">use automatic or semi-automatic methods to increase the</text>
<text top="525" left="309" width="213" height="9" font="font5" id="p5_t55" reading_order_no="54" segment_no="8" tag_type="text">number of data sets at least several times the original.</text>
<text top="537" left="321" width="224" height="9" font="font5" id="p5_t56" reading_order_no="55" segment_no="9" tag_type="text">Another direction is data augmentation on the hateful-</text>
<text top="549" left="309" width="236" height="9" font="font5" id="p5_t57" reading_order_no="56" segment_no="9" tag_type="text">meme dataset. Some color-related operations, such as color</text>
<text top="560" left="309" width="236" height="9" font="font5" id="p5_t58" reading_order_no="57" segment_no="9" tag_type="text">jitter, inversion, channel swap, or deblurring, might help the</text>
<text top="572" left="309" width="236" height="9" font="font5" id="p5_t59" reading_order_no="58" segment_no="9" tag_type="text">model get out from being trapped object-unrelated details in</text>
<text top="584" left="309" width="236" height="9" font="font5" id="p5_t60" reading_order_no="59" segment_no="9" tag_type="text">the images. Shape-related techniques like rotation and per-</text>
<text top="596" left="309" width="236" height="9" font="font5" id="p5_t61" reading_order_no="60" segment_no="9" tag_type="text">spective transforming might get rid of some biased informa-</text>
<text top="608" left="309" width="236" height="9" font="font5" id="p5_t62" reading_order_no="61" segment_no="9" tag_type="text">tion from the images. Also, recent GAN-based techniques,</text>
<text top="620" left="309" width="236" height="9" font="font5" id="p5_t63" reading_order_no="62" segment_no="9" tag_type="text">like SRNet[ 10 ], can directly replace the text on an image<a href="deeplearning_paper32.html#6">ImageNet[</a></text>
<text top="632" left="309" width="236" height="9" font="font5" id="p5_t64" reading_order_no="63" segment_no="9" tag_type="text">with a new one while keeping its original style. This tech-<a href="deeplearning_paper32.html#6">8</a></text>
<text top="644" left="309" width="236" height="9" font="font5" id="p5_t65" reading_order_no="64" segment_no="9" tag_type="text">nique could augment the data by giving us a lot of edited<a href="deeplearning_paper32.html#6">] </a>is a well-known image classification pre-</text>
<text top="656" left="309" width="236" height="9" font="font5" id="p5_t66" reading_order_no="65" segment_no="9" tag_type="text">memes with any labels we want. Through those augmenta-</text>
<text top="668" left="309" width="236" height="9" font="font5" id="p5_t67" reading_order_no="66" segment_no="9" tag_type="text">tions can the data set for surely be enlarged for a consider-</text>
<text top="680" left="309" width="227" height="9" font="font5" id="p5_t68" reading_order_no="67" segment_no="9" tag_type="text">able amount, and our model can finally benefit from that.</text>
<text top="734" left="295" width="5" height="9" font="font5" id="p5_t69" reading_order_no="68" segment_no="12" tag_type="text">5</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font27" size="9" family="NimbusRomNo9L-Regu" color="#ff0000"/>
<text top="74" left="50" width="56" height="11" font="font2" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="title">References</text>
<text top="94" left="55" width="232" height="8" font="font19" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">[1] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-</text>
<text top="105" left="70" width="216" height="8" font="font19" id="p6_t3" reading_order_no="2" segment_no="1" tag_type="text">tam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zit-</text>
<text top="116" left="70" width="216" height="8" font="font19" id="p6_t4" reading_order_no="3" segment_no="1" tag_type="text">nick. Microsoft coco captions: Data collection and evalu-</text>
<text top="127" left="70" width="74" height="8" font="font19" id="p6_t5" reading_order_no="4" segment_no="1" tag_type="text">ation server, 2015. 2</text>
<text top="139" left="55" width="232" height="8" font="font19" id="p6_t6" reading_order_no="5" segment_no="2" tag_type="text">[2] Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez,<a href="deeplearning_paper32.html#2">2</a></text>
<text top="150" left="70" width="216" height="8" font="font19" id="p6_t7" reading_order_no="6" segment_no="2" tag_type="text">and Davide Testuggine. Supervised multimodal bitransform-</text>
<text top="161" left="70" width="155" height="8" font="font19" id="p6_t8" reading_order_no="7" segment_no="2" tag_type="text">ers for classifying images and text, 2019. 3</text>
<text top="173" left="55" width="232" height="8" font="font19" id="p6_t9" reading_order_no="8" segment_no="3" tag_type="text">[3] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj</text>
<text top="183" left="70" width="216" height="8" font="font19" id="p6_t10" reading_order_no="9" segment_no="3" tag_type="text">Goswami, Amanpreet Singh, Pratik Ringshia, and Davide<a href="deeplearning_paper32.html#3">3</a></text>
<text top="194" left="70" width="216" height="8" font="font19" id="p6_t11" reading_order_no="10" segment_no="3" tag_type="text">Testuggine. The hateful memes challenge: Detecting hate</text>
<text top="205" left="70" width="146" height="8" font="font19" id="p6_t12" reading_order_no="11" segment_no="3" tag_type="text">speech in multimodal memes, 2020. 1 , 2</text>
<text top="217" left="55" width="232" height="8" font="font19" id="p6_t13" reading_order_no="12" segment_no="4" tag_type="text">[4] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,</text>
<text top="228" left="70" width="216" height="8" font="font19" id="p6_t14" reading_order_no="13" segment_no="4" tag_type="text">and Kai-Wei Chang. Visualbert: A simple and performant</text>
<text top="239" left="70" width="157" height="8" font="font19" id="p6_t15" reading_order_no="14" segment_no="4" tag_type="text">baseline for vision and language, 2019. 1 , 3<a href="deeplearning_paper32.html#1">1</a></text>
<text top="251" left="55" width="232" height="8" font="font19" id="p6_t16" reading_order_no="15" segment_no="5" tag_type="text">[5] Tsung-Yi Lin, Priya Goyal, Kaiming He Ross Girshick, and<a href="deeplearning_paper32.html#1">,</a></text>
<text top="262" left="70" width="216" height="8" font="font19" id="p6_t17" reading_order_no="16" segment_no="5" tag_type="text">Piotr Doll´ar. Focal loss for dense object detection, 2017. 2 ,<a href="deeplearning_paper32.html#2">2</a></text>
<text top="273" left="70" width="4" height="8" font="font27" id="p6_t18" reading_order_no="17" segment_no="5" tag_type="text">4</text>
<text top="285" left="55" width="232" height="8" font="font19" id="p6_t19" reading_order_no="18" segment_no="6" tag_type="text">[6] Phillip Lippe, Nithin Holla, Shantanu Chandra, Santhosh</text>
<text top="296" left="70" width="216" height="8" font="font19" id="p6_t20" reading_order_no="19" segment_no="6" tag_type="text">Rajamanickam, Georgios Antoniou, Ekaterina Shutova, and</text>
<text top="307" left="70" width="216" height="8" font="font19" id="p6_t21" reading_order_no="20" segment_no="6" tag_type="text">Helen Yannakoudakis. A multimodal framework for the de-<a href="deeplearning_paper32.html#1">1</a></text>
<text top="318" left="70" width="121" height="8" font="font19" id="p6_t22" reading_order_no="21" segment_no="6" tag_type="text">tection of hateful memes, 2020. 1<a href="deeplearning_paper32.html#1">,</a></text>
<text top="330" left="55" width="232" height="8" font="font19" id="p6_t23" reading_order_no="22" segment_no="7" tag_type="text">[7] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:<a href="deeplearning_paper32.html#3">3</a></text>
<text top="341" left="70" width="216" height="8" font="font19" id="p6_t24" reading_order_no="23" segment_no="7" tag_type="text">Pretraining task-agnostic visiolinguistic representations for</text>
<text top="352" left="70" width="125" height="8" font="font19" id="p6_t25" reading_order_no="24" segment_no="7" tag_type="text">vision-and-language tasks, 2019. 1</text>
<text top="364" left="55" width="232" height="8" font="font19" id="p6_t26" reading_order_no="25" segment_no="8" tag_type="text">[8] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-<a href="deeplearning_paper32.html#2">2</a></text>
<text top="375" left="70" width="216" height="8" font="font19" id="p6_t27" reading_order_no="26" segment_no="8" tag_type="text">jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,<a href="deeplearning_paper32.html#2">,</a></text>
<text top="386" left="70" width="216" height="8" font="font19" id="p6_t28" reading_order_no="27" segment_no="8" tag_type="text">Aditya Khosla, Michael Bernstein, Alexander C. Berg, and<a href="deeplearning_paper32.html#4">4</a></text>
<text top="397" left="70" width="40" height="8" font="font19" id="p6_t29" reading_order_no="28" segment_no="8" tag_type="text">Li Fei-Fei.</text>
<text top="397" left="119" width="168" height="8" font="font19" id="p6_t30" reading_order_no="29" segment_no="8" tag_type="text">Imagenet large-scale visual recognition chal-</text>
<text top="408" left="70" width="51" height="8" font="font19" id="p6_t31" reading_order_no="30" segment_no="8" tag_type="text">lenge, 2014. 5</text>
<text top="420" left="55" width="232" height="8" font="font19" id="p6_t32" reading_order_no="31" segment_no="9" tag_type="text">[9] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu</text>
<text top="431" left="70" width="216" height="8" font="font19" id="p6_t33" reading_order_no="32" segment_no="9" tag_type="text">Soricut. Conceptual captions: A cleaned, hypernymed, im-<a href="deeplearning_paper32.html#1">1</a></text>
<text top="442" left="70" width="216" height="8" font="font19" id="p6_t34" reading_order_no="33" segment_no="9" tag_type="text">age alt-text dataset for automatic image captioning, July</text>
<text top="452" left="70" width="27" height="8" font="font19" id="p6_t35" reading_order_no="34" segment_no="9" tag_type="text">2018. 2</text>
<text top="464" left="50" width="236" height="8" font="font19" id="p6_t36" reading_order_no="35" segment_no="10" tag_type="text">[10] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jing-</text>
<text top="475" left="70" width="216" height="8" font="font19" id="p6_t37" reading_order_no="36" segment_no="10" tag_type="text">tuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild,<a href="deeplearning_paper32.html#1">1</a></text>
<text top="486" left="70" width="27" height="8" font="font19" id="p6_t38" reading_order_no="37" segment_no="10" tag_type="text">2019. 5</text>
<text top="734" left="295" width="5" height="9" font="font5" id="p6_t39" reading_order_no="38" segment_no="11" tag_type="text">6</text>
</page>
</pdf2xml>
