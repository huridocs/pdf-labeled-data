<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="7" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="17" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font2" size="11" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="9" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font4" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font5" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font6" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font7" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font8" size="9" family="CMSY9" color="#000000"/>
	<fontspec id="font9" size="9" family="NimbusMonL-Regu" color="#000000"/>
	<fontspec id="font10" size="8" family="ArialMT" color="#000000"/>
	<fontspec id="font11" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font12" size="20" family="Times" color="#7f7f7f"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="text">1</text>
<text top="60" left="61" width="490" height="15" font="font1" id="p1_t2" reading_order_no="2" segment_no="2" tag_type="title">Place recognition survey: An update on deep learning approaches</text>
<text top="88" left="117" width="377" height="10" font="font2" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="text">Tiago Barros, Ricardo Pereira, Lu´ıs Garrote, Cristiano Premebida, Urbano J. Nunes</text>
<text top="142" left="59" width="241" height="8" font="font3" id="p1_t4" reading_order_no="4" segment_no="3" tag_type="text">Abstract —Autonomous Vehicles (AV) are becoming more ca-</text>
<text top="152" left="49" width="251" height="8" font="font4" id="p1_t5" reading_order_no="5" segment_no="3" tag_type="text">pable of navigating in complex environments with dynamic</text>
<text top="162" left="49" width="251" height="8" font="font4" id="p1_t6" reading_order_no="6" segment_no="3" tag_type="text">and changing conditions. A key component that enables these</text>
<text top="172" left="49" width="251" height="8" font="font4" id="p1_t7" reading_order_no="7" segment_no="3" tag_type="text">intelligent vehicles to overcome such conditions and become</text>
<text top="182" left="49" width="251" height="8" font="font4" id="p1_t8" reading_order_no="8" segment_no="3" tag_type="text">more autonomous is the sophistication of the perception and</text>
<text top="192" left="49" width="251" height="8" font="font4" id="p1_t9" reading_order_no="9" segment_no="3" tag_type="text">localization systems. As part of the localization system, place</text>
<text top="202" left="49" width="251" height="8" font="font4" id="p1_t10" reading_order_no="10" segment_no="3" tag_type="text">recognition has benefited from recent developments in other per-</text>
<text top="212" left="49" width="251" height="8" font="font4" id="p1_t11" reading_order_no="11" segment_no="3" tag_type="text">ception tasks such as place categorization or object recognition,</text>
<text top="222" left="49" width="251" height="8" font="font4" id="p1_t12" reading_order_no="12" segment_no="3" tag_type="text">namely with the emergence of deep learning (DL) frameworks.</text>
<text top="232" left="49" width="251" height="8" font="font4" id="p1_t13" reading_order_no="13" segment_no="3" tag_type="text">This paper surveys recent approaches and methods used in</text>
<text top="242" left="49" width="251" height="8" font="font4" id="p1_t14" reading_order_no="14" segment_no="3" tag_type="text">place recognition, particularly those based on deep learning. The</text>
<text top="252" left="49" width="251" height="8" font="font4" id="p1_t15" reading_order_no="15" segment_no="3" tag_type="text">contributions of this work are twofold: surveying recent sensors</text>
<text top="261" left="49" width="251" height="8" font="font4" id="p1_t16" reading_order_no="16" segment_no="3" tag_type="text">such as 3D LiDARs and RADARs, applied in place recognition;</text>
<text top="271" left="49" width="251" height="8" font="font4" id="p1_t17" reading_order_no="17" segment_no="3" tag_type="text">and categorizing the various DL-based place recognition works</text>
<text top="281" left="49" width="251" height="8" font="font4" id="p1_t18" reading_order_no="18" segment_no="3" tag_type="text">into supervised, unsupervised, semi-supervised, parallel, and</text>
<text top="291" left="49" width="251" height="8" font="font4" id="p1_t19" reading_order_no="19" segment_no="3" tag_type="text">hierarchical categories. First, this survey introduces key place</text>
<text top="301" left="49" width="251" height="8" font="font4" id="p1_t20" reading_order_no="20" segment_no="3" tag_type="text">recognition concepts to contextualize the reader. Then, sensor</text>
<text top="311" left="49" width="251" height="8" font="font4" id="p1_t21" reading_order_no="21" segment_no="3" tag_type="text">characteristics are addressed. This survey proceeds by elaborat-</text>
<text top="321" left="49" width="251" height="8" font="font4" id="p1_t22" reading_order_no="22" segment_no="3" tag_type="text">ing on the various DL-based works, presenting summaries for</text>
<text top="331" left="49" width="251" height="8" font="font4" id="p1_t23" reading_order_no="23" segment_no="3" tag_type="text">each framework. Some lessons learned from this survey include:</text>
<text top="341" left="49" width="251" height="8" font="font4" id="p1_t24" reading_order_no="24" segment_no="3" tag_type="text">the importance of NetVLAD for supervised end-to-end learning;</text>
<text top="351" left="49" width="251" height="8" font="font4" id="p1_t25" reading_order_no="25" segment_no="3" tag_type="text">the advantages of unsupervised approaches in place recognition,</text>
<text top="361" left="49" width="251" height="8" font="font4" id="p1_t26" reading_order_no="26" segment_no="3" tag_type="text">namely for cross-domain applications; or the increasing tendency</text>
<text top="371" left="49" width="251" height="8" font="font4" id="p1_t27" reading_order_no="27" segment_no="3" tag_type="text">of recent works to seek, not only for higher performance but also</text>
<text top="381" left="49" width="80" height="8" font="font4" id="p1_t28" reading_order_no="28" segment_no="3" tag_type="text">for higher efficiency.</text>
<text top="397" left="59" width="241" height="8" font="font3" id="p1_t29" reading_order_no="29" segment_no="6" tag_type="text">Index Terms —Place recognition, Deep Learning, Localization.</text>
<text top="435" left="136" width="77" height="9" font="font5" id="p1_t30" reading_order_no="30" segment_no="8" tag_type="title">I. I NTRODUCTION</text>
<text top="451" left="59" width="241" height="9" font="font5" id="p1_t31" reading_order_no="31" segment_no="9" tag_type="text">Self-driving vehicles are increasingly able to deal with</text>
<text top="463" left="49" width="251" height="9" font="font5" id="p1_t32" reading_order_no="32" segment_no="9" tag_type="text">unstructured and dynamic environments, which is mainly due</text>
<text top="475" left="49" width="251" height="9" font="font5" id="p1_t33" reading_order_no="33" segment_no="9" tag_type="text">to the development of more robust long-term localization and</text>
<text top="487" left="49" width="251" height="9" font="font5" id="p1_t34" reading_order_no="34" segment_no="9" tag_type="text">perception systems. A critical aspect of long-term localization</text>
<text top="499" left="49" width="251" height="9" font="font5" id="p1_t35" reading_order_no="35" segment_no="9" tag_type="text">is to guarantee coherent mapping and bounded error over</text>
<text top="511" left="49" width="251" height="9" font="font5" id="p1_t36" reading_order_no="36" segment_no="9" tag_type="text">time, which is achieved by finding loops in revisited areas.</text>
<text top="523" left="49" width="251" height="9" font="font5" id="p1_t37" reading_order_no="37" segment_no="9" tag_type="text">Revisited places are detected in long-term localization systems</text>
<text top="535" left="49" width="251" height="9" font="font5" id="p1_t38" reading_order_no="38" segment_no="9" tag_type="text">by resorting to approaches such as place recognition and</text>
<text top="547" left="49" width="251" height="9" font="font5" id="p1_t39" reading_order_no="39" segment_no="9" tag_type="text">loop closure. Namely, place recognition is a perception based</text>
<text top="559" left="49" width="251" height="9" font="font5" id="p1_t40" reading_order_no="40" segment_no="9" tag_type="text">approach that recognizes previously visited places based on</text>
<text top="571" left="49" width="144" height="9" font="font5" id="p1_t41" reading_order_no="41" segment_no="9" tag_type="text">visual, structural, or semantic cues.</text>
<text top="583" left="59" width="241" height="9" font="font5" id="p1_t42" reading_order_no="42" segment_no="15" tag_type="text">Place recognition has been the focus of much research over</text>
<text top="595" left="49" width="251" height="9" font="font5" id="p1_t43" reading_order_no="43" segment_no="15" tag_type="text">the last decade. The efforts of the intelligent vehicle and</text>
<text top="607" left="49" width="251" height="9" font="font5" id="p1_t44" reading_order_no="44" segment_no="15" tag_type="text">machine vision communities, including those devoted to place</text>
<text top="618" left="49" width="251" height="9" font="font5" id="p1_t45" reading_order_no="45" segment_no="15" tag_type="text">recognition, resulted in great achievements, namely evolving</text>
<text top="630" left="49" width="251" height="9" font="font5" id="p1_t46" reading_order_no="46" segment_no="15" tag_type="text">towards systems that achieve promising performances in ap-</text>
<text top="642" left="49" width="251" height="9" font="font5" id="p1_t47" reading_order_no="47" segment_no="15" tag_type="text">pearance changing and extreme viewpoint variation conditions.</text>
<text top="654" left="49" width="251" height="9" font="font5" id="p1_t48" reading_order_no="48" segment_no="15" tag_type="text">Despite the recent achievements, the fundamental challenges</text>
<text top="666" left="49" width="151" height="9" font="font5" id="p1_t49" reading_order_no="49" segment_no="15" tag_type="text">remain unsolved, which occur when:</text>
<text top="680" left="55" width="245" height="9" font="font7" id="p1_t50" reading_order_no="50" segment_no="17" tag_type="list">− two distinct places look similar (also known as perceptual</text>
<text top="692" left="69" width="37" height="9" font="font5" id="p1_t51" reading_order_no="51" segment_no="17" tag_type="list">aliasing);</text>
<text top="714" left="57" width="243" height="7" font="font6" id="p1_t52" reading_order_no="89" segment_no="18" tag_type="footnote">The authors are with the University of Coimbra, Institute of Systems</text>
<text top="723" left="49" width="251" height="7" font="font6" id="p1_t53" reading_order_no="90" segment_no="18" tag_type="footnote">and Robotics, Department of Electrical and Computer Engineering, Portu-</text>
<text top="730" left="49" width="251" height="9" font="font8" id="p1_t54" reading_order_no="91" segment_no="18" tag_type="footnote">gal. E-mail: { tiagobarros, ricardo.pereira, garrote,</text>
<text top="739" left="49" width="155" height="8" font="font9" id="p1_t55" reading_order_no="92" segment_no="18" tag_type="footnote">cpremebida, urbano } @isr.uc.pt</text>
<text top="209" left="469" width="52" height="8" font="font10" id="p1_t56" reading_order_no="55" segment_no="4" tag_type="figure">Place Matching</text>
<text top="228" left="466" width="58" height="8" font="font10" id="p1_t57" reading_order_no="56" segment_no="4" tag_type="figure">Belief Generation</text>
<text top="228" left="362" width="52" height="8" font="font10" id="p1_t58" reading_order_no="53" segment_no="4" tag_type="figure">Place Modeling</text>
<text top="143" left="455" width="79" height="8" font="font10" id="p1_t59" reading_order_no="54" segment_no="4" tag_type="figure">Internal Representation</text>
<text top="211" left="557" width="0" height="8" font="font10" id="p1_t60" reading_order_no="57" segment_no="4" tag_type="figure">Loop Candidates</text>
<text top="203" left="323" width="0" height="8" font="font10" id="p1_t61" reading_order_no="52" segment_no="4" tag_type="figure">Sensor Data</text>
<text top="253" left="312" width="251" height="7" font="font6" id="p1_t62" reading_order_no="58" segment_no="5" tag_type="text">Fig. 1. Generic place recognition pipeline with the following modules: place</text>
<text top="262" left="312" width="251" height="7" font="font6" id="p1_t63" reading_order_no="59" segment_no="5" tag_type="text">modeling, belief generation and place mapping. Place modeling creates an</text>
<text top="271" left="312" width="251" height="7" font="font6" id="p1_t64" reading_order_no="60" segment_no="5" tag_type="text">internal place representation. Place mapping is concerned with maintaining</text>
<text top="280" left="312" width="251" height="7" font="font6" id="p1_t65" reading_order_no="61" segment_no="5" tag_type="text">a coherent representation of places over time. And Belief generation, finally,</text>
<text top="289" left="312" width="245" height="7" font="font6" id="p1_t66" reading_order_no="62" segment_no="5" tag_type="text">generates, based on the current place model and the map, loop candidates.</text>
<text top="418" left="312" width="251" height="7" font="font6" id="p1_t67" reading_order_no="63" segment_no="7" tag_type="text">Fig. 2. Illustration of the seasonal environment changes. Images taken from</text>
<text top="427" left="312" width="166" height="7" font="font6" id="p1_t68" reading_order_no="64" segment_no="7" tag_type="text">the Oxford Robotcar [1] and Nordland dataset [2].</text>
<text top="458" left="318" width="245" height="10" font="font7" id="p1_t69" reading_order_no="65" segment_no="10" tag_type="list">− the same places exhibit significant appearance changes</text>
<text top="471" left="332" width="231" height="9" font="font5" id="p1_t70" reading_order_no="66" segment_no="10" tag_type="list">over time due to day-night variation, weather, seasonal</text>
<text top="483" left="332" width="174" height="9" font="font5" id="p1_t71" reading_order_no="67" segment_no="10" tag_type="list">or structural changes (as shown in Fig. 2);</text>
<text top="494" left="318" width="245" height="10" font="font7" id="p1_t72" reading_order_no="68" segment_no="11" tag_type="list">− same places are perceived from different viewpoints or</text>
<text top="507" left="332" width="38" height="9" font="font5" id="p1_t73" reading_order_no="69" segment_no="11" tag_type="list">positions.</text>
<text top="522" left="312" width="251" height="9" font="font5" id="p1_t74" reading_order_no="70" segment_no="12" tag_type="text">Solving these challenges is essential to enable robust place</text>
<text top="534" left="312" width="215" height="9" font="font5" id="p1_t75" reading_order_no="71" segment_no="12" tag_type="text">recognition and consequently long-term localization.</text>
<text top="547" left="322" width="241" height="9" font="font5" id="p1_t76" reading_order_no="72" segment_no="14" tag_type="text">The primary motivation for writing this survey paper is to<a href="deeplearning_paper41.html#13">[1] </a>and Nordland dataset <a href="deeplearning_paper41.html#13">[2].</a></text>
<text top="558" left="312" width="251" height="9" font="font5" id="p1_t77" reading_order_no="73" segment_no="14" tag_type="text">provide an updated review of the recent place recognition</text>
<text top="570" left="312" width="251" height="9" font="font5" id="p1_t78" reading_order_no="74" segment_no="14" tag_type="text">approaches and methods since the publication of previous</text>
<text top="582" left="312" width="251" height="9" font="font5" id="p1_t79" reading_order_no="75" segment_no="14" tag_type="text">surveys [3], [4]. The goal is, in particular, to focus on the</text>
<text top="594" left="312" width="159" height="9" font="font5" id="p1_t80" reading_order_no="76" segment_no="14" tag_type="text">works that are based on deep-learning.<a href="deeplearning_paper41.html#1">2);</a></text>
<text top="607" left="322" width="241" height="9" font="font5" id="p1_t81" reading_order_no="77" segment_no="16" tag_type="text">Lowry et al. [3] presented a comprehensive overview of</text>
<text top="619" left="312" width="251" height="9" font="font5" id="p1_t82" reading_order_no="78" segment_no="16" tag_type="text">the existing visual place recognition methods up to 2016. The</text>
<text top="631" left="312" width="251" height="9" font="font5" id="p1_t83" reading_order_no="79" segment_no="16" tag_type="text">work summarizes and discusses several fundamentals to deal</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p1_t84" reading_order_no="80" segment_no="16" tag_type="text">with appearance changing environments and viewpoint varia-</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p1_t85" reading_order_no="81" segment_no="16" tag_type="text">tions. However, the rapid developments in deep learning (DL)</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p1_t86" reading_order_no="82" segment_no="16" tag_type="text">and new sensor modalities ( e.g. , 3D LiDARs and RADARs)</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p1_t87" reading_order_no="83" segment_no="16" tag_type="text">are setting unprecedented performances, shifting the place</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p1_t88" reading_order_no="84" segment_no="16" tag_type="text">recognition state of the art from traditional (handcrafted-only)</text>
<text top="703" left="312" width="194" height="9" font="font5" id="p1_t89" reading_order_no="85" segment_no="16" tag_type="text">feature extraction towards data-driven methods.<a href="deeplearning_paper41.html#13">[3], [4]. </a>The goal is, in particular, to focus on the</text>
<text top="715" left="322" width="241" height="9" font="font5" id="p1_t90" reading_order_no="86" segment_no="19" tag_type="text">A key advantage of these data-driven approaches is the end-</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p1_t91" reading_order_no="87" segment_no="19" tag_type="text">to-end training, which enables to learn a task directly from<a href="deeplearning_paper41.html#13">[3] </a>presented a comprehensive overview of</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p1_t92" reading_order_no="88" segment_no="19" tag_type="text">the sensory data without requiring domain knowledge for fea-</text>
<text top="546" left="32" width="0" height="18" font="font12" id="p1_t93" reading_order_no="0" segment_no="13" tag_type="title">arXiv:2106.10458v2  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font13" size="7" family="CMSY10" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="text">2</text>
<text top="58" left="49" width="251" height="9" font="font5" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">ture extraction. Instead, features are learning during training,</text>
<text top="70" left="49" width="251" height="9" font="font5" id="p2_t3" reading_order_no="2" segment_no="1" tag_type="text">using Convolutional Neural Networks (CNNs). These feature</text>
<text top="82" left="49" width="251" height="9" font="font5" id="p2_t4" reading_order_no="3" segment_no="1" tag_type="text">extraction approaches have been ultimately the driving force</text>
<text top="94" left="49" width="251" height="9" font="font5" id="p2_t5" reading_order_no="4" segment_no="1" tag_type="text">that has inspired recent works to use supervised, unsupervised,</text>
<text top="106" left="49" width="251" height="9" font="font5" id="p2_t6" reading_order_no="5" segment_no="1" tag_type="text">or both learning approaches combined (semi-supervised) to</text>
<text top="117" left="49" width="251" height="9" font="font5" id="p2_t7" reading_order_no="6" segment_no="1" tag_type="text">improve performance. The influence of DL frameworks in</text>
<text top="129" left="49" width="251" height="9" font="font5" id="p2_t8" reading_order_no="7" segment_no="1" tag_type="text">place recognition is, in particular, observable when considering</text>
<text top="141" left="49" width="251" height="9" font="font5" id="p2_t9" reading_order_no="8" segment_no="1" tag_type="text">the vast amount of place recognition works published in the</text>
<text top="153" left="49" width="174" height="9" font="font5" id="p2_t10" reading_order_no="9" segment_no="1" tag_type="text">last few years that resort to such methods.</text>
<text top="165" left="59" width="241" height="9" font="font5" id="p2_t11" reading_order_no="10" segment_no="3" tag_type="text">On the other hand, a disadvantage of DL methods is the re-</text>
<text top="177" left="49" width="251" height="9" font="font5" id="p2_t12" reading_order_no="11" segment_no="3" tag_type="text">quirement of a vast amount of training data. This requirement</text>
<text top="189" left="49" width="251" height="9" font="font5" id="p2_t13" reading_order_no="12" segment_no="3" tag_type="text">is in particular critical since the creation of suitable datasets</text>
<text top="201" left="49" width="251" height="9" font="font5" id="p2_t14" reading_order_no="13" segment_no="3" tag_type="text">is a demanding and expensive process. In this regard, place</text>
<text top="213" left="49" width="251" height="9" font="font5" id="p2_t15" reading_order_no="14" segment_no="3" tag_type="text">recognition has benefited considerably from the availability of</text>
<text top="225" left="49" width="251" height="9" font="font5" id="p2_t16" reading_order_no="15" segment_no="3" tag_type="text">autonomous vehicle datasets, which are becoming more and</text>
<text top="237" left="49" width="251" height="9" font="font5" id="p2_t17" reading_order_no="16" segment_no="3" tag_type="text">more realistic. Besides more realistic real-world conditions,</text>
<text top="249" left="49" width="251" height="9" font="font5" id="p2_t18" reading_order_no="17" segment_no="3" tag_type="text">also data from new sensor modalities are becoming available,</text>
<text top="261" left="49" width="251" height="9" font="font5" id="p2_t19" reading_order_no="18" segment_no="3" tag_type="text">for example, new camera types, 3D LiDARs, and, more re-</text>
<text top="273" left="49" width="251" height="9" font="font5" id="p2_t20" reading_order_no="19" segment_no="3" tag_type="text">cently, RADARs [5], [6]. This work does not address datasets<a href="deeplearning_paper41.html#13">[5], [6]. </a>This work does not address datasets</text>
<text top="284" left="49" width="251" height="9" font="font5" id="p2_t21" reading_order_no="20" segment_no="3" tag_type="text">since this topic is already overviewed in other works such</text>
<text top="296" left="49" width="251" height="9" font="font5" id="p2_t22" reading_order_no="21" segment_no="3" tag_type="text">as in [7] what place recognition concerns, and in [8] broader<a href="deeplearning_paper41.html#13">[7] </a>what place recognition concerns, and in <a href="deeplearning_paper41.html#13">[8] </a>broader</text>
<text top="308" left="49" width="118" height="9" font="font5" id="p2_t23" reading_order_no="22" segment_no="3" tag_type="text">autonomous driving datasets.</text>
<text top="320" left="59" width="241" height="9" font="font5" id="p2_t24" reading_order_no="23" segment_no="8" tag_type="text">The contribution of this work is to provide a comprehensive</text>
<text top="332" left="49" width="251" height="9" font="font5" id="p2_t25" reading_order_no="24" segment_no="8" tag_type="text">review of the recent methods and approaches, focusing in</text>
<text top="344" left="49" width="54" height="9" font="font5" id="p2_t26" reading_order_no="25" segment_no="8" tag_type="text">particular on:</text>
<text top="357" left="59" width="241" height="9" font="font5" id="p2_t27" reading_order_no="26" segment_no="9" tag_type="list">• the recent introduced sensors in the context of place</text>
<text top="369" left="69" width="231" height="9" font="font5" id="p2_t28" reading_order_no="27" segment_no="9" tag_type="list">recognition, an outline of advantages and disadvantages</text>
<text top="381" left="69" width="231" height="9" font="font5" id="p2_t29" reading_order_no="28" segment_no="9" tag_type="list">is presented in Table I and outline is illustrated in Fig. 4;</text>
<text top="393" left="59" width="241" height="9" font="font5" id="p2_t30" reading_order_no="29" segment_no="10" tag_type="list">• the categorization of the various DL-based works into su-<a href="deeplearning_paper41.html#5">I </a>and outline is illustrated in Fig. <a href="deeplearning_paper41.html#4">4;</a></text>
<text top="405" left="69" width="231" height="9" font="font5" id="p2_t31" reading_order_no="30" segment_no="10" tag_type="list">pervised, unsupervised, semi-supervised and other frame-</text>
<text top="416" left="69" width="231" height="9" font="font5" id="p2_t32" reading_order_no="31" segment_no="10" tag_type="list">works (as illustrated in Fig. 3), in order to provide</text>
<text top="428" left="69" width="231" height="9" font="font5" id="p2_t33" reading_order_no="32" segment_no="10" tag_type="list">to the reader a more comprehensive and meaningful</text>
<text top="440" left="69" width="112" height="9" font="font5" id="p2_t34" reading_order_no="33" segment_no="10" tag_type="list">understanding of this topic.<a href="deeplearning_paper41.html#3">3), </a>in order to provide</text>
<text top="453" left="59" width="241" height="9" font="font5" id="p2_t35" reading_order_no="34" segment_no="11" tag_type="text">The remainder of this paper is organized as follows. Section</text>
<text top="465" left="49" width="251" height="9" font="font5" id="p2_t36" reading_order_no="35" segment_no="11" tag_type="text">II is dedicated to the key concepts regarding place recogni-</text>
<text top="477" left="49" width="251" height="9" font="font5" id="p2_t37" reading_order_no="36" segment_no="11" tag_type="text">tion. Section III addresses the supervised place recognition</text>
<text top="489" left="49" width="251" height="9" font="font5" id="p2_t38" reading_order_no="37" segment_no="11" tag_type="text">approaches, which include pre-trained and end-to-end frame-<a href="deeplearning_paper41.html#2">II </a>is dedicated to the key concepts regarding place recogni-</text>
<text top="501" left="49" width="251" height="9" font="font5" id="p2_t39" reading_order_no="38" segment_no="11" tag_type="text">works. Section V addresses the unsupervised place recognition<a href="deeplearning_paper41.html#4">III </a>addresses the supervised place recognition</text>
<text top="513" left="49" width="251" height="9" font="font5" id="p2_t40" reading_order_no="39" segment_no="11" tag_type="text">approaches. Section VI addresses approaches that combine</text>
<text top="525" left="49" width="251" height="9" font="font5" id="p2_t41" reading_order_no="40" segment_no="11" tag_type="text">both supervised and unsupervised. Section VII addresses al-<a href="deeplearning_paper41.html#9">V </a>addresses the unsupervised place recognition</text>
<text top="537" left="49" width="251" height="9" font="font5" id="p2_t42" reading_order_no="41" segment_no="11" tag_type="text">ternative frameworks that resort to parallel and hierarchical<a href="deeplearning_paper41.html#10">VI </a>addresses approaches that combine</text>
<text top="549" left="49" width="224" height="9" font="font5" id="p2_t43" reading_order_no="42" segment_no="11" tag_type="text">architectures. Lastly, Section VIII concludes the paper.<a href="deeplearning_paper41.html#10">VII </a>addresses al-</text>
<text top="569" left="81" width="187" height="9" font="font5" id="p2_t44" reading_order_no="43" segment_no="14" tag_type="title">II. K EY CONCEPTS OF PLACE RECOGNITION</text>
<text top="583" left="59" width="241" height="9" font="font5" id="p2_t45" reading_order_no="44" segment_no="15" tag_type="text">This section introduces the fundamentals and key concepts<a href="deeplearning_paper41.html#12">VIII </a>concludes the paper.</text>
<text top="595" left="49" width="251" height="9" font="font5" id="p2_t46" reading_order_no="45" segment_no="15" tag_type="text">of place recognition. Most of the concepts here discussed</text>
<text top="607" left="49" width="251" height="9" font="font5" id="p2_t47" reading_order_no="46" segment_no="15" tag_type="text">have been already presented in [3] [9] [10] [11], but they are</text>
<text top="619" left="49" width="251" height="9" font="font5" id="p2_t48" reading_order_no="47" segment_no="15" tag_type="text">concisely revisited in this section to contextualize the reader</text>
<text top="631" left="49" width="158" height="9" font="font5" id="p2_t49" reading_order_no="48" segment_no="15" tag_type="text">and thus facilitate the reading process.</text>
<text top="643" left="59" width="241" height="9" font="font5" id="p2_t50" reading_order_no="49" segment_no="17" tag_type="text">Thus, before diving into more details, some fundamental<a href="deeplearning_paper41.html#13">[3] [9] [10] [11], </a>but they are</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p2_t51" reading_order_no="50" segment_no="17" tag_type="text">questions have to be addressed beforehand. What is a ‘place’ in</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p2_t52" reading_order_no="51" segment_no="17" tag_type="text">the place recognition context? How are places recognized and</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p2_t53" reading_order_no="52" segment_no="17" tag_type="text">remembered? Moreover, what are the difficulties/challenges</text>
<text top="690" left="49" width="128" height="9" font="font5" id="p2_t54" reading_order_no="53" segment_no="17" tag_type="text">when places change over time?</text>
<text top="713" left="49" width="83" height="9" font="font11" id="p2_t55" reading_order_no="54" segment_no="18" tag_type="title">A. What is a place?</text>
<text top="727" left="59" width="241" height="9" font="font5" id="p2_t56" reading_order_no="55" segment_no="19" tag_type="text">Places are segments of the physical world that can have</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p2_t57" reading_order_no="56" segment_no="19" tag_type="text">any given scale - at the limit, a place may represent a</text>
<text top="58" left="312" width="251" height="9" font="font5" id="p2_t58" reading_order_no="57" segment_no="2" tag_type="text">single location to an entire region of discrete locations [3]</text>
<text top="70" left="312" width="251" height="9" font="font5" id="p2_t59" reading_order_no="58" segment_no="2" tag_type="text">(see examples in Fig. 2). The segments’ physical bounds</text>
<text top="82" left="312" width="251" height="9" font="font5" id="p2_t60" reading_order_no="59" segment_no="2" tag_type="text">can be defined, resorting to different segmentation criteria:</text>
<text top="94" left="312" width="251" height="9" font="font5" id="p2_t61" reading_order_no="60" segment_no="2" tag_type="text">time step, traveled distance, or appearance. In particular, the<a href="deeplearning_paper41.html#13">[3]</a></text>
<text top="106" left="312" width="251" height="9" font="font5" id="p2_t62" reading_order_no="61" segment_no="2" tag_type="text">appearance criterion is widely used in place recognition. In<a href="deeplearning_paper41.html#1">2). </a>The segments’ physical bounds</text>
<text top="117" left="312" width="251" height="9" font="font5" id="p2_t63" reading_order_no="62" segment_no="2" tag_type="text">such a case, a new place is created whenever the appearance</text>
<text top="129" left="312" width="251" height="9" font="font5" id="p2_t64" reading_order_no="63" segment_no="2" tag_type="text">of the current location differs significantly from locations that</text>
<text top="141" left="312" width="122" height="9" font="font5" id="p2_t65" reading_order_no="64" segment_no="2" tag_type="text">were previously observed [3].</text>
<text top="172" left="312" width="200" height="9" font="font11" id="p2_t66" reading_order_no="65" segment_no="4" tag_type="title">B. How are places recognized and remembered?</text>
<text top="187" left="322" width="241" height="9" font="font5" id="p2_t67" reading_order_no="66" segment_no="5" tag_type="text">Place recognition is the process of recognizing places within</text>
<text top="199" left="312" width="251" height="9" font="font5" id="p2_t68" reading_order_no="67" segment_no="5" tag_type="text">a global map, utilizing cues from surrounding environments.<a href="deeplearning_paper41.html#13">[3].</a></text>
<text top="211" left="312" width="251" height="9" font="font5" id="p2_t69" reading_order_no="68" segment_no="5" tag_type="text">This process is typically divided into three modules (as illus-</text>
<text top="223" left="312" width="251" height="9" font="font5" id="p2_t70" reading_order_no="69" segment_no="5" tag_type="text">trated in Fig.1: place modeling, belief generation, and place</text>
<text top="235" left="312" width="37" height="9" font="font5" id="p2_t71" reading_order_no="70" segment_no="5" tag_type="text">mapping.</text>
<text top="248" left="322" width="241" height="9" font="font11" id="p2_t72" reading_order_no="71" segment_no="6" tag_type="text">1) Place Modeling: Place modeling is the module that</text>
<text top="260" left="312" width="251" height="9" font="font5" id="p2_t73" reading_order_no="72" segment_no="6" tag_type="text">maps the data from a sensor space into a descriptor space. Sen-<a href="deeplearning_paper41.html#1">Fig.1: </a>place modeling, belief generation, and place</text>
<text top="272" left="312" width="251" height="9" font="font5" id="p2_t74" reading_order_no="73" segment_no="6" tag_type="text">sory data from cameras, 3D LiDARs [12], [13] or RADARs</text>
<text top="284" left="312" width="251" height="9" font="font5" id="p2_t75" reading_order_no="74" segment_no="6" tag_type="text">[14] are used to model the surrounding environment, which is</text>
<text top="296" left="312" width="178" height="9" font="font5" id="p2_t76" reading_order_no="75" segment_no="6" tag_type="text">achieved by extracting meaningful features.</text>
<text top="308" left="322" width="241" height="9" font="font5" id="p2_t77" reading_order_no="76" segment_no="7" tag_type="text">Feature extraction approaches have evolved immensely over</text>
<text top="320" left="312" width="251" height="9" font="font5" id="p2_t78" reading_order_no="77" segment_no="7" tag_type="text">the last decade. Classical approaches rely on handcrafted<a href="deeplearning_paper41.html#13">[12], [13] </a>or RADARs</text>
<text top="332" left="312" width="251" height="9" font="font5" id="p2_t79" reading_order_no="78" segment_no="7" tag_type="text">descriptors such as SWIFT [15], SURF [16], Multiscale Super-<a href="deeplearning_paper41.html#13">[14] </a>are used to model the surrounding environment, which is</text>
<text top="344" left="312" width="251" height="9" font="font5" id="p2_t80" reading_order_no="79" segment_no="7" tag_type="text">pixel Grids [17], HOG [18] or bag-of-words [19], which are</text>
<text top="356" left="312" width="251" height="9" font="font5" id="p2_t81" reading_order_no="80" segment_no="7" tag_type="text">mainly build based on the knowledge of domain experts (see</text>
<text top="368" left="312" width="251" height="9" font="font5" id="p2_t82" reading_order_no="81" segment_no="7" tag_type="text">[3] for further understanding). On the other hand, DL-based</text>
<text top="379" left="312" width="251" height="9" font="font5" id="p2_t83" reading_order_no="82" segment_no="7" tag_type="text">techniques, namely CNNs, are optimized to learn the best<a href="deeplearning_paper41.html#13">[15], </a>SURF <a href="deeplearning_paper41.html#13">[16], </a>Multiscale Super-</text>
<text top="391" left="312" width="251" height="9" font="font5" id="p2_t84" reading_order_no="83" segment_no="7" tag_type="text">features for a given task [20]. With the increasing dominance<a href="deeplearning_paper41.html#13">[17], </a>HOG <a href="deeplearning_paper41.html#13">[18] </a>or bag-of-words <a href="deeplearning_paper41.html#13">[19], </a>which are</text>
<text top="403" left="312" width="251" height="9" font="font5" id="p2_t85" reading_order_no="84" segment_no="7" tag_type="text">of DL in the various perception tasks, also place recognition</text>
<text top="415" left="312" width="251" height="9" font="font5" id="p2_t86" reading_order_no="85" segment_no="7" tag_type="text">slowly benefited from these techniques, using initially pre-<a href="deeplearning_paper41.html#13">[3] </a>for further understanding). On the other hand, DL-based</text>
<text top="427" left="312" width="251" height="9" font="font5" id="p2_t87" reading_order_no="86" segment_no="7" tag_type="text">trained models from other tasks ( e.g. , object recognition [21]</text>
<text top="439" left="312" width="251" height="9" font="font5" id="p2_t88" reading_order_no="87" segment_no="7" tag_type="text">or place categorization [22], [23]), and more recently using<a href="deeplearning_paper41.html#13">[20]. </a>With the increasing dominance</text>
<text top="451" left="312" width="251" height="9" font="font5" id="p2_t89" reading_order_no="88" segment_no="7" tag_type="text">end-to-end learning techniques trained directly on place recog-</text>
<text top="463" left="312" width="92" height="9" font="font5" id="p2_t90" reading_order_no="89" segment_no="7" tag_type="text">nition tasks [24], [25].</text>
<text top="476" left="322" width="241" height="9" font="font11" id="p2_t91" reading_order_no="90" segment_no="12" tag_type="text">2) Place Mapping: Place mapping refers to the process of</text>
<text top="488" left="312" width="251" height="9" font="font5" id="p2_t92" reading_order_no="91" segment_no="12" tag_type="text">maintaining a faithful representation of the physical world. To</text>
<text top="499" left="312" width="251" height="9" font="font5" id="p2_t93" reading_order_no="92" segment_no="12" tag_type="text">this end, place recognition approaches rely on various map-<a href="deeplearning_paper41.html#13">[21]</a></text>
<text top="511" left="312" width="251" height="9" font="font5" id="p2_t94" reading_order_no="93" segment_no="12" tag_type="text">ping frameworks and map update mechanisms. Regarding the<a href="deeplearning_paper41.html#14">[22], [23]), </a>and more recently using</text>
<text top="523" left="312" width="251" height="9" font="font5" id="p2_t95" reading_order_no="94" segment_no="12" tag_type="text">mapping frameworks, three main approaches are highlighted:</text>
<text top="535" left="312" width="251" height="9" font="font5" id="p2_t96" reading_order_no="95" segment_no="12" tag_type="text">database [26], [27], topological [28]–[30] or topological-<a href="deeplearning_paper41.html#14">[24], [25].</a></text>
<text top="547" left="312" width="71" height="9" font="font5" id="p2_t97" reading_order_no="96" segment_no="12" tag_type="text">metric [31], [32].</text>
<text top="560" left="322" width="241" height="9" font="font5" id="p2_t98" reading_order_no="97" segment_no="13" tag_type="text">Database frameworks are abstract map structures, which</text>
<text top="571" left="312" width="251" height="9" font="font5" id="p2_t99" reading_order_no="98" segment_no="13" tag_type="text">store arbitrary amounts of data without any relation between</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p2_t100" reading_order_no="99" segment_no="13" tag_type="text">them. These frameworks are mainly used in pure retrieval tasks</text>
<text top="595" left="312" width="251" height="9" font="font5" id="p2_t101" reading_order_no="100" segment_no="13" tag_type="text">and resort, for retrieval efficiency, to k-dimensional [33], [34],</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p2_t102" reading_order_no="101" segment_no="13" tag_type="text">Chow Liu trees [35] or Hierarchical Navigable Small World</text>
<text top="619" left="312" width="205" height="9" font="font5" id="p2_t103" reading_order_no="102" segment_no="13" tag_type="text">(NSW) [36] to accelerate nearest neighbor search.<a href="deeplearning_paper41.html#14">[26], [27], </a>topological <a href="deeplearning_paper41.html#14">[28]–[30] </a>or topological-</text>
<text top="632" left="322" width="241" height="9" font="font5" id="p2_t104" reading_order_no="103" segment_no="16" tag_type="text">Topological(-metric) maps, on the other hand, are graph-<a href="deeplearning_paper41.html#14">[31], [32].</a></text>
<text top="643" left="312" width="251" height="9" font="font5" id="p2_t105" reading_order_no="104" segment_no="16" tag_type="text">based frameworks, which represent the map through nodes</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p2_t106" reading_order_no="105" segment_no="16" tag_type="text">and edges. The nodes represent places in the physical world,</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p2_t107" reading_order_no="106" segment_no="16" tag_type="text">while the edges represent the relationships among the nodes</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p2_t108" reading_order_no="107" segment_no="16" tag_type="text">( e.g. , the similarity between two nodes). A node may represent<a href="deeplearning_paper41.html#14">[33], [34],</a></text>
<text top="691" left="312" width="251" height="9" font="font5" id="p2_t109" reading_order_no="108" segment_no="16" tag_type="text">one, or several locations, defining in the latter case a region in<a href="deeplearning_paper41.html#14">[35] </a>or Hierarchical Navigable Small World</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p2_t110" reading_order_no="109" segment_no="16" tag_type="text">the physical world. The topological-metric map differs from<a href="deeplearning_paper41.html#14">[36] </a>to accelerate nearest neighbor search.</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p2_t111" reading_order_no="110" segment_no="16" tag_type="text">pure topological in respect of how nodes relate i.e. , while in</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p2_t112" reading_order_no="111" segment_no="16" tag_type="text">pure topological maps no metric information is used in the</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p2_t113" reading_order_no="112" segment_no="16" tag_type="text">edges; in topological-metric maps, nodes may relate with other</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font14" size="10" family="Arial,Bold" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="text">3</text>
<text top="71" left="217" width="178" height="11" font="font14" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="figure">Deep-learning-based Place recognition<b>Deep-learning-based Place recognition</b></text>
<text top="129" left="208" width="63" height="11" font="font14" id="p3_t3" reading_order_no="7" segment_no="1" tag_type="figure">Unsupervised<b>Unsupervised</b></text>
<text top="129" left="333" width="77" height="11" font="font14" id="p3_t4" reading_order_no="8" segment_no="1" tag_type="figure">Semi-supervised<b>Semi-supervised</b></text>
<text top="129" left="463" width="81" height="11" font="font14" id="p3_t5" reading_order_no="9" segment_no="1" tag_type="figure">Other Franmweks<b>Other Franmweks</b></text>
<text top="129" left="81" width="52" height="11" font="font14" id="p3_t6" reading_order_no="2" segment_no="1" tag_type="figure">Supervised<b>Supervised</b></text>
<text top="184" left="178" width="51" height="11" font="font14" id="p3_t7" reading_order_no="5" segment_no="1" tag_type="figure">End-to-end<b>End-to-end</b></text>
<text top="196" left="179" width="49" height="11" font="font14" id="p3_t8" reading_order_no="6" segment_no="1" tag_type="figure">framework<b>framework</b></text>
<text top="184" left="72" width="51" height="11" font="font14" id="p3_t9" reading_order_no="3" segment_no="1" tag_type="figure">Pre-trained<b>Pre-trained</b></text>
<text top="196" left="73" width="49" height="11" font="font14" id="p3_t10" reading_order_no="4" segment_no="1" tag_type="figure">framework<b>framework</b></text>
<text top="190" left="365" width="85" height="11" font="font14" id="p3_t11" reading_order_no="10" segment_no="1" tag_type="figure">Parallel framework<b>Parallel framework</b></text>
<text top="184" left="486" width="55" height="11" font="font14" id="p3_t12" reading_order_no="11" segment_no="1" tag_type="figure">Hierarchical<b>Hierarchical</b></text>
<text top="196" left="489" width="49" height="11" font="font14" id="p3_t13" reading_order_no="12" segment_no="1" tag_type="figure">framework<b>framework</b></text>
<text top="229" left="49" width="233" height="7" font="font6" id="p3_t14" reading_order_no="13" segment_no="2" tag_type="text">Fig. 3. A taxonomy of recent DL-based place recognition approaches.</text>
<text top="260" left="49" width="251" height="9" font="font5" id="p3_t15" reading_order_no="14" segment_no="3" tag_type="text">nodes through relative position, orientation, or metric distance</text>
<text top="272" left="49" width="251" height="9" font="font5" id="p3_t16" reading_order_no="15" segment_no="3" tag_type="text">[3]. An example of such a mapping approach is the HTMap<a href="deeplearning_paper41.html#13">[3]. </a>An example of such a mapping approach is the HTMap</text>
<text top="283" left="49" width="59" height="9" font="font5" id="p3_t17" reading_order_no="16" segment_no="3" tag_type="text">approach [37].<a href="deeplearning_paper41.html#14">[37].</a></text>
<text top="296" left="59" width="241" height="9" font="font5" id="p3_t18" reading_order_no="17" segment_no="5" tag_type="text">Regarding map updating, database frameworks usually are</text>
<text top="308" left="49" width="251" height="9" font="font5" id="p3_t19" reading_order_no="18" segment_no="5" tag_type="text">not updated during operation time, while topological frame-</text>
<text top="320" left="49" width="251" height="9" font="font5" id="p3_t20" reading_order_no="19" segment_no="5" tag_type="text">works can be updated. Update strategies include simple meth-</text>
<text top="332" left="49" width="251" height="9" font="font5" id="p3_t21" reading_order_no="20" segment_no="5" tag_type="text">ods, which update nodes as loops occur [28], or more sophis-<a href="deeplearning_paper41.html#14">[28], </a>or more sophis-</text>
<text top="344" left="49" width="251" height="9" font="font5" id="p3_t22" reading_order_no="21" segment_no="5" tag_type="text">ticated ones, where long- short-term memory-based methods</text>
<text top="356" left="49" width="56" height="9" font="font5" id="p3_t23" reading_order_no="22" segment_no="5" tag_type="text">are used [38].<a href="deeplearning_paper41.html#14">[38].</a></text>
<text top="368" left="59" width="241" height="9" font="font11" id="p3_t24" reading_order_no="23" segment_no="7" tag_type="text">3) Belief Generation: The belief generation module refers</text>
<text top="380" left="49" width="251" height="9" font="font5" id="p3_t25" reading_order_no="24" segment_no="7" tag_type="text">to the process of generating a belief distribution, which repre-</text>
<text top="392" left="49" width="251" height="9" font="font5" id="p3_t26" reading_order_no="25" segment_no="7" tag_type="text">sents the likelihood or confidence of the input data matching a</text>
<text top="404" left="49" width="251" height="9" font="font5" id="p3_t27" reading_order_no="26" segment_no="7" tag_type="text">place in the map. This module is thus responsible to generate</text>
<text top="416" left="49" width="251" height="9" font="font5" id="p3_t28" reading_order_no="27" segment_no="7" tag_type="text">loop candidates based on the belief scores, which can be</text>
<text top="428" left="49" width="251" height="9" font="font5" id="p3_t29" reading_order_no="28" segment_no="7" tag_type="text">computed using methods based on frame-to-frame [39]–[41],</text>
<text top="440" left="49" width="251" height="9" font="font5" id="p3_t30" reading_order_no="29" segment_no="7" tag_type="text">sequence of frames [14], [42]–[44], hierarchical, graphs [45]<a href="deeplearning_paper41.html#14">[39]–[41],</a></text>
<text top="452" left="49" width="110" height="9" font="font5" id="p3_t31" reading_order_no="30" segment_no="7" tag_type="text">or probabilistics [46]–[48].<a href="deeplearning_paper41.html#13">[14], </a><a href="deeplearning_paper41.html#14">[42]–[44], </a>hierarchical, graphs <a href="deeplearning_paper41.html#14">[45]</a></text>
<text top="464" left="59" width="241" height="9" font="font5" id="p3_t32" reading_order_no="31" segment_no="8" tag_type="text">The frame-to-frame matching approach is the most common<a href="deeplearning_paper41.html#14">[46]–[48].</a></text>
<text top="476" left="49" width="251" height="9" font="font5" id="p3_t33" reading_order_no="32" segment_no="8" tag_type="text">in place recognition. This approach usually computes the belief</text>
<text top="488" left="49" width="251" height="9" font="font5" id="p3_t34" reading_order_no="33" segment_no="8" tag_type="text">distribution by matching only one frame at the time; and uses</text>
<text top="500" left="49" width="251" height="9" font="font5" id="p3_t35" reading_order_no="34" segment_no="8" tag_type="text">KD trees [33], [34] or Chow Liu trees [35] for nearest neighbor</text>
<text top="512" left="49" width="251" height="9" font="font5" id="p3_t36" reading_order_no="35" segment_no="8" tag_type="text">search, and cosine [41], Euclidean distance [49], Hamming<a href="deeplearning_paper41.html#14">[33], [34] </a>or Chow Liu trees <a href="deeplearning_paper41.html#14">[35] </a>for nearest neighbor</text>
<text top="524" left="49" width="186" height="9" font="font5" id="p3_t37" reading_order_no="36" segment_no="8" tag_type="text">distance [50] to compute the similarity score.<a href="deeplearning_paper41.html#14">[41], </a>Euclidean distance <a href="deeplearning_paper41.html#14">[49], </a>Hamming</text>
<text top="536" left="59" width="241" height="9" font="font5" id="p3_t38" reading_order_no="37" segment_no="10" tag_type="text">On the other hand, sequence-based approaches compute the<a href="deeplearning_paper41.html#14">[50] </a>to compute the similarity score.</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p3_t39" reading_order_no="38" segment_no="10" tag_type="text">scores based on sequences of consecutive frames, using, for</text>
<text top="560" left="49" width="251" height="9" font="font5" id="p3_t40" reading_order_no="39" segment_no="10" tag_type="text">example, cost flow minimization [51] to find matches in the</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p3_t41" reading_order_no="40" segment_no="10" tag_type="text">similarity matrix. Sequence matching is also implementable in<a href="deeplearning_paper41.html#14">[51] </a>to find matches in the</text>
<text top="584" left="49" width="251" height="9" font="font5" id="p3_t42" reading_order_no="41" segment_no="10" tag_type="text">a probabilistic framework using Hidden Markov Models [47]</text>
<text top="596" left="49" width="147" height="9" font="font5" id="p3_t43" reading_order_no="42" segment_no="10" tag_type="text">or Conditional Random Fields [48].<a href="deeplearning_paper41.html#14">[47]</a></text>
<text top="608" left="59" width="241" height="9" font="font5" id="p3_t44" reading_order_no="43" segment_no="11" tag_type="text">Hierarchical methods combine multiple matching ap-<a href="deeplearning_paper41.html#14">[48].</a></text>
<text top="620" left="49" width="251" height="9" font="font5" id="p3_t45" reading_order_no="44" segment_no="11" tag_type="text">proaches in a single place recognition framework. For ex-</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p3_t46" reading_order_no="45" segment_no="11" tag_type="text">ample, the coarse-to-fine architecture [27], [52] selects top</text>
<text top="644" left="49" width="251" height="9" font="font5" id="p3_t47" reading_order_no="46" segment_no="11" tag_type="text">candidates in a coarse tier, and from those, selects the best<a href="deeplearning_paper41.html#14">[27], [52] </a>selects top</text>
<text top="656" left="49" width="81" height="9" font="font5" id="p3_t48" reading_order_no="47" segment_no="11" tag_type="text">match in a fine tier.</text>
<text top="687" left="49" width="145" height="9" font="font11" id="p3_t49" reading_order_no="48" segment_no="13" tag_type="title">C. What are the major challenges?</text>
<text top="703" left="59" width="241" height="9" font="font5" id="p3_t50" reading_order_no="49" segment_no="14" tag_type="text">Place recognition approaches are becoming more and more</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p3_t51" reading_order_no="50" segment_no="14" tag_type="text">sophisticated as the environment and operation conditions</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p3_t52" reading_order_no="51" segment_no="14" tag_type="text">become more similar to real-world situations. An example</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p3_t53" reading_order_no="52" segment_no="14" tag_type="text">of this is the current state-of-the-art of place recognition</text>
<text top="260" left="312" width="251" height="9" font="font5" id="p3_t54" reading_order_no="53" segment_no="4" tag_type="text">approaches, which can operate over extended areas in real-</text>
<text top="272" left="312" width="251" height="9" font="font5" id="p3_t55" reading_order_no="54" segment_no="4" tag_type="text">world conditions with unprecedented performances. Despite</text>
<text top="283" left="312" width="251" height="9" font="font5" id="p3_t56" reading_order_no="55" segment_no="4" tag_type="text">these achievements, the major place recognition challenges</text>
<text top="295" left="312" width="251" height="9" font="font5" id="p3_t57" reading_order_no="56" segment_no="4" tag_type="text">remain unsolved, namely places with similar appearances;</text>
<text top="307" left="312" width="251" height="9" font="font5" id="p3_t58" reading_order_no="57" segment_no="4" tag_type="text">places that change in appearance over time; places that are</text>
<text top="319" left="312" width="251" height="9" font="font5" id="p3_t59" reading_order_no="58" segment_no="4" tag_type="text">perceived from different viewpoints; and scalability of the</text>
<text top="331" left="312" width="179" height="9" font="font5" id="p3_t60" reading_order_no="59" segment_no="4" tag_type="text">proposed approaches in large environments.</text>
<text top="344" left="322" width="61" height="9" font="font11" id="p3_t61" reading_order_no="60" segment_no="6" tag_type="text">1) Appearance</text>
<text top="344" left="394" width="31" height="9" font="font11" id="p3_t62" reading_order_no="61" segment_no="6" tag_type="text">Change</text>
<text top="344" left="435" width="15" height="9" font="font11" id="p3_t63" reading_order_no="62" segment_no="6" tag_type="text">and</text>
<text top="344" left="461" width="43" height="9" font="font11" id="p3_t64" reading_order_no="63" segment_no="6" tag_type="text">Perceptual</text>
<text top="344" left="514" width="37" height="9" font="font11" id="p3_t65" reading_order_no="64" segment_no="6" tag_type="text">Aliasing:</text>
<text top="356" left="312" width="251" height="9" font="font5" id="p3_t66" reading_order_no="65" segment_no="6" tag_type="text">Appearance-changing environments and perceptual aliasing</text>
<text top="368" left="312" width="251" height="9" font="font5" id="p3_t67" reading_order_no="66" segment_no="6" tag_type="text">have been in particular the focus of much research. As</text>
<text top="380" left="312" width="251" height="9" font="font5" id="p3_t68" reading_order_no="67" segment_no="6" tag_type="text">autonomous vehicles operate over extended periods, their</text>
<text top="392" left="312" width="251" height="9" font="font5" id="p3_t69" reading_order_no="68" segment_no="6" tag_type="text">perception systems have to deal with environments that</text>
<text top="404" left="312" width="251" height="9" font="font5" id="p3_t70" reading_order_no="69" segment_no="6" tag_type="text">change over time due to for example different weather or</text>
<text top="416" left="312" width="251" height="9" font="font5" id="p3_t71" reading_order_no="70" segment_no="6" tag_type="text">seasonal conditions or due to structural changes. While the</text>
<text top="427" left="312" width="251" height="9" font="font5" id="p3_t72" reading_order_no="71" segment_no="6" tag_type="text">appearance changing problem is originated when the same</text>
<text top="439" left="312" width="251" height="9" font="font5" id="p3_t73" reading_order_no="72" segment_no="6" tag_type="text">place changes over time in appearance, perceptual aliasing</text>
<text top="451" left="312" width="251" height="9" font="font5" id="p3_t74" reading_order_no="73" segment_no="6" tag_type="text">is caused when different places have a similar appearance.</text>
<text top="463" left="312" width="251" height="9" font="font5" id="p3_t75" reading_order_no="74" segment_no="6" tag_type="text">These conditions affect in particular place recognition since</text>
<text top="475" left="312" width="239" height="9" font="font5" id="p3_t76" reading_order_no="75" segment_no="6" tag_type="text">the loop decisions are affected directly by the appearance.</text>
<text top="488" left="322" width="241" height="9" font="font5" id="p3_t77" reading_order_no="76" segment_no="9" tag_type="text">A variety of works have been addressing these challenges</text>
<text top="500" left="312" width="251" height="9" font="font5" id="p3_t78" reading_order_no="77" segment_no="9" tag_type="text">from various perspectives. From the belief generation perspec-</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p3_t79" reading_order_no="78" segment_no="9" tag_type="text">tive, sequence-based matching approaches [42], [48], [53]–</text>
<text top="523" left="312" width="251" height="9" font="font5" id="p3_t80" reading_order_no="79" segment_no="9" tag_type="text">[55] are highlighted as very effective in these conditions. Se-<a href="deeplearning_paper41.html#14">[42], [48], [53]–</a></text>
<text top="535" left="312" width="251" height="9" font="font5" id="p3_t81" reading_order_no="80" segment_no="9" tag_type="text">quence matching is the task of aligning a pair of a template and<a href="deeplearning_paper41.html#14">[55] </a>are highlighted as very effective in these conditions. Se-</text>
<text top="547" left="312" width="251" height="9" font="font5" id="p3_t82" reading_order_no="81" segment_no="9" tag_type="text">query sequences, which can be implemented through minimum</text>
<text top="559" left="312" width="251" height="9" font="font5" id="p3_t83" reading_order_no="82" segment_no="9" tag_type="text">cost flow [42], [56], or probabilistically using Hidden Markov</text>
<text top="571" left="312" width="251" height="9" font="font5" id="p3_t84" reading_order_no="83" segment_no="9" tag_type="text">models [47] or Conditional Random Fields [48]. Another<a href="deeplearning_paper41.html#14">[42], [56], </a>or probabilistically using Hidden Markov</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p3_t85" reading_order_no="84" segment_no="9" tag_type="text">way is to address this problem from the place modeling<a href="deeplearning_paper41.html#14">[47] </a>or Conditional Random Fields <a href="deeplearning_paper41.html#14">[48]. </a>Another</text>
<text top="595" left="312" width="251" height="9" font="font5" id="p3_t86" reading_order_no="85" segment_no="9" tag_type="text">perspective: extracting condition-invariant features [57], [58],</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p3_t87" reading_order_no="86" segment_no="9" tag_type="text">extracting for example features from the middle layers of<a href="deeplearning_paper41.html#14">[57], [58],</a></text>
<text top="619" left="312" width="251" height="9" font="font5" id="p3_t88" reading_order_no="87" segment_no="9" tag_type="text">CNN’s [22]. On the other hand, matching quality of descriptors</text>
<text top="631" left="312" width="251" height="9" font="font5" id="p3_t89" reading_order_no="88" segment_no="9" tag_type="text">can be improved through descriptors normalization [23], [59]<a href="deeplearning_paper41.html#14">[22]. </a>On the other hand, matching quality of descriptors</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p3_t90" reading_order_no="89" segment_no="9" tag_type="text">or through unsupervised techniques such as dimensionality<a href="deeplearning_paper41.html#14">[23], [59]</a></text>
<text top="655" left="312" width="251" height="9" font="font5" id="p3_t91" reading_order_no="90" segment_no="9" tag_type="text">reduction [60], change removal [61], K-STD [62] or delta</text>
<text top="667" left="312" width="66" height="9" font="font5" id="p3_t92" reading_order_no="91" segment_no="9" tag_type="text">descriptors [43].<a href="deeplearning_paper41.html#14">[60], </a>change removal <a href="deeplearning_paper41.html#14">[61], </a>K-STD <a href="deeplearning_paper41.html#14">[62] </a>or delta</text>
<text top="679" left="322" width="241" height="9" font="font11" id="p3_t93" reading_order_no="92" segment_no="12" tag_type="text">2) Viewpoint Changing : Revisiting a place from different<a href="deeplearning_paper41.html#14">[43].</a></text>
<text top="691" left="312" width="251" height="9" font="font5" id="p3_t94" reading_order_no="93" segment_no="12" tag_type="text">viewpoints - at the limit opposite direction (180º viewpoint</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p3_t95" reading_order_no="94" segment_no="12" tag_type="text">variation) [23] - is also challenging for place recognition. That</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p3_t96" reading_order_no="95" segment_no="12" tag_type="text">is, in particular, true for approaches that rely on sensors with a</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p3_t97" reading_order_no="96" segment_no="12" tag_type="text">restricted field-of-view (FoV) or without geometrical sensing<a href="deeplearning_paper41.html#14">[23] </a>- is also challenging for place recognition. That</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p3_t98" reading_order_no="97" segment_no="12" tag_type="text">capabilities. When visiting a place, these sensors only capture</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font15" size="9" family="Arial,Bold" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">4</text>
<text top="58" left="49" width="251" height="9" font="font5" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="text">a fraction of the environment, and when revisiting from a</text>
<text top="70" left="49" width="251" height="9" font="font5" id="p4_t3" reading_order_no="2" segment_no="1" tag_type="text">different angle or position, the appearance of the scene may</text>
<text top="82" left="49" width="251" height="9" font="font5" id="p4_t4" reading_order_no="3" segment_no="1" tag_type="text">differ or even additional elements may be sensed, generating</text>
<text top="94" left="49" width="137" height="9" font="font5" id="p4_t5" reading_order_no="4" segment_no="1" tag_type="text">a complete different place model.</text>
<text top="106" left="59" width="241" height="9" font="font5" id="p4_t6" reading_order_no="5" segment_no="3" tag_type="text">To overcome these shortcomings, visual-based approaches</text>
<text top="118" left="49" width="251" height="9" font="font5" id="p4_t7" reading_order_no="6" segment_no="3" tag_type="text">have resorted to semantic-based features [41], [63]. For exam-<a href="deeplearning_paper41.html#14">[41], [63]. </a>For exam-</text>
<text top="130" left="49" width="251" height="9" font="font5" id="p4_t8" reading_order_no="7" segment_no="3" tag_type="text">ple extracting features from higher-order CNN layers, which</text>
<text top="142" left="49" width="251" height="9" font="font5" id="p4_t9" reading_order_no="8" segment_no="3" tag_type="text">have a semantic meaning, have demonstrated to be more</text>
<text top="154" left="49" width="251" height="9" font="font5" id="p4_t10" reading_order_no="9" segment_no="3" tag_type="text">robust to viewpoint variation [22]. Other works propose the<a href="deeplearning_paper41.html#14">[22]. </a>Other works propose the</text>
<text top="166" left="49" width="251" height="9" font="font5" id="p4_t11" reading_order_no="10" segment_no="3" tag_type="text">use of panoramic cameras [64] or 3DLiDAR [65], being thus<a href="deeplearning_paper41.html#14">[64] </a>or 3DLiDAR <a href="deeplearning_paper41.html#15">[65], </a>being thus</text>
<text top="178" left="49" width="251" height="9" font="font5" id="p4_t12" reading_order_no="11" segment_no="3" tag_type="text">irrelevant what orientation places are perceived in future visits.</text>
<text top="190" left="49" width="251" height="9" font="font5" id="p4_t13" reading_order_no="12" segment_no="3" tag_type="text">Thus relying on sensors and methods that do not depend</text>
<text top="202" left="49" width="251" height="9" font="font5" id="p4_t14" reading_order_no="13" segment_no="3" tag_type="text">on orientation (also called viewpoint-invariant) turn place</text>
<text top="214" left="49" width="100" height="9" font="font5" id="p4_t15" reading_order_no="14" segment_no="3" tag_type="text">recognition more robust.</text>
<text top="226" left="59" width="241" height="9" font="font11" id="p4_t16" reading_order_no="15" segment_no="5" tag_type="text">3) Scalability: Another critical factor of place recognition</text>
<text top="238" left="49" width="251" height="9" font="font5" id="p4_t17" reading_order_no="16" segment_no="5" tag_type="text">is concerned with scalability [50], [66], [67], [67]–[70]. As</text>
<text top="250" left="49" width="251" height="9" font="font5" id="p4_t18" reading_order_no="17" segment_no="5" tag_type="text">self-driving vehicles operate in increasingly larger areas, more<a href="deeplearning_paper41.html#14">[50], </a><a href="deeplearning_paper41.html#15">[66], [67], [67]–[70]. </a>As</text>
<text top="262" left="49" width="251" height="9" font="font5" id="p4_t19" reading_order_no="18" segment_no="5" tag_type="text">places are visited and maps become larger and larger, in-</text>
<text top="274" left="49" width="251" height="9" font="font5" id="p4_t20" reading_order_no="19" segment_no="5" tag_type="text">creasing thus computational demand, which affects negatively</text>
<text top="286" left="49" width="251" height="9" font="font5" id="p4_t21" reading_order_no="20" segment_no="5" tag_type="text">the inference efficiency. Thus, to boost inference efficiency,</text>
<text top="298" left="49" width="251" height="9" font="font5" id="p4_t22" reading_order_no="21" segment_no="5" tag_type="text">approaches include: efficient indexing [71], [72], hierarchical</text>
<text top="310" left="49" width="251" height="9" font="font5" id="p4_t23" reading_order_no="22" segment_no="5" tag_type="text">searching [73], [74], hashing [22], [50], [68], [70], [75],<a href="deeplearning_paper41.html#15">[71], [72], </a>hierarchical</text>
<text top="322" left="49" width="251" height="9" font="font5" id="p4_t24" reading_order_no="23" segment_no="5" tag_type="text">scalar quantization [70], Hidden Markov Models (HMMs)<a href="deeplearning_paper41.html#15">[73], [74], </a>hashing <a href="deeplearning_paper41.html#14">[22], [50], </a><a href="deeplearning_paper41.html#15">[68], [70], [75],</a></text>
<text top="334" left="49" width="251" height="9" font="font5" id="p4_t25" reading_order_no="24" segment_no="5" tag_type="text">[67], [69] or learning regularly repeating visual patterns [66].<a href="deeplearning_paper41.html#15">[70], </a>Hidden Markov Models (HMMs)</text>
<text top="346" left="49" width="251" height="9" font="font5" id="p4_t26" reading_order_no="25" segment_no="5" tag_type="text">For example in [70] a hashing-based approach is used in a<a href="deeplearning_paper41.html#15">[67], [69] </a>or learning regularly repeating visual patterns <a href="deeplearning_paper41.html#15">[66].</a></text>
<text top="358" left="49" width="251" height="9" font="font5" id="p4_t27" reading_order_no="26" segment_no="5" tag_type="text">visual place recognition task with a large database to both<a href="deeplearning_paper41.html#15">[70] </a>a hashing-based approach is used in a</text>
<text top="370" left="49" width="251" height="9" font="font5" id="p4_t28" reading_order_no="27" segment_no="5" tag_type="text">maintain the storage footprint of the descriptor space small</text>
<text top="381" left="49" width="78" height="9" font="font5" id="p4_t29" reading_order_no="28" segment_no="5" tag_type="text">and boost retrieval.</text>
<text top="410" left="145" width="59" height="9" font="font5" id="p4_t30" reading_order_no="29" segment_no="8" tag_type="title">III. S ENSORS</text>
<text top="428" left="59" width="241" height="9" font="font5" id="p4_t31" reading_order_no="30" segment_no="9" tag_type="text">An important aspect of any perception-based application is</text>
<text top="440" left="49" width="251" height="9" font="font5" id="p4_t32" reading_order_no="31" segment_no="9" tag_type="text">the selection of appropriate sensors. To this end, the selection</text>
<text top="452" left="49" width="251" height="9" font="font5" id="p4_t33" reading_order_no="32" segment_no="9" tag_type="text">criterion has to consider the specificities of both the application</text>
<text top="464" left="49" width="251" height="9" font="font5" id="p4_t34" reading_order_no="33" segment_no="9" tag_type="text">and the environment for the task in hand. In place recognition,</text>
<text top="476" left="49" width="251" height="9" font="font5" id="p4_t35" reading_order_no="34" segment_no="9" tag_type="text">the must used sensors are cameras [23], [24], [26], [27],</text>
<text top="488" left="49" width="251" height="9" font="font5" id="p4_t36" reading_order_no="35" segment_no="9" tag_type="text">[32], [41], [42], [63], [76], LiDARs [13], [28], [34], [46],</text>
<text top="500" left="49" width="251" height="9" font="font5" id="p4_t37" reading_order_no="36" segment_no="9" tag_type="text">[65], [77]–[80] and RADARs [14], [81]–[83]. Although in a<a href="deeplearning_paper41.html#14">[23], [24], [26], [27],</a></text>
<text top="512" left="49" width="251" height="9" font="font5" id="p4_t38" reading_order_no="37" segment_no="9" tag_type="text">broader AV context, these sensors are widely adopted [84],<a href="deeplearning_paper41.html#14">[32], [41], [42], [63], </a><a href="deeplearning_paper41.html#15">[76], </a>LiDARs <a href="deeplearning_paper41.html#13">[13], </a><a href="deeplearning_paper41.html#14">[28], [34], [46],</a></text>
<text top="524" left="49" width="251" height="9" font="font5" id="p4_t39" reading_order_no="38" segment_no="9" tag_type="text">[85], in place recognition, cameras are the most popular in<a href="deeplearning_paper41.html#15">[65], [77]–[80] </a>and RADARs <a href="deeplearning_paper41.html#13">[14], </a><a href="deeplearning_paper41.html#15">[81]–[83]. </a>Although in a</text>
<text top="535" left="49" width="251" height="9" font="font5" id="p4_t40" reading_order_no="39" segment_no="9" tag_type="text">the literature, followed by LiDARs, while RADARs are a very<a href="deeplearning_paper41.html#15">[84],</a></text>
<text top="547" left="49" width="251" height="9" font="font5" id="p4_t41" reading_order_no="40" segment_no="9" tag_type="text">recent technology in this domain. For the remaining of this<a href="deeplearning_paper41.html#15">[85], </a>in place recognition, cameras are the most popular in</text>
<text top="559" left="49" width="251" height="9" font="font5" id="p4_t42" reading_order_no="41" segment_no="9" tag_type="text">section, each sensor is detailed and an outline is presented in</text>
<text top="571" left="49" width="29" height="9" font="font5" id="p4_t43" reading_order_no="42" segment_no="9" tag_type="text">Table I.</text>
<text top="584" left="59" width="241" height="9" font="font5" id="p4_t44" reading_order_no="43" segment_no="11" tag_type="text">In the place recognition literature, cameras are by far the</text>
<text top="596" left="49" width="251" height="9" font="font5" id="p4_t45" reading_order_no="44" segment_no="11" tag_type="text">most used sensor. The vision category includes camera sensors<a href="deeplearning_paper41.html#5">I.</a></text>
<text top="608" left="49" width="251" height="9" font="font5" id="p4_t46" reading_order_no="45" segment_no="11" tag_type="text">such as monocular [26], stereo [86], RGBD [87], thermal</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p4_t47" reading_order_no="46" segment_no="11" tag_type="text">[88] or event-triggered [89]. Cameras provide dense and rich</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p4_t48" reading_order_no="47" segment_no="11" tag_type="text">visual information, which can be provided at a high frame<a href="deeplearning_paper41.html#14">[26], </a>stereo <a href="deeplearning_paper41.html#15">[86], </a>RGBD <a href="deeplearning_paper41.html#15">[87], </a>thermal</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p4_t49" reading_order_no="48" segment_no="11" tag_type="text">rate (ranging up to 60Hz) with a relatively low cost. On the<a href="deeplearning_paper41.html#15">[88] </a>or event-triggered <a href="deeplearning_paper41.html#15">[89]. </a>Cameras provide dense and rich</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p4_t50" reading_order_no="49" segment_no="11" tag_type="text">other hand, vision data is very sensitive when faced with</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p4_t51" reading_order_no="50" segment_no="11" tag_type="text">visual appearance change and viewpoint variation, which is a</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p4_t52" reading_order_no="51" segment_no="11" tag_type="text">tremendous disadvantage compared with the other modalities.</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p4_t53" reading_order_no="52" segment_no="11" tag_type="text">Besides vision data, cameras are also able to return depth</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p4_t54" reading_order_no="53" segment_no="11" tag_type="text">maps. This is achieved either with RGB-D [87], [90], stereo</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p4_t55" reading_order_no="54" segment_no="11" tag_type="text">cameras [86], or trough structure from motion (SfM) [91]</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p4_t56" reading_order_no="55" segment_no="11" tag_type="text">methods. In outdoor environments, the limited field-of-view<a href="deeplearning_paper41.html#15">[87], [90], </a>stereo</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p4_t57" reading_order_no="56" segment_no="11" tag_type="text">(FoV) and noisy depth measurements are a clear disadvantage<a href="deeplearning_paper41.html#15">[86], </a>or trough structure from motion (SfM) <a href="deeplearning_paper41.html#15">[91]</a></text>
<text top="181" left="476" width="25" height="10" font="font15" id="p4_t58" reading_order_no="64" segment_no="2" tag_type="figure">Event</text>
<text top="192" left="467" width="42" height="10" font="font15" id="p4_t59" reading_order_no="65" segment_no="2" tag_type="figure">Triggered</text>
<text top="187" left="524" width="31" height="10" font="font15" id="p4_t60" reading_order_no="66" segment_no="2" tag_type="figure">RGB-D<b>Event</b></text>
<text top="187" left="419" width="36" height="10" font="font15" id="p4_t61" reading_order_no="63" segment_no="2" tag_type="figure">Thermal<b>Triggered</b></text>
<text top="127" left="421" width="34" height="10" font="font15" id="p4_t62" reading_order_no="59" segment_no="2" tag_type="figure">Camera<b>RGB-D</b></text>
<text top="187" left="324" width="21" height="10" font="font15" id="p4_t63" reading_order_no="61" segment_no="2" tag_type="figure">RGB<b>Thermal</b></text>
<text top="187" left="371" width="29" height="10" font="font15" id="p4_t64" reading_order_no="62" segment_no="2" tag_type="figure">Stereo<b>Camera</b></text>
<text top="127" left="507" width="34" height="10" font="font15" id="p4_t65" reading_order_no="60" segment_no="2" tag_type="figure">RADAR<b>RGB</b></text>
<text top="71" left="419" width="37" height="10" font="font15" id="p4_t66" reading_order_no="57" segment_no="2" tag_type="figure">Sensors<b>Stereo</b></text>
<text top="127" left="328" width="43" height="10" font="font15" id="p4_t67" reading_order_no="58" segment_no="2" tag_type="figure">3D LiDAR<b>RADAR</b></text>
<text top="225" left="312" width="148" height="7" font="font6" id="p4_t68" reading_order_no="67" segment_no="4" tag_type="text">Fig. 4. Popular sensors in place recognition.<b>Sensors</b></text>
<text top="258" left="312" width="251" height="9" font="font5" id="p4_t69" reading_order_no="68" segment_no="6" tag_type="text">when compared with the depth measurements of recent 3D<b>3D LiDAR</b></text>
<text top="270" left="312" width="36" height="9" font="font5" id="p4_t70" reading_order_no="69" segment_no="6" tag_type="text">LiDARs.</text>
<text top="282" left="322" width="241" height="9" font="font5" id="p4_t71" reading_order_no="70" segment_no="7" tag_type="text">LiDAR sensors gained more attention, in place recogni-</text>
<text top="294" left="312" width="251" height="9" font="font5" id="p4_t72" reading_order_no="71" segment_no="7" tag_type="text">tion, with the emergence of the 3D rotating version. 3D</text>
<text top="306" left="312" width="251" height="9" font="font5" id="p4_t73" reading_order_no="72" segment_no="7" tag_type="text">LiDARs capture the spatial (or geometrical) structure of the</text>
<text top="318" left="312" width="251" height="9" font="font5" id="p4_t74" reading_order_no="73" segment_no="7" tag_type="text">surrounding environment in a single 360°swift, measuring the</text>
<text top="330" left="312" width="251" height="9" font="font5" id="p4_t75" reading_order_no="74" segment_no="7" tag_type="text">time-of-flight (ToF) of reflected laser beams. These sensors</text>
<text top="342" left="312" width="251" height="9" font="font5" id="p4_t76" reading_order_no="75" segment_no="7" tag_type="text">have a sensing capacity of up to 120m with a frame rate</text>
<text top="354" left="312" width="251" height="9" font="font5" id="p4_t77" reading_order_no="76" segment_no="7" tag_type="text">of 10 to 15 Hz. Such features are particularly suitable for</text>
<text top="366" left="312" width="251" height="9" font="font5" id="p4_t78" reading_order_no="77" segment_no="7" tag_type="text">outdoor environment, since measuring depth through ToF is</text>
<text top="378" left="312" width="251" height="9" font="font5" id="p4_t79" reading_order_no="78" segment_no="7" tag_type="text">not influenced by lighting or visual appearance conditions.</text>
<text top="390" left="312" width="251" height="9" font="font5" id="p4_t80" reading_order_no="79" segment_no="7" tag_type="text">This is a major advantage when compared with cameras.</text>
<text top="402" left="312" width="251" height="9" font="font5" id="p4_t81" reading_order_no="80" segment_no="7" tag_type="text">On the other hand, disadvantages are related to the high</text>
<text top="414" left="312" width="251" height="9" font="font5" id="p4_t82" reading_order_no="81" segment_no="7" tag_type="text">cost and the large size, which have been promised to be</text>
<text top="426" left="312" width="251" height="9" font="font5" id="p4_t83" reading_order_no="82" segment_no="7" tag_type="text">surpassed by the solid-state versions. An additional weak point</text>
<text top="438" left="312" width="251" height="9" font="font5" id="p4_t84" reading_order_no="83" segment_no="7" tag_type="text">is the sensitiveness of this technology towards the reflectance</text>
<text top="450" left="312" width="251" height="9" font="font5" id="p4_t85" reading_order_no="84" segment_no="7" tag_type="text">property of objects. For example, glass, mirror, smoke, fog,</text>
<text top="462" left="312" width="149" height="9" font="font5" id="p4_t86" reading_order_no="85" segment_no="7" tag_type="text">and dust reduce sensing capabilities.</text>
<text top="474" left="322" width="241" height="9" font="font5" id="p4_t87" reading_order_no="86" segment_no="10" tag_type="text">Radar sensors measure distance through time delay or phase</text>
<text top="486" left="312" width="251" height="9" font="font5" id="p4_t88" reading_order_no="87" segment_no="10" tag_type="text">shift of radio signals, which makes them very robust to dif-</text>
<text top="498" left="312" width="251" height="9" font="font5" id="p4_t89" reading_order_no="88" segment_no="10" tag_type="text">ferent weather or lighting conditions. The reasonable cost and</text>
<text top="510" left="312" width="251" height="9" font="font5" id="p4_t90" reading_order_no="89" segment_no="10" tag_type="text">long-range capability [92] are features that are popularizing</text>
<text top="522" left="312" width="251" height="9" font="font5" id="p4_t91" reading_order_no="90" segment_no="10" tag_type="text">radars in tasks such as environment understanding [93] and</text>
<text top="534" left="312" width="251" height="9" font="font5" id="p4_t92" reading_order_no="91" segment_no="10" tag_type="text">place recognition [14]. However, radars continue to face weak-<a href="deeplearning_paper41.html#15">[92] </a>are features that are popularizing</text>
<text top="546" left="312" width="251" height="9" font="font5" id="p4_t93" reading_order_no="92" segment_no="10" tag_type="text">nesses in terms of low spatial resolution and interoperability<a href="deeplearning_paper41.html#15">[93] </a>and</text>
<text top="558" left="312" width="251" height="9" font="font5" id="p4_t94" reading_order_no="93" segment_no="10" tag_type="text">[93], disadvantages when compared with LiDARs or cameras.<a href="deeplearning_paper41.html#13">[14]. </a>However, radars continue to face weak-</text>
<text top="589" left="356" width="163" height="9" font="font5" id="p4_t95" reading_order_no="94" segment_no="12" tag_type="title">IV. S UPERVISED PLACE RECOGNITION</text>
<text top="608" left="322" width="241" height="9" font="font5" id="p4_t96" reading_order_no="95" segment_no="13" tag_type="text">This section addresses the place recognition approaches that<a href="deeplearning_paper41.html#15">[93], </a>disadvantages when compared with LiDARs or cameras.</text>
<text top="620" left="312" width="251" height="9" font="font5" id="p4_t97" reading_order_no="96" segment_no="13" tag_type="text">resort to supervised deep learning. Supervised machine learn-</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p4_t98" reading_order_no="97" segment_no="13" tag_type="text">ing techniques learn a function that maps an input representa-</text>
<text top="643" left="312" width="251" height="10" font="font5" id="p4_t99" reading_order_no="98" segment_no="13" tag_type="text">tion ( e.g. , images, point clouds) into an output representation</text>
<text top="655" left="312" width="251" height="10" font="font5" id="p4_t100" reading_order_no="99" segment_no="13" tag_type="text">( e.g. categories, scores, bounding boxes, descriptor) utilizing</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p4_t101" reading_order_no="100" segment_no="13" tag_type="text">labeled data. In deep learning, this function assumes the form</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p4_t102" reading_order_no="101" segment_no="13" tag_type="text">of weights in a network with staked layers. The weights</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p4_t103" reading_order_no="102" segment_no="13" tag_type="text">are learned progressively by computing the error between</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p4_t104" reading_order_no="103" segment_no="13" tag_type="text">predictions and ground-truth in a first step, and in a second</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p4_t105" reading_order_no="104" segment_no="13" tag_type="text">step, the error is backpropagated using gradient vectors [20].</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p4_t106" reading_order_no="105" segment_no="13" tag_type="text">This procedure (i.e., error measuring and weight adjusting)</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p4_t107" reading_order_no="106" segment_no="13" tag_type="text">is repeated until the network’s predictions achieve adequate</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font16" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font17" size="8" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font18" size="9" family="ArialMT" color="#000000"/>
	<fontspec id="font19" size="9" family="ArialMT" color="#e6e6e6"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">5</text>
<text top="59" left="159" width="30" height="7" font="font6" id="p5_t2" reading_order_no="1" segment_no="3" tag_type="title">TABLE I</text>
<text top="68" left="83" width="183" height="7" font="font6" id="p5_t3" reading_order_no="2" segment_no="4" tag_type="text">S ENSORS FOR PLACE RECOGNITION : PROS AND CONS .</text>
<text top="88" left="55" width="23" height="7" font="font17" id="p5_t4" reading_order_no="3" segment_no="5" tag_type="table">Sensor</text>
<text top="88" left="106" width="37" height="7" font="font17" id="p5_t5" reading_order_no="4" segment_no="5" tag_type="table">Advantage</text>
<text top="88" left="220" width="46" height="7" font="font17" id="p5_t6" reading_order_no="5" segment_no="5" tag_type="table">Disadvantage</text>
<text top="134" left="60" width="0" height="7" font="font6" id="p5_t7" reading_order_no="6" segment_no="5" tag_type="table">Camera</text>
<text top="105" left="74" width="35" height="7" font="font6" id="p5_t8" reading_order_no="7" segment_no="5" tag_type="table">- Low cost</text>
<text top="114" left="74" width="85" height="7" font="font6" id="p5_t9" reading_order_no="8" segment_no="5" tag_type="table">- Dense color information</text>
<text top="123" left="74" width="88" height="7" font="font6" id="p5_t10" reading_order_no="9" segment_no="5" tag_type="table">- Low energy consumption</text>
<text top="132" left="74" width="87" height="7" font="font6" id="p5_t11" reading_order_no="10" segment_no="5" tag_type="table">- High precision/resolution</text>
<text top="141" left="74" width="57" height="7" font="font6" id="p5_t12" reading_order_no="11" segment_no="5" tag_type="table">- High frame rate</text>
<text top="105" left="187" width="43" height="7" font="font6" id="p5_t13" reading_order_no="12" segment_no="5" tag_type="table">- Short range</text>
<text top="114" left="187" width="61" height="7" font="font6" id="p5_t14" reading_order_no="13" segment_no="5" tag_type="table">- Sensitive to light</text>
<text top="123" left="187" width="80" height="7" font="font6" id="p5_t15" reading_order_no="14" segment_no="5" tag_type="table">- Sensitive to calibration</text>
<text top="132" left="187" width="48" height="7" font="font6" id="p5_t16" reading_order_no="15" segment_no="5" tag_type="table">- Limited FoV</text>
<text top="141" left="187" width="113" height="7" font="font6" id="p5_t17" reading_order_no="16" segment_no="5" tag_type="table">- Difficulty in textureless environ-</text>
<text top="150" left="187" width="16" height="7" font="font6" id="p5_t18" reading_order_no="17" segment_no="5" tag_type="table">ment</text>
<text top="198" left="60" width="0" height="7" font="font6" id="p5_t19" reading_order_no="20" segment_no="5" tag_type="table">3D</text>
<text top="186" left="60" width="0" height="7" font="font6" id="p5_t20" reading_order_no="19" segment_no="5" tag_type="table">LiD</text>
<text top="173" left="60" width="0" height="7" font="font6" id="p5_t21" reading_order_no="18" segment_no="5" tag_type="table">AR</text>
<text top="164" left="74" width="43" height="7" font="font6" id="p5_t22" reading_order_no="21" segment_no="5" tag_type="table">- Long range</text>
<text top="173" left="74" width="37" height="7" font="font6" id="p5_t23" reading_order_no="22" segment_no="5" tag_type="table">- 360º FoV</text>
<text top="182" left="74" width="100" height="7" font="font6" id="p5_t24" reading_order_no="23" segment_no="5" tag_type="table">- Robust to appearance chang-</text>
<text top="191" left="74" width="46" height="7" font="font6" id="p5_t25" reading_order_no="24" segment_no="5" tag_type="table">ing conditions</text>
<text top="200" left="74" width="83" height="7" font="font6" id="p5_t26" reading_order_no="25" segment_no="5" tag_type="table">-high precision/resolution</text>
<text top="164" left="187" width="37" height="7" font="font6" id="p5_t27" reading_order_no="26" segment_no="5" tag_type="table">- High cost</text>
<text top="173" left="187" width="113" height="7" font="font6" id="p5_t28" reading_order_no="27" segment_no="5" tag_type="table">- Sensitive to reflective and foggy</text>
<text top="182" left="187" width="43" height="7" font="font6" id="p5_t29" reading_order_no="28" segment_no="5" tag_type="table">environments</text>
<text top="191" left="187" width="25" height="7" font="font6" id="p5_t30" reading_order_no="29" segment_no="5" tag_type="table">- Bulky</text>
<text top="200" left="187" width="64" height="7" font="font6" id="p5_t31" reading_order_no="30" segment_no="5" tag_type="table">- Fragile mechanics</text>
<text top="244" left="60" width="0" height="7" font="font6" id="p5_t32" reading_order_no="32" segment_no="5" tag_type="table">RAD</text>
<text top="228" left="60" width="0" height="7" font="font6" id="p5_t33" reading_order_no="31" segment_no="5" tag_type="table">AR</text>
<text top="214" left="74" width="35" height="7" font="font6" id="p5_t34" reading_order_no="33" segment_no="5" tag_type="table">- Low cost</text>
<text top="223" left="74" width="61" height="7" font="font6" id="p5_t35" reading_order_no="34" segment_no="5" tag_type="table">- Very Long range</text>
<text top="232" left="74" width="93" height="7" font="font6" id="p5_t36" reading_order_no="35" segment_no="5" tag_type="table">- Precise velocity estimation</text>
<text top="241" left="74" width="100" height="7" font="font6" id="p5_t37" reading_order_no="36" segment_no="5" tag_type="table">- Insensitive to weather condi-</text>
<text top="250" left="74" width="16" height="7" font="font6" id="p5_t38" reading_order_no="37" segment_no="5" tag_type="table">tions</text>
<text top="214" left="187" width="46" height="7" font="font6" id="p5_t39" reading_order_no="38" segment_no="5" tag_type="table">- Narrow FoV</text>
<text top="223" left="187" width="55" height="7" font="font6" id="p5_t40" reading_order_no="39" segment_no="5" tag_type="table">- Low resolution</text>
<text top="587" left="83" width="8" height="10" font="font18" id="p5_t41" reading_order_no="47" segment_no="1" tag_type="figure">a)</text>
<text top="587" left="170" width="8" height="10" font="font18" id="p5_t42" reading_order_no="57" segment_no="1" tag_type="figure">b)</text>
<text top="587" left="258" width="7" height="10" font="font18" id="p5_t43" reading_order_no="67" segment_no="1" tag_type="figure">c)</text>
<text top="526" left="67" width="43" height="10" font="font18" id="p5_t44" reading_order_no="44" segment_no="1" tag_type="figure">Descriptor</text>
<text top="537" left="69" width="38" height="10" font="font18" id="p5_t45" reading_order_no="45" segment_no="1" tag_type="figure">Matching</text>
<text top="565" left="57" width="63" height="9" font="font10" id="p5_t46" reading_order_no="46" segment_no="1" tag_type="figure">Loop Candidates</text>
<text top="353" left="153" width="42" height="10" font="font18" id="p5_t47" reading_order_no="48" segment_no="1" tag_type="figure">Landmark</text>
<text top="364" left="154" width="40" height="10" font="font18" id="p5_t48" reading_order_no="49" segment_no="1" tag_type="figure">Detection</text>
<text top="526" left="152" width="45" height="10" font="font18" id="p5_t49" reading_order_no="54" segment_no="1" tag_type="figure">Descriptor</text>
<text top="537" left="155" width="38" height="10" font="font18" id="p5_t50" reading_order_no="55" segment_no="1" tag_type="figure">Matching</text>
<text top="565" left="143" width="63" height="9" font="font10" id="p5_t51" reading_order_no="56" segment_no="1" tag_type="figure">Loop Candidates</text>
<text top="414" left="165" width="32" height="10" font="font19" id="p5_t52" reading_order_no="50" segment_no="1" tag_type="figure">Feature</text>
<text top="425" left="160" width="42" height="10" font="font19" id="p5_t53" reading_order_no="51" segment_no="1" tag_type="figure">Extraction</text>
<text top="454" left="153" width="43" height="10" font="font18" id="p5_t54" reading_order_no="52" segment_no="1" tag_type="figure">Descriptor</text>
<text top="465" left="157" width="35" height="10" font="font18" id="p5_t55" reading_order_no="53" segment_no="1" tag_type="figure">Creation</text>
<text top="454" left="67" width="43" height="10" font="font18" id="p5_t56" reading_order_no="42" segment_no="1" tag_type="figure">Descriptor</text>
<text top="465" left="71" width="35" height="10" font="font18" id="p5_t57" reading_order_no="43" segment_no="1" tag_type="figure">Creation</text>
<text top="353" left="78" width="32" height="10" font="font19" id="p5_t58" reading_order_no="40" segment_no="1" tag_type="figure">Feature</text>
<text top="365" left="74" width="42" height="10" font="font19" id="p5_t59" reading_order_no="41" segment_no="1" tag_type="figure">Extraction</text>
<text top="389" left="224" width="72" height="10" font="font18" id="p5_t60" reading_order_no="60" segment_no="1" tag_type="figure">Salient Activation</text>
<text top="400" left="240" width="40" height="10" font="font18" id="p5_t61" reading_order_no="61" segment_no="1" tag_type="figure">Detection</text>
<text top="565" left="228" width="63" height="9" font="font10" id="p5_t62" reading_order_no="66" segment_no="1" tag_type="figure">Loop Candidates</text>
<text top="526" left="239" width="43" height="10" font="font18" id="p5_t63" reading_order_no="64" segment_no="1" tag_type="figure">Descriptor</text>
<text top="537" left="241" width="38" height="10" font="font18" id="p5_t64" reading_order_no="65" segment_no="1" tag_type="figure">Matching</text>
<text top="353" left="253" width="32" height="10" font="font19" id="p5_t65" reading_order_no="58" segment_no="1" tag_type="figure">Feature</text>
<text top="365" left="248" width="42" height="10" font="font19" id="p5_t66" reading_order_no="59" segment_no="1" tag_type="figure">Extraction</text>
<text top="486" left="239" width="43" height="10" font="font18" id="p5_t67" reading_order_no="62" segment_no="1" tag_type="figure">Descriptor</text>
<text top="496" left="242" width="35" height="10" font="font18" id="p5_t68" reading_order_no="63" segment_no="1" tag_type="figure">Creation</text>
<text top="610" left="49" width="24" height="7" font="font6" id="p5_t69" reading_order_no="68" segment_no="11" tag_type="text">Fig. 5.</text>
<text top="610" left="83" width="217" height="7" font="font6" id="p5_t70" reading_order_no="69" segment_no="11" tag_type="text">Block diagram of pre-trained frameworks a) holistic-based b)</text>
<text top="619" left="49" width="120" height="7" font="font6" id="p5_t71" reading_order_no="70" segment_no="11" tag_type="text">landmark-based and c) region-based.</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p5_t72" reading_order_no="71" segment_no="12" tag_type="text">performance. The advantage of such a learning process, par-</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p5_t73" reading_order_no="72" segment_no="12" tag_type="text">ticularly when using convolutional networks (CNN), is the</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p5_t74" reading_order_no="73" segment_no="12" tag_type="text">capability of automatically learning features from the training</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p5_t75" reading_order_no="74" segment_no="12" tag_type="text">data, which, in classical approaches, required a considerable</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p5_t76" reading_order_no="75" segment_no="12" tag_type="text">amount of engineering skill and domain expertise. On the</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p5_t77" reading_order_no="76" segment_no="12" tag_type="text">other hand, the disadvantages are related to the necessity</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p5_t78" reading_order_no="77" segment_no="12" tag_type="text">of a massive amount of labeled data for training, which is</text>
<text top="739" left="49" width="101" height="9" font="font5" id="p5_t79" reading_order_no="78" segment_no="12" tag_type="text">expensive to obtain [94].</text>
<text top="58" left="322" width="241" height="9" font="font5" id="p5_t80" reading_order_no="79" segment_no="2" tag_type="text">In place recognition, deep supervised learning enabled</text>
<text top="70" left="312" width="251" height="9" font="font5" id="p5_t81" reading_order_no="80" segment_no="2" tag_type="text">breakthroughs. Especially, the capability of CNNs to extract</text>
<text top="82" left="312" width="251" height="9" font="font5" id="p5_t82" reading_order_no="81" segment_no="2" tag_type="text">features led to more descriptive place models, improving place</text>
<text top="94" left="312" width="251" height="9" font="font5" id="p5_t83" reading_order_no="82" segment_no="2" tag_type="text">matching. Early approaches relied mostly on pre-trained (or<a href="deeplearning_paper41.html#15">[94].</a></text>
<text top="106" left="312" width="251" height="9" font="font5" id="p5_t84" reading_order_no="83" segment_no="2" tag_type="text">off-the-shelf) CNNs that were trained on other vision tasks</text>
<text top="117" left="312" width="251" height="9" font="font5" id="p5_t85" reading_order_no="84" segment_no="2" tag_type="text">[21], [22]. But more recently, new approaches enabled the</text>
<text top="129" left="312" width="251" height="9" font="font5" id="p5_t86" reading_order_no="85" segment_no="2" tag_type="text">training of DL networks directly on place recognition tasks in</text>
<text top="141" left="312" width="174" height="9" font="font5" id="p5_t87" reading_order_no="86" segment_no="2" tag_type="text">a end-to-end fashion [24], [65], [95], [96].</text>
<text top="172" left="312" width="139" height="9" font="font11" id="p5_t88" reading_order_no="87" segment_no="6" tag_type="title">A. Pre-trained-based Frameworks</text>
<text top="188" left="322" width="241" height="9" font="font5" id="p5_t89" reading_order_no="88" segment_no="7" tag_type="text">In this work, pre-trained place recognition frameworks refer<a href="deeplearning_paper41.html#13">[21], </a><a href="deeplearning_paper41.html#14">[22]. </a>But more recently, new approaches enabled the</text>
<text top="200" left="312" width="251" height="9" font="font5" id="p5_t90" reading_order_no="89" segment_no="7" tag_type="text">to approaches that extract features from pre-trained CNN</text>
<text top="212" left="312" width="251" height="9" font="font5" id="p5_t91" reading_order_no="90" segment_no="7" tag_type="text">models, which are originally trained on other perception tasks<a href="deeplearning_paper41.html#14">[24], </a><a href="deeplearning_paper41.html#15">[65], [95], [96].</a></text>
<text top="224" left="312" width="251" height="9" font="font5" id="p5_t92" reading_order_no="91" segment_no="7" tag_type="text">( e.g. , object recognition [21], place categorization [22], [23]</text>
<text top="236" left="312" width="251" height="9" font="font5" id="p5_t93" reading_order_no="92" segment_no="7" tag_type="text">or segmentation [59]). Works using such models fall into three</text>
<text top="248" left="312" width="251" height="9" font="font5" id="p5_t94" reading_order_no="93" segment_no="7" tag_type="text">categories: holistic-based, landmark-based, and region-based.</text>
<text top="260" left="312" width="251" height="9" font="font5" id="p5_t95" reading_order_no="94" segment_no="7" tag_type="text">Figure 5 illustrates such approaches applied to an input image.</text>
<text top="272" left="322" width="241" height="9" font="font11" id="p5_t96" reading_order_no="95" segment_no="8" tag_type="text">1) Holistic-based: Holistic approaches refer to works that</text>
<text top="284" left="312" width="251" height="9" font="font5" id="p5_t97" reading_order_no="96" segment_no="8" tag_type="text">feed the whole image to a CNN and use all activations from</text>
<text top="296" left="312" width="251" height="9" font="font5" id="p5_t98" reading_order_no="97" segment_no="8" tag_type="text">a layer as a descriptor. The hierarchical nature of CNNs<a href="deeplearning_paper41.html#13">[21], </a>place categorization <a href="deeplearning_paper41.html#14">[22], [23]</a></text>
<text top="308" left="312" width="251" height="9" font="font5" id="p5_t99" reading_order_no="98" segment_no="8" tag_type="text">makes that the various layers contain features with different<a href="deeplearning_paper41.html#14">[59]). </a>Works using such models fall into three</text>
<text top="320" left="312" width="251" height="9" font="font5" id="p5_t100" reading_order_no="99" segment_no="8" tag_type="text">semantic meanings. Thus, to assess which layers generate</text>
<text top="332" left="312" width="251" height="9" font="font5" id="p5_t101" reading_order_no="100" segment_no="8" tag_type="text">the best features for place recognition, works have conducted<a href="deeplearning_paper41.html#5">5 </a>illustrates such approaches applied to an input image.</text>
<text top="344" left="312" width="251" height="9" font="font5" id="p5_t102" reading_order_no="101" segment_no="8" tag_type="text">ablation studies, which compared the performance of the</text>
<text top="356" left="312" width="251" height="9" font="font5" id="p5_t103" reading_order_no="102" segment_no="8" tag_type="text">various layers towards appearance and viewpoint robustness</text>
<text top="368" left="312" width="251" height="9" font="font5" id="p5_t104" reading_order_no="103" segment_no="8" tag_type="text">and compared the performance of object-centric, place-centric,</text>
<text top="380" left="312" width="251" height="9" font="font5" id="p5_t105" reading_order_no="104" segment_no="8" tag_type="text">and hybrid networks (i.e., networks trained respectively for</text>
<text top="392" left="312" width="251" height="9" font="font5" id="p5_t106" reading_order_no="105" segment_no="8" tag_type="text">object recognition, place categorization and both). Moreover,</text>
<text top="403" left="312" width="251" height="9" font="font5" id="p5_t107" reading_order_no="106" segment_no="8" tag_type="text">as CNN layers tend to have many activations, the proposed</text>
<text top="415" left="312" width="251" height="9" font="font5" id="p5_t108" reading_order_no="107" segment_no="8" tag_type="text">approaches compress the descriptor to a more tractable sized</text>
<text top="427" left="312" width="90" height="9" font="font5" id="p5_t109" reading_order_no="108" segment_no="8" tag_type="text">for efficiency reasons.</text>
<text top="440" left="322" width="241" height="9" font="font5" id="p5_t110" reading_order_no="109" segment_no="9" tag_type="text">Ng et al. [21] study the performance of each layer, using</text>
<text top="452" left="312" width="251" height="9" font="font5" id="p5_t111" reading_order_no="110" segment_no="9" tag_type="text">pre-trained object-centric networks such as OxfordNet [97]</text>
<text top="464" left="312" width="251" height="9" font="font5" id="p5_t112" reading_order_no="111" segment_no="9" tag_type="text">and GoogLeNet [98] to extract feature from images. The</text>
<text top="475" left="312" width="251" height="9" font="font5" id="p5_t113" reading_order_no="112" segment_no="9" tag_type="text">features are encoded into VLAD descriptors and compressed</text>
<text top="487" left="312" width="251" height="9" font="font5" id="p5_t114" reading_order_no="113" segment_no="9" tag_type="text">using PCA [100]. Results show that performance increases as</text>
<text top="499" left="312" width="251" height="9" font="font5" id="p5_t115" reading_order_no="114" segment_no="9" tag_type="text">features are extracted from deeper layers, but drops again at</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p5_t116" reading_order_no="115" segment_no="9" tag_type="text">the latest layers. Matching is achieved by computing the L2</text>
<text top="523" left="312" width="112" height="9" font="font5" id="p5_t117" reading_order_no="116" segment_no="9" tag_type="text">distance of two descriptors.<a href="deeplearning_paper41.html#13">[21] </a>study the performance of each layer, using</text>
<text top="536" left="322" width="241" height="9" font="font5" id="p5_t118" reading_order_no="117" segment_no="10" tag_type="text">A similar conclusion is reached by S¨underhauf et al. [22],<a href="deeplearning_paper41.html#15">[97]</a></text>
<text top="548" left="312" width="251" height="9" font="font5" id="p5_t119" reading_order_no="118" segment_no="10" tag_type="text">using holistic image descriptors extracted from AlexNet [124].<a href="deeplearning_paper41.html#15">[98] </a>to extract feature from images. The</text>
<text top="559" left="312" width="251" height="9" font="font5" id="p5_t120" reading_order_no="119" segment_no="10" tag_type="text">Authors argue that the semantic information encoded in the</text>
<text top="571" left="312" width="251" height="9" font="font5" id="p5_t121" reading_order_no="120" segment_no="10" tag_type="text">middle layers improves place recognition when faced with<a href="deeplearning_paper41.html#15">[100]. </a>Results show that performance increases as</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p5_t122" reading_order_no="121" segment_no="10" tag_type="text">severe appearance change, while features from higher lay-</text>
<text top="595" left="312" width="251" height="9" font="font5" id="p5_t123" reading_order_no="122" segment_no="10" tag_type="text">ers are more robust to viewpoint change. The work further</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p5_t124" reading_order_no="123" segment_no="10" tag_type="text">compares AlexNet (object-centric) with Places205 and Hybrid</text>
<text top="619" left="312" width="251" height="9" font="font5" id="p5_t125" reading_order_no="124" segment_no="10" tag_type="text">[125], both trained on a scene categorization task (i.e., place-<a href="deeplearning_paper41.html#14">[22],</a></text>
<text top="631" left="312" width="251" height="9" font="font5" id="p5_t126" reading_order_no="125" segment_no="10" tag_type="text">centric networks) [125], concluding that, for place recognition,<a href="deeplearning_paper41.html#16">[124].</a></text>
<text top="643" left="312" width="251" height="9" font="font5" id="p5_t127" reading_order_no="126" segment_no="10" tag_type="text">place-centric networks outperform object-centric CNNs. The</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p5_t128" reading_order_no="127" segment_no="10" tag_type="text">networks are tested using a cosine-based KNN approach for</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p5_t129" reading_order_no="128" segment_no="10" tag_type="text">matching, but for efficiency reason, the cosine similarity was</text>
<text top="679" left="312" width="191" height="9" font="font5" id="p5_t130" reading_order_no="129" segment_no="10" tag_type="text">approximated by the Hamming distance [126].</text>
<text top="691" left="322" width="241" height="9" font="font5" id="p5_t131" reading_order_no="130" segment_no="13" tag_type="text">On the other hand, Arroyo et al. [104] fuse features from</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p5_t132" reading_order_no="131" segment_no="13" tag_type="text">multiple convolutional layers at several levels and granularities<a href="deeplearning_paper41.html#16">[125], </a>both trained on a scene categorization task (i.e., place-</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p5_t133" reading_order_no="132" segment_no="13" tag_type="text">and show that this approach outperforms approaches that only<a href="deeplearning_paper41.html#16">[125], </a>concluding that, for place recognition,</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p5_t134" reading_order_no="133" segment_no="13" tag_type="text">use features from a single layer. The CNN architecture is</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p5_t135" reading_order_no="134" segment_no="13" tag_type="text">based on the VGG-F [105], and the output features are further</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font20" size="7" family="NimbusRomNo9L-Medi" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="text">6</text>
<text top="59" left="289" width="33" height="7" font="font6" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE II</text>
<text top="68" left="50" width="512" height="7" font="font6" id="p6_t3" reading_order_no="2" segment_no="2" tag_type="text">S UMMARY OF RECENT WORKS ON SUPERVISED PLACE RECOGNITION USING PRE - TRAINED FRAMEWORKS . A LL THE WORKS USE CAMERA - BASED DATA .</text>
<text top="77" left="210" width="192" height="7" font="font6" id="p6_t4" reading_order_no="3" segment_no="2" tag_type="text">BG = B ELIEF G ENERATION AND PM = P LACE MAPPING .</text>
<text top="103" left="56" width="15" height="7" font="font20" id="p6_t5" reading_order_no="4" segment_no="3" tag_type="table">Type</text>
<text top="103" left="88" width="11" height="7" font="font20" id="p6_t6" reading_order_no="5" segment_no="3" tag_type="table">Ref</text>
<text top="103" left="198" width="20" height="7" font="font20" id="p6_t7" reading_order_no="6" segment_no="3" tag_type="table">Model</text>
<text top="103" left="347" width="24" height="7" font="font20" id="p6_t8" reading_order_no="7" segment_no="3" tag_type="table">BG/PM</text>
<text top="103" left="476" width="23" height="7" font="font20" id="p6_t9" reading_order_no="8" segment_no="3" tag_type="table">Dataset</text>
<text top="185" left="59" width="0" height="7" font="font0" id="p6_t10" reading_order_no="9" segment_no="3" tag_type="table">Holistic-based</text>
<text top="126" left="87" width="12" height="7" font="font0" id="p6_t11" reading_order_no="10" segment_no="3" tag_type="table">[21]</text>
<text top="126" left="114" width="172" height="7" font="font0" id="p6_t12" reading_order_no="11" segment_no="3" tag_type="table">Feature Extraction: OxfordNet [97] and GoogLeNet [98];</text>
<text top="134" left="114" width="112" height="7" font="font0" id="p6_t13" reading_order_no="12" segment_no="3" tag_type="table">Descriptor: VLAD [99] + PCA [100]</text>
<text top="126" left="312" width="63" height="7" font="font0" id="p6_t14" reading_order_no="13" segment_no="3" tag_type="table">L2 distance/Database</text>
<text top="126" left="417" width="90" height="7" font="font0" id="p6_t15" reading_order_no="14" segment_no="3" tag_type="table">Holidays [101]; Oxford [102];</text>
<text top="134" left="417" width="33" height="7" font="font0" id="p6_t16" reading_order_no="15" segment_no="3" tag_type="table">Paris [103]</text>
<text top="151" left="87" width="16" height="7" font="font0" id="p6_t17" reading_order_no="16" segment_no="3" tag_type="table">[104]</text>
<text top="151" left="114" width="139" height="7" font="font0" id="p6_t18" reading_order_no="17" segment_no="3" tag_type="table">Feature Extraction: CNN-VTL (VGG-F [105])</text>
<text top="159" left="114" width="174" height="7" font="font0" id="p6_t19" reading_order_no="18" segment_no="3" tag_type="table">Descriptor: Conv5 layer + Random selection (LDB [106])</text>
<text top="151" left="312" width="84" height="7" font="font0" id="p6_t20" reading_order_no="19" segment_no="3" tag_type="table">Hamming distance/Database</text>
<text top="151" left="417" width="141" height="7" font="font0" id="p6_t21" reading_order_no="20" segment_no="3" tag_type="table">Nordland [2]; CMU-CVG Visual Localization</text>
<text top="159" left="417" width="62" height="7" font="font0" id="p6_t22" reading_order_no="21" segment_no="3" tag_type="table">[107]; Alderley [51];</text>
<text top="175" left="87" width="12" height="7" font="font0" id="p6_t23" reading_order_no="22" segment_no="3" tag_type="table">[22]</text>
<text top="175" left="114" width="104" height="7" font="font0" id="p6_t24" reading_order_no="23" segment_no="3" tag_type="table">Feature Extraction: AlexNet [108];</text>
<text top="183" left="114" width="71" height="7" font="font0" id="p6_t25" reading_order_no="24" segment_no="3" tag_type="table">Descriptor: Conv3 layer<a href="deeplearning_paper41.html#13">[21]</a></text>
<text top="175" left="312" width="76" height="7" font="font0" id="p6_t26" reading_order_no="25" segment_no="3" tag_type="table">Hamming KNN/Database<a href="deeplearning_paper41.html#15">[97] </a>and GoogLeNet <a href="deeplearning_paper41.html#15">[98];</a></text>
<text top="175" left="417" width="141" height="7" font="font0" id="p6_t27" reading_order_no="26" segment_no="3" tag_type="table">Nordland [2]; Gardens Point [22]; The Campus<a href="deeplearning_paper41.html#15">[99] </a>+ PCA <a href="deeplearning_paper41.html#15">[100]</a></text>
<text top="183" left="417" width="116" height="7" font="font0" id="p6_t28" reading_order_no="27" segment_no="3" tag_type="table">Human vs. Robot; The St. Lucia [109]</text>
<text top="267" left="59" width="0" height="7" font="font0" id="p6_t29" reading_order_no="28" segment_no="3" tag_type="table">Landmark-based<a href="deeplearning_paper41.html#15">[101]; </a>Oxford <a href="deeplearning_paper41.html#15">[102];</a></text>
<text top="205" left="87" width="12" height="7" font="font0" id="p6_t30" reading_order_no="29" segment_no="3" tag_type="table">[23]<a href="deeplearning_paper41.html#15">[103]</a></text>
<text top="205" left="114" width="151" height="7" font="font0" id="p6_t31" reading_order_no="30" segment_no="3" tag_type="table">Landmark Detection: Left and right image regions<a href="deeplearning_paper41.html#15">[104]</a></text>
<text top="213" left="114" width="125" height="7" font="font0" id="p6_t32" reading_order_no="31" segment_no="3" tag_type="table">Feature Extraction: CNN Places365 [110]<a href="deeplearning_paper41.html#15">[105])</a></text>
<text top="221" left="114" width="144" height="7" font="font0" id="p6_t33" reading_order_no="32" segment_no="3" tag_type="table">Descriptor: fc6 + normalization + concatenation<a href="deeplearning_paper41.html#15">[106])</a></text>
<text top="205" left="312" width="78" height="7" font="font0" id="p6_t34" reading_order_no="33" segment_no="3" tag_type="table">Sequence Match/Database</text>
<text top="205" left="417" width="71" height="7" font="font0" id="p6_t35" reading_order_no="34" segment_no="3" tag_type="table">Oxford Robotcar [102];<a href="deeplearning_paper41.html#13">[2]; </a>CMU-CVG Visual Localization</text>
<text top="213" left="417" width="59" height="7" font="font0" id="p6_t36" reading_order_no="35" segment_no="3" tag_type="table">University Campus;<a href="deeplearning_paper41.html#16">[107]; </a>Alderley <a href="deeplearning_paper41.html#14">[51];</a></text>
<text top="237" left="87" width="16" height="7" font="font0" id="p6_t37" reading_order_no="36" segment_no="3" tag_type="table">[111]<a href="deeplearning_paper41.html#14">[22]</a></text>
<text top="237" left="114" width="101" height="7" font="font0" id="p6_t38" reading_order_no="37" segment_no="3" tag_type="table">Landmark Detection: Edge Boxes<a href="deeplearning_paper41.html#16">[108];</a></text>
<text top="245" left="114" width="86" height="7" font="font0" id="p6_t39" reading_order_no="38" segment_no="3" tag_type="table">Feature Extraction: ALexNet</text>
<text top="254" left="114" width="185" height="7" font="font0" id="p6_t40" reading_order_no="39" segment_no="3" tag_type="table">Descriptor: Conv3 layer + Gaussian Random Projection [112]</text>
<text top="237" left="312" width="67" height="7" font="font0" id="p6_t41" reading_order_no="40" segment_no="3" tag_type="table">Cosine KNN/Database<a href="deeplearning_paper41.html#13">[2]; </a>Gardens Point <a href="deeplearning_paper41.html#14">[22]; </a>The Campus</text>
<text top="237" left="417" width="92" height="7" font="font0" id="p6_t42" reading_order_no="41" segment_no="3" tag_type="table">Gardens Point [22]; Mapillary;<a href="deeplearning_paper41.html#16">[109]</a></text>
<text top="245" left="417" width="109" height="7" font="font0" id="p6_t43" reading_order_no="42" segment_no="3" tag_type="table">Library Robot Indoor; Nordland [2];</text>
<text top="270" left="87" width="16" height="7" font="font0" id="p6_t44" reading_order_no="43" segment_no="3" tag_type="table">[113]<a href="deeplearning_paper41.html#14">[23]</a></text>
<text top="270" left="114" width="96" height="7" font="font0" id="p6_t45" reading_order_no="44" segment_no="3" tag_type="table">Landmrk detection: BING [114]</text>
<text top="278" left="114" width="84" height="7" font="font0" id="p6_t46" reading_order_no="45" segment_no="3" tag_type="table">Feature Extraction: AlexNet<a href="deeplearning_paper41.html#16">[110]</a></text>
<text top="286" left="114" width="188" height="7" font="font0" id="p6_t47" reading_order_no="46" segment_no="3" tag_type="table">Descriptor: pool 5 layer + Gaussian Random Projection [112],</text>
<text top="295" left="114" width="65" height="7" font="font0" id="p6_t48" reading_order_no="47" segment_no="3" tag_type="table">[115] + normalization</text>
<text top="270" left="312" width="57" height="7" font="font0" id="p6_t49" reading_order_no="48" segment_no="3" tag_type="table">L2- KNN/Database<a href="deeplearning_paper41.html#15">[102];</a></text>
<text top="270" left="417" width="24" height="7" font="font0" id="p6_t50" reading_order_no="49" segment_no="3" tag_type="table">Gardens</text>
<text top="270" left="448" width="15" height="7" font="font0" id="p6_t51" reading_order_no="50" segment_no="3" tag_type="table">Point<a href="deeplearning_paper41.html#16">[111]</a></text>
<text top="270" left="470" width="38" height="7" font="font0" id="p6_t52" reading_order_no="51" segment_no="3" tag_type="table">[22]; Berlin</text>
<text top="270" left="515" width="42" height="7" font="font0" id="p6_t53" reading_order_no="52" segment_no="3" tag_type="table">A100, Berlin</text>
<text top="278" left="417" width="141" height="7" font="font0" id="p6_t54" reading_order_no="53" segment_no="3" tag_type="table">Halenseestrasse and Berlin Kudamm [111];<a href="deeplearning_paper41.html#16">[112]</a></text>
<text top="286" left="417" width="90" height="7" font="font0" id="p6_t55" reading_order_no="54" segment_no="3" tag_type="table">Nordland [2]; St. Lucia [109];</text>
<text top="373" left="59" width="0" height="7" font="font0" id="p6_t56" reading_order_no="56" segment_no="3" tag_type="table">Re<a href="deeplearning_paper41.html#14">[22]; </a>Mapillary;</text>
<text top="365" left="59" width="0" height="7" font="font0" id="p6_t57" reading_order_no="55" segment_no="3" tag_type="table">gion-based<a href="deeplearning_paper41.html#13">[2];</a></text>
<text top="316" left="87" width="12" height="7" font="font0" id="p6_t58" reading_order_no="57" segment_no="3" tag_type="table">[59]<a href="deeplearning_paper41.html#16">[113]</a></text>
<text top="316" left="114" width="125" height="7" font="font0" id="p6_t59" reading_order_no="58" segment_no="3" tag_type="table">Feature Extraction: Fast-Net (VGG) [116]<a href="deeplearning_paper41.html#16">[114]</a></text>
<text top="324" left="114" width="188" height="7" font="font0" id="p6_t60" reading_order_no="59" segment_no="3" tag_type="table">Descriptor: conv3 + L2-normalization + Sparse Random Pro-</text>
<text top="332" left="114" width="38" height="7" font="font0" id="p6_t61" reading_order_no="60" segment_no="3" tag_type="table">jection [117]<a href="deeplearning_paper41.html#16">[112],</a></text>
<text top="316" left="312" width="75" height="7" font="font0" id="p6_t62" reading_order_no="61" segment_no="3" tag_type="table">Cosine distance/Database<a href="deeplearning_paper41.html#16">[115] </a>+ normalization</text>
<text top="316" left="417" width="52" height="7" font="font0" id="p6_t63" reading_order_no="62" segment_no="3" tag_type="table">Cityscapes [118];</text>
<text top="324" left="417" width="62" height="7" font="font0" id="p6_t64" reading_order_no="63" segment_no="3" tag_type="table">Virtual KITTI [119];</text>
<text top="332" left="417" width="27" height="7" font="font0" id="p6_t65" reading_order_no="64" segment_no="3" tag_type="table">Freiburg;</text>
<text top="348" left="87" width="12" height="7" font="font0" id="p6_t66" reading_order_no="65" segment_no="3" tag_type="table">[40]<a href="deeplearning_paper41.html#14">[22];</a></text>
<text top="348" left="114" width="100" height="7" font="font0" id="p6_t67" reading_order_no="66" segment_no="3" tag_type="table">Feature Extraction: VGG16 [120]</text>
<text top="357" left="114" width="188" height="7" font="font0" id="p6_t68" reading_order_no="67" segment_no="3" tag_type="table">Descriptor: Salient regions from different layers + Bag-of-</text>
<text top="365" left="114" width="37" height="7" font="font0" id="p6_t69" reading_order_no="68" segment_no="3" tag_type="table">Words [121]</text>
<text top="348" left="312" width="77" height="7" font="font0" id="p6_t70" reading_order_no="69" segment_no="3" tag_type="table">Cross matching/ Database<a href="deeplearning_paper41.html#16">[111];</a></text>
<text top="348" left="417" width="141" height="7" font="font0" id="p6_t71" reading_order_no="70" segment_no="3" tag_type="table">Gardens Point [22]; Nordland [2]; Berlin A100,<a href="deeplearning_paper41.html#13">[2]; </a>St. Lucia <a href="deeplearning_paper41.html#16">[109];</a></text>
<text top="357" left="417" width="141" height="7" font="font0" id="p6_t72" reading_order_no="71" segment_no="3" tag_type="table">Berlin Halenseestrasse and Berlin Kudamm</text>
<text top="365" left="417" width="18" height="7" font="font0" id="p6_t73" reading_order_no="72" segment_no="3" tag_type="table">[111];</text>
<text top="381" left="87" width="16" height="7" font="font0" id="p6_t74" reading_order_no="73" segment_no="3" tag_type="table">[122]<a href="deeplearning_paper41.html#14">[59]</a></text>
<text top="381" left="114" width="113" height="7" font="font0" id="p6_t75" reading_order_no="74" segment_no="3" tag_type="table">Feature Extraction: AlexNet365 [123]<a href="deeplearning_paper41.html#16">[116]</a></text>
<text top="389" left="114" width="158" height="7" font="font0" id="p6_t76" reading_order_no="75" segment_no="3" tag_type="table">Descriptor: (Region-VLAD) salient regions + VLAD</text>
<text top="381" left="312" width="75" height="7" font="font0" id="p6_t77" reading_order_no="76" segment_no="3" tag_type="table">Cosine distance/Database<a href="deeplearning_paper41.html#16">[117]</a></text>
<text top="381" left="417" width="141" height="7" font="font0" id="p6_t78" reading_order_no="77" segment_no="3" tag_type="table">Mapillary; Gardens Point [22]; Nordland [2];</text>
<text top="389" left="417" width="141" height="7" font="font0" id="p6_t79" reading_order_no="78" segment_no="3" tag_type="table">Berlin A100, Berlin Halenseestrasse and Berlin<a href="deeplearning_paper41.html#16">[118];</a></text>
<text top="397" left="417" width="47" height="7" font="font0" id="p6_t80" reading_order_no="79" segment_no="3" tag_type="table">Kudamm [111];<a href="deeplearning_paper41.html#16">[119];</a></text>
<text top="433" left="49" width="251" height="9" font="font5" id="p6_t81" reading_order_no="80" segment_no="4" tag_type="text">compressed using a random selection approach for efficient</text>
<text top="445" left="49" width="40" height="9" font="font5" id="p6_t82" reading_order_no="81" segment_no="4" tag_type="text">matching.<a href="deeplearning_paper41.html#14">[40]</a></text>
<text top="460" left="59" width="241" height="9" font="font11" id="p6_t83" reading_order_no="82" segment_no="6" tag_type="text">2) Landmark-based: Landmark-based approaches, contrary<a href="deeplearning_paper41.html#16">[120]</a></text>
<text top="472" left="49" width="251" height="9" font="font5" id="p6_t84" reading_order_no="83" segment_no="6" tag_type="text">to the aforementioned methods, do not feed the entire image to</text>
<text top="484" left="49" width="251" height="9" font="font5" id="p6_t85" reading_order_no="84" segment_no="6" tag_type="text">the network; instead, these approaches use, in a pre-processing<a href="deeplearning_paper41.html#16">[121]</a></text>
<text top="495" left="49" width="251" height="9" font="font5" id="p6_t86" reading_order_no="85" segment_no="6" tag_type="text">stage, object proposal techniques to identify potential land-</text>
<text top="507" left="49" width="251" height="9" font="font5" id="p6_t87" reading_order_no="86" segment_no="6" tag_type="text">marks in the images, which are feed to the CNN. Contrary<a href="deeplearning_paper41.html#14">[22]; </a>Nordland <a href="deeplearning_paper41.html#13">[2]; </a>Berlin A100,</text>
<text top="519" left="49" width="251" height="9" font="font5" id="p6_t88" reading_order_no="87" segment_no="6" tag_type="text">to holistic-based approaches, where all image features are</text>
<text top="531" left="49" width="251" height="9" font="font5" id="p6_t89" reading_order_no="88" segment_no="6" tag_type="text">transformed to descriptors, in landmark-based approaches,<a href="deeplearning_paper41.html#16">[111];</a></text>
<text top="543" left="49" width="251" height="9" font="font5" id="p6_t90" reading_order_no="89" segment_no="6" tag_type="text">only the features from the detected landmarks are converted to<a href="deeplearning_paper41.html#16">[122]</a></text>
<text top="555" left="49" width="251" height="9" font="font5" id="p6_t91" reading_order_no="90" segment_no="6" tag_type="text">descriptors. Detection approaches used in these works include<a href="deeplearning_paper41.html#16">[123]</a></text>
<text top="567" left="49" width="168" height="9" font="font5" id="p6_t92" reading_order_no="91" segment_no="6" tag_type="text">Edge Boxes, BING, or simple heuristics.</text>
<text top="581" left="59" width="241" height="9" font="font5" id="p6_t93" reading_order_no="92" segment_no="7" tag_type="text">With the aim of addressing the extreme appearance and</text>
<text top="593" left="49" width="251" height="9" font="font5" id="p6_t94" reading_order_no="93" segment_no="7" tag_type="text">viewpoint variations problem in place recognition, S¨underhauf<a href="deeplearning_paper41.html#14">[22]; </a>Nordland <a href="deeplearning_paper41.html#13">[2];</a></text>
<text top="605" left="49" width="251" height="9" font="font5" id="p6_t95" reading_order_no="94" segment_no="7" tag_type="text">et al. [111] propose such a landmark detection approach. Land-</text>
<text top="617" left="49" width="251" height="9" font="font5" id="p6_t96" reading_order_no="95" segment_no="7" tag_type="text">marks are detected using Edge Boxes [127] and are mapped<a href="deeplearning_paper41.html#16">[111];</a></text>
<text top="628" left="49" width="251" height="9" font="font5" id="p6_t97" reading_order_no="96" segment_no="7" tag_type="text">into a feature space using the features from Alexnet’s [124]</text>
<text top="640" left="49" width="251" height="9" font="font5" id="p6_t98" reading_order_no="97" segment_no="7" tag_type="text">conv3 layer. The descriptor is also compressed for efficiency</text>
<text top="652" left="49" width="251" height="9" font="font5" id="p6_t99" reading_order_no="98" segment_no="7" tag_type="text">reasons, using a Gaussian Random Projection approach [112].</text>
<text top="666" left="59" width="241" height="9" font="font5" id="p6_t100" reading_order_no="99" segment_no="9" tag_type="text">A similar approach is proposed by Kong et al. [113].</text>
<text top="678" left="49" width="251" height="9" font="font5" id="p6_t101" reading_order_no="100" segment_no="9" tag_type="text">However, instead of detecting landmarks using Edge Boxes</text>
<text top="690" left="49" width="251" height="9" font="font5" id="p6_t102" reading_order_no="101" segment_no="9" tag_type="text">[127] and extracting features from conv3 layer of Alexnet,</text>
<text top="702" left="49" width="251" height="9" font="font5" id="p6_t103" reading_order_no="102" segment_no="9" tag_type="text">landmarks are detected using BING [114] and features are</text>
<text top="714" left="49" width="126" height="9" font="font5" id="p6_t104" reading_order_no="103" segment_no="9" tag_type="text">extracted from a pooling layer.</text>
<text top="727" left="59" width="241" height="9" font="font5" id="p6_t105" reading_order_no="104" segment_no="11" tag_type="text">A slightly different approach is proposed by Garg et al.</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p6_t106" reading_order_no="105" segment_no="11" tag_type="text">[23], which resorting to Places365 [110], also highlights the</text>
<text top="433" left="312" width="251" height="9" font="font5" id="p6_t107" reading_order_no="106" segment_no="5" tag_type="text">effectiveness of place-centric semantic information in extreme</text>
<text top="445" left="312" width="251" height="9" font="font5" id="p6_t108" reading_order_no="107" segment_no="5" tag_type="text">variations such as front versus rear view. In particular, this</text>
<text top="457" left="312" width="251" height="9" font="font5" id="p6_t109" reading_order_no="108" segment_no="5" tag_type="text">work crops the right and left regions of images, which has been</text>
<text top="469" left="312" width="251" height="9" font="font5" id="p6_t110" reading_order_no="109" segment_no="5" tag_type="text">demonstrated to possess useful information [59], for place</text>
<text top="481" left="312" width="251" height="9" font="font5" id="p6_t111" reading_order_no="110" segment_no="5" tag_type="text">description. The work highlights the importance of semantics-</text>
<text top="493" left="312" width="251" height="9" font="font5" id="p6_t112" reading_order_no="111" segment_no="5" tag_type="text">aware features from higher-order layers for viewpoint and con-<a href="deeplearning_paper41.html#16">[111] </a>propose such a landmark detection approach. Land-</text>
<text top="505" left="312" width="251" height="9" font="font5" id="p6_t113" reading_order_no="112" segment_no="5" tag_type="text">dition invariance. Additionally, to improve robustness against<a href="deeplearning_paper41.html#16">[127] </a>and are mapped</text>
<text top="517" left="312" width="251" height="9" font="font5" id="p6_t114" reading_order_no="113" segment_no="5" tag_type="text">appearance, a descriptor normalization approach is proposed.<a href="deeplearning_paper41.html#16">[124]</a></text>
<text top="529" left="312" width="251" height="9" font="font5" id="p6_t115" reading_order_no="114" segment_no="5" tag_type="text">Descriptor normalization of the query and reference descrip-</text>
<text top="541" left="312" width="251" height="9" font="font5" id="p6_t116" reading_order_no="115" segment_no="5" tag_type="text">tors are computed independently since the image conditions<a href="deeplearning_paper41.html#16">[112].</a></text>
<text top="553" left="312" width="251" height="9" font="font5" id="p6_t117" reading_order_no="116" segment_no="5" tag_type="text">differ (i.e., day-time vs. night-time). While matching is com-<a href="deeplearning_paper41.html#16">[113].</a></text>
<text top="565" left="312" width="116" height="9" font="font5" id="p6_t118" reading_order_no="117" segment_no="5" tag_type="text">puted using SeqSLAM [42].</text>
<text top="581" left="322" width="71" height="9" font="font11" id="p6_t119" reading_order_no="118" segment_no="8" tag_type="text">3) Region-based:<a href="deeplearning_paper41.html#16">[127] </a>and extracting features from conv3 layer of Alexnet,</text>
<text top="581" left="403" width="160" height="9" font="font5" id="p6_t120" reading_order_no="119" segment_no="8" tag_type="text">Region-based methods, similarly to<a href="deeplearning_paper41.html#16">[114] </a>and features are</text>
<text top="593" left="312" width="251" height="9" font="font5" id="p6_t121" reading_order_no="120" segment_no="8" tag_type="text">landmark-based approaches, rely on local features; however,</text>
<text top="605" left="312" width="251" height="9" font="font5" id="p6_t122" reading_order_no="121" segment_no="8" tag_type="text">instead of utilizing object proposal methods, the regions of</text>
<text top="617" left="312" width="251" height="9" font="font5" id="p6_t123" reading_order_no="122" segment_no="8" tag_type="text">interest are identified on the CNN layers, detecting salient<a href="deeplearning_paper41.html#14">[23], </a>which resorting to Places365 <a href="deeplearning_paper41.html#16">[110], </a>also highlights the</text>
<text top="629" left="312" width="251" height="9" font="font5" id="p6_t124" reading_order_no="123" segment_no="8" tag_type="text">layer activations. Therefore, region-based methods feed the</text>
<text top="641" left="312" width="251" height="9" font="font5" id="p6_t125" reading_order_no="124" segment_no="8" tag_type="text">entire image to the DL model and use, for the descriptors,</text>
<text top="653" left="312" width="186" height="9" font="font5" id="p6_t126" reading_order_no="125" segment_no="8" tag_type="text">only the salient activation in the CNN layers.</text>
<text top="667" left="322" width="241" height="9" font="font5" id="p6_t127" reading_order_no="126" segment_no="10" tag_type="text">Addressing the problem of viewpoint and appearance chang-<a href="deeplearning_paper41.html#14">[59], </a>for place</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p6_t128" reading_order_no="127" segment_no="10" tag_type="text">ing in place recognition, Chen et al. [40] propose such a</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p6_t129" reading_order_no="128" segment_no="10" tag_type="text">region-based approach that extracts salient regions without</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p6_t130" reading_order_no="129" segment_no="10" tag_type="text">relying on external landmark proposal techniques. Regions of</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p6_t131" reading_order_no="130" segment_no="10" tag_type="text">interest are extracted from various CNN layers of a pre-trained</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p6_t132" reading_order_no="131" segment_no="10" tag_type="text">VGG16 [120]. The approach extracts explicitly local features</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p6_t133" reading_order_no="132" segment_no="10" tag_type="text">from the early layers and semantic information from later</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font21" size="6" family="ArialMT" color="#000000"/>
	<fontspec id="font22" size="7" family="ArialMT" color="#000000"/>
	<fontspec id="font23" size="7" family="ArialMT" color="#e6e6e6"/>
	<fontspec id="font24" size="4" family="ArialMT" color="#000000"/>
	<fontspec id="font25" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font26" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font27" size="7" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="text">7</text>
<text top="199" left="103" width="6" height="7" font="font21" id="p7_t2" reading_order_no="12" segment_no="1" tag_type="figure">a)</text>
<text top="199" left="246" width="6" height="7" font="font21" id="p7_t3" reading_order_no="31" segment_no="1" tag_type="figure">b)</text>
<text top="199" left="447" width="5" height="7" font="font21" id="p7_t4" reading_order_no="56" segment_no="1" tag_type="figure">c)</text>
<text top="156" left="406" width="30" height="7" font="font22" id="p7_t5" reading_order_no="49" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="409" width="25" height="7" font="font22" id="p7_t6" reading_order_no="50" segment_no="1" tag_type="figure">Creation</text>
<text top="156" left="462" width="30" height="7" font="font22" id="p7_t7" reading_order_no="51" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="465" width="25" height="7" font="font22" id="p7_t8" reading_order_no="52" segment_no="1" tag_type="figure">Creation</text>
<text top="185" left="424" width="48" height="7" font="font22" id="p7_t9" reading_order_no="55" segment_no="1" tag_type="figure">Quadruplet Loss</text>
<text top="130" left="361" width="23" height="7" font="font23" id="p7_t10" reading_order_no="39" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="357" width="29" height="7" font="font23" id="p7_t11" reading_order_no="40" segment_no="1" tag_type="figure">Extraction</text>
<text top="156" left="351" width="30" height="7" font="font22" id="p7_t12" reading_order_no="47" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="353" width="25" height="7" font="font22" id="p7_t13" reading_order_no="48" segment_no="1" tag_type="figure">Creation</text>
<text top="130" left="528" width="23" height="7" font="font23" id="p7_t14" reading_order_no="45" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="524" width="29" height="7" font="font23" id="p7_t15" reading_order_no="46" segment_no="1" tag_type="figure">Extraction</text>
<text top="155" left="518" width="30" height="7" font="font22" id="p7_t16" reading_order_no="53" segment_no="1" tag_type="figure">Descriptor</text>
<text top="163" left="520" width="25" height="7" font="font22" id="p7_t17" reading_order_no="54" segment_no="1" tag_type="figure">Creation</text>
<text top="77" left="414" width="13" height="5" font="font24" id="p7_t18" reading_order_no="36" segment_no="1" tag_type="figure">Anchor</text>
<text top="77" left="469" width="16" height="5" font="font24" id="p7_t19" reading_order_no="37" segment_no="1" tag_type="figure">Negative</text>
<text top="77" left="358" width="15" height="5" font="font24" id="p7_t20" reading_order_no="35" segment_no="1" tag_type="figure">Positive</text>
<text top="77" left="525" width="16" height="5" font="font24" id="p7_t21" reading_order_no="38" segment_no="1" tag_type="figure">Negative</text>
<text top="58" left="372" width="39" height="5" font="font24" id="p7_t22" reading_order_no="32" segment_no="1" tag_type="figure">Positive Pair Probe A</text>
<text top="58" left="428" width="41" height="5" font="font24" id="p7_t23" reading_order_no="33" segment_no="1" tag_type="figure">Negative Pair Probe A</text>
<text top="58" left="485" width="41" height="5" font="font24" id="p7_t24" reading_order_no="34" segment_no="1" tag_type="figure">Negative Pair Probe B</text>
<text top="130" left="242" width="23" height="7" font="font23" id="p7_t25" reading_order_no="20" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="239" width="29" height="7" font="font23" id="p7_t26" reading_order_no="21" segment_no="1" tag_type="figure">Extraction</text>
<text top="130" left="298" width="23" height="7" font="font23" id="p7_t27" reading_order_no="22" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="295" width="29" height="7" font="font23" id="p7_t28" reading_order_no="23" segment_no="1" tag_type="figure">Extraction</text>
<text top="156" left="234" width="30" height="7" font="font22" id="p7_t29" reading_order_no="26" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="236" width="25" height="7" font="font22" id="p7_t30" reading_order_no="27" segment_no="1" tag_type="figure">Creation</text>
<text top="156" left="288" width="30" height="7" font="font22" id="p7_t31" reading_order_no="28" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="291" width="25" height="7" font="font22" id="p7_t32" reading_order_no="29" segment_no="1" tag_type="figure">Creation</text>
<text top="185" left="232" width="34" height="7" font="font22" id="p7_t33" reading_order_no="30" segment_no="1" tag_type="figure">Triplet Loss</text>
<text top="77" left="241" width="13" height="5" font="font24" id="p7_t34" reading_order_no="16" segment_no="1" tag_type="figure">Anchor</text>
<text top="77" left="296" width="16" height="5" font="font24" id="p7_t35" reading_order_no="17" segment_no="1" tag_type="figure">Negative</text>
<text top="130" left="186" width="23" height="7" font="font23" id="p7_t36" reading_order_no="18" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="183" width="29" height="7" font="font23" id="p7_t37" reading_order_no="19" segment_no="1" tag_type="figure">Extraction</text>
<text top="156" left="178" width="30" height="7" font="font22" id="p7_t38" reading_order_no="24" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="180" width="25" height="7" font="font22" id="p7_t39" reading_order_no="25" segment_no="1" tag_type="figure">Creation</text>
<text top="77" left="185" width="15" height="5" font="font24" id="p7_t40" reading_order_no="15" segment_no="1" tag_type="figure">Positive</text>
<text top="58" left="266" width="25" height="5" font="font24" id="p7_t41" reading_order_no="14" segment_no="1" tag_type="figure">Negative Pair</text>
<text top="58" left="209" width="23" height="5" font="font24" id="p7_t42" reading_order_no="13" segment_no="1" tag_type="figure">Positive Pair</text>
<text top="130" left="415" width="23" height="7" font="font23" id="p7_t43" reading_order_no="41" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="412" width="29" height="7" font="font23" id="p7_t44" reading_order_no="42" segment_no="1" tag_type="figure">Extraction</text>
<text top="130" left="73" width="23" height="7" font="font23" id="p7_t45" reading_order_no="3" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="69" width="29" height="7" font="font23" id="p7_t46" reading_order_no="4" segment_no="1" tag_type="figure">Extraction</text>
<text top="130" left="128" width="23" height="7" font="font23" id="p7_t47" reading_order_no="5" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="125" width="29" height="7" font="font23" id="p7_t48" reading_order_no="6" segment_no="1" tag_type="figure">Extraction</text>
<text top="156" left="63" width="30" height="7" font="font22" id="p7_t49" reading_order_no="7" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="66" width="25" height="7" font="font22" id="p7_t50" reading_order_no="8" segment_no="1" tag_type="figure">Creation</text>
<text top="156" left="118" width="30" height="7" font="font22" id="p7_t51" reading_order_no="9" segment_no="1" tag_type="figure">Descriptor</text>
<text top="164" left="121" width="25" height="7" font="font22" id="p7_t52" reading_order_no="10" segment_no="1" tag_type="figure">Creation</text>
<text top="185" left="67" width="79" height="7" font="font22" id="p7_t53" reading_order_no="11" segment_no="1" tag_type="figure">Contrastive or margin loss</text>
<text top="77" left="71" width="13" height="5" font="font24" id="p7_t54" reading_order_no="1" segment_no="1" tag_type="figure">Anchor</text>
<text top="77" left="125" width="16" height="5" font="font24" id="p7_t55" reading_order_no="2" segment_no="1" tag_type="figure">Negative</text>
<text top="130" left="472" width="23" height="7" font="font23" id="p7_t56" reading_order_no="43" segment_no="1" tag_type="figure">Feature</text>
<text top="139" left="469" width="29" height="7" font="font23" id="p7_t57" reading_order_no="44" segment_no="1" tag_type="figure">Extraction</text>
<text top="220" left="49" width="432" height="7" font="font6" id="p7_t58" reading_order_no="57" segment_no="2" tag_type="text">Fig. 6. Block diagram of training strategies using a) contrastive-based and margin-based, b) triplet and c) quadruplet loss function.</text>
<text top="251" left="49" width="251" height="9" font="font5" id="p7_t59" reading_order_no="58" segment_no="3" tag_type="text">layers. The extracted regions are encoded into a descriptor,</text>
<text top="263" left="49" width="251" height="9" font="font5" id="p7_t60" reading_order_no="59" segment_no="3" tag_type="text">using a bag-of-words-based approach [121] which is matched<a href="deeplearning_paper41.html#16">[121] </a>which is matched</text>
<text top="275" left="49" width="136" height="9" font="font5" id="p7_t61" reading_order_no="60" segment_no="3" tag_type="text">using a cross-matching approach.</text>
<text top="287" left="59" width="241" height="9" font="font5" id="p7_t62" reading_order_no="61" segment_no="6" tag_type="text">Naseer et al. [59], on the other hand, learn the activation<a href="deeplearning_paper41.html#14">[59], </a>on the other hand, learn the activation</text>
<text top="299" left="49" width="251" height="9" font="font5" id="p7_t63" reading_order_no="62" segment_no="6" tag_type="text">regions of interest resorting to segmentation. In this work,</text>
<text top="311" left="49" width="251" height="9" font="font5" id="p7_t64" reading_order_no="63" segment_no="6" tag_type="text">regions of interest represent stable image areas, which are</text>
<text top="323" left="49" width="251" height="9" font="font5" id="p7_t65" reading_order_no="64" segment_no="6" tag_type="text">learned using Fast-Net [116]. Fast-Net is an up-convolutional<a href="deeplearning_paper41.html#16">[116]. </a>Fast-Net is an up-convolutional</text>
<text top="335" left="49" width="251" height="9" font="font5" id="p7_t66" reading_order_no="65" segment_no="6" tag_type="text">Network that provides near real-time image segmentation.</text>
<text top="347" left="49" width="251" height="9" font="font5" id="p7_t67" reading_order_no="66" segment_no="6" tag_type="text">Due to being too large for real-time matching, the features</text>
<text top="359" left="49" width="251" height="9" font="font5" id="p7_t68" reading_order_no="67" segment_no="6" tag_type="text">resulting from the learned segments are encoded into a lower</text>
<text top="371" left="49" width="251" height="9" font="font5" id="p7_t69" reading_order_no="68" segment_no="6" tag_type="text">dimensionality using L2-normalization and Sparse Random</text>
<text top="383" left="49" width="251" height="9" font="font5" id="p7_t70" reading_order_no="69" segment_no="6" tag_type="text">Projection [117]. This approach, in particular, learns human-<a href="deeplearning_paper41.html#16">[117]. </a>This approach, in particular, learns human-</text>
<text top="394" left="49" width="251" height="9" font="font5" id="p7_t71" reading_order_no="70" segment_no="6" tag_type="text">made structure due to being more stable for more extended</text>
<text top="406" left="49" width="32" height="9" font="font5" id="p7_t72" reading_order_no="71" segment_no="6" tag_type="text">periods.</text>
<text top="419" left="59" width="241" height="9" font="font5" id="p7_t73" reading_order_no="72" segment_no="9" tag_type="text">With the aim of reducing the memory and computational</text>
<text top="431" left="49" width="251" height="9" font="font5" id="p7_t74" reading_order_no="73" segment_no="9" tag_type="text">cost, Khaliq et al. [122] propose Region-VLAD. This ap-<a href="deeplearning_paper41.html#16">[122] </a>propose Region-VLAD. This ap-</text>
<text top="443" left="49" width="251" height="9" font="font5" id="p7_t75" reading_order_no="74" segment_no="9" tag_type="text">proach leverages a lightweight place-centric CNN architecture</text>
<text top="454" left="49" width="251" height="9" font="font5" id="p7_t76" reading_order_no="75" segment_no="9" tag_type="text">(AlexNet365 [123]) to extract regional features. These features<a href="deeplearning_paper41.html#16">[123]) </a>to extract regional features. These features</text>
<text top="466" left="49" width="251" height="9" font="font5" id="p7_t77" reading_order_no="76" segment_no="9" tag_type="text">are encoded using a VLAD method, which is specially adapted</text>
<text top="478" left="49" width="242" height="9" font="font5" id="p7_t78" reading_order_no="77" segment_no="9" tag_type="text">to gain computation-efficiency and environment invariance.</text>
<text top="508" left="49" width="113" height="9" font="font11" id="p7_t79" reading_order_no="78" segment_no="11" tag_type="title">B. End-to-End Frameworks</text>
<text top="524" left="59" width="241" height="9" font="font5" id="p7_t80" reading_order_no="79" segment_no="12" tag_type="text">Conversely to pre-trained frameworks, end-to-end frame-</text>
<text top="536" left="49" width="251" height="9" font="font5" id="p7_t81" reading_order_no="80" segment_no="12" tag_type="text">works resort to machine learning approaches that learn the</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p7_t82" reading_order_no="81" segment_no="12" tag_type="text">feature representation and obtain a descriptor directly from the</text>
<text top="559" left="49" width="251" height="9" font="font5" id="p7_t83" reading_order_no="82" segment_no="12" tag_type="text">sensor data while training on a place recognition task. A key</text>
<text top="571" left="49" width="251" height="9" font="font5" id="p7_t84" reading_order_no="83" segment_no="12" tag_type="text">aspect of end-to-end learning is concerned with the definition</text>
<text top="583" left="49" width="251" height="9" font="font5" id="p7_t85" reading_order_no="84" segment_no="12" tag_type="text">of the training objective: i.e., what are the networks optimized</text>
<text top="595" left="49" width="251" height="9" font="font5" id="p7_t86" reading_order_no="85" segment_no="12" tag_type="text">for, and how are they optimized. In place recognition, networks</text>
<text top="607" left="49" width="251" height="9" font="font5" id="p7_t87" reading_order_no="86" segment_no="12" tag_type="text">are mostly optimized to generate unique descriptors that can</text>
<text top="619" left="49" width="251" height="9" font="font5" id="p7_t88" reading_order_no="87" segment_no="12" tag_type="text">identify the same physical place regardless of the appearance</text>
<text top="631" left="49" width="251" height="9" font="font5" id="p7_t89" reading_order_no="88" segment_no="12" tag_type="text">or viewpoint. The achievement of such an objective is deter-</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p7_t90" reading_order_no="89" segment_no="12" tag_type="text">mined by selecting, for the task in hands, an adequate network</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p7_t91" reading_order_no="90" segment_no="12" tag_type="text">, and adequate network training, which depends on the loss</text>
<text top="667" left="49" width="36" height="9" font="font5" id="p7_t92" reading_order_no="91" segment_no="12" tag_type="text">function.</text>
<text top="679" left="59" width="77" height="9" font="font11" id="p7_t93" reading_order_no="92" segment_no="15" tag_type="text">1) Loss functions:</text>
<text top="679" left="144" width="156" height="9" font="font5" id="p7_t94" reading_order_no="93" segment_no="15" tag_type="text">The loss function is in particular a</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p7_t95" reading_order_no="94" segment_no="15" tag_type="text">major concern in the training phase, since it represents the</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p7_t96" reading_order_no="95" segment_no="15" tag_type="text">matematical interpretation of the training objective, and thus</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p7_t97" reading_order_no="96" segment_no="15" tag_type="text">determining the successful convergence of the optimization</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p7_t98" reading_order_no="97" segment_no="15" tag_type="text">process. In place recognition loss functions include triplet-</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p7_t99" reading_order_no="98" segment_no="15" tag_type="text">based [24], [34], [50], [65], [81], [96], [137], margin-based<a href="deeplearning_paper41.html#14">[24], [34], [50], </a><a href="deeplearning_paper41.html#15">[65], [81], [96], </a><a href="deeplearning_paper41.html#16">[137], </a>margin-based</text>
<text top="251" left="312" width="251" height="9" font="font5" id="p7_t100" reading_order_no="99" segment_no="4" tag_type="text">[25], quadruplet-based [49], and contrastive-based [13]. Figure<a href="deeplearning_paper41.html#14">[25], </a>quadruplet-based <a href="deeplearning_paper41.html#14">[49], </a>and contrastive-based <a href="deeplearning_paper41.html#13">[13]. </a>Figure</text>
<text top="263" left="312" width="251" height="9" font="font5" id="p7_t101" reading_order_no="100" segment_no="4" tag_type="text">6 illustrates the various training strategies of the loss functions.<a href="deeplearning_paper41.html#7">6 </a>illustrates the various training strategies of the loss functions.</text>
<text top="275" left="322" width="241" height="9" font="font5" id="p7_t102" reading_order_no="101" segment_no="5" tag_type="text">The contrastive loss is used in siamese networks [13],<a href="deeplearning_paper41.html#13">[13],</a></text>
<text top="287" left="312" width="251" height="9" font="font5" id="p7_t103" reading_order_no="102" segment_no="5" tag_type="text">[143], which have two branches with shared parameters. This<a href="deeplearning_paper41.html#16">[143], </a>which have two branches with shared parameters. This</text>
<text top="299" left="312" width="251" height="9" font="font5" id="p7_t104" reading_order_no="103" segment_no="5" tag_type="text">function computes the similarity distance between the output</text>
<text top="311" left="312" width="251" height="9" font="font5" id="p7_t105" reading_order_no="104" segment_no="5" tag_type="text">descriptors of the branches, forcing the netwroks to decrease</text>
<text top="323" left="312" width="251" height="9" font="font5" id="p7_t106" reading_order_no="105" segment_no="5" tag_type="text">the distance between positive pairs (input data from the same</text>
<text top="335" left="312" width="251" height="9" font="font5" id="p7_t107" reading_order_no="106" segment_no="5" tag_type="text">place) and increase the distance between negative pairs. The</text>
<text top="347" left="312" width="149" height="9" font="font5" id="p7_t108" reading_order_no="107" segment_no="5" tag_type="text">function can be described as follow:</text>
<text top="366" left="360" width="24" height="16" font="font11" id="p7_t109" reading_order_no="108" segment_no="7" tag_type="formula">L = 1</text>
<text top="366" left="379" width="41" height="23" font="font5" id="p7_t110" reading_order_no="109" segment_no="7" tag_type="formula">2 Y D 2 + 1</text>
<text top="371" left="415" width="99" height="18" font="font5" id="p7_t111" reading_order_no="110" segment_no="7" tag_type="formula">2 ( 1 − Y ) max ( 0 , m − D ) 2</text>
<text top="372" left="551" width="12" height="9" font="font5" id="p7_t112" reading_order_no="111" segment_no="7" tag_type="text">(1)</text>
<text top="394" left="312" width="251" height="11" font="font5" id="p7_t113" reading_order_no="112" segment_no="8" tag_type="text">where D = || R a − R x || 2 represents the Euclidean distance</text>
<text top="407" left="312" width="251" height="9" font="font5" id="p7_t114" reading_order_no="113" segment_no="8" tag_type="text">between the descriptor representation from the branch of the</text>
<text top="419" left="312" width="251" height="9" font="font5" id="p7_t115" reading_order_no="114" segment_no="8" tag_type="text">anchor image ( R a ) and the descriptor representation from the</text>
<text top="431" left="312" width="250" height="9" font="font5" id="p7_t116" reading_order_no="115" segment_no="8" tag_type="text">other branch ( R x ). While m represents a margin parameter, Y</text>
<text top="443" left="312" width="251" height="9" font="font5" id="p7_t117" reading_order_no="116" segment_no="8" tag_type="text">represents the label, where Y = 1 refers to a positive pair and</text>
<text top="455" left="311" width="69" height="9" font="font11" id="p7_t118" reading_order_no="117" segment_no="8" tag_type="text">Y = 0 otherwise.</text>
<text top="467" left="322" width="241" height="9" font="font5" id="p7_t119" reading_order_no="118" segment_no="10" tag_type="text">Similar to the former loss, the triplet loss also relies on</text>
<text top="479" left="312" width="251" height="9" font="font5" id="p7_t120" reading_order_no="119" segment_no="10" tag_type="text">more than one branch during training. However, instead of</text>
<text top="491" left="312" width="251" height="9" font="font5" id="p7_t121" reading_order_no="120" segment_no="10" tag_type="text">computing the distance between positive or negative pairs at</text>
<text top="503" left="312" width="251" height="9" font="font5" id="p7_t122" reading_order_no="121" segment_no="10" tag_type="text">each iteration, the triplet loss function computes the distance</text>
<text top="515" left="312" width="251" height="9" font="font5" id="p7_t123" reading_order_no="122" segment_no="10" tag_type="text">between a positive and a negative pair at the same iteration,</text>
<text top="527" left="312" width="251" height="9" font="font5" id="p7_t124" reading_order_no="123" segment_no="10" tag_type="text">relying, thus, on three branches. As in the former loss function,</text>
<text top="539" left="312" width="251" height="9" font="font5" id="p7_t125" reading_order_no="124" segment_no="10" tag_type="text">the objective is to train a network to keep positive pairs close</text>
<text top="551" left="312" width="251" height="9" font="font5" id="p7_t126" reading_order_no="125" segment_no="10" tag_type="text">and negative pairs apart. The Triplet loss function can be</text>
<text top="563" left="312" width="91" height="9" font="font5" id="p7_t127" reading_order_no="126" segment_no="10" tag_type="text">formulated as follows:</text>
<text top="585" left="386" width="59" height="12" font="font11" id="p7_t128" reading_order_no="127" segment_no="13" tag_type="formula">L = max ( 0 , D 2</text>
<text top="585" left="441" width="26" height="13" font="font7" id="p7_t129" reading_order_no="128" segment_no="13" tag_type="formula">p − D 2</text>
<text top="588" left="463" width="26" height="10" font="font25" id="p7_t130" reading_order_no="129" segment_no="13" tag_type="formula">n + m )</text>
<text top="588" left="551" width="12" height="9" font="font5" id="p7_t131" reading_order_no="130" segment_no="13" tag_type="text">(2)</text>
<text top="607" left="312" width="251" height="10" font="font5" id="p7_t132" reading_order_no="131" segment_no="14" tag_type="text">where D p refers to the distance between positive pairs (i.e.,</text>
<text top="619" left="312" width="251" height="10" font="font5" id="p7_t133" reading_order_no="132" segment_no="14" tag_type="text">between anchor and positive sample) and D n refers to the</text>
<text top="631" left="312" width="251" height="9" font="font5" id="p7_t134" reading_order_no="133" segment_no="14" tag_type="text">distance between the negative pair. This function is widely</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p7_t135" reading_order_no="134" segment_no="14" tag_type="text">used in place recognition, namely in frameworks that use input</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p7_t136" reading_order_no="135" segment_no="14" tag_type="text">data from the camera, 3d LiDARs, and RADARs, which adapt</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p7_t137" reading_order_no="136" segment_no="14" tag_type="text">the function to fit the training requirements. Loss functions that</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p7_t138" reading_order_no="137" segment_no="14" tag_type="text">drive from the triplet loss include Lazy triplet [65], weighted</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p7_t139" reading_order_no="138" segment_no="14" tag_type="text">triplet loss [96] and weakly supervised triplet ranking loss</text>
<text top="703" left="312" width="19" height="9" font="font5" id="p7_t140" reading_order_no="139" segment_no="14" tag_type="text">[24].</text>
<text top="715" left="322" width="241" height="9" font="font5" id="p7_t141" reading_order_no="140" segment_no="16" tag_type="text">The quadruplet is an extension of the triplet loss, introducing</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p7_t142" reading_order_no="141" segment_no="16" tag_type="text">an additional constraint to push the negative pairs [145] from</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p7_t143" reading_order_no="142" segment_no="16" tag_type="text">the positives pairs w.r.t different probe samples, while triplet</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font28" size="10" family="StandardSymL-Slant_167" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="text">8</text>
<text top="59" left="288" width="36" height="7" font="font6" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE III</text>
<text top="68" left="69" width="474" height="7" font="font6" id="p8_t3" reading_order_no="2" segment_no="2" tag_type="text">S UMMARY OF RECENT WORKS ON SUPERVISED END - TO - END PLACE RECOGNITION . BG = B ELIEF G ENERATION AND PM = P LACE MAPPING .</text>
<text top="94" left="55" width="22" height="7" font="font17" id="p8_t4" reading_order_no="3" segment_no="3" tag_type="table">Sensor</text>
<text top="94" left="92" width="11" height="7" font="font17" id="p8_t5" reading_order_no="4" segment_no="3" tag_type="table">Ref</text>
<text top="94" left="171" width="41" height="7" font="font17" id="p8_t6" reading_order_no="5" segment_no="3" tag_type="table">Architecture</text>
<text top="94" left="302" width="47" height="7" font="font17" id="p8_t7" reading_order_no="6" segment_no="3" tag_type="table">Loss Function</text>
<text top="94" left="404" width="25" height="7" font="font17" id="p8_t8" reading_order_no="7" segment_no="3" tag_type="table">BG / PM</text>
<text top="94" left="496" width="24" height="7" font="font17" id="p8_t9" reading_order_no="8" segment_no="3" tag_type="table">Dataset</text>
<text top="212" left="60" width="0" height="7" font="font6" id="p8_t10" reading_order_no="9" segment_no="3" tag_type="table">Camera</text>
<text top="119" left="90" width="13" height="7" font="font6" id="p8_t11" reading_order_no="10" segment_no="3" tag_type="table">[24]</text>
<text top="118" left="119" width="35" height="7" font="font17" id="p8_t12" reading_order_no="11" segment_no="3" tag_type="table">NetVLAD:</text>
<text top="127" left="119" width="103" height="7" font="font6" id="p8_t13" reading_order_no="12" segment_no="3" tag_type="table">VGG/AlexNet + NetVLAD layer</text>
<text top="119" left="276" width="35" height="7" font="font6" id="p8_t14" reading_order_no="13" segment_no="3" tag_type="table">Triplet loss</text>
<text top="119" left="385" width="49" height="7" font="font6" id="p8_t15" reading_order_no="14" segment_no="3" tag_type="table">KNN /Database</text>
<text top="119" left="460" width="98" height="7" font="font6" id="p8_t16" reading_order_no="15" segment_no="3" tag_type="table">Google Street View Time Ma-</text>
<text top="127" left="460" width="71" height="7" font="font6" id="p8_t17" reading_order_no="16" segment_no="3" tag_type="table">chine; Pitts250k [128];</text>
<text top="144" left="90" width="13" height="7" font="font6" id="p8_t18" reading_order_no="17" segment_no="3" tag_type="table">[25]</text>
<text top="144" left="119" width="146" height="7" font="font6" id="p8_t19" reading_order_no="18" segment_no="3" tag_type="table">2D CNN visual and 3D CNN structural feature</text>
<text top="153" left="119" width="113" height="7" font="font6" id="p8_t20" reading_order_no="19" segment_no="3" tag_type="table">extraction + Feature fusion network;</text>
<text top="144" left="276" width="75" height="7" font="font6" id="p8_t21" reading_order_no="20" segment_no="3" tag_type="table">Margin-based loss [129]</text>
<text top="144" left="385" width="49" height="7" font="font6" id="p8_t22" reading_order_no="21" segment_no="3" tag_type="table">KNN /Database</text>
<text top="144" left="460" width="67" height="7" font="font6" id="p8_t23" reading_order_no="22" segment_no="3" tag_type="table">Oxford RobotCar [1];</text>
<text top="170" left="90" width="13" height="7" font="font6" id="p8_t24" reading_order_no="23" segment_no="3" tag_type="table">[96]</text>
<text top="170" left="121" width="40" height="7" font="font17" id="p8_t25" reading_order_no="24" segment_no="3" tag_type="table">SPE-VLAD:<a href="deeplearning_paper41.html#14">[24]</a></text>
<text top="178" left="119" width="146" height="7" font="font6" id="p8_t26" reading_order_no="25" segment_no="3" tag_type="table">(VGG-16 network or ResNet18) + spatial pyra-</text>
<text top="187" left="119" width="100" height="7" font="font6" id="p8_t27" reading_order_no="26" segment_no="3" tag_type="table">mid structure + NetVLAD layer</text>
<text top="170" left="276" width="52" height="7" font="font6" id="p8_t28" reading_order_no="27" segment_no="3" tag_type="table">Weighted Triplet</text>
<text top="170" left="385" width="41" height="7" font="font6" id="p8_t29" reading_order_no="28" segment_no="3" tag_type="table">L2 /Database</text>
<text top="170" left="460" width="52" height="7" font="font6" id="p8_t30" reading_order_no="29" segment_no="3" tag_type="table">Pittsburgh [128];</text>
<text top="178" left="460" width="82" height="7" font="font6" id="p8_t31" reading_order_no="30" segment_no="3" tag_type="table">TokyoTimeMachine [130];<a href="deeplearning_paper41.html#16">[128];</a></text>
<text top="187" left="460" width="81" height="7" font="font6" id="p8_t32" reading_order_no="31" segment_no="3" tag_type="table">Places365-Standard [110];<a href="deeplearning_paper41.html#14">[25]</a></text>
<text top="204" left="90" width="16" height="7" font="font6" id="p8_t33" reading_order_no="32" segment_no="3" tag_type="table">[131]</text>
<text top="204" left="119" width="54" height="7" font="font17" id="p8_t34" reading_order_no="33" segment_no="3" tag_type="table">Siamese-ResNet:</text>
<text top="212" left="119" width="97" height="7" font="font6" id="p8_t35" reading_order_no="34" segment_no="3" tag_type="table">ResNet in the siamese network<a href="deeplearning_paper41.html#16">[129]</a></text>
<text top="204" left="276" width="62" height="7" font="font6" id="p8_t36" reading_order_no="35" segment_no="3" tag_type="table">L2-based loss [132]</text>
<text top="204" left="385" width="41" height="7" font="font6" id="p8_t37" reading_order_no="36" segment_no="3" tag_type="table">L2 /Database<a href="deeplearning_paper41.html#13">[1];</a></text>
<text top="204" left="460" width="38" height="7" font="font6" id="p8_t38" reading_order_no="37" segment_no="3" tag_type="table">TUM [133];<a href="deeplearning_paper41.html#15">[96]</a></text>
<text top="229" left="90" width="13" height="7" font="font6" id="p8_t39" reading_order_no="38" segment_no="3" tag_type="table">[50]</text>
<text top="229" left="119" width="52" height="7" font="font6" id="p8_t40" reading_order_no="39" segment_no="3" tag_type="table">MobileNet [134]</text>
<text top="229" left="276" width="75" height="7" font="font6" id="p8_t41" reading_order_no="40" segment_no="3" tag_type="table">Triplet loss [135], [136]</text>
<text top="229" left="385" width="30" height="7" font="font6" id="p8_t42" reading_order_no="41" segment_no="3" tag_type="table">Hamming</text>
<text top="229" left="430" width="19" height="7" font="font6" id="p8_t43" reading_order_no="43" segment_no="3" tag_type="table">K-NN</text>
<text top="238" left="385" width="30" height="7" font="font6" id="p8_t44" reading_order_no="42" segment_no="3" tag_type="table">/Database<a href="deeplearning_paper41.html#16">[128];</a></text>
<text top="229" left="460" width="42" height="7" font="font6" id="p8_t45" reading_order_no="44" segment_no="3" tag_type="table">Nordland [2];<a href="deeplearning_paper41.html#16">[130];</a></text>
<text top="238" left="460" width="59" height="7" font="font6" id="p8_t46" reading_order_no="45" segment_no="3" tag_type="table">Gardens Point [22]<a href="deeplearning_paper41.html#16">[110];</a></text>
<text top="255" left="90" width="16" height="7" font="font6" id="p8_t47" reading_order_no="46" segment_no="3" tag_type="table">[137]<a href="deeplearning_paper41.html#16">[131]</a></text>
<text top="255" left="119" width="51" height="7" font="font6" id="p8_t48" reading_order_no="47" segment_no="3" tag_type="table">HybridNet [138]</text>
<text top="255" left="276" width="37" height="7" font="font6" id="p8_t49" reading_order_no="48" segment_no="3" tag_type="table">Triplet Loss</text>
<text top="255" left="385" width="53" height="7" font="font6" id="p8_t50" reading_order_no="49" segment_no="3" tag_type="table">Cosine /Database<a href="deeplearning_paper41.html#16">[132]</a></text>
<text top="255" left="460" width="98" height="7" font="font6" id="p8_t51" reading_order_no="50" segment_no="3" tag_type="table">Oxford RobotCar [6]; Nordland</text>
<text top="263" left="460" width="75" height="7" font="font6" id="p8_t52" reading_order_no="51" segment_no="3" tag_type="table">[2]; Gardens Point [22];<a href="deeplearning_paper41.html#16">[133];</a></text>
<text top="358" left="60" width="0" height="7" font="font6" id="p8_t53" reading_order_no="54" segment_no="3" tag_type="table">3D<a href="deeplearning_paper41.html#14">[50]</a></text>
<text top="346" left="60" width="0" height="7" font="font6" id="p8_t54" reading_order_no="53" segment_no="3" tag_type="table">LiD<a href="deeplearning_paper41.html#16">[134]</a></text>
<text top="335" left="60" width="0" height="7" font="font6" id="p8_t55" reading_order_no="52" segment_no="3" tag_type="table">AR<a href="deeplearning_paper41.html#16">[135], [136]</a></text>
<text top="285" left="90" width="13" height="7" font="font6" id="p8_t56" reading_order_no="55" segment_no="3" tag_type="table">[49]</text>
<text top="285" left="119" width="31" height="7" font="font17" id="p8_t57" reading_order_no="56" segment_no="3" tag_type="table">LPD-Net:</text>
<text top="294" left="119" width="146" height="7" font="font6" id="p8_t58" reading_order_no="57" segment_no="3" tag_type="table">Adaptive feature extraction + a graph-based</text>
<text top="302" left="119" width="139" height="7" font="font6" id="p8_t59" reading_order_no="58" segment_no="3" tag_type="table">neighborhood aggregation + NetVLAD layer<a href="deeplearning_paper41.html#13">[2];</a></text>
<text top="285" left="276" width="65" height="7" font="font6" id="p8_t60" reading_order_no="59" segment_no="3" tag_type="table">Lazy quadruplet loss<a href="deeplearning_paper41.html#14">[22]</a></text>
<text top="285" left="385" width="41" height="7" font="font6" id="p8_t61" reading_order_no="60" segment_no="3" tag_type="table">L2 /Database<a href="deeplearning_paper41.html#16">[137]</a></text>
<text top="285" left="460" width="66" height="7" font="font6" id="p8_t62" reading_order_no="61" segment_no="3" tag_type="table">Oxford Robotcar [1];<a href="deeplearning_paper41.html#16">[138]</a></text>
<text top="319" left="90" width="13" height="7" font="font6" id="p8_t63" reading_order_no="62" segment_no="3" tag_type="table">[65]</text>
<text top="319" left="119" width="52" height="7" font="font17" id="p8_t64" reading_order_no="63" segment_no="3" tag_type="table">PointNetVLAD:</text>
<text top="328" left="119" width="86" height="7" font="font6" id="p8_t65" reading_order_no="64" segment_no="3" tag_type="table">PointNet + NetVLAD layer<a href="deeplearning_paper41.html#13">[6]; </a>Nordland</text>
<text top="319" left="276" width="98" height="7" font="font6" id="p8_t66" reading_order_no="65" segment_no="3" tag_type="table">Lazy triplet and quadruplet loss<a href="deeplearning_paper41.html#13">[2]; </a>Gardens Point <a href="deeplearning_paper41.html#14">[22];</a></text>
<text top="319" left="385" width="49" height="7" font="font6" id="p8_t67" reading_order_no="66" segment_no="3" tag_type="table">KNN /Database</text>
<text top="319" left="460" width="67" height="7" font="font6" id="p8_t68" reading_order_no="67" segment_no="3" tag_type="table">Oxford RobotCar [1];</text>
<text top="345" left="90" width="13" height="7" font="font6" id="p8_t69" reading_order_no="68" segment_no="3" tag_type="table">[34]</text>
<text top="345" left="119" width="29" height="7" font="font17" id="p8_t70" reading_order_no="69" segment_no="3" tag_type="table">OREOS:<a href="deeplearning_paper41.html#14">[49]</a></text>
<text top="353" left="119" width="73" height="7" font="font6" id="p8_t71" reading_order_no="70" segment_no="3" tag_type="table">CNN as in [120], [139]</text>
<text top="345" left="276" width="54" height="7" font="font6" id="p8_t72" reading_order_no="71" segment_no="3" tag_type="table">Triplet loss [140]</text>
<text top="345" left="385" width="49" height="7" font="font6" id="p8_t73" reading_order_no="72" segment_no="3" tag_type="table">KNN /Database</text>
<text top="345" left="460" width="40" height="7" font="font6" id="p8_t74" reading_order_no="73" segment_no="3" tag_type="table">NCLT [141];</text>
<text top="353" left="460" width="41" height="7" font="font6" id="p8_t75" reading_order_no="74" segment_no="3" tag_type="table">KITTI [142];</text>
<text top="370" left="90" width="13" height="7" font="font6" id="p8_t76" reading_order_no="75" segment_no="3" tag_type="table">[13]<a href="deeplearning_paper41.html#13">[1];</a></text>
<text top="370" left="119" width="26" height="7" font="font17" id="p8_t77" reading_order_no="76" segment_no="3" tag_type="table">LocNet:<a href="deeplearning_paper41.html#15">[65]</a></text>
<text top="379" left="119" width="53" height="7" font="font6" id="p8_t78" reading_order_no="77" segment_no="3" tag_type="table">Siamese network</text>
<text top="370" left="276" width="96" height="7" font="font6" id="p8_t79" reading_order_no="78" segment_no="3" tag_type="table">Contrastive loss function [143]</text>
<text top="370" left="385" width="60" height="7" font="font6" id="p8_t80" reading_order_no="79" segment_no="3" tag_type="table">L2 KNN /Database</text>
<text top="370" left="460" width="39" height="7" font="font6" id="p8_t81" reading_order_no="80" segment_no="3" tag_type="table">KITTI [142]</text>
<text top="379" left="460" width="47" height="7" font="font6" id="p8_t82" reading_order_no="81" segment_no="3" tag_type="table">inhouse dataset<a href="deeplearning_paper41.html#13">[1];</a></text>
<text top="396" left="90" width="16" height="7" font="font6" id="p8_t83" reading_order_no="82" segment_no="3" tag_type="table">[144]<a href="deeplearning_paper41.html#14">[34]</a></text>
<text top="396" left="119" width="53" height="7" font="font6" id="p8_t84" reading_order_no="83" segment_no="3" tag_type="table">Siamese network</text>
<text top="396" left="276" width="68" height="7" font="font6" id="p8_t85" reading_order_no="84" segment_no="3" tag_type="table">Contrastive loss [143]<a href="deeplearning_paper41.html#16">[120], [139]</a></text>
<text top="396" left="385" width="60" height="7" font="font6" id="p8_t86" reading_order_no="85" segment_no="3" tag_type="table">L2 KNN /Database<a href="deeplearning_paper41.html#16">[140]</a></text>
<text top="396" left="460" width="93" height="7" font="font6" id="p8_t87" reading_order_no="86" segment_no="3" tag_type="table">KITTI [142]; inhouse dataset;</text>
<text top="418" left="55" width="26" height="7" font="font6" id="p8_t88" reading_order_no="87" segment_no="3" tag_type="table">RADAR<a href="deeplearning_paper41.html#16">[141];</a></text>
<text top="418" left="90" width="13" height="7" font="font6" id="p8_t89" reading_order_no="88" segment_no="3" tag_type="table">[81]<a href="deeplearning_paper41.html#16">[142];</a></text>
<text top="418" left="119" width="86" height="7" font="font6" id="p8_t90" reading_order_no="89" segment_no="3" tag_type="table">VGG-16 + NetVLAD layer<a href="deeplearning_paper41.html#13">[13]</a></text>
<text top="418" left="276" width="35" height="7" font="font6" id="p8_t91" reading_order_no="90" segment_no="3" tag_type="table">Triplet loss</text>
<text top="418" left="385" width="49" height="7" font="font6" id="p8_t92" reading_order_no="91" segment_no="3" tag_type="table">KNN /Database</text>
<text top="418" left="460" width="86" height="7" font="font6" id="p8_t93" reading_order_no="92" segment_no="3" tag_type="table">Oxford Radar RobotCar [6]<a href="deeplearning_paper41.html#16">[143]</a></text>
<text top="454" left="49" width="251" height="9" font="font5" id="p8_t94" reading_order_no="93" segment_no="4" tag_type="text">loss only pushes the negatives from the positives w.r.t from the</text>
<text top="466" left="49" width="251" height="9" font="font5" id="p8_t95" reading_order_no="94" segment_no="4" tag_type="text">same probe. The additional constrain of the quadruplet loss<a href="deeplearning_paper41.html#16">[142]</a></text>
<text top="478" left="49" width="251" height="9" font="font5" id="p8_t96" reading_order_no="95" segment_no="4" tag_type="text">reduces the intra-class variations and enlarges the inter-class</text>
<text top="490" left="49" width="204" height="9" font="font5" id="p8_t97" reading_order_no="96" segment_no="4" tag_type="text">variations. This function is formulated as follows:<a href="deeplearning_paper41.html#16">[144]</a></text>
<text top="508" left="65" width="181" height="12" font="font11" id="p8_t98" reading_order_no="97" segment_no="6" tag_type="formula">L = max ( 0 , D 2 p − D 2 n + m 1 ) + max ( 0 , D 2 p − D 2</text>
<text top="510" left="242" width="31" height="11" font="font25" id="p8_t99" reading_order_no="98" segment_no="6" tag_type="formula">d + m 2 )<a href="deeplearning_paper41.html#16">[143]</a></text>
<text top="510" left="288" width="12" height="9" font="font5" id="p8_t100" reading_order_no="99" segment_no="6" tag_type="text">(3)</text>
<text top="531" left="49" width="251" height="9" font="font5" id="p8_t101" reading_order_no="100" segment_no="8" tag_type="text">where D p and D n represent the distance between the positive<a href="deeplearning_paper41.html#16">[142]; </a>inhouse dataset;</text>
<text top="543" left="49" width="252" height="10" font="font5" id="p8_t102" reading_order_no="101" segment_no="8" tag_type="text">and negative pairs, respectively. The m 1 and m 2 represent</text>
<text top="555" left="49" width="251" height="9" font="font5" id="p8_t103" reading_order_no="102" segment_no="8" tag_type="text">margin parameters, while R d corresponds to the additional con-<a href="deeplearning_paper41.html#15">[81]</a></text>
<text top="567" left="49" width="251" height="9" font="font5" id="p8_t104" reading_order_no="103" segment_no="8" tag_type="text">straint, representing the distance between negative pairs from</text>
<text top="578" left="49" width="251" height="9" font="font5" id="p8_t105" reading_order_no="104" segment_no="8" tag_type="text">different probes. In [49], [65], the quadruplet loss function is</text>
<text top="590" left="49" width="251" height="9" font="font5" id="p8_t106" reading_order_no="105" segment_no="8" tag_type="text">used to train networks for the task of place recognition using</text>
<text top="602" left="49" width="68" height="9" font="font5" id="p8_t107" reading_order_no="106" segment_no="8" tag_type="text">3D LiDAR data.<a href="deeplearning_paper41.html#13">[6]</a></text>
<text top="615" left="59" width="241" height="9" font="font5" id="p8_t108" reading_order_no="107" segment_no="9" tag_type="text">The margin-based loss function is a simple extension to the</text>
<text top="627" left="49" width="251" height="9" font="font5" id="p8_t109" reading_order_no="108" segment_no="9" tag_type="text">contrastive loss [129]. While the contrastive function enforces</text>
<text top="639" left="49" width="251" height="9" font="font5" id="p8_t110" reading_order_no="109" segment_no="9" tag_type="text">the positive pairs to be as close as possible, the margin-based</text>
<text top="651" left="49" width="251" height="9" font="font5" id="p8_t111" reading_order_no="110" segment_no="9" tag_type="text">function only encourages the positive pairs to be within a</text>
<text top="663" left="49" width="92" height="9" font="font5" id="p8_t112" reading_order_no="111" segment_no="9" tag_type="text">distance of each other.</text>
<text top="682" left="121" width="108" height="10" font="font11" id="p8_t113" reading_order_no="112" segment_no="11" tag_type="formula">L = max ( 0 , α + Y ( D − m ))</text>
<text top="683" left="288" width="12" height="9" font="font5" id="p8_t114" reading_order_no="113" segment_no="11" tag_type="text">(4)</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p8_t115" reading_order_no="114" segment_no="12" tag_type="text">where Y ∈ 1 , − 1 represents the labels of Y = 1 when the pair is</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p8_t116" reading_order_no="115" segment_no="12" tag_type="text">positive and Y = − 1 otherwise. α is a variable that determines</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p8_t117" reading_order_no="116" segment_no="12" tag_type="text">the boundary between positive and negative pairs. The margin-</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p8_t118" reading_order_no="117" segment_no="12" tag_type="text">based loss function was proposed in [129] to demonstrate that</text>
<text top="454" left="312" width="251" height="9" font="font5" id="p8_t119" reading_order_no="118" segment_no="5" tag_type="text">state-of-art performance could be achieved with a simple loss</text>
<text top="466" left="312" width="251" height="9" font="font5" id="p8_t120" reading_order_no="119" segment_no="5" tag_type="text">function, only by having an adequate sampling strategy of the</text>
<text top="478" left="312" width="251" height="9" font="font5" id="p8_t121" reading_order_no="120" segment_no="5" tag_type="text">input data during training. This function is used in [25] to train</text>
<text top="490" left="312" width="251" height="9" font="font5" id="p8_t122" reading_order_no="121" segment_no="5" tag_type="text">a multi-modal network. The network is jointly trained based</text>
<text top="502" left="312" width="251" height="9" font="font5" id="p8_t123" reading_order_no="122" segment_no="5" tag_type="text">on information extracted from images and structural data in the</text>
<text top="514" left="312" width="248" height="9" font="font5" id="p8_t124" reading_order_no="123" segment_no="5" tag_type="text">format of voxel grids, which are generated from the images.</text>
<text top="530" left="322" width="241" height="9" font="font11" id="p8_t125" reading_order_no="124" segment_no="7" tag_type="text">2) Camera-based Networks: A key contribution to super-</text>
<text top="542" left="312" width="251" height="9" font="font5" id="p8_t126" reading_order_no="125" segment_no="7" tag_type="text">vised end-to-end-based place recognition is the NetVLAD</text>
<text top="554" left="312" width="251" height="9" font="font5" id="p8_t127" reading_order_no="126" segment_no="7" tag_type="text">layer [24]. Inspired by the Vector of Locally Aggregated</text>
<text top="566" left="312" width="251" height="9" font="font5" id="p8_t128" reading_order_no="127" segment_no="7" tag_type="text">Descriptors (VLAD) [146], Arandjelovi´c et al. [24] propose</text>
<text top="578" left="312" width="251" height="9" font="font5" id="p8_t129" reading_order_no="128" segment_no="7" tag_type="text">NetVLAD as a ‘pluggable’ layer into any CNN architecture to</text>
<text top="590" left="312" width="251" height="9" font="font5" id="p8_t130" reading_order_no="129" segment_no="7" tag_type="text">output a compact image descriptor. The network’s parameters</text>
<text top="602" left="312" width="251" height="9" font="font5" id="p8_t131" reading_order_no="130" segment_no="7" tag_type="text">are learned using a weakly supervised triplet ranking loss</text>
<text top="614" left="312" width="36" height="9" font="font5" id="p8_t132" reading_order_no="131" segment_no="7" tag_type="text">function.</text>
<text top="629" left="322" width="241" height="9" font="font5" id="p8_t133" reading_order_no="132" segment_no="10" tag_type="text">Yu et al. [96] also exploited VLAD descriptors for images,</text>
<text top="641" left="312" width="251" height="9" font="font5" id="p8_t134" reading_order_no="133" segment_no="10" tag_type="text">proposing a spatial pyramid-enhanced VLAD (SPE-VLAD)</text>
<text top="653" left="312" width="251" height="9" font="font5" id="p8_t135" reading_order_no="134" segment_no="10" tag_type="text">layer. The proposed layer leverages the spatial pyramid struc-</text>
<text top="665" left="312" width="251" height="9" font="font5" id="p8_t136" reading_order_no="135" segment_no="10" tag_type="text">ture of images to enhance place description, using for feature</text>
<text top="676" left="312" width="251" height="9" font="font5" id="p8_t137" reading_order_no="136" segment_no="10" tag_type="text">extraction a VGG-16 [120] and a ResNet-18 [147], and as</text>
<text top="688" left="312" width="251" height="9" font="font5" id="p8_t138" reading_order_no="137" segment_no="10" tag_type="text">loss function the weighted T-loss. The network’s parameters</text>
<text top="700" left="312" width="251" height="9" font="font5" id="p8_t139" reading_order_no="138" segment_no="10" tag_type="text">are learned under weakly supervised scenarios, using GPS tags</text>
<text top="712" left="312" width="251" height="9" font="font5" id="p8_t140" reading_order_no="139" segment_no="10" tag_type="text">and the Euclidean distance between the image representations.</text>
<text top="727" left="322" width="241" height="9" font="font5" id="p8_t141" reading_order_no="140" segment_no="13" tag_type="text">Qiu et al. [131] apply a siamese-based network to loop</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p8_t142" reading_order_no="141" segment_no="13" tag_type="text">closure detection. Siamese networks are twin neural networks,</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="560" width="3" height="6" font="font0" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="text">9</text>
<text top="58" left="49" width="251" height="9" font="font5" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="text">which share the same parameters and are particularly useful</text>
<text top="70" left="49" width="251" height="9" font="font5" id="p9_t3" reading_order_no="2" segment_no="1" tag_type="text">when limited training data is available. Both sub-networks</text>
<text top="82" left="49" width="251" height="9" font="font5" id="p9_t4" reading_order_no="3" segment_no="1" tag_type="text">share the same parameters and mirror the update of the</text>
<text top="94" left="49" width="251" height="9" font="font5" id="p9_t5" reading_order_no="4" segment_no="1" tag_type="text">parameters. In this work, the sub-networks are replaced by</text>
<text top="106" left="49" width="251" height="9" font="font5" id="p9_t6" reading_order_no="5" segment_no="1" tag_type="text">ResNet to improve feature representation, and the network is</text>
<text top="117" left="49" width="240" height="9" font="font5" id="p9_t7" reading_order_no="6" segment_no="1" tag_type="text">trained, resorting to an L2-based loss function as in [132].<a href="deeplearning_paper41.html#16">[132].</a></text>
<text top="129" left="59" width="241" height="9" font="font5" id="p9_t8" reading_order_no="7" segment_no="4" tag_type="text">Wu et al. [50] jointly addresses the place recognition<a href="deeplearning_paper41.html#14">[50] </a>jointly addresses the place recognition</text>
<text top="141" left="49" width="251" height="9" font="font5" id="p9_t9" reading_order_no="8" segment_no="4" tag_type="text">problem from the efficiency and performance perspective,</text>
<text top="153" left="49" width="251" height="9" font="font5" id="p9_t10" reading_order_no="9" segment_no="4" tag_type="text">proposing to this end a deep supervised hashing approach</text>
<text top="165" left="49" width="251" height="9" font="font5" id="p9_t11" reading_order_no="10" segment_no="4" tag_type="text">with a similar hierarchy. Hashing is an encoding technique that</text>
<text top="177" left="49" width="251" height="9" font="font5" id="p9_t12" reading_order_no="11" segment_no="4" tag_type="text">maps high dimensional data into a set of binary codes, having</text>
<text top="189" left="49" width="251" height="9" font="font5" id="p9_t13" reading_order_no="12" segment_no="4" tag_type="text">low computational requirements and high storage efficiency.</text>
<text top="201" left="49" width="251" height="9" font="font5" id="p9_t14" reading_order_no="13" segment_no="4" tag_type="text">The proposed framework comprises three modules: features</text>
<text top="213" left="49" width="251" height="9" font="font5" id="p9_t15" reading_order_no="14" segment_no="4" tag_type="text">extraction based on MobileNet [134]; hash code learning,<a href="deeplearning_paper41.html#16">[134]; </a>hash code learning,</text>
<text top="225" left="49" width="251" height="9" font="font5" id="p9_t16" reading_order_no="15" segment_no="4" tag_type="text">obtained using the last fully connected layer of MobileNet;</text>
<text top="237" left="49" width="251" height="9" font="font5" id="p9_t17" reading_order_no="16" segment_no="4" tag_type="text">and loss function, which is based on the likelihood [135],<a href="deeplearning_paper41.html#16">[135],</a></text>
<text top="249" left="49" width="251" height="9" font="font5" id="p9_t18" reading_order_no="17" segment_no="4" tag_type="text">[136]. This work proposes a similar hierarchy method to<a href="deeplearning_paper41.html#16">[136]. </a>This work proposes a similar hierarchy method to</text>
<text top="261" left="49" width="251" height="9" font="font5" id="p9_t19" reading_order_no="18" segment_no="4" tag_type="text">distinguish similar images. To this end, the distance of hashing</text>
<text top="273" left="49" width="251" height="9" font="font5" id="p9_t20" reading_order_no="19" segment_no="4" tag_type="text">codes between a pair of images must increase as similar</text>
<text top="285" left="49" width="251" height="9" font="font5" id="p9_t21" reading_order_no="20" segment_no="4" tag_type="text">images are more distinct and must remain the same between</text>
<text top="297" left="49" width="251" height="9" font="font5" id="p9_t22" reading_order_no="21" segment_no="4" tag_type="text">different images. These two conditions are essential to use</text>
<text top="309" left="49" width="186" height="9" font="font5" id="p9_t23" reading_order_no="22" segment_no="4" tag_type="text">deep supervised hashing in place recognition.</text>
<text top="321" left="59" width="241" height="9" font="font5" id="p9_t24" reading_order_no="23" segment_no="6" tag_type="text">Another efficiency improving technique for deep networks</text>
<text top="333" left="49" width="251" height="9" font="font5" id="p9_t25" reading_order_no="24" segment_no="6" tag_type="text">is network pruning. This technique aims to reduce the size</text>
<text top="345" left="49" width="251" height="9" font="font5" id="p9_t26" reading_order_no="25" segment_no="6" tag_type="text">of the network by removing unnecessary neurons or setting</text>
<text top="357" left="49" width="251" height="9" font="font5" id="p9_t27" reading_order_no="26" segment_no="6" tag_type="text">the weights to zero [148]. Hausler et al. [137] propose a<a href="deeplearning_paper41.html#16">[148]. </a>Hausler et al. <a href="deeplearning_paper41.html#16">[137] </a>propose a</text>
<text top="369" left="49" width="251" height="9" font="font5" id="p9_t28" reading_order_no="27" segment_no="6" tag_type="text">feature filtering approach, which removes feature maps at the</text>
<text top="380" left="49" width="251" height="9" font="font5" id="p9_t29" reading_order_no="28" segment_no="6" tag_type="text">beginning of the network while using for matching late feature</text>
<text top="392" left="49" width="251" height="9" font="font5" id="p9_t30" reading_order_no="29" segment_no="6" tag_type="text">maps to foster efficiency and performance simultaneously.</text>
<text top="404" left="49" width="251" height="9" font="font5" id="p9_t31" reading_order_no="30" segment_no="6" tag_type="text">Feature maps to be removed are determined based on a Triplet</text>
<text top="416" left="49" width="251" height="9" font="font5" id="p9_t32" reading_order_no="31" segment_no="6" tag_type="text">Loss calibration procedure. As a feature extraction framework,</text>
<text top="428" left="49" width="162" height="9" font="font5" id="p9_t33" reading_order_no="32" segment_no="6" tag_type="text">the approach uses the HybridNet [138].<a href="deeplearning_paper41.html#16">[138].</a></text>
<text top="440" left="59" width="241" height="9" font="font5" id="p9_t34" reading_order_no="33" segment_no="8" tag_type="text">Contrary to former single modality works, Oertel et al.</text>
<text top="452" left="49" width="251" height="9" font="font5" id="p9_t35" reading_order_no="34" segment_no="8" tag_type="text">[25] propose a place description approach that uses vision<a href="deeplearning_paper41.html#14">[25] </a>propose a place description approach that uses vision</text>
<text top="464" left="49" width="251" height="9" font="font5" id="p9_t36" reading_order_no="35" segment_no="8" tag_type="text">and structural information, both originated from camera data.</text>
<text top="476" left="49" width="251" height="9" font="font5" id="p9_t37" reading_order_no="36" segment_no="8" tag_type="text">This approach jointly uses vision and depth data from a stereo</text>
<text top="488" left="49" width="251" height="9" font="font5" id="p9_t38" reading_order_no="37" segment_no="8" tag_type="text">camera in an end-to-end pipeline. The structural information</text>
<text top="500" left="49" width="251" height="9" font="font5" id="p9_t39" reading_order_no="38" segment_no="8" tag_type="text">is first obtained utilizing the Direct Sparse Odometry (DSO)</text>
<text top="512" left="49" width="251" height="9" font="font5" id="p9_t40" reading_order_no="39" segment_no="8" tag_type="text">framework [149] and then discretized into regular voxel grids<a href="deeplearning_paper41.html#16">[149] </a>and then discretized into regular voxel grids</text>
<text top="524" left="49" width="251" height="9" font="font5" id="p9_t41" reading_order_no="40" segment_no="8" tag_type="text">to serve as inputs along with the corresponding image. The</text>
<text top="536" left="49" width="251" height="9" font="font5" id="p9_t42" reading_order_no="41" segment_no="8" tag_type="text">pipeline has two parallel branches: one for vision and another</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p9_t43" reading_order_no="42" segment_no="8" tag_type="text">for the structural data, which use 2D and 3D convolutional</text>
<text top="560" left="49" width="251" height="9" font="font5" id="p9_t44" reading_order_no="43" segment_no="8" tag_type="text">layers, respectively, for feature extraction. Both branches are</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p9_t45" reading_order_no="44" segment_no="8" tag_type="text">learned jointly through a margin-based loss function. The</text>
<text top="584" left="49" width="251" height="9" font="font5" id="p9_t46" reading_order_no="45" segment_no="8" tag_type="text">outputs of the branches are concatenated into a single vector,</text>
<text top="596" left="49" width="246" height="9" font="font5" id="p9_t47" reading_order_no="46" segment_no="8" tag_type="text">which is fed to a fusion network that outputs the descriptor.</text>
<text top="608" left="59" width="241" height="9" font="font11" id="p9_t48" reading_order_no="47" segment_no="11" tag_type="text">3) 3D LiDAR-based Network: Although NetVLAD was</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p9_t49" reading_order_no="48" segment_no="11" tag_type="text">originally used for images, it has also been used on 3D</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p9_t50" reading_order_no="49" segment_no="11" tag_type="text">LiDAR data [49], [65]. Uy et al. [65] and Liu et al. [49]</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p9_t51" reading_order_no="50" segment_no="11" tag_type="text">propose respectively PointNetVLAD and LPD-Net, which are<a href="deeplearning_paper41.html#14">[49], </a><a href="deeplearning_paper41.html#15">[65]. </a>Uy et al. <a href="deeplearning_paper41.html#15">[65] </a>and Liu et al. <a href="deeplearning_paper41.html#14">[49]</a></text>
<text top="655" left="49" width="251" height="9" font="font5" id="p9_t52" reading_order_no="51" segment_no="11" tag_type="text">NetVLAD-based global descriptor learning approaches for</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p9_t53" reading_order_no="52" segment_no="11" tag_type="text">3D LiDAR data. Both have compatible inputs and outputs,</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p9_t54" reading_order_no="53" segment_no="11" tag_type="text">receiving as input raw point clouds and outputting a descriptor.</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p9_t55" reading_order_no="54" segment_no="11" tag_type="text">The difference relies on the feature extraction and feature</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p9_t56" reading_order_no="55" segment_no="11" tag_type="text">processing methods. PointNetVLAD [65] relies on PointNet</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p9_t57" reading_order_no="56" segment_no="11" tag_type="text">[150], a 3D object detection and segmentation approach,<a href="deeplearning_paper41.html#15">[65] </a>relies on PointNet</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p9_t58" reading_order_no="57" segment_no="11" tag_type="text">for feature extraction. In contrast, LPD-Net relies on an<a href="deeplearning_paper41.html#16">[150], </a>a 3D object detection and segmentation approach,</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p9_t59" reading_order_no="58" segment_no="11" tag_type="text">adaptive local feature extraction module and a graph-based</text>
<text top="58" left="312" width="251" height="9" font="font5" id="p9_t60" reading_order_no="59" segment_no="2" tag_type="text">neighborhood aggregation module, aggregating both in the</text>
<text top="70" left="312" width="251" height="9" font="font5" id="p9_t61" reading_order_no="60" segment_no="2" tag_type="text">Feature Space and the Cartesian Space. Regarding the network</text>
<text top="82" left="312" width="251" height="9" font="font5" id="p9_t62" reading_order_no="61" segment_no="2" tag_type="text">training, Uy et al. [65] showed that the lazy quadruplet loss</text>
<text top="94" left="312" width="251" height="9" font="font5" id="p9_t63" reading_order_no="62" segment_no="2" tag_type="text">function enables higher performance than the lazy triplet loss<a href="deeplearning_paper41.html#15">[65] </a>showed that the lazy quadruplet loss</text>
<text top="106" left="312" width="244" height="9" font="font5" id="p9_t64" reading_order_no="63" segment_no="2" tag_type="text">function, motivating Liu et al. [49] to follow this approach.</text>
<text top="118" left="322" width="241" height="9" font="font5" id="p9_t65" reading_order_no="64" segment_no="3" tag_type="text">A different 3D-LiDAR-based place recognition approach is<a href="deeplearning_paper41.html#14">[49] </a>to follow this approach.</text>
<text top="130" left="312" width="251" height="9" font="font5" id="p9_t66" reading_order_no="65" segment_no="3" tag_type="text">proposed in [34]. Schaupp et al. propose OREOS, which is</text>
<text top="142" left="312" width="251" height="9" font="font5" id="p9_t67" reading_order_no="66" segment_no="3" tag_type="text">a triplet DL network-based architecture [140]. The OREOS<a href="deeplearning_paper41.html#14">[34]. </a>Schaupp et al. propose OREOS, which is</text>
<text top="154" left="312" width="251" height="9" font="font5" id="p9_t68" reading_order_no="67" segment_no="3" tag_type="text">approach receives as input 2D range images and outputs<a href="deeplearning_paper41.html#16">[140]. </a>The OREOS</text>
<text top="166" left="312" width="251" height="9" font="font5" id="p9_t69" reading_order_no="68" segment_no="3" tag_type="text">orientation-and place-dependent descriptors. The 2D range</text>
<text top="178" left="312" width="251" height="9" font="font5" id="p9_t70" reading_order_no="69" segment_no="3" tag_type="text">images are the result of the 3D point clouds projections</text>
<text top="190" left="312" width="251" height="9" font="font5" id="p9_t71" reading_order_no="70" segment_no="3" tag_type="text">onto an image representation. The network is trained using</text>
<text top="202" left="312" width="251" height="9" font="font5" id="p9_t72" reading_order_no="71" segment_no="3" tag_type="text">an L2 distance-based triplet loss function to compute the</text>
<text top="213" left="312" width="251" height="9" font="font5" id="p9_t73" reading_order_no="72" segment_no="3" tag_type="text">similarity between anchor-positive and anchor-negative. Place</text>
<text top="225" left="312" width="251" height="9" font="font5" id="p9_t74" reading_order_no="73" segment_no="3" tag_type="text">recognition is validated using a k-nearest neighbor framework</text>
<text top="237" left="312" width="55" height="9" font="font5" id="p9_t75" reading_order_no="74" segment_no="3" tag_type="text">for matching.</text>
<text top="250" left="322" width="241" height="9" font="font5" id="p9_t76" reading_order_no="75" segment_no="5" tag_type="text">Yin et al. [13] uses 3D point clouds to address the global</text>
<text top="262" left="312" width="251" height="9" font="font5" id="p9_t77" reading_order_no="76" segment_no="5" tag_type="text">localization problem, proposing a place recognition and metric<a href="deeplearning_paper41.html#13">[13] </a>uses 3D point clouds to address the global</text>
<text top="274" left="312" width="251" height="9" font="font5" id="p9_t78" reading_order_no="77" segment_no="5" tag_type="text">pose estimation approach. Place recognition is achieved using</text>
<text top="286" left="312" width="251" height="9" font="font5" id="p9_t79" reading_order_no="78" segment_no="5" tag_type="text">the siamese LocNets, which is a semi-handcrafted represen-</text>
<text top="298" left="312" width="251" height="9" font="font5" id="p9_t80" reading_order_no="79" segment_no="5" tag_type="text">tation learning method for LiDAR point clouds. As input,</text>
<text top="310" left="312" width="251" height="9" font="font5" id="p9_t81" reading_order_no="80" segment_no="5" tag_type="text">LocNets receives a handcrafted rotational invariant represen-</text>
<text top="321" left="312" width="251" height="9" font="font5" id="p9_t82" reading_order_no="81" segment_no="5" tag_type="text">tation extracted from point clouds in a pre-processing step and</text>
<text top="333" left="312" width="251" height="9" font="font5" id="p9_t83" reading_order_no="82" segment_no="5" tag_type="text">outputs a low-dimensional fingerprint. The network follows a</text>
<text top="345" left="312" width="251" height="9" font="font5" id="p9_t84" reading_order_no="83" segment_no="5" tag_type="text">Siamese architecture and uses for learning Euclidean distance-</text>
<text top="357" left="312" width="251" height="9" font="font5" id="p9_t85" reading_order_no="84" segment_no="5" tag_type="text">based contrastive loss function [143]. For belief generation,</text>
<text top="369" left="312" width="251" height="9" font="font5" id="p9_t86" reading_order_no="85" segment_no="5" tag_type="text">an L2-based KNN approach is used. A similar LocNets-based<a href="deeplearning_paper41.html#16">[143]. </a>For belief generation,</text>
<text top="381" left="312" width="125" height="9" font="font5" id="p9_t87" reading_order_no="86" segment_no="5" tag_type="text">approach is proposed in [144].</text>
<text top="394" left="322" width="241" height="9" font="font11" id="p9_t88" reading_order_no="87" segment_no="7" tag_type="text">4) RADAR-based: Regarding RADAR-based place recog-<a href="deeplearning_paper41.html#16">[144].</a></text>
<text top="406" left="312" width="251" height="9" font="font5" id="p9_t89" reading_order_no="88" segment_no="7" tag_type="text">nition, Saftescu et al. [95] also propose a NetVLAD-based</text>
<text top="418" left="312" width="251" height="9" font="font5" id="p9_t90" reading_order_no="89" segment_no="7" tag_type="text">approach to map FMCW RADAR scans to a descriptor space.</text>
<text top="430" left="312" width="251" height="9" font="font5" id="p9_t91" reading_order_no="90" segment_no="7" tag_type="text">Features are extracted using a specially tailored CNN based on<a href="deeplearning_paper41.html#15">[95] </a>also propose a NetVLAD-based</text>
<text top="442" left="312" width="251" height="9" font="font5" id="p9_t92" reading_order_no="91" segment_no="7" tag_type="text">cylindrical convolutions, anti-aliasing blurring, and azimuth-</text>
<text top="454" left="312" width="251" height="9" font="font5" id="p9_t93" reading_order_no="92" segment_no="7" tag_type="text">wise max-pooling to bolster the rotational invariance of polar</text>
<text top="466" left="312" width="251" height="9" font="font5" id="p9_t94" reading_order_no="93" segment_no="7" tag_type="text">radar images. Regarding training, the network uses a triplet</text>
<text top="477" left="312" width="143" height="9" font="font5" id="p9_t95" reading_order_no="94" segment_no="7" tag_type="text">loss function as proposed in [151].</text>
<text top="506" left="350" width="175" height="9" font="font5" id="p9_t96" reading_order_no="95" segment_no="9" tag_type="title">V. U NSUPERVISED P LACE R ECOGNITION</text>
<text top="524" left="322" width="241" height="9" font="font5" id="p9_t97" reading_order_no="96" segment_no="10" tag_type="text">The aforementioned supervised learning approaches achieve<a href="deeplearning_paper41.html#17">[151].</a></text>
<text top="536" left="312" width="251" height="9" font="font5" id="p9_t98" reading_order_no="97" segment_no="10" tag_type="text">excellent results in learning discriminative place models. How-</text>
<text top="547" left="312" width="251" height="9" font="font5" id="p9_t99" reading_order_no="98" segment_no="10" tag_type="text">ever, these methods have the inconvenience of requiring a vast</text>
<text top="559" left="312" width="251" height="9" font="font5" id="p9_t100" reading_order_no="99" segment_no="10" tag_type="text">amount of labeled data to perform well, as it is common in</text>
<text top="571" left="312" width="251" height="9" font="font5" id="p9_t101" reading_order_no="100" segment_no="10" tag_type="text">supervised DL-based approaches. Contrary to supervised, un-</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p9_t102" reading_order_no="101" segment_no="10" tag_type="text">supervised learning does not require labeled data, an advantage</text>
<text top="595" left="312" width="197" height="9" font="font5" id="p9_t103" reading_order_no="102" segment_no="10" tag_type="text">when annotated data are not available or scarce.</text>
<text top="608" left="322" width="241" height="9" font="font5" id="p9_t104" reading_order_no="103" segment_no="12" tag_type="text">Place recognition works use unsupervised approaches such</text>
<text top="620" left="312" width="251" height="9" font="font5" id="p9_t105" reading_order_no="104" segment_no="12" tag_type="text">as Generative Adversarial Networks (GAN) for domain trans-</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p9_t106" reading_order_no="105" segment_no="12" tag_type="text">lation [152]. An example of such an approach is proposed</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p9_t107" reading_order_no="106" segment_no="12" tag_type="text">by Latif et al. [152], which address the cross-season place</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p9_t108" reading_order_no="107" segment_no="12" tag_type="text">recognition problem as a domain translation task. GANS are</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p9_t109" reading_order_no="108" segment_no="12" tag_type="text">used to learn the relationship between two domains without</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p9_t110" reading_order_no="109" segment_no="12" tag_type="text">requiring cross-domain image correspondences. The proposed</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p9_t111" reading_order_no="110" segment_no="12" tag_type="text">architecture is presented as two coupled GANs. The generator</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p9_t112" reading_order_no="111" segment_no="12" tag_type="text">integrated an encoder-decoder network, while the discrimi-</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p9_t113" reading_order_no="112" segment_no="12" tag_type="text">nator integrates an encoder network followed by two fully<a href="deeplearning_paper41.html#17">[152]. </a>An example of such an approach is proposed</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p9_t114" reading_order_no="113" segment_no="12" tag_type="text">connected layers. The output of the discriminator is used<a href="deeplearning_paper41.html#17">[152], </a>which address the cross-season place</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p9_t115" reading_order_no="114" segment_no="12" tag_type="text">as a descriptor for place recognition. Authors show that the</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p10_t1" reading_order_no="0" segment_no="0" tag_type="text">10</text>
<text top="58" left="49" width="251" height="9" font="font5" id="p10_t2" reading_order_no="1" segment_no="1" tag_type="text">discriminator’s feature space is more informative than image</text>
<text top="70" left="49" width="155" height="9" font="font5" id="p10_t3" reading_order_no="2" segment_no="1" tag_type="text">pixels translated to the target domain.</text>
<text top="82" left="59" width="241" height="9" font="font5" id="p10_t4" reading_order_no="3" segment_no="3" tag_type="text">Yin et al. [153] also proposes a GAN-based approach,<a href="deeplearning_paper41.html#17">[153] </a>also proposes a GAN-based approach,</text>
<text top="94" left="49" width="251" height="9" font="font5" id="p10_t5" reading_order_no="4" segment_no="3" tag_type="text">but for 3D LiDAR-based. LiDAR data are first mapped</text>
<text top="106" left="49" width="251" height="9" font="font5" id="p10_t6" reading_order_no="5" segment_no="3" tag_type="text">into dynamic octree maps, from which bird-view images are</text>
<text top="118" left="49" width="251" height="9" font="font5" id="p10_t7" reading_order_no="6" segment_no="3" tag_type="text">extracted. These images are used in a GAN-based pipeline</text>
<text top="130" left="49" width="251" height="9" font="font5" id="p10_t8" reading_order_no="7" segment_no="3" tag_type="text">to learn stable and generalized place features. The network</text>
<text top="142" left="49" width="251" height="9" font="font5" id="p10_t9" reading_order_no="8" segment_no="3" tag_type="text">trained using adversarial and conditional entropy strategies to</text>
<text top="154" left="49" width="251" height="9" font="font5" id="p10_t10" reading_order_no="9" segment_no="3" tag_type="text">produce a higher generalization ability and capture the unique</text>
<text top="166" left="49" width="251" height="9" font="font5" id="p10_t11" reading_order_no="10" segment_no="3" tag_type="text">mapping between the original data space and the compressed</text>
<text top="178" left="49" width="73" height="9" font="font5" id="p10_t12" reading_order_no="11" segment_no="3" tag_type="text">latent code space.</text>
<text top="190" left="59" width="241" height="9" font="font5" id="p10_t13" reading_order_no="12" segment_no="4" tag_type="text">Han et al.,(2020) [88] propose a Multispectral Domain<a href="deeplearning_paper41.html#15">[88] </a>propose a Multispectral Domain</text>
<text top="202" left="49" width="251" height="9" font="font5" id="p10_t14" reading_order_no="13" segment_no="4" tag_type="text">Invariant framework for the translation between unpaired RGB</text>
<text top="214" left="49" width="251" height="9" font="font5" id="p10_t15" reading_order_no="14" segment_no="4" tag_type="text">and thermal imagery. The proposed approach is based on</text>
<text top="226" left="49" width="251" height="9" font="font5" id="p10_t16" reading_order_no="15" segment_no="4" tag_type="text">CycleGAN [154], which relies, for training, on the single<a href="deeplearning_paper41.html#17">[154], </a>which relies, for training, on the single</text>
<text top="238" left="49" width="251" height="9" font="font5" id="p10_t17" reading_order_no="16" segment_no="4" tag_type="text">scale structural similarity index (SSIM [155]) loss, triplet loss,<a href="deeplearning_paper41.html#17">[155]) </a>loss, triplet loss,</text>
<text top="250" left="49" width="251" height="9" font="font5" id="p10_t18" reading_order_no="17" segment_no="4" tag_type="text">adversarial loss, and two types of consistency losses (cyclic</text>
<text top="262" left="49" width="251" height="9" font="font5" id="p10_t19" reading_order_no="18" segment_no="4" tag_type="text">loss [154] and pixel-wise loss). The proposed framework<a href="deeplearning_paper41.html#17">[154] </a>and pixel-wise loss). The proposed framework</text>
<text top="274" left="49" width="251" height="9" font="font5" id="p10_t20" reading_order_no="19" segment_no="4" tag_type="text">is further validated on semantic segmentation and domain</text>
<text top="286" left="49" width="67" height="9" font="font5" id="p10_t21" reading_order_no="20" segment_no="4" tag_type="text">adaptation tasks.</text>
<text top="299" left="59" width="241" height="9" font="font5" id="p10_t22" reading_order_no="21" segment_no="6" tag_type="text">Contrary to the former works, which were mainly based on</text>
<text top="310" left="49" width="251" height="9" font="font5" id="p10_t23" reading_order_no="22" segment_no="6" tag_type="text">GAN approaches, Merril and Huang [94] propose, for visual<a href="deeplearning_paper41.html#15">[94] </a>propose, for visual</text>
<text top="322" left="49" width="251" height="9" font="font5" id="p10_t24" reading_order_no="23" segment_no="6" tag_type="text">loop closure, an autoencoder-based approach to handle the</text>
<text top="334" left="49" width="251" height="9" font="font5" id="p10_t25" reading_order_no="24" segment_no="6" tag_type="text">feature embedding. Instead of reconstructing original images,</text>
<text top="346" left="49" width="251" height="9" font="font5" id="p10_t26" reading_order_no="25" segment_no="6" tag_type="text">this unsupervised approach is specifically tailored to map</text>
<text top="358" left="49" width="251" height="9" font="font5" id="p10_t27" reading_order_no="26" segment_no="6" tag_type="text">images to a HOG descriptor space. The autoencoder network</text>
<text top="370" left="49" width="251" height="9" font="font5" id="p10_t28" reading_order_no="27" segment_no="6" tag_type="text">is trained, having as input a pre-processing stage, where two</text>
<text top="382" left="49" width="251" height="9" font="font5" id="p10_t29" reading_order_no="28" segment_no="6" tag_type="text">classical geometric vision techniques are exploited: histogram</text>
<text top="394" left="49" width="251" height="9" font="font5" id="p10_t30" reading_order_no="29" segment_no="6" tag_type="text">of oriented gradients (HOG) [156], and the projective trans-<a href="deeplearning_paper41.html#17">[156], </a>and the projective trans-</text>
<text top="406" left="49" width="251" height="9" font="font5" id="p10_t31" reading_order_no="30" segment_no="6" tag_type="text">formation (homography) [157]. HOG enables the compression<a href="deeplearning_paper41.html#17">[157]. </a>HOG enables the compression</text>
<text top="418" left="49" width="251" height="9" font="font5" id="p10_t32" reading_order_no="31" segment_no="6" tag_type="text">of images while preserving salient features. On the other</text>
<text top="430" left="49" width="251" height="9" font="font5" id="p10_t33" reading_order_no="32" segment_no="6" tag_type="text">hand, the projective transformation allows the relation of</text>
<text top="442" left="49" width="251" height="9" font="font5" id="p10_t34" reading_order_no="33" segment_no="6" tag_type="text">images with differing viewpoints. The network has a minimal</text>
<text top="454" left="49" width="251" height="9" font="font5" id="p10_t35" reading_order_no="34" segment_no="6" tag_type="text">architecture, enabling fast and reliable close-loop detection in</text>
<text top="466" left="49" width="177" height="9" font="font5" id="p10_t36" reading_order_no="35" segment_no="6" tag_type="text">real-time with no dimensionality reduction.</text>
<text top="496" left="78" width="193" height="9" font="font5" id="p10_t37" reading_order_no="36" segment_no="9" tag_type="title">VI. S EMI - SUPERVISED P LACE R ECOGNITION</text>
<text top="515" left="59" width="241" height="9" font="font5" id="p10_t38" reading_order_no="37" segment_no="10" tag_type="text">In this work, Semi-supervised approaches refer to works</text>
<text top="527" left="49" width="251" height="9" font="font5" id="p10_t39" reading_order_no="38" segment_no="10" tag_type="text">that jointly rely on supervised and unsupervised methods. The</text>
<text top="539" left="49" width="251" height="9" font="font5" id="p10_t40" reading_order_no="39" segment_no="10" tag_type="text">combination of these two learning approaches is particularly</text>
<text top="551" left="49" width="251" height="9" font="font5" id="p10_t41" reading_order_no="40" segment_no="10" tag_type="text">used for the cross-domain problem. However, rather than</text>
<text top="562" left="49" width="251" height="9" font="font5" id="p10_t42" reading_order_no="41" segment_no="10" tag_type="text">translating one domain to another, these learning techniques</text>
<text top="574" left="49" width="251" height="9" font="font5" id="p10_t43" reading_order_no="42" segment_no="10" tag_type="text">are used to learn features that are independent of the domain</text>
<text top="586" left="49" width="251" height="9" font="font5" id="p10_t44" reading_order_no="43" segment_no="10" tag_type="text">appearance. A summary of recent works is presented in Table</text>
<text top="598" left="49" width="10" height="9" font="font5" id="p10_t45" reading_order_no="44" segment_no="10" tag_type="text">V.</text>
<text top="611" left="59" width="241" height="9" font="font5" id="p10_t46" reading_order_no="45" segment_no="13" tag_type="text">To learn domain-invariant features for cross-domain visual</text>
<text top="623" left="49" width="251" height="9" font="font5" id="p10_t47" reading_order_no="46" segment_no="13" tag_type="text">place recognition, Wang et al. [160] propose an approach that</text>
<text top="635" left="49" width="251" height="9" font="font5" id="p10_t48" reading_order_no="47" segment_no="13" tag_type="text">combines weakly supervised learning with unsupervised learn-</text>
<text top="647" left="49" width="251" height="9" font="font5" id="p10_t49" reading_order_no="48" segment_no="13" tag_type="text">ing. The proposed architecture has three primary modules: an</text>
<text top="659" left="49" width="251" height="9" font="font5" id="p10_t50" reading_order_no="49" segment_no="13" tag_type="text">attention module, an attention-aware VLAD module, and a</text>
<text top="671" left="49" width="251" height="9" font="font5" id="p10_t51" reading_order_no="50" segment_no="13" tag_type="text">domain adaptation module. The supervised branch is trained</text>
<text top="683" left="49" width="251" height="9" font="font5" id="p10_t52" reading_order_no="51" segment_no="13" tag_type="text">with a triplet ranking loss function, while the unsupervised<a href="deeplearning_paper41.html#11">V.</a></text>
<text top="695" left="49" width="251" height="9" font="font5" id="p10_t53" reading_order_no="52" segment_no="13" tag_type="text">branch resorts to a multi-kernel maximum mean discrepancy</text>
<text top="707" left="49" width="109" height="9" font="font5" id="p10_t54" reading_order_no="53" segment_no="13" tag_type="text">(MK-MMD) loss function.<a href="deeplearning_paper41.html#17">[160] </a>propose an approach that</text>
<text top="729" left="57" width="90" height="9" font="font6" id="p10_t55" reading_order_no="108" segment_no="15" tag_type="footnote">1 https://www.mapillary.com</text>
<text top="739" left="57" width="138" height="8" font="font6" id="p10_t56" reading_order_no="109" segment_no="16" tag_type="footnote">2 https://beeldbank.amsterdam.nl/beeldbank</text>
<text top="58" left="322" width="241" height="9" font="font5" id="p10_t57" reading_order_no="54" segment_no="2" tag_type="text">On the other hand, Tang et al. [162] propose a self-</text>
<text top="70" left="312" width="251" height="9" font="font5" id="p10_t58" reading_order_no="55" segment_no="2" tag_type="text">supervised learning approach to disentangle place-rated fea-</text>
<text top="82" left="312" width="251" height="9" font="font5" id="p10_t59" reading_order_no="56" segment_no="2" tag_type="text">tures from domain-related features. The backbone architecture</text>
<text top="94" left="312" width="251" height="9" font="font5" id="p10_t60" reading_order_no="57" segment_no="2" tag_type="text">of the proposed approach is a modified autoencoder for</text>
<text top="106" left="312" width="251" height="9" font="font5" id="p10_t61" reading_order_no="58" segment_no="2" tag_type="text">adversarial learning: i.e., two input encoder branches con-</text>
<text top="117" left="312" width="251" height="9" font="font5" id="p10_t62" reading_order_no="59" segment_no="2" tag_type="text">verging into one output decoder. The disentanglement of the</text>
<text top="129" left="312" width="251" height="9" font="font5" id="p10_t63" reading_order_no="60" segment_no="2" tag_type="text">two feature domains is solved through adversarial learning,</text>
<text top="141" left="312" width="251" height="9" font="font5" id="p10_t64" reading_order_no="61" segment_no="2" tag_type="text">which constrains the learning of domain specific features (i.e.,</text>
<text top="153" left="312" width="251" height="9" font="font5" id="p10_t65" reading_order_no="62" segment_no="2" tag_type="text">features depending on the appearance); a task that is not</text>
<text top="165" left="312" width="251" height="9" font="font5" id="p10_t66" reading_order_no="63" segment_no="2" tag_type="text">guaranteed by the reconstruction loss of autoencoders. For<a href="deeplearning_paper41.html#17">[162] </a>propose a self-</text>
<text top="177" left="312" width="251" height="9" font="font5" id="p10_t67" reading_order_no="64" segment_no="2" tag_type="text">adversarial learning, the proposed loss function is the least</text>
<text top="189" left="312" width="251" height="9" font="font5" id="p10_t68" reading_order_no="65" segment_no="2" tag_type="text">square adversarial loss [163]; while for reconstruction, the loss</text>
<text top="201" left="312" width="112" height="9" font="font5" id="p10_t69" reading_order_no="66" segment_no="2" tag_type="text">function is the L2 distance.</text>
<text top="214" left="322" width="241" height="9" font="font5" id="p10_t70" reading_order_no="67" segment_no="5" tag_type="text">Dub´e et al. [77] propose SegMap, an data-driven learn-</text>
<text top="225" left="312" width="251" height="9" font="font5" id="p10_t71" reading_order_no="68" segment_no="5" tag_type="text">ing approach for the task of localization and mapping. The</text>
<text top="237" left="312" width="251" height="9" font="font5" id="p10_t72" reading_order_no="69" segment_no="5" tag_type="text">approach uses as the main framework an autoencoder-like</text>
<text top="249" left="312" width="251" height="9" font="font5" id="p10_t73" reading_order_no="70" segment_no="5" tag_type="text">architecture to learn object segments of 3D point clouds.</text>
<text top="261" left="312" width="251" height="9" font="font5" id="p10_t74" reading_order_no="71" segment_no="5" tag_type="text">The framework is used for two tasks: (supervised) classifi-</text>
<text top="273" left="312" width="251" height="9" font="font5" id="p10_t75" reading_order_no="72" segment_no="5" tag_type="text">cation and (unsupervised) reconstruction. The work proposes</text>
<text top="285" left="312" width="251" height="9" font="font5" id="p10_t76" reading_order_no="73" segment_no="5" tag_type="text">a customized learning technique to train the network, which</text>
<text top="297" left="312" width="251" height="9" font="font5" id="p10_t77" reading_order_no="74" segment_no="5" tag_type="text">comprises, for classification, the softmax cross-entropy loss<a href="deeplearning_paper41.html#17">[163]; </a>while for reconstruction, the loss</text>
<text top="309" left="312" width="251" height="9" font="font5" id="p10_t78" reading_order_no="75" segment_no="5" tag_type="text">function in conjunction with the N-ways classification problem</text>
<text top="321" left="312" width="251" height="9" font="font5" id="p10_t79" reading_order_no="76" segment_no="5" tag_type="text">learning technique [164], and, for reconstruction, the binary<a href="deeplearning_paper41.html#15">[77] </a>propose SegMap, an data-driven learn-</text>
<text top="333" left="312" width="251" height="9" font="font5" id="p10_t80" reading_order_no="77" segment_no="5" tag_type="text">cross-entropy loss function [165]. The latent space, which</text>
<text top="345" left="312" width="251" height="9" font="font5" id="p10_t81" reading_order_no="78" segment_no="5" tag_type="text">is jointly learned on the two tasks, is used as a descriptor</text>
<text top="357" left="312" width="251" height="9" font="font5" id="p10_t82" reading_order_no="79" segment_no="5" tag_type="text">for segment retrieval. The proposed framework can be used</text>
<text top="369" left="312" width="251" height="9" font="font5" id="p10_t83" reading_order_no="80" segment_no="5" tag_type="text">in global localization, 3D dense map reconstruction, and</text>
<text top="381" left="312" width="155" height="9" font="font5" id="p10_t84" reading_order_no="81" segment_no="5" tag_type="text">semantic information extraction tasks.</text>
<text top="410" left="379" width="118" height="9" font="font5" id="p10_t85" reading_order_no="82" segment_no="7" tag_type="title">VII. O THER F RAMEWORKS</text>
<text top="428" left="322" width="241" height="9" font="font5" id="p10_t86" reading_order_no="83" segment_no="8" tag_type="text">This section is dedicated to the frameworks that have more</text>
<text top="440" left="312" width="251" height="9" font="font5" id="p10_t87" reading_order_no="84" segment_no="8" tag_type="text">complex and entangled architectures: i.e., containing more</text>
<text top="452" left="312" width="251" height="9" font="font5" id="p10_t88" reading_order_no="85" segment_no="8" tag_type="text">than one place recognition approach for the purpose of finding<a href="deeplearning_paper41.html#17">[164], </a>and, for reconstruction, the binary</text>
<text top="464" left="312" width="251" height="9" font="font5" id="p10_t89" reading_order_no="86" segment_no="8" tag_type="text">the best loop candidates. Two main frameworks are high-<a href="deeplearning_paper41.html#17">[165]. </a>The latent space, which</text>
<text top="476" left="312" width="251" height="9" font="font5" id="p10_t90" reading_order_no="87" segment_no="8" tag_type="text">lighted: parallel and hierarchical. While parallel frameworks</text>
<text top="487" left="312" width="251" height="9" font="font5" id="p10_t91" reading_order_no="88" segment_no="8" tag_type="text">have a very defined structure, hierarchical frameworks may</text>
<text top="499" left="312" width="251" height="9" font="font5" id="p10_t92" reading_order_no="89" segment_no="8" tag_type="text">assume very complex and entangled configurations, but both</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p10_t93" reading_order_no="90" segment_no="8" tag_type="text">frameworks have the end goal of representing more performant</text>
<text top="523" left="312" width="110" height="9" font="font5" id="p10_t94" reading_order_no="91" segment_no="8" tag_type="text">place recognition methods.</text>
<text top="555" left="312" width="98" height="9" font="font11" id="p10_t95" reading_order_no="92" segment_no="11" tag_type="title">A. Parallel Frameworks</text>
<text top="571" left="322" width="241" height="9" font="font5" id="p10_t96" reading_order_no="93" segment_no="12" tag_type="text">Parallel frameworks refer to approaches that rely on multiple</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p10_t97" reading_order_no="94" segment_no="12" tag_type="text">information streams, which are fused into one branch to gen-</text>
<text top="595" left="312" width="251" height="9" font="font5" id="p10_t98" reading_order_no="95" segment_no="12" tag_type="text">erate place recognition decisions. These parallel architectures</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p10_t99" reading_order_no="96" segment_no="12" tag_type="text">fuse the various branches utilizing methods such as feature</text>
<text top="619" left="312" width="251" height="9" font="font5" id="p10_t100" reading_order_no="97" segment_no="12" tag_type="text">concatenation [44], HMM [166] or multiplying normalized</text>
<text top="631" left="312" width="251" height="9" font="font5" id="p10_t101" reading_order_no="98" segment_no="12" tag_type="text">data across Gaussian-distributed clusters [167]. Approaches</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p10_t102" reading_order_no="99" segment_no="12" tag_type="text">such as proposed by Oertel et al. [25], where vision and struc-</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p10_t103" reading_order_no="100" segment_no="12" tag_type="text">tural data are fused in an end-to-end fashion, are considered to</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p10_t104" reading_order_no="101" segment_no="12" tag_type="text">belong to the Section IV-B because features are jointly learned</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p10_t105" reading_order_no="102" segment_no="12" tag_type="text">in an end-to-end pipeline. An example of parallel frameworks</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p10_t106" reading_order_no="103" segment_no="12" tag_type="text">is illustrated in Fig. 7, and a summary of recent works is</text>
<text top="703" left="312" width="91" height="9" font="font5" id="p10_t107" reading_order_no="104" segment_no="12" tag_type="text">presented in Table VI.</text>
<text top="715" left="322" width="241" height="9" font="font5" id="p10_t108" reading_order_no="105" segment_no="14" tag_type="text">Relying on multiple information streams allows overcoming</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p10_t109" reading_order_no="106" segment_no="14" tag_type="text">individual sensory data limitations, which can be, for instance,</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p10_t110" reading_order_no="107" segment_no="14" tag_type="text">due to environment changing conditions. Zhang et al. [44]</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font29" size="5" family="NimbusRomNo9L-Regu" color="#000000"/>
<text top="26" left="556" width="7" height="6" font="font0" id="p11_t1" reading_order_no="0" segment_no="0" tag_type="text">11</text>
<text top="59" left="288" width="36" height="7" font="font6" id="p11_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE IV</text>
<text top="68" left="105" width="402" height="7" font="font6" id="p11_t3" reading_order_no="2" segment_no="2" tag_type="text">S UMMARY OF RECENT WORKS USING UNSUPERVISED END - TO - END LEARNING TECHNIQUES FOR PLACE RECOGNITION .</text>
<text top="94" left="58" width="20" height="6" font="font20" id="p11_t4" reading_order_no="3" segment_no="3" tag_type="table">Sensor</text>
<text top="94" left="96" width="10" height="6" font="font20" id="p11_t5" reading_order_no="4" segment_no="3" tag_type="table">Ref</text>
<text top="94" left="169" width="38" height="6" font="font20" id="p11_t6" reading_order_no="5" segment_no="3" tag_type="table">Architecture</text>
<text top="94" left="290" width="43" height="6" font="font20" id="p11_t7" reading_order_no="6" segment_no="3" tag_type="table">Loss Function</text>
<text top="94" left="405" width="14" height="6" font="font20" id="p11_t8" reading_order_no="7" segment_no="3" tag_type="table">Task</text>
<text top="94" left="501" width="23" height="6" font="font20" id="p11_t9" reading_order_no="8" segment_no="3" tag_type="table">Dataset</text>
<text top="117" left="54" width="22" height="6" font="font0" id="p11_t10" reading_order_no="9" segment_no="3" tag_type="table">Camera</text>
<text top="117" left="94" width="15" height="6" font="font0" id="p11_t11" reading_order_no="10" segment_no="3" tag_type="table">[152]</text>
<text top="117" left="120" width="135" height="6" font="font0" id="p11_t12" reading_order_no="11" segment_no="3" tag_type="table">Architecture: Coupled GANs + encoder-decoder</text>
<text top="125" left="120" width="23" height="6" font="font0" id="p11_t13" reading_order_no="12" segment_no="3" tag_type="table">network</text>
<text top="117" left="266" width="90" height="6" font="font0" id="p11_t14" reading_order_no="13" segment_no="3" tag_type="table">Minimization of the cyclic re-</text>
<text top="125" left="266" width="66" height="6" font="font0" id="p11_t15" reading_order_no="14" segment_no="3" tag_type="table">construction loss [158]</text>
<text top="117" left="367" width="90" height="6" font="font0" id="p11_t16" reading_order_no="15" segment_no="3" tag_type="table">Domain translation for cross</text>
<text top="125" left="367" width="73" height="6" font="font0" id="p11_t17" reading_order_no="16" segment_no="3" tag_type="table">domain place recognition<a href="deeplearning_paper41.html#17">[152]</a></text>
<text top="117" left="468" width="35" height="6" font="font0" id="p11_t18" reading_order_no="17" segment_no="3" tag_type="table">Norland [2];</text>
<text top="141" left="54" width="22" height="6" font="font0" id="p11_t19" reading_order_no="18" segment_no="3" tag_type="table">Camera</text>
<text top="141" left="94" width="12" height="6" font="font0" id="p11_t20" reading_order_no="19" segment_no="3" tag_type="table">[94]</text>
<text top="141" left="120" width="135" height="6" font="font0" id="p11_t21" reading_order_no="20" segment_no="3" tag_type="table">Pre-processing: HOG [156] and homography<a href="deeplearning_paper41.html#17">[158]</a></text>
<text top="148" left="120" width="15" height="6" font="font0" id="p11_t22" reading_order_no="21" segment_no="3" tag_type="table">[157]</text>
<text top="156" left="120" width="93" height="6" font="font0" id="p11_t23" reading_order_no="22" segment_no="3" tag_type="table">Architecture: small Autoencoder</text>
<text top="141" left="266" width="47" height="6" font="font0" id="p11_t24" reading_order_no="23" segment_no="3" tag_type="table">L2 loss function<a href="deeplearning_paper41.html#13">[2];</a></text>
<text top="141" left="367" width="90" height="6" font="font0" id="p11_t25" reading_order_no="24" segment_no="3" tag_type="table">Unsupervised feature embed-</text>
<text top="148" left="367" width="80" height="6" font="font0" id="p11_t26" reading_order_no="25" segment_no="3" tag_type="table">ding for visual loop closure<a href="deeplearning_paper41.html#15">[94]</a></text>
<text top="141" left="468" width="41" height="6" font="font0" id="p11_t27" reading_order_no="26" segment_no="3" tag_type="table">Places [110];<a href="deeplearning_paper41.html#17">[156] </a>and homography</text>
<text top="141" left="516" width="18" height="6" font="font0" id="p11_t28" reading_order_no="27" segment_no="3" tag_type="table">KITTI<a href="deeplearning_paper41.html#17">[157]</a></text>
<text top="141" left="541" width="17" height="6" font="font0" id="p11_t29" reading_order_no="28" segment_no="3" tag_type="table">[142];</text>
<text top="148" left="468" width="90" height="6" font="font0" id="p11_t30" reading_order_no="29" segment_no="3" tag_type="table">Alderley [42]; Norland [2]; Gar-</text>
<text top="156" left="468" width="46" height="6" font="font0" id="p11_t31" reading_order_no="30" segment_no="3" tag_type="table">dens Point [22];</text>
<text top="172" left="54" width="14" height="6" font="font0" id="p11_t32" reading_order_no="31" segment_no="3" tag_type="table">RGB</text>
<text top="172" left="77" width="4" height="6" font="font0" id="p11_t33" reading_order_no="32" segment_no="3" tag_type="table">+</text>
<text top="180" left="54" width="24" height="6" font="font0" id="p11_t34" reading_order_no="33" segment_no="3" tag_type="table">Thermal<a href="deeplearning_paper41.html#16">[110];</a></text>
<text top="172" left="94" width="12" height="6" font="font0" id="p11_t35" reading_order_no="34" segment_no="3" tag_type="table">[88]</text>
<text top="172" left="120" width="109" height="6" font="font0" id="p11_t36" reading_order_no="35" segment_no="3" tag_type="table">Multispectral Domain Invariant model<a href="deeplearning_paper41.html#16">[142];</a></text>
<text top="180" left="120" width="88" height="6" font="font0" id="p11_t37" reading_order_no="36" segment_no="3" tag_type="table">Architecture: CycleGAN [154]<a href="deeplearning_paper41.html#14">[42]; </a>Norland <a href="deeplearning_paper41.html#13">[2]; </a>Gar-</text>
<text top="172" left="266" width="90" height="6" font="font0" id="p11_t38" reading_order_no="37" segment_no="3" tag_type="table">SSIM [155] + triplet + adversar-<a href="deeplearning_paper41.html#14">[22];</a></text>
<text top="180" left="266" width="90" height="6" font="font0" id="p11_t39" reading_order_no="38" segment_no="3" tag_type="table">ial + cyclic loss [154] + pixel-</text>
<text top="188" left="266" width="26" height="6" font="font0" id="p11_t40" reading_order_no="39" segment_no="3" tag_type="table">wise loss</text>
<text top="172" left="367" width="90" height="6" font="font0" id="p11_t41" reading_order_no="40" segment_no="3" tag_type="table">Unsupervised multispectral im-</text>
<text top="180" left="367" width="61" height="6" font="font0" id="p11_t42" reading_order_no="41" segment_no="3" tag_type="table">agery translation task<a href="deeplearning_paper41.html#15">[88]</a></text>
<text top="172" left="468" width="40" height="6" font="font0" id="p11_t43" reading_order_no="42" segment_no="3" tag_type="table">KAIST [159];</text>
<text top="208" left="54" width="9" height="6" font="font0" id="p11_t44" reading_order_no="43" segment_no="3" tag_type="table">3D<a href="deeplearning_paper41.html#17">[154]</a></text>
<text top="216" left="54" width="21" height="6" font="font0" id="p11_t45" reading_order_no="44" segment_no="3" tag_type="table">LiDAR<a href="deeplearning_paper41.html#17">[155] </a>+ triplet + adversar-</text>
<text top="208" left="94" width="15" height="6" font="font0" id="p11_t46" reading_order_no="45" segment_no="3" tag_type="table">[152]<a href="deeplearning_paper41.html#17">[154] </a>+ pixel-</text>
<text top="208" left="120" width="135" height="6" font="font0" id="p11_t47" reading_order_no="46" segment_no="3" tag_type="table">Pre-processing: Mapping LiDAR to dynamic</text>
<text top="216" left="120" width="94" height="6" font="font0" id="p11_t48" reading_order_no="47" segment_no="3" tag_type="table">octree maps to bird-view images</text>
<text top="224" left="120" width="135" height="6" font="font0" id="p11_t49" reading_order_no="48" segment_no="3" tag_type="table">Architecture: GAN + encoder-decoder network</text>
<text top="208" left="266" width="90" height="6" font="font0" id="p11_t50" reading_order_no="49" segment_no="3" tag_type="table">Adversarial learning and condi-<a href="deeplearning_paper41.html#17">[159];</a></text>
<text top="216" left="266" width="40" height="6" font="font0" id="p11_t51" reading_order_no="50" segment_no="3" tag_type="table">tional entropy</text>
<text top="208" left="367" width="90" height="6" font="font0" id="p11_t52" reading_order_no="51" segment_no="3" tag_type="table">Unsupervised Feature learning</text>
<text top="216" left="367" width="90" height="6" font="font0" id="p11_t53" reading_order_no="52" segment_no="3" tag_type="table">for a 3D LiDAR-based place<a href="deeplearning_paper41.html#17">[152]</a></text>
<text top="224" left="367" width="46" height="6" font="font0" id="p11_t54" reading_order_no="53" segment_no="3" tag_type="table">recognition task</text>
<text top="208" left="468" width="38" height="6" font="font0" id="p11_t55" reading_order_no="54" segment_no="3" tag_type="table">KITTI [142];</text>
<text top="216" left="468" width="38" height="6" font="font0" id="p11_t56" reading_order_no="55" segment_no="3" tag_type="table">NCTL [141];</text>
<text top="252" left="289" width="33" height="7" font="font6" id="p11_t57" reading_order_no="56" segment_no="4" tag_type="title">TABLE V</text>
<text top="261" left="116" width="380" height="7" font="font6" id="p11_t58" reading_order_no="57" segment_no="5" tag_type="text">R ECENT WORKS THAT COMBINE SUPERVISED AND UNSUPERVISED LEARNING IN PLACE RECOGNITION SYSTEMS .</text>
<text top="287" left="58" width="20" height="6" font="font20" id="p11_t59" reading_order_no="58" segment_no="6" tag_type="table">Sensor</text>
<text top="287" left="96" width="10" height="6" font="font20" id="p11_t60" reading_order_no="59" segment_no="6" tag_type="table">Ref</text>
<text top="287" left="169" width="38" height="6" font="font20" id="p11_t61" reading_order_no="60" segment_no="6" tag_type="table">Architecture</text>
<text top="287" left="290" width="43" height="6" font="font20" id="p11_t62" reading_order_no="61" segment_no="6" tag_type="table">Loss Function<a href="deeplearning_paper41.html#16">[142];</a></text>
<text top="287" left="405" width="14" height="6" font="font20" id="p11_t63" reading_order_no="62" segment_no="6" tag_type="table">Task<a href="deeplearning_paper41.html#16">[141];</a></text>
<text top="287" left="501" width="23" height="6" font="font20" id="p11_t64" reading_order_no="63" segment_no="6" tag_type="table">Dataset</text>
<text top="341" left="59" width="0" height="6" font="font0" id="p11_t65" reading_order_no="64" segment_no="6" tag_type="table">Camera</text>
<text top="309" left="94" width="15" height="6" font="font0" id="p11_t66" reading_order_no="65" segment_no="6" tag_type="table">[160]</text>
<text top="309" left="120" width="126" height="6" font="font0" id="p11_t67" reading_order_no="66" segment_no="6" tag_type="table">Feature extraction: AlexNet cropped(conv5)</text>
<text top="317" left="120" width="112" height="6" font="font0" id="p11_t68" reading_order_no="67" segment_no="6" tag_type="table">Supervised: VLAD + attention module</text>
<text top="325" left="120" width="95" height="6" font="font0" id="p11_t69" reading_order_no="68" segment_no="6" tag_type="table">Unsupervised: domain adaptation</text>
<text top="309" left="266" width="78" height="6" font="font0" id="p11_t70" reading_order_no="69" segment_no="6" tag_type="table">Supervised: triplet ranking;</text>
<text top="317" left="266" width="90" height="6" font="font0" id="p11_t71" reading_order_no="70" segment_no="6" tag_type="table">Unsupervised: MK-MMD [161]</text>
<text top="309" left="367" width="87" height="6" font="font0" id="p11_t72" reading_order_no="71" segment_no="6" tag_type="table">Single and cross-domain VPR</text>
<text top="308" left="468" width="33" height="7" font="font0" id="p11_t73" reading_order_no="72" segment_no="6" tag_type="table">Mapillary 1</text>
<text top="316" left="468" width="35" height="7" font="font0" id="p11_t74" reading_order_no="73" segment_no="6" tag_type="table">Beeldbank 2</text>
<text top="341" left="94" width="15" height="6" font="font0" id="p11_t75" reading_order_no="74" segment_no="6" tag_type="table">[162]<a href="deeplearning_paper41.html#17">[160]</a></text>
<text top="341" left="120" width="91" height="6" font="font0" id="p11_t76" reading_order_no="75" segment_no="6" tag_type="table">Supervised: adversarial learning</text>
<text top="349" left="120" width="77" height="6" font="font0" id="p11_t77" reading_order_no="76" segment_no="6" tag_type="table">Unsupervised: autoencoder</text>
<text top="341" left="266" width="33" height="6" font="font0" id="p11_t78" reading_order_no="77" segment_no="6" tag_type="table">Adversarial</text>
<text top="341" left="306" width="27" height="6" font="font0" id="p11_t79" reading_order_no="78" segment_no="6" tag_type="table">Learning:</text>
<text top="341" left="341" width="15" height="6" font="font0" id="p11_t80" reading_order_no="79" segment_no="6" tag_type="table">Least<a href="deeplearning_paper41.html#17">[161]</a></text>
<text top="349" left="266" width="38" height="6" font="font0" id="p11_t81" reading_order_no="80" segment_no="6" tag_type="table">square [163];</text>
<text top="357" left="266" width="80" height="6" font="font0" id="p11_t82" reading_order_no="81" segment_no="6" tag_type="table">Reconstruction: L2 distance</text>
<text top="341" left="367" width="90" height="6" font="font0" id="p11_t83" reading_order_no="82" segment_no="6" tag_type="table">Disentanglement of place and<a href="deeplearning_paper41.html#1">1</a></text>
<text top="349" left="367" width="90" height="6" font="font0" id="p11_t84" reading_order_no="83" segment_no="6" tag_type="table">appearance features in a cross</text>
<text top="357" left="367" width="37" height="6" font="font0" id="p11_t85" reading_order_no="84" segment_no="6" tag_type="table">domain VPR<a href="deeplearning_paper41.html#1">2</a></text>
<text top="341" left="468" width="39" height="6" font="font0" id="p11_t86" reading_order_no="85" segment_no="6" tag_type="table">Nordland [2];<a href="deeplearning_paper41.html#17">[162]</a></text>
<text top="349" left="468" width="40" height="6" font="font0" id="p11_t87" reading_order_no="86" segment_no="6" tag_type="table">Alderley [42];</text>
<text top="377" left="54" width="9" height="6" font="font0" id="p11_t88" reading_order_no="87" segment_no="6" tag_type="table">3D</text>
<text top="385" left="54" width="21" height="6" font="font0" id="p11_t89" reading_order_no="88" segment_no="6" tag_type="table">LiDAR</text>
<text top="377" left="94" width="12" height="6" font="font0" id="p11_t90" reading_order_no="89" segment_no="6" tag_type="table">[77]</text>
<text top="377" left="120" width="135" height="6" font="font0" id="p11_t91" reading_order_no="90" segment_no="6" tag_type="table">Supervised: latent space + classification net-</text>
<text top="385" left="120" width="14" height="6" font="font0" id="p11_t92" reading_order_no="91" segment_no="6" tag_type="table">work<a href="deeplearning_paper41.html#17">[163];</a></text>
<text top="393" left="120" width="117" height="6" font="font0" id="p11_t93" reading_order_no="92" segment_no="6" tag_type="table">Unsupervised: Autoencoder-like network</text>
<text top="377" left="266" width="40" height="6" font="font0" id="p11_t94" reading_order_no="93" segment_no="6" tag_type="table">Classification:</text>
<text top="377" left="313" width="43" height="6" font="font0" id="p11_t95" reading_order_no="94" segment_no="6" tag_type="table">softmax cross</text>
<text top="385" left="266" width="39" height="6" font="font0" id="p11_t96" reading_order_no="95" segment_no="6" tag_type="table">entropy [164]</text>
<text top="393" left="266" width="44" height="6" font="font0" id="p11_t97" reading_order_no="96" segment_no="6" tag_type="table">Reconstruction:<a href="deeplearning_paper41.html#13">[2];</a></text>
<text top="393" left="317" width="18" height="6" font="font0" id="p11_t98" reading_order_no="97" segment_no="6" tag_type="table">binary<a href="deeplearning_paper41.html#14">[42];</a></text>
<text top="393" left="342" width="14" height="6" font="font0" id="p11_t99" reading_order_no="98" segment_no="6" tag_type="table">cross</text>
<text top="400" left="266" width="39" height="6" font="font0" id="p11_t100" reading_order_no="99" segment_no="6" tag_type="table">entropy [165]</text>
<text top="377" left="367" width="90" height="6" font="font0" id="p11_t101" reading_order_no="100" segment_no="6" tag_type="table">Global localization, 3D dense<a href="deeplearning_paper41.html#15">[77]</a></text>
<text top="385" left="367" width="90" height="6" font="font0" id="p11_t102" reading_order_no="101" segment_no="6" tag_type="table">map reconstruction, and seman-</text>
<text top="393" left="367" width="73" height="6" font="font0" id="p11_t103" reading_order_no="102" segment_no="6" tag_type="table">tic information extraction</text>
<text top="377" left="468" width="67" height="6" font="font0" id="p11_t104" reading_order_no="103" segment_no="6" tag_type="table">KITTI odometry [142];</text>
<text top="436" left="49" width="251" height="9" font="font5" id="p11_t105" reading_order_no="104" segment_no="7" tag_type="text">address the loop closure detection problem under strong per-</text>
<text top="448" left="49" width="251" height="9" font="font5" id="p11_t106" reading_order_no="105" segment_no="7" tag_type="text">ceptual aliasing and appearance variations, proposing Robust</text>
<text top="460" left="49" width="251" height="9" font="font5" id="p11_t107" reading_order_no="106" segment_no="7" tag_type="text">Multimodal Sequence-based (ROMS). ROMS concatenates</text>
<text top="472" left="49" width="251" height="9" font="font5" id="p11_t108" reading_order_no="107" segment_no="7" tag_type="text">LDB features [168], GIST features [169], CNN-based deep<a href="deeplearning_paper41.html#17">[164]</a></text>
<text top="484" left="49" width="251" height="9" font="font5" id="p11_t109" reading_order_no="108" segment_no="7" tag_type="text">features [170] and ORB local features [171] in a single vector.</text>
<text top="496" left="49" width="251" height="9" font="font5" id="p11_t110" reading_order_no="109" segment_no="7" tag_type="text">A similar (parallel) architecture is proposed by Hausler et</text>
<text top="508" left="49" width="251" height="9" font="font5" id="p11_t111" reading_order_no="110" segment_no="7" tag_type="text">al. [166], where an approach, called Multi-Process Fusion,</text>
<text top="520" left="49" width="251" height="9" font="font5" id="p11_t112" reading_order_no="111" segment_no="7" tag_type="text">fuses four image processing methods: SAD with patch nor-<a href="deeplearning_paper41.html#17">[165]</a></text>
<text top="532" left="49" width="251" height="9" font="font5" id="p11_t113" reading_order_no="112" segment_no="7" tag_type="text">malization [42], [172]; HOG [173], [174]; multiple spatial</text>
<text top="543" left="49" width="251" height="9" font="font5" id="p11_t114" reading_order_no="113" segment_no="7" tag_type="text">regions of CNN features [138], [175]; and spatial coordinates</text>
<text top="555" left="49" width="251" height="9" font="font5" id="p11_t115" reading_order_no="114" segment_no="7" tag_type="text">of maximum CNN activations [41]. However, instead of fusing</text>
<text top="567" left="49" width="251" height="9" font="font5" id="p11_t116" reading_order_no="115" segment_no="7" tag_type="text">all features to generate one descriptor as proposed in [44],<a href="deeplearning_paper41.html#16">[142];</a></text>
<text top="579" left="49" width="251" height="9" font="font5" id="p11_t117" reading_order_no="116" segment_no="7" tag_type="text">here, each feature stream is matched separately using cosine</text>
<text top="591" left="49" width="251" height="9" font="font5" id="p11_t118" reading_order_no="117" segment_no="7" tag_type="text">distance, and only the resulting similarity values are fused</text>
<text top="603" left="49" width="136" height="9" font="font5" id="p11_t119" reading_order_no="118" segment_no="7" tag_type="text">using the Hidden Markov model.</text>
<text top="629" left="49" width="117" height="9" font="font11" id="p11_t120" reading_order_no="119" segment_no="10" tag_type="title">B. Hierarchical Frameworks<a href="deeplearning_paper41.html#17">[168], </a>GIST features <a href="deeplearning_paper41.html#17">[169], </a>CNN-based deep</text>
<text top="643" left="59" width="241" height="9" font="font5" id="p11_t121" reading_order_no="120" segment_no="11" tag_type="text">In this work, hierarchical frameworks refer to place recog-<a href="deeplearning_paper41.html#17">[170] </a>and ORB local features <a href="deeplearning_paper41.html#17">[171] </a>in a single vector.</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p11_t122" reading_order_no="121" segment_no="11" tag_type="text">nition approaches that, similarly to parallel frameworks, rely</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p11_t123" reading_order_no="122" segment_no="11" tag_type="text">on multiple methods; however, instead of having as main<a href="deeplearning_paper41.html#17">[166], </a>where an approach, called Multi-Process Fusion,</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p11_t124" reading_order_no="123" segment_no="11" tag_type="text">framework a parallel architecture, the architecture is formed</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p11_t125" reading_order_no="124" segment_no="11" tag_type="text">by various stacked tiers. The hierarchical architectures find the<a href="deeplearning_paper41.html#14">[42], </a><a href="deeplearning_paper41.html#17">[172]; </a>HOG <a href="deeplearning_paper41.html#17">[173], [174]; </a>multiple spatial</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p11_t126" reading_order_no="125" segment_no="11" tag_type="text">best loop candidate by filter candidates progressively in each<a href="deeplearning_paper41.html#16">[138], </a><a href="deeplearning_paper41.html#17">[175]; </a>and spatial coordinates</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p11_t127" reading_order_no="126" segment_no="11" tag_type="text">tier. An example of such a framework is the coarse-to-fine<a href="deeplearning_paper41.html#14">[41]. </a>However, instead of fusing</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p11_t128" reading_order_no="127" segment_no="11" tag_type="text">architecture, which has a coarse and a fine tier. The coarse<a href="deeplearning_paper41.html#14">[44],</a></text>
<text top="739" left="49" width="251" height="9" font="font5" id="p11_t129" reading_order_no="128" segment_no="11" tag_type="text">tier is mostly dedicated to retrieving top candidates utilizing</text>
<text top="436" left="312" width="251" height="9" font="font5" id="p11_t130" reading_order_no="129" segment_no="8" tag_type="text">methods that rather are computer efficient than accurate. These</text>
<text top="448" left="312" width="251" height="9" font="font5" id="p11_t131" reading_order_no="130" segment_no="8" tag_type="text">top candidates are feed to the fine tier, which can use more</text>
<text top="460" left="312" width="251" height="9" font="font5" id="p11_t132" reading_order_no="131" segment_no="8" tag_type="text">computer demanding methods to find the best loop candidate.</text>
<text top="472" left="312" width="251" height="9" font="font5" id="p11_t133" reading_order_no="132" segment_no="8" tag_type="text">The coarse-to-fine architecture, while being the most common,</text>
<text top="484" left="312" width="251" height="9" font="font5" id="p11_t134" reading_order_no="133" segment_no="8" tag_type="text">is not the only. Other architectures exist, for example Fig.</text>
<text top="496" left="312" width="251" height="9" font="font5" id="p11_t135" reading_order_no="134" segment_no="8" tag_type="text">7 illustrates a framework proposed in [176] and Table VII</text>
<text top="508" left="312" width="151" height="9" font="font5" id="p11_t136" reading_order_no="135" segment_no="8" tag_type="text">presents a summary of recent works.</text>
<text top="522" left="322" width="241" height="9" font="font5" id="p11_t137" reading_order_no="136" segment_no="9" tag_type="text">Hausler and Milford [176] show that parallel fusion strate-</text>
<text top="534" left="312" width="251" height="9" font="font5" id="p11_t138" reading_order_no="137" segment_no="9" tag_type="text">gies have inferior performance compared with hierarchical</text>
<text top="546" left="312" width="251" height="9" font="font5" id="p11_t139" reading_order_no="138" segment_no="9" tag_type="text">approaches and therefore propose Hierarchical Multi-Process</text>
<text top="558" left="312" width="251" height="9" font="font5" id="p11_t140" reading_order_no="139" segment_no="9" tag_type="text">Fusion, which has a three-tier hierarchy. In the first tier, top</text>
<text top="570" left="312" width="251" height="9" font="font5" id="p11_t141" reading_order_no="140" segment_no="9" tag_type="text">candidates are retrieved from the database based on HybridNet</text>
<text top="582" left="312" width="251" height="9" font="font5" id="p11_t142" reading_order_no="141" segment_no="9" tag_type="text">[138] and Gist [177] features. In the second tier, from the</text>
<text top="593" left="312" width="251" height="9" font="font5" id="p11_t143" reading_order_no="142" segment_no="9" tag_type="text">top candidates of the previous tier, a more narrow selection is</text>
<text top="605" left="312" width="251" height="9" font="font5" id="p11_t144" reading_order_no="143" segment_no="9" tag_type="text">performed based on KAZE [178] and Only Look Once (OLO)</text>
<text top="617" left="312" width="251" height="9" font="font5" id="p11_t145" reading_order_no="144" segment_no="9" tag_type="text">[123] features. Finally, the best loop candidate is obtained</text>
<text top="629" left="312" width="251" height="9" font="font5" id="p11_t146" reading_order_no="145" segment_no="9" tag_type="text">in the third tier using NetVLAD [24] and HOG [18]. An</text>
<text top="641" left="312" width="213" height="9" font="font5" id="p11_t147" reading_order_no="146" segment_no="9" tag_type="text">illustration of this framework is presented in Fig. 7.<a href="deeplearning_paper41.html#12">7 </a>illustrates a framework proposed in <a href="deeplearning_paper41.html#17">[176] </a>and Table <a href="deeplearning_paper41.html#13">VII</a></text>
<text top="655" left="322" width="241" height="9" font="font5" id="p11_t148" reading_order_no="147" segment_no="12" tag_type="text">Garg et al. [41] also follow a similar framework, proposing</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p11_t149" reading_order_no="148" segment_no="12" tag_type="text">a hierarchical place recognition approach, called X-Lost. In<a href="deeplearning_paper41.html#17">[176] </a>show that parallel fusion strate-</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p11_t150" reading_order_no="149" segment_no="12" tag_type="text">the coarse tier, top candidates are found by matching the</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p11_t151" reading_order_no="150" segment_no="12" tag_type="text">Local Semantic Tensor (LoST) descriptor, which comprises</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p11_t152" reading_order_no="151" segment_no="12" tag_type="text">feature maps from the RefineNet [179] (a dense segmentation</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p11_t153" reading_order_no="152" segment_no="12" tag_type="text">network) and semantic label scores of the road, building, and</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p11_t154" reading_order_no="153" segment_no="12" tag_type="text">vegetation classes. The best match is found in the fine tier by<a href="deeplearning_paper41.html#16">[138] </a>and Gist <a href="deeplearning_paper41.html#17">[177] </a>features. In the second tier, from the</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p11_t155" reading_order_no="154" segment_no="12" tag_type="text">verifying the spatial layout of semantically salient keypoint</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font30" size="6" family="Arial,Bold" color="#e6e6e6"/>
	<fontspec id="font31" size="6" family="Arial,Bold" color="#000000"/>
	<fontspec id="font32" size="5" family="Arial,Bold" color="#000000"/>
	<fontspec id="font33" size="6" family="Georgia,Bold" color="#e6e6e6"/>
	<fontspec id="font34" size="6" family="NimbusRomNo9L-Medi" color="#000000"/>
<text top="26" left="556" width="7" height="6" font="font0" id="p12_t1" reading_order_no="0" segment_no="0" tag_type="text">12</text>
<text top="144" left="440" width="22" height="7" font="font30" id="p12_t2" reading_order_no="25" segment_no="1" tag_type="figure">Feature<b>Feature</b></text>
<text top="151" left="431" width="32" height="7" font="font30" id="p12_t3" reading_order_no="26" segment_no="1" tag_type="figure">Extractor 1<b>Extractor 1</b></text>
<text top="145" left="517" width="22" height="7" font="font30" id="p12_t4" reading_order_no="27" segment_no="1" tag_type="figure">Feature<b>Feature</b></text>
<text top="152" left="507" width="32" height="7" font="font30" id="p12_t5" reading_order_no="28" segment_no="1" tag_type="figure">Extractor 2<b>Extractor 2</b></text>
<text top="201" left="429" width="17" height="7" font="font31" id="p12_t6" reading_order_no="29" segment_no="1" tag_type="figure">Belief<b>Belief</b></text>
<text top="208" left="421" width="32" height="7" font="font31" id="p12_t7" reading_order_no="30" segment_no="1" tag_type="figure">Generation<b>Generation</b></text>
<text top="175" left="541" width="12" height="7" font="font31" id="p12_t8" reading_order_no="31" segment_no="1" tag_type="figure">Map<b>Map</b></text>
<text top="201" left="508" width="17" height="7" font="font31" id="p12_t9" reading_order_no="32" segment_no="1" tag_type="figure">Belief<b>Belief</b></text>
<text top="208" left="500" width="32" height="7" font="font31" id="p12_t10" reading_order_no="33" segment_no="1" tag_type="figure">Generation<b>Generation</b></text>
<text top="229" left="470" width="20" height="7" font="font31" id="p12_t11" reading_order_no="34" segment_no="1" tag_type="figure">Fusion<b>Fusion</b></text>
<text top="259" left="454" width="50" height="7" font="font31" id="p12_t12" reading_order_no="35" segment_no="1" tag_type="figure">Loop Candidates<b>Loop Candidates</b></text>
<text top="244" left="122" width="4" height="7" font="font31" id="p12_t13" reading_order_no="11" segment_no="1" tag_type="figure">+<b>+</b></text>
<text top="201" left="122" width="4" height="7" font="font31" id="p12_t14" reading_order_no="7" segment_no="1" tag_type="figure">U<b>U</b></text>
<text top="153" left="122" width="4" height="7" font="font31" id="p12_t15" reading_order_no="3" segment_no="1" tag_type="figure">U<b>U</b></text>
<text top="157" left="179" width="21" height="6" font="font32" id="p12_t16" reading_order_no="4" segment_no="1" tag_type="figure">1st Tier<b> 1st Tier </b></text>
<text top="205" left="179" width="23" height="6" font="font32" id="p12_t17" reading_order_no="8" segment_no="1" tag_type="figure">2nd Tier<b> 2nd Tier </b></text>
<text top="250" left="181" width="20" height="6" font="font32" id="p12_t18" reading_order_no="12" segment_no="1" tag_type="figure">3thTier<b> 3thTier </b></text>
<text top="138" left="70" width="34" height="7" font="font33" id="p12_t19" reading_order_no="1" segment_no="1" tag_type="figure">HybridNet<b>HybridNet</b></text>
<text top="183" left="74" width="18" height="7" font="font33" id="p12_t20" reading_order_no="5" segment_no="1" tag_type="figure">KAZE<b>KAZE</b></text>
<text top="182" left="142" width="48" height="7" font="font33" id="p12_t21" reading_order_no="6" segment_no="1" tag_type="figure">Only look Once<b>Only look Once</b></text>
<text top="230" left="152" width="28" height="7" font="font33" id="p12_t22" reading_order_no="10" segment_no="1" tag_type="figure">NetVLAD<b>NetVLAD</b></text>
<text top="230" left="76" width="16" height="7" font="font33" id="p12_t23" reading_order_no="9" segment_no="1" tag_type="figure">HOG<b>HOG</b></text>
<text top="138" left="160" width="13" height="7" font="font33" id="p12_t24" reading_order_no="2" segment_no="1" tag_type="figure">Gist<b>Gist</b></text>
<text top="261" left="100" width="50" height="7" font="font31" id="p12_t25" reading_order_no="13" segment_no="1" tag_type="figure">Loop Candidates<b>Loop Candidates</b></text>
<text top="145" left="268" width="22" height="7" font="font30" id="p12_t26" reading_order_no="15" segment_no="1" tag_type="figure">Feature<b>Feature</b></text>
<text top="152" left="258" width="32" height="7" font="font30" id="p12_t27" reading_order_no="16" segment_no="1" tag_type="figure">Extractor 1<b>Extractor 1</b></text>
<text top="184" left="293" width="20" height="7" font="font31" id="p12_t28" reading_order_no="19" segment_no="1" tag_type="figure">Fusion<b>Fusion</b></text>
<text top="146" left="344" width="22" height="7" font="font30" id="p12_t29" reading_order_no="17" segment_no="1" tag_type="figure">Feature<b>Feature</b></text>
<text top="153" left="334" width="32" height="7" font="font30" id="p12_t30" reading_order_no="18" segment_no="1" tag_type="figure">Extractor 2<b>Extractor 2</b></text>
<text top="221" left="294" width="17" height="7" font="font31" id="p12_t31" reading_order_no="20" segment_no="1" tag_type="figure">Belief<b>Belief</b></text>
<text top="228" left="287" width="32" height="7" font="font31" id="p12_t32" reading_order_no="21" segment_no="1" tag_type="figure">Generation<b>Generation</b></text>
<text top="224" left="345" width="12" height="7" font="font31" id="p12_t33" reading_order_no="22" segment_no="1" tag_type="figure">Map<b>Map</b></text>
<text top="260" left="277" width="50" height="7" font="font31" id="p12_t34" reading_order_no="23" segment_no="1" tag_type="figure">Loop Candidates<b>Loop Candidates</b></text>
<text top="274" left="122" width="7" height="8" font="font10" id="p12_t35" reading_order_no="14" segment_no="1" tag_type="figure">a)</text>
<text top="274" left="303" width="7" height="8" font="font10" id="p12_t36" reading_order_no="24" segment_no="1" tag_type="figure">b)</text>
<text top="274" left="479" width="6" height="8" font="font10" id="p12_t37" reading_order_no="36" segment_no="1" tag_type="figure">c)</text>
<text top="296" left="49" width="514" height="7" font="font6" id="p12_t38" reading_order_no="37" segment_no="2" tag_type="text">Fig. 7. Block diagram of a) hierarchical b) and c) parallel place recognition frameworks. The example in b) fuses the descriptors, while in c) the belief scores</text>
<text top="305" left="49" width="32" height="7" font="font6" id="p12_t39" reading_order_no="38" segment_no="2" tag_type="text">are fused.</text>
<text top="327" left="288" width="36" height="7" font="font6" id="p12_t40" reading_order_no="39" segment_no="3" tag_type="title">TABLE VI</text>
<text top="336" left="54" width="505" height="7" font="font6" id="p12_t41" reading_order_no="40" segment_no="4" tag_type="text">S UMMARY OF RECENT WORKS ON SUPERVISED PLACE RECOGNITION USING PARALLEL FRAMEWORKS . BG = B ELIEF G ENERATION AND PM = P LACE</text>
<text top="345" left="289" width="34" height="7" font="font6" id="p12_t42" reading_order_no="41" segment_no="4" tag_type="text">MAPPING .</text>
<text top="370" left="57" width="19" height="6" font="font34" id="p12_t43" reading_order_no="42" segment_no="5" tag_type="table">Sensor</text>
<text top="370" left="92" width="10" height="6" font="font34" id="p12_t44" reading_order_no="43" segment_no="5" tag_type="table">Ref</text>
<text top="370" left="199" width="18" height="6" font="font34" id="p12_t45" reading_order_no="44" segment_no="5" tag_type="table">Model</text>
<text top="370" left="343" width="19" height="6" font="font34" id="p12_t46" reading_order_no="45" segment_no="5" tag_type="table">Fusion</text>
<text top="370" left="424" width="21" height="6" font="font34" id="p12_t47" reading_order_no="46" segment_no="5" tag_type="table">BG/PM</text>
<text top="370" left="506" width="21" height="6" font="font34" id="p12_t48" reading_order_no="47" segment_no="5" tag_type="table">Dataset</text>
<text top="420" left="58" width="0" height="6" font="font16" id="p12_t49" reading_order_no="48" segment_no="5" tag_type="table">Camera</text>
<text top="391" left="91" width="11" height="6" font="font16" id="p12_t50" reading_order_no="49" segment_no="5" tag_type="table">[44]</text>
<text top="391" left="115" width="187" height="6" font="font16" id="p12_t51" reading_order_no="50" segment_no="5" tag_type="table">Feacture Extraction: LDB [168] + GIST [169] + CNN [170] + ORB</text>
<text top="398" left="115" width="14" height="6" font="font16" id="p12_t52" reading_order_no="51" segment_no="5" tag_type="table">[171]</text>
<text top="391" left="311" width="76" height="6" font="font16" id="p12_t53" reading_order_no="52" segment_no="5" tag_type="table">Concatenation of all features</text>
<text top="391" left="403" width="52" height="6" font="font16" id="p12_t54" reading_order_no="53" segment_no="5" tag_type="table">Sequence /Database</text>
<text top="391" left="475" width="83" height="6" font="font16" id="p12_t55" reading_order_no="54" segment_no="5" tag_type="table">St Lucia [109]; CMU-VL [107];</text>
<text top="398" left="475" width="36" height="6" font="font16" id="p12_t56" reading_order_no="55" segment_no="5" tag_type="table">Nordland [2];</text>
<text top="412" left="91" width="14" height="6" font="font16" id="p12_t57" reading_order_no="56" segment_no="5" tag_type="table">[166]</text>
<text top="412" left="115" width="187" height="6" font="font16" id="p12_t58" reading_order_no="57" segment_no="5" tag_type="table">Feacture Extraction: SAD [42], [172] + HOG [173], [174] + spatial<a href="deeplearning_paper41.html#14">[44]</a></text>
<text top="420" left="115" width="187" height="6" font="font16" id="p12_t59" reading_order_no="58" segment_no="5" tag_type="table">regions of HybridNet(Conv-5 layer) [138], [175] + spatial coordinates<a href="deeplearning_paper41.html#17">[168] </a>+ GIST <a href="deeplearning_paper41.html#17">[169] </a>+ CNN <a href="deeplearning_paper41.html#17">[170] </a>+ ORB</text>
<text top="427" left="115" width="145" height="6" font="font16" id="p12_t60" reading_order_no="59" segment_no="5" tag_type="table">of maximum activations HybridNet(Conv-5 layer) [41]<a href="deeplearning_paper41.html#17">[171]</a></text>
<text top="434" left="115" width="95" height="6" font="font16" id="p12_t61" reading_order_no="60" segment_no="5" tag_type="table">Descriptor: features + normalization</text>
<text top="412" left="311" width="83" height="6" font="font16" id="p12_t62" reading_order_no="61" segment_no="5" tag_type="table">Hidden Markov Model of the</text>
<text top="420" left="311" width="83" height="6" font="font16" id="p12_t63" reading_order_no="62" segment_no="5" tag_type="table">similarity distances of each fea-<a href="deeplearning_paper41.html#16">[109]; </a>CMU-VL <a href="deeplearning_paper41.html#16">[107];</a></text>
<text top="427" left="311" width="29" height="6" font="font16" id="p12_t64" reading_order_no="63" segment_no="5" tag_type="table">ture stream<a href="deeplearning_paper41.html#13">[2];</a></text>
<text top="412" left="403" width="24" height="6" font="font16" id="p12_t65" reading_order_no="64" segment_no="5" tag_type="table">Dynamic<a href="deeplearning_paper41.html#17">[166]</a></text>
<text top="412" left="442" width="24" height="6" font="font16" id="p12_t66" reading_order_no="66" segment_no="5" tag_type="table">sequence<a href="deeplearning_paper41.html#14">[42], </a><a href="deeplearning_paper41.html#17">[172] </a>+ HOG <a href="deeplearning_paper41.html#17">[173], [174] </a>+ spatial</text>
<text top="420" left="403" width="25" height="6" font="font16" id="p12_t67" reading_order_no="65" segment_no="5" tag_type="table">/Database<a href="deeplearning_paper41.html#16">[138], </a><a href="deeplearning_paper41.html#17">[175] </a>+ spatial coordinates</text>
<text top="412" left="475" width="38" height="6" font="font16" id="p12_t68" reading_order_no="67" segment_no="5" tag_type="table">St Lucia [109]<a href="deeplearning_paper41.html#14">[41]</a></text>
<text top="420" left="475" width="34" height="6" font="font16" id="p12_t69" reading_order_no="68" segment_no="5" tag_type="table">Nordland [2]</text>
<text top="427" left="475" width="62" height="6" font="font16" id="p12_t70" reading_order_no="69" segment_no="5" tag_type="table">Oxford RobotCar [102]</text>
<text top="469" left="49" width="69" height="9" font="font5" id="p12_t71" reading_order_no="70" segment_no="6" tag_type="text">correspondences.</text>
<text top="490" left="59" width="241" height="9" font="font5" id="p12_t72" reading_order_no="71" segment_no="9" tag_type="text">This semantic- and keypoint-based approach is further ex-</text>
<text top="502" left="49" width="251" height="9" font="font5" id="p12_t73" reading_order_no="72" segment_no="9" tag_type="text">ploited in [27], [63]. In [63], top candidates are obtained fusing</text>
<text top="514" left="49" width="251" height="9" font="font5" id="p12_t74" reading_order_no="73" segment_no="9" tag_type="text">NetVLAD [24] and LoST [41] descriptors in a coarse stage,</text>
<text top="526" left="49" width="251" height="9" font="font5" id="p12_t75" reading_order_no="74" segment_no="9" tag_type="text">while in [27], depth maps are computed from camera data</text>
<text top="538" left="49" width="251" height="9" font="font5" id="p12_t76" reading_order_no="75" segment_no="9" tag_type="text">in an intermediate stage to remove keypoints that are out of<a href="deeplearning_paper41.html#16">[109]</a></text>
<text top="550" left="49" width="25" height="9" font="font5" id="p12_t77" reading_order_no="76" segment_no="9" tag_type="text">range.<a href="deeplearning_paper41.html#13">[2]</a></text>
<text top="572" left="59" width="241" height="9" font="font5" id="p12_t78" reading_order_no="77" segment_no="11" tag_type="text">Contrary to the former approaches, where performance is<a href="deeplearning_paper41.html#15">[102]</a></text>
<text top="584" left="49" width="251" height="9" font="font5" id="p12_t79" reading_order_no="78" segment_no="11" tag_type="text">the primary goal, An et al. [180] address the efficiency problem</text>
<text top="596" left="49" width="251" height="9" font="font5" id="p12_t80" reading_order_no="79" segment_no="11" tag_type="text">of place recognition, proposing an approach based on an</text>
<text top="608" left="49" width="251" height="9" font="font5" id="p12_t81" reading_order_no="80" segment_no="11" tag_type="text">HNSW graph for efficient map management. HNSW graph<a href="deeplearning_paper41.html#14">[27], [63]. </a>In <a href="deeplearning_paper41.html#14">[63], </a>top candidates are obtained fusing</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p12_t82" reading_order_no="81" segment_no="11" tag_type="text">guarantees low map building and retrieval time. In the coarse<a href="deeplearning_paper41.html#14">[24] </a>and LoST <a href="deeplearning_paper41.html#14">[41] </a>descriptors in a coarse stage,</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p12_t83" reading_order_no="82" segment_no="11" tag_type="text">stage, top candidates are retrieved from the HNSW graph by<a href="deeplearning_paper41.html#14">[27], </a>depth maps are computed from camera data</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p12_t84" reading_order_no="83" segment_no="11" tag_type="text">matching features extracted from the MobileNetV2 [181] us-</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p12_t85" reading_order_no="84" segment_no="11" tag_type="text">ing normalized scalar product [182]. The final loop candidate</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p12_t86" reading_order_no="85" segment_no="11" tag_type="text">is obtained by matching hash codes from SURF features and</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p12_t87" reading_order_no="86" segment_no="11" tag_type="text">the top candidates retrieved in the coarse stage. On the other<a href="deeplearning_paper41.html#17">[180] </a>address the efficiency problem</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p12_t88" reading_order_no="87" segment_no="11" tag_type="text">hand, Liu et al. [52] exploits 3D point clouds instead of camera</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p12_t89" reading_order_no="88" segment_no="11" tag_type="text">data, proposing SeqLPD, which is a lightweight variant of</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p12_t90" reading_order_no="89" segment_no="11" tag_type="text">our LPD-Net [49]. This approach resorts to super keyframe</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p12_t91" reading_order_no="90" segment_no="11" tag_type="text">clusters for coarse search, while for fine search, local sequence</text>
<text top="739" left="49" width="90" height="9" font="font5" id="p12_t92" reading_order_no="91" segment_no="11" tag_type="text">matching is preferred.<a href="deeplearning_paper41.html#17">[181] </a>us-</text>
<text top="469" left="358" width="160" height="9" font="font5" id="p12_t93" reading_order_no="92" segment_no="7" tag_type="title">VIII. C ONCLUSION AND DISCUSSION<a href="deeplearning_paper41.html#17">[182]. </a>The final loop candidate</text>
<text top="487" left="322" width="241" height="9" font="font5" id="p12_t94" reading_order_no="93" segment_no="8" tag_type="text">This paper presents a critical survey on place recognition</text>
<text top="499" left="312" width="251" height="9" font="font5" id="p12_t95" reading_order_no="94" segment_no="8" tag_type="text">approaches, emphasizing the recent developments on deep</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p12_t96" reading_order_no="95" segment_no="8" tag_type="text">learning frameworks, namely supervised, unsupervised, semi-<a href="deeplearning_paper41.html#14">[52] </a>exploits 3D point clouds instead of camera</text>
<text top="523" left="312" width="200" height="9" font="font5" id="p12_t97" reading_order_no="96" segment_no="8" tag_type="text">supervised, parallel, and hierarchical approaches.</text>
<text top="536" left="322" width="241" height="9" font="font5" id="p12_t98" reading_order_no="97" segment_no="10" tag_type="text">An overview of each of these frameworks is presented.<a href="deeplearning_paper41.html#14">[49]. </a>This approach resorts to super keyframe</text>
<text top="548" left="312" width="251" height="9" font="font5" id="p12_t99" reading_order_no="98" segment_no="10" tag_type="text">In supervised approaches, the pre-trained frameworks tend</text>
<text top="560" left="312" width="251" height="9" font="font5" id="p12_t100" reading_order_no="99" segment_no="10" tag_type="text">to resort to semantic information by detecting landmarks</text>
<text top="572" left="312" width="251" height="9" font="font5" id="p12_t101" reading_order_no="100" segment_no="10" tag_type="text">or leveraging regional activation from CNN layers. On the</text>
<text top="584" left="312" width="251" height="9" font="font5" id="p12_t102" reading_order_no="101" segment_no="10" tag_type="text">other hand, among the end-to-end frameworks, the NetVLAD</text>
<text top="596" left="312" width="251" height="9" font="font5" id="p12_t103" reading_order_no="102" segment_no="10" tag_type="text">layer has inspired various works, which integrated this layer</text>
<text top="608" left="312" width="251" height="9" font="font5" id="p12_t104" reading_order_no="103" segment_no="10" tag_type="text">in deep architectures to train the model directly on place</text>
<text top="620" left="312" width="251" height="9" font="font5" id="p12_t105" reading_order_no="104" segment_no="10" tag_type="text">recognition using sensory data from the camera, 3D LiDAR,</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p12_t106" reading_order_no="105" segment_no="10" tag_type="text">or RADAR. The main application of unsupervised approaches,</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p12_t107" reading_order_no="106" segment_no="10" tag_type="text">such as GANs and autoencoders, is to address the domain</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p12_t108" reading_order_no="107" segment_no="10" tag_type="text">translation problem. While in semi-supervised, which in this</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p12_t109" reading_order_no="108" segment_no="10" tag_type="text">work refers to works that jointly leverage supervised and</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p12_t110" reading_order_no="109" segment_no="10" tag_type="text">unsupervised methods, the works address the cross-domain</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p12_t111" reading_order_no="110" segment_no="10" tag_type="text">problem, however instead of translating a source domain into</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p12_t112" reading_order_no="111" segment_no="10" tag_type="text">a target domain, these works seek to obtain a descriptor space</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p12_t113" reading_order_no="112" segment_no="10" tag_type="text">that is invariant to domains. Besides these traditional machine</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p12_t114" reading_order_no="113" segment_no="10" tag_type="text">learning frameworks, other frameworks have been suggested,</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p12_t115" reading_order_no="114" segment_no="10" tag_type="text">combining multiple DL or classical ML approaches into a par-</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font35" size="8" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="26" left="556" width="7" height="6" font="font0" id="p13_t1" reading_order_no="0" segment_no="0" tag_type="text">13</text>
<text top="59" left="287" width="39" height="7" font="font6" id="p13_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE VII</text>
<text top="68" left="92" width="428" height="7" font="font6" id="p13_t3" reading_order_no="2" segment_no="2" tag_type="text">S UMMARY OF RECENT WORKS USING HIERARCHICAL FRAMEWORKS . BG = B ELIEF G ENERATION AND PM = P LACE MAPPING .</text>
<text top="92" left="54" width="17" height="5" font="font34" id="p13_t4" reading_order_no="3" segment_no="3" tag_type="table">Sensor</text>
<text top="92" left="84" width="9" height="5" font="font34" id="p13_t5" reading_order_no="4" segment_no="3" tag_type="table">Ref</text>
<text top="92" left="162" width="33" height="5" font="font34" id="p13_t6" reading_order_no="5" segment_no="3" tag_type="table">Coarse Stage</text>
<text top="92" left="323" width="27" height="5" font="font34" id="p13_t7" reading_order_no="6" segment_no="3" tag_type="table">Fine Stage</text>
<text top="92" left="443" width="9" height="5" font="font34" id="p13_t8" reading_order_no="7" segment_no="3" tag_type="table">PM</text>
<text top="92" left="512" width="19" height="5" font="font34" id="p13_t9" reading_order_no="8" segment_no="3" tag_type="table">Dataset</text>
<text top="208" left="57" width="0" height="5" font="font16" id="p13_t10" reading_order_no="9" segment_no="3" tag_type="table">Camera</text>
<text top="110" left="83" width="10" height="5" font="font16" id="p13_t11" reading_order_no="10" segment_no="3" tag_type="table">[41]</text>
<text top="110" left="104" width="61" height="5" font="font16" id="p13_t12" reading_order_no="11" segment_no="3" tag_type="table">Features: RefineNet [179]</text>
<text top="117" left="104" width="149" height="5" font="font16" id="p13_t13" reading_order_no="12" segment_no="3" tag_type="table">Descriptor: LosT (semantic label scores + conv5 layer feature</text>
<text top="123" left="104" width="56" height="5" font="font16" id="p13_t14" reading_order_no="13" segment_no="3" tag_type="table">maps ) + normalization</text>
<text top="130" left="104" width="47" height="5" font="font16" id="p13_t15" reading_order_no="14" segment_no="3" tag_type="table">BG: cosine distance</text>
<text top="110" left="262" width="61" height="5" font="font16" id="p13_t16" reading_order_no="15" segment_no="3" tag_type="table">Features: RefineNet [179]</text>
<text top="117" left="262" width="139" height="5" font="font16" id="p13_t17" reading_order_no="16" segment_no="3" tag_type="table">Descriptor: keypoint extracted from CNN layer activations</text>
<text top="123" left="262" width="149" height="5" font="font16" id="p13_t18" reading_order_no="17" segment_no="3" tag_type="table">GB: spatial layout Verification ( Semantic Label Consistency +</text>
<text top="130" left="262" width="69" height="5" font="font16" id="p13_t19" reading_order_no="18" segment_no="3" tag_type="table">weighted Euclidean distance)<a href="deeplearning_paper41.html#14">[41]</a></text>
<text top="110" left="420" width="41" height="5" font="font16" id="p13_t20" reading_order_no="19" segment_no="3" tag_type="table">Coarse: Database<a href="deeplearning_paper41.html#17">[179]</a></text>
<text top="117" left="420" width="50" height="5" font="font16" id="p13_t21" reading_order_no="20" segment_no="3" tag_type="table">Fine: Top Candidates</text>
<text top="110" left="484" width="50" height="5" font="font16" id="p13_t22" reading_order_no="21" segment_no="3" tag_type="table">Oxford Robotcar [1];</text>
<text top="117" left="484" width="53" height="5" font="font16" id="p13_t23" reading_order_no="22" segment_no="3" tag_type="table">Synthia Dataset [183];</text>
<text top="143" left="83" width="10" height="5" font="font16" id="p13_t24" reading_order_no="23" segment_no="3" tag_type="table">[63]<a href="deeplearning_paper41.html#17">[179]</a></text>
<text top="143" left="104" width="137" height="5" font="font16" id="p13_t25" reading_order_no="24" segment_no="3" tag_type="table">Descriptor: concatenation of LoST [41] + NetVLAD [24]</text>
<text top="149" left="104" width="47" height="5" font="font16" id="p13_t26" reading_order_no="25" segment_no="3" tag_type="table">BG: cosine distance</text>
<text top="143" left="262" width="63" height="5" font="font16" id="p13_t27" reading_order_no="26" segment_no="3" tag_type="table">Features: pre-trained CNN</text>
<text top="149" left="262" width="125" height="5" font="font16" id="p13_t28" reading_order_no="27" segment_no="3" tag_type="table">Descriptor: keypoint extracted from CNN activations</text>
<text top="156" left="262" width="72" height="5" font="font16" id="p13_t29" reading_order_no="28" segment_no="3" tag_type="table">BG: spatial layout consistency</text>
<text top="143" left="420" width="41" height="5" font="font16" id="p13_t30" reading_order_no="29" segment_no="3" tag_type="table">Coarse: Database<a href="deeplearning_paper41.html#13">[1];</a></text>
<text top="149" left="420" width="50" height="5" font="font16" id="p13_t31" reading_order_no="30" segment_no="3" tag_type="table">Fine: Top Candidates<a href="deeplearning_paper41.html#17">[183];</a></text>
<text top="143" left="484" width="75" height="5" font="font16" id="p13_t32" reading_order_no="31" segment_no="3" tag_type="table">Oxford Robotcar [1]; MLFR;<a href="deeplearning_paper41.html#14">[63]</a></text>
<text top="149" left="484" width="75" height="5" font="font16" id="p13_t33" reading_order_no="32" segment_no="3" tag_type="table">Parking Lot; Residence Indoor<a href="deeplearning_paper41.html#14">[41] </a>+ NetVLAD <a href="deeplearning_paper41.html#14">[24]</a></text>
<text top="156" left="484" width="19" height="5" font="font16" id="p13_t34" reading_order_no="33" segment_no="3" tag_type="table">Outdoor</text>
<text top="169" left="83" width="10" height="5" font="font16" id="p13_t35" reading_order_no="34" segment_no="3" tag_type="table">[27]</text>
<text top="169" left="104" width="89" height="5" font="font16" id="p13_t36" reading_order_no="35" segment_no="3" tag_type="table">Features: RefineNet(Resnet101) [179]</text>
<text top="175" left="104" width="76" height="5" font="font16" id="p13_t37" reading_order_no="36" segment_no="3" tag_type="table">Descriptor: conv 5 feature maps</text>
<text top="182" left="104" width="47" height="5" font="font16" id="p13_t38" reading_order_no="37" segment_no="3" tag_type="table">BG: cosine distance</text>
<text top="169" left="262" width="127" height="5" font="font16" id="p13_t39" reading_order_no="38" segment_no="3" tag_type="table">Filtering: out-of-range keypoins based on depth maps</text>
<text top="175" left="262" width="100" height="5" font="font16" id="p13_t40" reading_order_no="39" segment_no="3" tag_type="table">Descriptor: same as coarse stage (filtered)<a href="deeplearning_paper41.html#13">[1]; </a>MLFR;</text>
<text top="182" left="262" width="47" height="5" font="font16" id="p13_t41" reading_order_no="40" segment_no="3" tag_type="table">BG: cosine distance</text>
<text top="169" left="420" width="41" height="5" font="font16" id="p13_t42" reading_order_no="41" segment_no="3" tag_type="table">Coarse: Database</text>
<text top="175" left="420" width="50" height="5" font="font16" id="p13_t43" reading_order_no="42" segment_no="3" tag_type="table">Fine: Top Candidates<a href="deeplearning_paper41.html#14">[27]</a></text>
<text top="169" left="484" width="49" height="5" font="font16" id="p13_t44" reading_order_no="43" segment_no="3" tag_type="table">Oxford Robotcar [1]<a href="deeplearning_paper41.html#17">[179]</a></text>
<text top="175" left="484" width="75" height="5" font="font16" id="p13_t45" reading_order_no="44" segment_no="3" tag_type="table">Synthia [183] (for depth evalu-</text>
<text top="182" left="484" width="13" height="5" font="font16" id="p13_t46" reading_order_no="45" segment_no="3" tag_type="table">ation)</text>
<text top="195" left="83" width="13" height="5" font="font16" id="p13_t47" reading_order_no="46" segment_no="3" tag_type="table">[180]</text>
<text top="195" left="104" width="38" height="5" font="font16" id="p13_t48" reading_order_no="47" segment_no="3" tag_type="table">Top Candidates:</text>
<text top="201" left="104" width="70" height="5" font="font16" id="p13_t49" reading_order_no="48" segment_no="3" tag_type="table">Features: MobileNetV2 [181]</text>
<text top="208" left="104" width="91" height="5" font="font16" id="p13_t50" reading_order_no="49" segment_no="3" tag_type="table">Descriptor: final average pooling layer</text>
<text top="214" left="104" width="136" height="5" font="font16" id="p13_t51" reading_order_no="50" segment_no="3" tag_type="table">BG: nearest neighbors + normalized scalar product [182]</text>
<text top="195" left="262" width="38" height="5" font="font16" id="p13_t52" reading_order_no="51" segment_no="3" tag_type="table">Features: SURF<a href="deeplearning_paper41.html#13">[1]</a></text>
<text top="201" left="262" width="54" height="5" font="font16" id="p13_t53" reading_order_no="52" segment_no="3" tag_type="table">Descriptor: hash codes<a href="deeplearning_paper41.html#17">[183] </a>(for depth evalu-</text>
<text top="208" left="262" width="149" height="5" font="font16" id="p13_t54" reading_order_no="53" segment_no="3" tag_type="table">BG: Hamming Distance between top candidates and SURF-</text>
<text top="214" left="262" width="111" height="5" font="font16" id="p13_t55" reading_order_no="54" segment_no="3" tag_type="table">based descriptor + ratio test [184] + RANSAC<a href="deeplearning_paper41.html#17">[180]</a></text>
<text top="195" left="420" width="54" height="5" font="font16" id="p13_t56" reading_order_no="55" segment_no="3" tag_type="table">Coarse: HNSW graphs</text>
<text top="201" left="420" width="49" height="5" font="font16" id="p13_t57" reading_order_no="56" segment_no="3" tag_type="table">Fine: Top candidates<a href="deeplearning_paper41.html#17">[181]</a></text>
<text top="195" left="484" width="30" height="5" font="font16" id="p13_t58" reading_order_no="57" segment_no="3" tag_type="table">KITTI [142]</text>
<text top="201" left="484" width="74" height="5" font="font16" id="p13_t59" reading_order_no="58" segment_no="3" tag_type="table">Malaga 2009 Parking 6L [185]<a href="deeplearning_paper41.html#17">[182]</a></text>
<text top="208" left="484" width="45" height="5" font="font16" id="p13_t60" reading_order_no="59" segment_no="3" tag_type="table">New College [186]</text>
<text top="227" left="83" width="13" height="5" font="font16" id="p13_t61" reading_order_no="60" segment_no="3" tag_type="table">[176]</text>
<text top="227" left="104" width="20" height="5" font="font16" id="p13_t62" reading_order_no="61" segment_no="3" tag_type="table">1st Tier:</text>
<text top="234" left="104" width="122" height="5" font="font16" id="p13_t63" reading_order_no="62" segment_no="3" tag_type="table">Features: HybridNet(AlexNet) [138] and Gist [177]<a href="deeplearning_paper41.html#17">[184] </a>+ RANSAC</text>
<text top="240" left="104" width="93" height="5" font="font16" id="p13_t64" reading_order_no="63" segment_no="3" tag_type="table">BG: Difference Scores + normalization</text>
<text top="247" left="104" width="22" height="5" font="font16" id="p13_t65" reading_order_no="64" segment_no="3" tag_type="table">2nd Tier:</text>
<text top="253" left="104" width="117" height="5" font="font16" id="p13_t66" reading_order_no="65" segment_no="3" tag_type="table">Features: KAZE [178] and Only Look Once [40]<a href="deeplearning_paper41.html#16">[142]</a></text>
<text top="260" left="104" width="149" height="5" font="font16" id="p13_t67" reading_order_no="66" segment_no="3" tag_type="table">BG: (KAZE) sum of the residual distances and difference scores<a href="deeplearning_paper41.html#17">[185]</a></text>
<text top="266" left="104" width="37" height="5" font="font16" id="p13_t68" reading_order_no="67" segment_no="3" tag_type="table">+ normalization<a href="deeplearning_paper41.html#17">[186]</a></text>
<text top="227" left="262" width="16" height="5" font="font16" id="p13_t69" reading_order_no="68" segment_no="3" tag_type="table">3 Tier:<a href="deeplearning_paper41.html#17">[176]</a></text>
<text top="234" left="262" width="96" height="5" font="font16" id="p13_t70" reading_order_no="69" segment_no="3" tag_type="table">Features: NetVLAD [24] and HOG [18]</text>
<text top="240" left="262" width="95" height="5" font="font16" id="p13_t71" reading_order_no="70" segment_no="3" tag_type="table">BG: max(Average of Difference Scores)<a href="deeplearning_paper41.html#16">[138] </a>and Gist <a href="deeplearning_paper41.html#17">[177]</a></text>
<text top="227" left="420" width="41" height="5" font="font16" id="p13_t72" reading_order_no="71" segment_no="3" tag_type="table">1st tier: Database</text>
<text top="234" left="420" width="56" height="5" font="font16" id="p13_t73" reading_order_no="72" segment_no="3" tag_type="table">2nd tier: Top candidates</text>
<text top="240" left="420" width="32" height="5" font="font16" id="p13_t74" reading_order_no="73" segment_no="3" tag_type="table">of the 1st tier<a href="deeplearning_paper41.html#17">[178] </a>and Only Look Once <a href="deeplearning_paper41.html#14">[40]</a></text>
<text top="247" left="420" width="56" height="5" font="font16" id="p13_t75" reading_order_no="74" segment_no="3" tag_type="table">3th tier: Top candidates</text>
<text top="253" left="420" width="34" height="5" font="font16" id="p13_t76" reading_order_no="75" segment_no="3" tag_type="table">of the 2nd tier</text>
<text top="227" left="484" width="36" height="5" font="font16" id="p13_t77" reading_order_no="76" segment_no="3" tag_type="table">Nordland [187]</text>
<text top="234" left="484" width="70" height="5" font="font16" id="p13_t78" reading_order_no="77" segment_no="3" tag_type="table">Berlin Kurfurstendamm [111]<a href="deeplearning_paper41.html#14">[24] </a>and HOG <a href="deeplearning_paper41.html#13">[18]</a></text>
<text top="283" left="53" width="19" height="5" font="font16" id="p13_t79" reading_order_no="78" segment_no="3" tag_type="table">3D Li-</text>
<text top="290" left="53" width="12" height="5" font="font16" id="p13_t80" reading_order_no="79" segment_no="3" tag_type="table">DAR</text>
<text top="283" left="83" width="10" height="5" font="font16" id="p13_t81" reading_order_no="80" segment_no="3" tag_type="table">[52]</text>
<text top="283" left="104" width="39" height="5" font="font16" id="p13_t82" reading_order_no="81" segment_no="3" tag_type="table">Find the cluster:</text>
<text top="290" left="104" width="95" height="5" font="font16" id="p13_t83" reading_order_no="82" segment_no="3" tag_type="table">Descriptor: lightweight variant LPD-Net</text>
<text top="296" left="104" width="149" height="5" font="font16" id="p13_t84" reading_order_no="83" segment_no="3" tag_type="table">Matching: the nearest L2 distance to the cluster center is</text>
<text top="303" left="104" width="73" height="5" font="font16" id="p13_t85" reading_order_no="84" segment_no="3" tag_type="table">selected as the super keyframe<a href="deeplearning_paper41.html#17">[187]</a></text>
<text top="283" left="262" width="70" height="5" font="font16" id="p13_t86" reading_order_no="85" segment_no="3" tag_type="table">Descriptor: same as in coarse<a href="deeplearning_paper41.html#16">[111]</a></text>
<text top="290" left="262" width="72" height="5" font="font16" id="p13_t87" reading_order_no="86" segment_no="3" tag_type="table">BG: Local sequence matching</text>
<text top="283" left="420" width="56" height="5" font="font16" id="p13_t88" reading_order_no="87" segment_no="3" tag_type="table">Super keyframe clusters</text>
<text top="283" left="484" width="49" height="5" font="font16" id="p13_t89" reading_order_no="88" segment_no="3" tag_type="table">Oxford Robotcar [1]<a href="deeplearning_paper41.html#14">[52]</a></text>
<text top="290" left="484" width="30" height="5" font="font16" id="p13_t90" reading_order_no="89" segment_no="3" tag_type="table">KITTI [142]</text>
<text top="336" left="49" width="251" height="9" font="font5" id="p13_t91" reading_order_no="90" segment_no="4" tag_type="text">allel or hierarchical architecture. In particular, the hierarchical</text>
<text top="348" left="49" width="251" height="9" font="font5" id="p13_t92" reading_order_no="91" segment_no="4" tag_type="text">approach has been shown to improve performances in general.</text>
<text top="360" left="49" width="251" height="9" font="font5" id="p13_t93" reading_order_no="92" segment_no="4" tag_type="text">Until recently, the primary motivation of the majority of</text>
<text top="372" left="49" width="251" height="9" font="font5" id="p13_t94" reading_order_no="93" segment_no="4" tag_type="text">the published articles was to increase performance. However,</text>
<text top="384" left="49" width="251" height="9" font="font5" id="p13_t95" reading_order_no="94" segment_no="4" tag_type="text">recent works additionally to high performance are also seeking</text>
<text top="396" left="49" width="41" height="9" font="font5" id="p13_t96" reading_order_no="95" segment_no="4" tag_type="text">efficiency.</text>
<text top="418" left="129" width="91" height="9" font="font5" id="p13_t97" reading_order_no="96" segment_no="8" tag_type="title">A CKNOWLEDGMENTS<a href="deeplearning_paper41.html#13">[1]</a></text>
<text top="433" left="59" width="241" height="9" font="font5" id="p13_t98" reading_order_no="97" segment_no="9" tag_type="text">This work has been supported by the projects MATIS-<a href="deeplearning_paper41.html#16">[142]</a></text>
<text top="445" left="49" width="251" height="9" font="font5" id="p13_t99" reading_order_no="98" segment_no="9" tag_type="text">CENTRO-01-0145-FEDER-000014 and SafeForest CENTRO-</text>
<text top="457" left="49" width="251" height="9" font="font5" id="p13_t100" reading_order_no="99" segment_no="9" tag_type="text">01-0247-FEDER-045931, Portugal. It was also partially sup-</text>
<text top="469" left="49" width="215" height="9" font="font5" id="p13_t101" reading_order_no="100" segment_no="9" tag_type="text">ported by FCT through grant UID/EEA/00048/2019.</text>
<text top="492" left="147" width="56" height="9" font="font5" id="p13_t102" reading_order_no="101" segment_no="12" tag_type="title">R EFERENCES</text>
<text top="507" left="57" width="243" height="7" font="font6" id="p13_t103" reading_order_no="102" segment_no="14" tag_type="text">[1] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year,</text>
<text top="516" left="71" width="229" height="7" font="font6" id="p13_t104" reading_order_no="103" segment_no="14" tag_type="text">1000km: The Oxford RobotCar Dataset,” The International Journal</text>
<text top="525" left="71" width="229" height="7" font="font35" id="p13_t105" reading_order_no="104" segment_no="14" tag_type="text">of Robotics Research (IJRR) , vol. 36, no. 1, pp. 3–15, 2017. [Online].</text>
<text top="534" left="71" width="181" height="7" font="font6" id="p13_t106" reading_order_no="105" segment_no="14" tag_type="text">Available: http://dx.doi.org/10.1177/0278364916679498</text>
<text top="543" left="57" width="243" height="7" font="font6" id="p13_t107" reading_order_no="106" segment_no="16" tag_type="text">[2] D. Olid, J. M. F´acil, and J. Civera, “Single-view place recognition</text>
<text top="552" left="71" width="221" height="7" font="font6" id="p13_t108" reading_order_no="107" segment_no="16" tag_type="text">under seasonal changes,” in PPNIV Workshop at IROS 2018 , 2018.</text>
<text top="561" left="57" width="243" height="7" font="font6" id="p13_t109" reading_order_no="108" segment_no="18" tag_type="text">[3] S. Lowry, N. S¨underhauf, P. Newman, J. J. Leonard, D. Cox, P. Corke,</text>
<text top="570" left="71" width="229" height="7" font="font6" id="p13_t110" reading_order_no="109" segment_no="18" tag_type="text">and M. J. Milford, “Visual place recognition: A survey,” IEEE Trans-</text>
<text top="579" left="71" width="170" height="7" font="font35" id="p13_t111" reading_order_no="110" segment_no="18" tag_type="text">actions on Robotics , vol. 32, no. 1, pp. 1–19, 2015.</text>
<text top="588" left="57" width="243" height="7" font="font6" id="p13_t112" reading_order_no="111" segment_no="20" tag_type="text">[4] E. Garcia-Fidalgo and A. Ortiz, “Vision-based topological mapping and</text>
<text top="597" left="71" width="229" height="7" font="font6" id="p13_t113" reading_order_no="112" segment_no="20" tag_type="text">localization methods: A survey,” Robotics and Autonomous Systems ,</text>
<text top="606" left="71" width="79" height="7" font="font6" id="p13_t114" reading_order_no="113" segment_no="20" tag_type="text">vol. 64, pp. 1–20, 2015.</text>
<text top="615" left="57" width="243" height="7" font="font6" id="p13_t115" reading_order_no="114" segment_no="21" tag_type="text">[5] G. Kim, Y. S. Park, Y. Cho, J. Jeong, and A. Kim, “Mulran: Multimodal</text>
<text top="624" left="71" width="229" height="7" font="font6" id="p13_t116" reading_order_no="115" segment_no="21" tag_type="text">range dataset for urban place recognition,” in 2020 IEEE International</text>
<text top="633" left="71" width="229" height="7" font="font35" id="p13_t117" reading_order_no="116" segment_no="21" tag_type="text">Conference on Robotics and Automation (ICRA) , 2020, pp. 6246–6253.</text>
<text top="642" left="57" width="243" height="7" font="font6" id="p13_t118" reading_order_no="117" segment_no="23" tag_type="text">[6] D. Barnes, M. Gadd, P. Murcutt, P. Newman, and I. Posner, “The<a href="http://dx.doi.org/10.1177/0278364916679498">http://dx.doi.org/10.1177/0278364916679498</a></text>
<text top="651" left="71" width="229" height="7" font="font6" id="p13_t119" reading_order_no="118" segment_no="23" tag_type="text">oxford radar robotcar dataset: A radar extension to the oxford robotcar</text>
<text top="660" left="71" width="230" height="7" font="font6" id="p13_t120" reading_order_no="119" segment_no="23" tag_type="text">dataset,” in 2020 IEEE International Conference on Robotics and</text>
<text top="669" left="71" width="65" height="7" font="font35" id="p13_t121" reading_order_no="120" segment_no="23" tag_type="text">Automation (ICRA) .</text>
<text top="669" left="144" width="93" height="7" font="font6" id="p13_t122" reading_order_no="121" segment_no="23" tag_type="text">IEEE, 2020, pp. 6433–6438.</text>
<text top="678" left="57" width="243" height="7" font="font6" id="p13_t123" reading_order_no="122" segment_no="25" tag_type="text">[7] F. Warburg, S. Hauberg, M. L´opez-Antequera, P. Gargallo, Y. Kuang,</text>
<text top="687" left="71" width="229" height="7" font="font6" id="p13_t124" reading_order_no="123" segment_no="25" tag_type="text">and J. Civera, “Mapillary street-level sequences: A dataset for lifelong</text>
<text top="696" left="71" width="229" height="7" font="font6" id="p13_t125" reading_order_no="124" segment_no="25" tag_type="text">place recognition,” in Proceedings of the IEEE/CVF Conference on</text>
<text top="705" left="71" width="213" height="7" font="font35" id="p13_t126" reading_order_no="125" segment_no="25" tag_type="text">Computer Vision and Pattern Recognition , 2020, pp. 2626–2635.</text>
<text top="714" left="57" width="243" height="7" font="font6" id="p13_t127" reading_order_no="126" segment_no="27" tag_type="text">[8] Y. Kang, H. Yin, and C. Berger, “Test your self-driving algorithm:</text>
<text top="723" left="71" width="229" height="7" font="font6" id="p13_t128" reading_order_no="127" segment_no="27" tag_type="text">An overview of publicly available driving datasets and virtual testing</text>
<text top="732" left="71" width="229" height="7" font="font6" id="p13_t129" reading_order_no="128" segment_no="27" tag_type="text">environments,” IEEE Transactions on Intelligent Vehicles , vol. 4, no. 2,</text>
<text top="740" left="71" width="63" height="7" font="font6" id="p13_t130" reading_order_no="129" segment_no="27" tag_type="text">pp. 171–185, 2019.</text>
<text top="337" left="320" width="243" height="7" font="font6" id="p13_t131" reading_order_no="130" segment_no="5" tag_type="text">[9] I. Kostavelis and A. Gasteratos, “Semantic mapping for mobile robotics</text>
<text top="346" left="334" width="229" height="7" font="font6" id="p13_t132" reading_order_no="131" segment_no="5" tag_type="text">tasks: A survey,” Robotics and Autonomous Systems , vol. 66, pp. 86–</text>
<text top="355" left="334" width="35" height="7" font="font6" id="p13_t133" reading_order_no="132" segment_no="5" tag_type="text">103, 2015.</text>
<text top="365" left="316" width="247" height="7" font="font6" id="p13_t134" reading_order_no="133" segment_no="6" tag_type="text">[10] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira,</text>
<text top="374" left="334" width="229" height="7" font="font6" id="p13_t135" reading_order_no="134" segment_no="6" tag_type="text">I. Reid, and J. J. Leonard, “Past, present, and future of simultaneous</text>
<text top="383" left="334" width="229" height="7" font="font6" id="p13_t136" reading_order_no="135" segment_no="6" tag_type="text">localization and mapping: Toward the robust-perception age,” IEEE</text>
<text top="392" left="334" width="208" height="7" font="font35" id="p13_t137" reading_order_no="136" segment_no="6" tag_type="text">Transactions on Robotics , vol. 32, no. 6, pp. 1309–1332, 2016.</text>
<text top="402" left="316" width="247" height="7" font="font6" id="p13_t138" reading_order_no="137" segment_no="7" tag_type="text">[11] G. Bresson, Z. Alsayed, L. Yu, and S. Glaser, “Simultaneous localiza-</text>
<text top="411" left="334" width="229" height="7" font="font6" id="p13_t139" reading_order_no="138" segment_no="7" tag_type="text">tion and mapping: A survey of current trends in autonomous driving,”</text>
<text top="420" left="334" width="229" height="7" font="font35" id="p13_t140" reading_order_no="139" segment_no="7" tag_type="text">IEEE Transactions on Intelligent Vehicles , vol. 2, no. 3, pp. 194–220,</text>
<text top="429" left="334" width="18" height="7" font="font6" id="p13_t141" reading_order_no="140" segment_no="7" tag_type="text">2017.</text>
<text top="438" left="316" width="247" height="7" font="font6" id="p13_t142" reading_order_no="141" segment_no="10" tag_type="text">[12] G. Kim and A. Kim, “Scan context: Egocentric spatial descriptor</text>
<text top="447" left="334" width="229" height="7" font="font6" id="p13_t143" reading_order_no="142" segment_no="10" tag_type="text">for place recognition within 3d point cloud map,” in 2018 IEEE/RSJ</text>
<text top="456" left="334" width="229" height="7" font="font35" id="p13_t144" reading_order_no="143" segment_no="10" tag_type="text">International Conference on Intelligent Robots and Systems (IROS) ,</text>
<text top="465" left="334" width="71" height="7" font="font6" id="p13_t145" reading_order_no="144" segment_no="10" tag_type="text">2018, pp. 4802–4809.</text>
<text top="475" left="316" width="247" height="7" font="font6" id="p13_t146" reading_order_no="145" segment_no="11" tag_type="text">[13] H. Yin, L. Tang, X. Ding, Y. Wang, and R. Xiong, “Locnet: Global</text>
<text top="484" left="334" width="229" height="7" font="font6" id="p13_t147" reading_order_no="146" segment_no="11" tag_type="text">localization in 3d point clouds for mobile vehicles,” in 2018 IEEE</text>
<text top="493" left="334" width="185" height="7" font="font35" id="p13_t148" reading_order_no="147" segment_no="11" tag_type="text">Intelligent Vehicles Symposium (IV) , 2018, pp. 728–733.</text>
<text top="502" left="316" width="247" height="7" font="font6" id="p13_t149" reading_order_no="148" segment_no="13" tag_type="text">[14] M. Gadd, D. De Martini, and P. Newman, “Look around you:</text>
<text top="511" left="334" width="229" height="7" font="font6" id="p13_t150" reading_order_no="149" segment_no="13" tag_type="text">Sequence-based radar place recognition with learned rotational invari-</text>
<text top="520" left="334" width="153" height="7" font="font6" id="p13_t151" reading_order_no="150" segment_no="13" tag_type="text">ance,” arXiv preprint arXiv:2003.04699 , 2020.</text>
<text top="530" left="316" width="247" height="7" font="font6" id="p13_t152" reading_order_no="151" segment_no="15" tag_type="text">[15] D. G. Lowe, “Object recognition from local scale-invariant features,”</text>
<text top="539" left="334" width="229" height="7" font="font6" id="p13_t153" reading_order_no="152" segment_no="15" tag_type="text">in Proceedings of the Seventh IEEE International Conference on</text>
<text top="548" left="334" width="173" height="7" font="font35" id="p13_t154" reading_order_no="153" segment_no="15" tag_type="text">Computer Vision , vol. 2, 1999, pp. 1150–1157 vol.2.</text>
<text top="558" left="316" width="247" height="7" font="font6" id="p13_t155" reading_order_no="154" segment_no="17" tag_type="text">[16] H. Bay, T. Tuytelaars, and L. Van Gool, “Surf: Speeded up robust</text>
<text top="567" left="334" width="229" height="7" font="font6" id="p13_t156" reading_order_no="155" segment_no="17" tag_type="text">features,” in European conference on computer vision . Springer, 2006,</text>
<text top="576" left="334" width="43" height="7" font="font6" id="p13_t157" reading_order_no="156" segment_no="17" tag_type="text">pp. 404–417.</text>
<text top="585" left="316" width="247" height="7" font="font6" id="p13_t158" reading_order_no="157" segment_no="19" tag_type="text">[17] P. Neubert and P. Protzel, “Beyond holistic descriptors, keypoints,</text>
<text top="594" left="334" width="229" height="7" font="font6" id="p13_t159" reading_order_no="158" segment_no="19" tag_type="text">and fixed patches: Multiscale superpixel grids for place recognition</text>
<text top="603" left="334" width="229" height="7" font="font6" id="p13_t160" reading_order_no="159" segment_no="19" tag_type="text">in changing environments,” IEEE Robotics and Automation Letters ,</text>
<text top="612" left="334" width="108" height="7" font="font6" id="p13_t161" reading_order_no="160" segment_no="19" tag_type="text">vol. 1, no. 1, pp. 484–491, 2016.</text>
<text top="622" left="316" width="247" height="7" font="font6" id="p13_t162" reading_order_no="161" segment_no="22" tag_type="text">[18] N. Dalal and B. Triggs, “Histograms of oriented gradients for human</text>
<text top="631" left="334" width="229" height="7" font="font6" id="p13_t163" reading_order_no="162" segment_no="22" tag_type="text">detection,” in 2005 IEEE computer society conference on computer</text>
<text top="640" left="334" width="165" height="7" font="font35" id="p13_t164" reading_order_no="163" segment_no="22" tag_type="text">vision and pattern recognition (CVPR’05) , vol. 1.</text>
<text top="640" left="509" width="54" height="7" font="font6" id="p13_t165" reading_order_no="164" segment_no="22" tag_type="text">IEEE, 2005, pp.</text>
<text top="649" left="334" width="30" height="7" font="font6" id="p13_t166" reading_order_no="165" segment_no="22" tag_type="text">886–893.</text>
<text top="658" left="316" width="247" height="7" font="font6" id="p13_t167" reading_order_no="166" segment_no="24" tag_type="text">[19] L. Fei-Fei and P. Perona, “A bayesian hierarchical model for learning</text>
<text top="667" left="334" width="229" height="7" font="font6" id="p13_t168" reading_order_no="167" segment_no="24" tag_type="text">natural scene categories,” in 2005 IEEE Computer Society Conference</text>
<text top="676" left="334" width="229" height="7" font="font35" id="p13_t169" reading_order_no="168" segment_no="24" tag_type="text">on Computer Vision and Pattern Recognition (CVPR’05) , vol. 2, 2005,</text>
<text top="685" left="334" width="64" height="7" font="font6" id="p13_t170" reading_order_no="169" segment_no="24" tag_type="text">pp. 524–531 vol. 2.</text>
<text top="695" left="316" width="247" height="7" font="font6" id="p13_t171" reading_order_no="170" segment_no="26" tag_type="text">[20] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature , vol. 521,</text>
<text top="704" left="334" width="97" height="7" font="font6" id="p13_t172" reading_order_no="171" segment_no="26" tag_type="text">no. 7553, pp. 436–444, 2015.</text>
<text top="714" left="316" width="247" height="7" font="font6" id="p13_t173" reading_order_no="172" segment_no="28" tag_type="text">[21] J. Yue-Hei Ng, F. Yang, and L. S. Davis, “Exploiting local features</text>
<text top="723" left="334" width="229" height="7" font="font6" id="p13_t174" reading_order_no="173" segment_no="28" tag_type="text">from deep networks for image retrieval,” in Proceedings of the IEEE</text>
<text top="732" left="334" width="229" height="7" font="font35" id="p13_t175" reading_order_no="174" segment_no="28" tag_type="text">conference on computer vision and pattern recognition workshops ,</text>
<text top="740" left="334" width="55" height="7" font="font6" id="p13_t176" reading_order_no="175" segment_no="28" tag_type="text">2015, pp. 53–61.</text>
</page>
<page number="14" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p14_t1" reading_order_no="0" segment_no="0" tag_type="text">14</text>
<text top="59" left="53" width="247" height="7" font="font6" id="p14_t2" reading_order_no="1" segment_no="1" tag_type="text">[22] N. S¨underhauf, S. Shirazi, F. Dayoub, B. Upcroft, and M. Milford, “On</text>
<text top="68" left="71" width="229" height="7" font="font6" id="p14_t3" reading_order_no="2" segment_no="1" tag_type="text">the performance of convnet features for place recognition,” in 2015</text>
<text top="77" left="71" width="229" height="7" font="font35" id="p14_t4" reading_order_no="3" segment_no="1" tag_type="text">IEEE/RSJ International Conference on Intelligent Robots and Systems</text>
<text top="86" left="71" width="24" height="7" font="font35" id="p14_t5" reading_order_no="4" segment_no="1" tag_type="text">(IROS) .</text>
<text top="86" left="103" width="93" height="7" font="font6" id="p14_t6" reading_order_no="5" segment_no="1" tag_type="text">IEEE, 2015, pp. 4297–4304.</text>
<text top="95" left="53" width="247" height="7" font="font6" id="p14_t7" reading_order_no="6" segment_no="4" tag_type="text">[23] S. Garg, N. Suenderhauf, and M. Milford, “Don’t look back: Robusti-</text>
<text top="104" left="71" width="229" height="7" font="font6" id="p14_t8" reading_order_no="7" segment_no="4" tag_type="text">fying place categorization for viewpoint- and condition-invariant place</text>
<text top="113" left="71" width="229" height="7" font="font6" id="p14_t9" reading_order_no="8" segment_no="4" tag_type="text">recognition,” in 2018 IEEE International Conference on Robotics and</text>
<text top="122" left="71" width="139" height="7" font="font35" id="p14_t10" reading_order_no="9" segment_no="4" tag_type="text">Automation (ICRA) , 2018, pp. 3645–3652.</text>
<text top="131" left="53" width="247" height="7" font="font6" id="p14_t11" reading_order_no="10" segment_no="6" tag_type="text">[24] R. Arandjelovi´c, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, “Netvlad:</text>
<text top="140" left="71" width="229" height="7" font="font6" id="p14_t12" reading_order_no="11" segment_no="6" tag_type="text">Cnn architecture for weakly supervised place recognition,” in IEEE</text>
<text top="149" left="71" width="229" height="7" font="font35" id="p14_t13" reading_order_no="12" segment_no="6" tag_type="text">Transactions on Pattern Analysis and Machine Intelligence , vol. 40,</text>
<text top="158" left="71" width="93" height="7" font="font6" id="p14_t14" reading_order_no="13" segment_no="6" tag_type="text">no. 6, 2018, pp. 1437–1451.</text>
<text top="167" left="53" width="247" height="7" font="font6" id="p14_t15" reading_order_no="14" segment_no="8" tag_type="text">[25] A. Oertel, T. Cieslewski, and D. Scaramuzza, “Augmenting visual</text>
<text top="176" left="71" width="229" height="7" font="font6" id="p14_t16" reading_order_no="15" segment_no="8" tag_type="text">place recognition with structural cues,” IEEE Robotics and Automation</text>
<text top="185" left="71" width="143" height="7" font="font35" id="p14_t17" reading_order_no="16" segment_no="8" tag_type="text">Letters , vol. 5, no. 4, pp. 5534–5541, 2020.</text>
<text top="194" left="53" width="247" height="7" font="font6" id="p14_t18" reading_order_no="17" segment_no="10" tag_type="text">[26] J. L. Sch¨onberger, M. Pollefeys, A. Geiger, and T. Sattler, “Semantic</text>
<text top="202" left="71" width="229" height="8" font="font6" id="p14_t19" reading_order_no="18" segment_no="10" tag_type="text">visual localization,” in Proceedings of the IEEE Conference on Com-</text>
<text top="211" left="71" width="198" height="8" font="font35" id="p14_t20" reading_order_no="19" segment_no="10" tag_type="text">puter Vision and Pattern Recognition , 2018, pp. 6896–6906.</text>
<text top="220" left="53" width="247" height="7" font="font6" id="p14_t21" reading_order_no="20" segment_no="12" tag_type="text">[27] S. Garg, M. Babu, T. Dharmasiri, S. Hausler, N. Suenderhauf, S. Ku-</text>
<text top="229" left="71" width="229" height="7" font="font6" id="p14_t22" reading_order_no="21" segment_no="12" tag_type="text">mar, T. Drummond, and M. Milford, “Look no deeper: Recognizing</text>
<text top="238" left="71" width="229" height="7" font="font6" id="p14_t23" reading_order_no="22" segment_no="12" tag_type="text">places from opposing viewpoints under varying scene appearance using</text>
<text top="247" left="71" width="229" height="7" font="font6" id="p14_t24" reading_order_no="23" segment_no="12" tag_type="text">single-view depth estimation,” in 2019 International Conference on</text>
<text top="256" left="71" width="110" height="7" font="font35" id="p14_t25" reading_order_no="24" segment_no="12" tag_type="text">Robotics and Automation (ICRA) .</text>
<text top="256" left="189" width="93" height="7" font="font6" id="p14_t26" reading_order_no="25" segment_no="12" tag_type="text">IEEE, 2019, pp. 4916–4923.</text>
<text top="265" left="53" width="247" height="7" font="font6" id="p14_t27" reading_order_no="26" segment_no="14" tag_type="text">[28] R. Paul and P. Newman, “Fab-map 3d: Topological mapping with</text>
<text top="274" left="71" width="229" height="7" font="font6" id="p14_t28" reading_order_no="27" segment_no="14" tag_type="text">spatial and visual appearance,” in 2010 IEEE International Conference</text>
<text top="283" left="71" width="95" height="7" font="font35" id="p14_t29" reading_order_no="28" segment_no="14" tag_type="text">on Robotics and Automation .</text>
<text top="283" left="174" width="93" height="7" font="font6" id="p14_t30" reading_order_no="29" segment_no="14" tag_type="text">IEEE, 2010, pp. 2649–2656.</text>
<text top="292" left="53" width="247" height="7" font="font6" id="p14_t31" reading_order_no="30" segment_no="16" tag_type="text">[29] H. Korrapati, J. Courbon, Y. Mezouar, and P. Martinet, “Image se-</text>
<text top="301" left="71" width="229" height="7" font="font6" id="p14_t32" reading_order_no="31" segment_no="16" tag_type="text">quence partitioning for outdoor mapping,” in 2012 IEEE International</text>
<text top="310" left="71" width="229" height="7" font="font35" id="p14_t33" reading_order_no="32" segment_no="16" tag_type="text">Conference on Robotics and Automation . IEEE, 2012, pp. 1650–1655.</text>
<text top="319" left="53" width="247" height="7" font="font6" id="p14_t34" reading_order_no="33" segment_no="18" tag_type="text">[30] Z. Hong, Y. Petillot, D. Lane, Y. Miao, and S. Wang, “Textplace:</text>
<text top="328" left="71" width="229" height="7" font="font6" id="p14_t35" reading_order_no="34" segment_no="18" tag_type="text">Visual place recognition and topological localization through reading</text>
<text top="337" left="71" width="229" height="7" font="font6" id="p14_t36" reading_order_no="35" segment_no="18" tag_type="text">scene texts,” in 2019 IEEE/CVF International Conference on Computer</text>
<text top="346" left="71" width="122" height="7" font="font35" id="p14_t37" reading_order_no="36" segment_no="18" tag_type="text">Vision (ICCV) , 2019, pp. 2861–2870.</text>
<text top="355" left="53" width="247" height="7" font="font6" id="p14_t38" reading_order_no="37" segment_no="20" tag_type="text">[31] F. Dayoub, G. Cielniak, and T. Duckett, “Long-term experiments with</text>
<text top="364" left="71" width="229" height="7" font="font6" id="p14_t39" reading_order_no="38" segment_no="20" tag_type="text">an adaptive spherical view representation for navigation in changing</text>
<text top="373" left="71" width="229" height="7" font="font6" id="p14_t40" reading_order_no="39" segment_no="20" tag_type="text">environments,” Robotics and Autonomous Systems , vol. 59, no. 5, pp.</text>
<text top="382" left="71" width="51" height="7" font="font6" id="p14_t41" reading_order_no="40" segment_no="20" tag_type="text">285–295, 2011.</text>
<text top="391" left="53" width="247" height="7" font="font6" id="p14_t42" reading_order_no="41" segment_no="23" tag_type="text">[32] W. Churchill and P. Newman, “Experience-based navigation for long-</text>
<text top="400" left="71" width="229" height="7" font="font6" id="p14_t43" reading_order_no="42" segment_no="23" tag_type="text">term localisation,” The International Journal of Robotics Research ,</text>
<text top="409" left="71" width="124" height="7" font="font6" id="p14_t44" reading_order_no="43" segment_no="23" tag_type="text">vol. 32, no. 14, pp. 1645–1661, 2013.</text>
<text top="418" left="53" width="247" height="7" font="font6" id="p14_t45" reading_order_no="44" segment_no="25" tag_type="text">[33] M. J. Milford, G. F. Wyeth, and D. Prasser, “Ratslam: a hippocampal</text>
<text top="427" left="71" width="229" height="7" font="font6" id="p14_t46" reading_order_no="45" segment_no="25" tag_type="text">model for simultaneous localization and mapping,” in IEEE Interna-</text>
<text top="436" left="71" width="229" height="7" font="font35" id="p14_t47" reading_order_no="46" segment_no="25" tag_type="text">tional Conference on Robotics and Automation, 2004. Proceedings.</text>
<text top="445" left="71" width="163" height="7" font="font35" id="p14_t48" reading_order_no="47" segment_no="25" tag_type="text">ICRA ’04. 2004 , vol. 1, 2004, pp. 403–408 Vol.1.</text>
<text top="454" left="53" width="247" height="7" font="font6" id="p14_t49" reading_order_no="48" segment_no="27" tag_type="text">[34] L. Schaupp, M. B¨urki, R. Dub´e, R. Siegwart, and C. Cadena, “Oreos:</text>
<text top="463" left="71" width="229" height="7" font="font6" id="p14_t50" reading_order_no="49" segment_no="27" tag_type="text">Oriented recognition of 3d point clouds in outdoor scenarios,” in 2019</text>
<text top="472" left="71" width="229" height="7" font="font35" id="p14_t51" reading_order_no="50" segment_no="27" tag_type="text">IEEE/RSJ International Conference on Intelligent Robots and Systems</text>
<text top="480" left="71" width="98" height="8" font="font35" id="p14_t52" reading_order_no="51" segment_no="27" tag_type="text">(IROS) , 2019, pp. 3255–3261.</text>
<text top="489" left="53" width="247" height="7" font="font6" id="p14_t53" reading_order_no="52" segment_no="29" tag_type="text">[35] M. Cummins and P. Newman, “Fab-map: Probabilistic localization and</text>
<text top="498" left="71" width="229" height="7" font="font6" id="p14_t54" reading_order_no="53" segment_no="29" tag_type="text">mapping in the space of appearance,” The International Journal of</text>
<text top="507" left="71" width="177" height="7" font="font35" id="p14_t55" reading_order_no="54" segment_no="29" tag_type="text">Robotics Research , vol. 27, no. 6, pp. 647–665, 2008.</text>
<text top="516" left="53" width="247" height="7" font="font6" id="p14_t56" reading_order_no="55" segment_no="30" tag_type="text">[36] Y. A. Malkov and D. A. Yashunin, “Efficient and robust approxi-</text>
<text top="525" left="71" width="229" height="7" font="font6" id="p14_t57" reading_order_no="56" segment_no="30" tag_type="text">mate nearest neighbor search using hierarchical navigable small world</text>
<text top="534" left="71" width="230" height="7" font="font6" id="p14_t58" reading_order_no="57" segment_no="30" tag_type="text">graphs,” IEEE Transactions on Pattern Analysis and Machine Intelli-</text>
<text top="543" left="71" width="45" height="7" font="font35" id="p14_t59" reading_order_no="58" segment_no="30" tag_type="text">gencee , 2018.</text>
<text top="552" left="53" width="247" height="7" font="font6" id="p14_t60" reading_order_no="59" segment_no="32" tag_type="text">[37] E. Garcia-Fidalgo and A. Ortiz, “Hierarchical place recognition for</text>
<text top="561" left="71" width="229" height="7" font="font6" id="p14_t61" reading_order_no="60" segment_no="32" tag_type="text">topological mapping,” IEEE Transactions on Robotics , vol. 33, no. 5,</text>
<text top="570" left="71" width="71" height="7" font="font6" id="p14_t62" reading_order_no="61" segment_no="32" tag_type="text">pp. 1061–1074, 2017.</text>
<text top="579" left="53" width="247" height="7" font="font6" id="p14_t63" reading_order_no="62" segment_no="34" tag_type="text">[38] T. Morris, F. Dayoub, P. Corke, G. Wyeth, and B. Upcroft, “Multiple</text>
<text top="588" left="71" width="229" height="7" font="font6" id="p14_t64" reading_order_no="63" segment_no="34" tag_type="text">map hypotheses for planning and navigating in non-stationary envi-</text>
<text top="597" left="71" width="229" height="7" font="font6" id="p14_t65" reading_order_no="64" segment_no="34" tag_type="text">ronments,” in 2014 IEEE International Conference on Robotics and</text>
<text top="606" left="71" width="139" height="7" font="font35" id="p14_t66" reading_order_no="65" segment_no="34" tag_type="text">Automation (ICRA) , 2014, pp. 2765–2770.</text>
<text top="615" left="53" width="247" height="7" font="font6" id="p14_t67" reading_order_no="66" segment_no="36" tag_type="text">[39] M. Zaffar, S. Ehsan, M. Milford, and K. McDonald-Maier, “Co-</text>
<text top="624" left="71" width="229" height="7" font="font6" id="p14_t68" reading_order_no="67" segment_no="36" tag_type="text">hog: A light-weight, compute-efficient, and training-free visual place</text>
<text top="633" left="71" width="229" height="7" font="font6" id="p14_t69" reading_order_no="68" segment_no="36" tag_type="text">recognition technique for changing environments,” IEEE Robotics and</text>
<text top="642" left="71" width="183" height="7" font="font35" id="p14_t70" reading_order_no="69" segment_no="36" tag_type="text">Automation Letters , vol. 5, no. 2, pp. 1835–1842, 2020.</text>
<text top="651" left="53" width="247" height="7" font="font6" id="p14_t71" reading_order_no="70" segment_no="39" tag_type="text">[40] Z. Chen, F. Maffra, I. Sa, and M. Chli, “Only look once, mining</text>
<text top="660" left="71" width="229" height="7" font="font6" id="p14_t72" reading_order_no="71" segment_no="39" tag_type="text">distinctive landmarks from convnet for visual place recognition,” in</text>
<text top="669" left="71" width="229" height="7" font="font35" id="p14_t73" reading_order_no="72" segment_no="39" tag_type="text">2017 IEEE/RSJ International Conference on Intelligent Robots and</text>
<text top="678" left="71" width="106" height="7" font="font35" id="p14_t74" reading_order_no="73" segment_no="39" tag_type="text">Systems (IROS) , 2017, pp. 9–16.</text>
<text top="687" left="53" width="247" height="7" font="font6" id="p14_t75" reading_order_no="74" segment_no="41" tag_type="text">[41] S. Garg, N. Suenderhauf, and M. Milford, “Lost? appearance-invariant</text>
<text top="696" left="71" width="229" height="7" font="font6" id="p14_t76" reading_order_no="75" segment_no="41" tag_type="text">place recognition for opposite viewpoints using visual semantics,”</text>
<text top="705" left="71" width="190" height="7" font="font35" id="p14_t77" reading_order_no="76" segment_no="41" tag_type="text">Proceedings of Robotics: Science and Systems XIV , 2018.</text>
<text top="714" left="53" width="247" height="7" font="font6" id="p14_t78" reading_order_no="77" segment_no="42" tag_type="text">[42] M. J. Milford and G. F. Wyeth, “SeqSLAM: Visual route-based</text>
<text top="723" left="71" width="229" height="7" font="font6" id="p14_t79" reading_order_no="78" segment_no="42" tag_type="text">navigation for sunny summer days and stormy winter nights,” in 2012</text>
<text top="732" left="71" width="229" height="7" font="font35" id="p14_t80" reading_order_no="79" segment_no="42" tag_type="text">IEEE International Conference on Robotics and Automation , 2012, pp.</text>
<text top="740" left="71" width="38" height="7" font="font6" id="p14_t81" reading_order_no="80" segment_no="42" tag_type="text">1643–1649.</text>
<text top="59" left="316" width="247" height="7" font="font6" id="p14_t82" reading_order_no="81" segment_no="2" tag_type="text">[43] S. Garg, B. Harwood, G. Anand, and M. Milford, “Delta descriptors:</text>
<text top="68" left="334" width="229" height="7" font="font6" id="p14_t83" reading_order_no="82" segment_no="2" tag_type="text">Change-based place representation for robust visual localization,” IEEE</text>
<text top="77" left="334" width="229" height="7" font="font35" id="p14_t84" reading_order_no="83" segment_no="2" tag_type="text">Robotics and Automation Letters , vol. 5, no. 4, pp. 5120–5127, 2020.</text>
<text top="86" left="316" width="247" height="7" font="font6" id="p14_t85" reading_order_no="84" segment_no="3" tag_type="text">[44] H. Zhang, F. Han, and H. Wang, “Robust multimodal sequence-based</text>
<text top="95" left="334" width="229" height="7" font="font6" id="p14_t86" reading_order_no="85" segment_no="3" tag_type="text">loop closure detection via structured sparsity.” in Robotics: Science and</text>
<text top="104" left="334" width="48" height="7" font="font35" id="p14_t87" reading_order_no="86" segment_no="3" tag_type="text">systems , 2016.</text>
<text top="113" left="316" width="247" height="7" font="font6" id="p14_t88" reading_order_no="87" segment_no="5" tag_type="text">[45] P. Gao and H. Zhang, “Long-term place recognition through worst-</text>
<text top="122" left="334" width="229" height="7" font="font6" id="p14_t89" reading_order_no="88" segment_no="5" tag_type="text">case graph matching to integrate landmark appearances and spatial</text>
<text top="131" left="334" width="229" height="7" font="font6" id="p14_t90" reading_order_no="89" segment_no="5" tag_type="text">relationships,” in 2020 IEEE International Conference on Robotics and</text>
<text top="140" left="334" width="139" height="7" font="font35" id="p14_t91" reading_order_no="90" segment_no="5" tag_type="text">Automation (ICRA) , 2020, pp. 1070–1076.</text>
<text top="149" left="316" width="247" height="7" font="font6" id="p14_t92" reading_order_no="91" segment_no="7" tag_type="text">[46] J. Guo, P. V. K. Borges, C. Park, and A. Gawel, “Local descriptor</text>
<text top="158" left="334" width="229" height="7" font="font6" id="p14_t93" reading_order_no="92" segment_no="7" tag_type="text">for robust place recognition using lidar intensity,” IEEE Robotics and</text>
<text top="167" left="334" width="183" height="7" font="font35" id="p14_t94" reading_order_no="93" segment_no="7" tag_type="text">Automation Letters , vol. 4, no. 2, pp. 1470–1477, 2019.</text>
<text top="176" left="316" width="247" height="7" font="font6" id="p14_t95" reading_order_no="94" segment_no="9" tag_type="text">[47] P. Hansen and B. Browning, “Visual place recognition using hmm</text>
<text top="185" left="334" width="229" height="7" font="font6" id="p14_t96" reading_order_no="95" segment_no="9" tag_type="text">sequence matching,” in 2014 IEEE/RSJ International Conference on</text>
<text top="194" left="334" width="177" height="7" font="font35" id="p14_t97" reading_order_no="96" segment_no="9" tag_type="text">Intelligent Robots and Systems , 2014, pp. 4549–4555.</text>
<text top="202" left="316" width="247" height="7" font="font6" id="p14_t98" reading_order_no="97" segment_no="11" tag_type="text">[48] C. Cadena, D. Galvez-L´opez, J. D. Tardos, and J. Neira, “Robust place</text>
<text top="211" left="334" width="229" height="8" font="font6" id="p14_t99" reading_order_no="98" segment_no="11" tag_type="text">recognition with stereo sequences,” IEEE Transactions on Robotics ,</text>
<text top="220" left="334" width="112" height="7" font="font6" id="p14_t100" reading_order_no="99" segment_no="11" tag_type="text">vol. 28, no. 4, pp. 871–885, 2012.</text>
<text top="229" left="316" width="247" height="7" font="font6" id="p14_t101" reading_order_no="100" segment_no="13" tag_type="text">[49] Z. Liu, S. Zhou, C. Suo, P. Yin, W. Chen, H. Wang, H. Li, and</text>
<text top="238" left="334" width="229" height="7" font="font6" id="p14_t102" reading_order_no="101" segment_no="13" tag_type="text">Y.-H. Liu, “LPD-Net: 3D point cloud learning for large-scale place</text>
<text top="247" left="334" width="229" height="7" font="font6" id="p14_t103" reading_order_no="102" segment_no="13" tag_type="text">recognition and environment analysis,” in Proceedings of the IEEE</text>
<text top="256" left="334" width="226" height="7" font="font35" id="p14_t104" reading_order_no="103" segment_no="13" tag_type="text">International Conference on Computer Vision , 2019, pp. 2831–2840.</text>
<text top="265" left="316" width="247" height="7" font="font6" id="p14_t105" reading_order_no="104" segment_no="15" tag_type="text">[50] L. Wu and Y. Wu, “Deep supervised hashing with similar hierarchy</text>
<text top="274" left="334" width="229" height="7" font="font6" id="p14_t106" reading_order_no="105" segment_no="15" tag_type="text">for place recognition,” in 2019 IEEE/RSJ International Conference on</text>
<text top="283" left="334" width="202" height="7" font="font35" id="p14_t107" reading_order_no="106" segment_no="15" tag_type="text">Intelligent Robots and Systems (IROS) , 2019, pp. 3781–3786.</text>
<text top="292" left="316" width="247" height="7" font="font6" id="p14_t108" reading_order_no="107" segment_no="17" tag_type="text">[51] S. M. Siam and H. Zhang, “Fast-seqslam: A fast appearance based</text>
<text top="301" left="334" width="229" height="7" font="font6" id="p14_t109" reading_order_no="108" segment_no="17" tag_type="text">place recognition algorithm,” in 2017 IEEE International Conference</text>
<text top="310" left="334" width="195" height="7" font="font35" id="p14_t110" reading_order_no="109" segment_no="17" tag_type="text">on Robotics and Automation (ICRA) , 2017, pp. 5702–5708.</text>
<text top="319" left="316" width="247" height="7" font="font6" id="p14_t111" reading_order_no="110" segment_no="19" tag_type="text">[52] Z. Liu, C. Suo, S. Zhou, H. Wei, Y. Liu, H. Wang, and Y.-H. Liu,</text>
<text top="328" left="334" width="229" height="7" font="font6" id="p14_t112" reading_order_no="111" segment_no="19" tag_type="text">“Seqlpd: Sequence matching enhanced loop-closure detection based</text>
<text top="337" left="334" width="229" height="7" font="font6" id="p14_t113" reading_order_no="112" segment_no="19" tag_type="text">on large-scale point cloud description for self-driving vehicles,” arXiv</text>
<text top="346" left="334" width="110" height="7" font="font35" id="p14_t114" reading_order_no="113" segment_no="19" tag_type="text">preprint arXiv:1904.13030 , 2019.</text>
<text top="355" left="316" width="247" height="7" font="font6" id="p14_t115" reading_order_no="114" segment_no="21" tag_type="text">[53] O. Vysotska and C. Stachniss, “Effective visual place recognition using</text>
<text top="364" left="334" width="230" height="7" font="font6" id="p14_t116" reading_order_no="115" segment_no="21" tag_type="text">multi-sequence maps,” IEEE Robotics and Automation Letters , vol. 4,</text>
<text top="373" left="334" width="93" height="7" font="font6" id="p14_t117" reading_order_no="116" segment_no="21" tag_type="text">no. 2, pp. 1730–1736, 2019.</text>
<text top="382" left="316" width="247" height="7" font="font6" id="p14_t118" reading_order_no="117" segment_no="22" tag_type="text">[54] O. Vysotska and C. Stachniss, “Lazy data association for image</text>
<text top="391" left="334" width="229" height="7" font="font6" id="p14_t119" reading_order_no="118" segment_no="22" tag_type="text">sequences matching under substantial appearance changes,” IEEE</text>
<text top="400" left="334" width="221" height="7" font="font35" id="p14_t120" reading_order_no="119" segment_no="22" tag_type="text">Robotics and Automation Letters , vol. 1, no. 1, pp. 213–220, 2015.</text>
<text top="409" left="316" width="247" height="7" font="font6" id="p14_t121" reading_order_no="120" segment_no="24" tag_type="text">[55] L. Bampis, A. Amanatiadis, and A. Gasteratos, “Encoding the de-</text>
<text top="418" left="334" width="229" height="7" font="font6" id="p14_t122" reading_order_no="121" segment_no="24" tag_type="text">scription of image sequences: A two-layered pipeline for loop closure</text>
<text top="427" left="334" width="229" height="7" font="font6" id="p14_t123" reading_order_no="122" segment_no="24" tag_type="text">detection,” in 2016 IEEE/RSJ International Conference on Intelligent</text>
<text top="436" left="334" width="166" height="7" font="font35" id="p14_t124" reading_order_no="123" segment_no="24" tag_type="text">Robots and Systems (IROS) , 2016, pp. 4530–4536.</text>
<text top="445" left="316" width="247" height="7" font="font6" id="p14_t125" reading_order_no="124" segment_no="26" tag_type="text">[56] T. Naseer, L. Spinello, W. Burgard, and C. Stachniss, “Robust visual</text>
<text top="454" left="334" width="229" height="7" font="font6" id="p14_t126" reading_order_no="125" segment_no="26" tag_type="text">robot localization across seasons using network flows,” in Proceedings</text>
<text top="463" left="334" width="229" height="7" font="font35" id="p14_t127" reading_order_no="126" segment_no="26" tag_type="text">of the Twenty-Eighth AAAI Conference on Artificial Intelligence , ser.</text>
<text top="471" left="334" width="33" height="7" font="font6" id="p14_t128" reading_order_no="127" segment_no="26" tag_type="text">AAAI’14.</text>
<text top="471" left="375" width="112" height="7" font="font6" id="p14_t129" reading_order_no="128" segment_no="26" tag_type="text">AAAI Press, 2014, p. 2564–2570.</text>
<text top="480" left="316" width="247" height="7" font="font6" id="p14_t130" reading_order_no="129" segment_no="28" tag_type="text">[57] P. Yin, L. Xu, X. Li, C. Yin, Y. Li, R. A. Srivatsan, L. Li, J. Ji, and</text>
<text top="489" left="334" width="229" height="7" font="font6" id="p14_t131" reading_order_no="130" segment_no="28" tag_type="text">Y. He, “A multi-domain feature learning method for visual place recog-</text>
<text top="498" left="334" width="229" height="7" font="font6" id="p14_t132" reading_order_no="131" segment_no="28" tag_type="text">nition,” in 2019 International Conference on Robotics and Automation</text>
<text top="507" left="334" width="91" height="7" font="font35" id="p14_t133" reading_order_no="132" segment_no="28" tag_type="text">(ICRA) , 2019, pp. 319–324.</text>
<text top="516" left="316" width="247" height="7" font="font6" id="p14_t134" reading_order_no="133" segment_no="31" tag_type="text">[58] M. Shakeri and H. Zhang, “Illumination invariant representation of</text>
<text top="525" left="334" width="230" height="7" font="font6" id="p14_t135" reading_order_no="134" segment_no="31" tag_type="text">natural images for visual place recognition,” in 2016 IEEE/RSJ Inter-</text>
<text top="534" left="334" width="229" height="7" font="font35" id="p14_t136" reading_order_no="135" segment_no="31" tag_type="text">national Conference on Intelligent Robots and Systems (IROS) , 2016,</text>
<text top="543" left="334" width="43" height="7" font="font6" id="p14_t137" reading_order_no="136" segment_no="31" tag_type="text">pp. 466–472.</text>
<text top="552" left="316" width="247" height="7" font="font6" id="p14_t138" reading_order_no="137" segment_no="33" tag_type="text">[59] T. Naseer, G. L. Oliveira, T. Brox, and W. Burgard, “Semantics-aware</text>
<text top="561" left="334" width="229" height="7" font="font6" id="p14_t139" reading_order_no="138" segment_no="33" tag_type="text">visual localization under challenging perceptual conditions,” in 2017</text>
<text top="570" left="334" width="229" height="7" font="font35" id="p14_t140" reading_order_no="139" segment_no="33" tag_type="text">IEEE International Conference on Robotics and Automation (ICRA) .</text>
<text top="579" left="334" width="93" height="7" font="font6" id="p14_t141" reading_order_no="140" segment_no="33" tag_type="text">IEEE, 2017, pp. 2614–2620.</text>
<text top="588" left="316" width="247" height="7" font="font6" id="p14_t142" reading_order_no="141" segment_no="35" tag_type="text">[60] Yang Liu and Hong Zhang, “Visual loop closure detection with a</text>
<text top="597" left="334" width="229" height="7" font="font6" id="p14_t143" reading_order_no="142" segment_no="35" tag_type="text">compact image descriptor,” in 2012 IEEE/RSJ International Conference</text>
<text top="606" left="334" width="187" height="7" font="font35" id="p14_t144" reading_order_no="143" segment_no="35" tag_type="text">on Intelligent Robots and Systems , 2012, pp. 1051–1056.</text>
<text top="615" left="316" width="247" height="7" font="font6" id="p14_t145" reading_order_no="144" segment_no="37" tag_type="text">[61] S. Lowry and M. J. Milford, “Supervised and unsupervised linear learn-</text>
<text top="624" left="334" width="229" height="7" font="font6" id="p14_t146" reading_order_no="145" segment_no="37" tag_type="text">ing techniques for visual place recognition in changing environments,”</text>
<text top="633" left="334" width="220" height="7" font="font35" id="p14_t147" reading_order_no="146" segment_no="37" tag_type="text">IEEE Transactions on Robotics , vol. 32, no. 3, pp. 600–613, 2016.</text>
<text top="642" left="316" width="247" height="7" font="font6" id="p14_t148" reading_order_no="147" segment_no="38" tag_type="text">[62] S. Schubert, P. Neubert, and P. Protzel, “Unsupervised learning methods</text>
<text top="651" left="334" width="229" height="7" font="font6" id="p14_t149" reading_order_no="148" segment_no="38" tag_type="text">for visual place recognition in discretely and continuously changing</text>
<text top="660" left="334" width="229" height="7" font="font6" id="p14_t150" reading_order_no="149" segment_no="38" tag_type="text">environments,” in 2020 IEEE International Conference on Robotics</text>
<text top="669" left="334" width="154" height="7" font="font35" id="p14_t151" reading_order_no="150" segment_no="38" tag_type="text">and Automation (ICRA) , 2020, pp. 4372–4378.</text>
<text top="678" left="316" width="247" height="7" font="font6" id="p14_t152" reading_order_no="151" segment_no="40" tag_type="text">[63] S. Garg, N. Suenderhauf, and M. Milford, “Semantic–geometric visual</text>
<text top="687" left="334" width="229" height="7" font="font6" id="p14_t153" reading_order_no="152" segment_no="40" tag_type="text">place recognition: a new perspective for reconciling opposing views,”</text>
<text top="696" left="334" width="229" height="7" font="font35" id="p14_t154" reading_order_no="153" segment_no="40" tag_type="text">The International Journal of Robotics Research , p. 0278364919839761,</text>
<text top="705" left="334" width="18" height="7" font="font6" id="p14_t155" reading_order_no="154" segment_no="40" tag_type="text">2019.</text>
<text top="714" left="316" width="247" height="7" font="font6" id="p14_t156" reading_order_no="155" segment_no="43" tag_type="text">[64] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, J. J. Yebes, and S. G´amez,</text>
<text top="723" left="334" width="229" height="7" font="font6" id="p14_t157" reading_order_no="156" segment_no="43" tag_type="text">“Bidirectional loop closure detection on panoramas for visual nav-</text>
<text top="732" left="334" width="229" height="7" font="font6" id="p14_t158" reading_order_no="157" segment_no="43" tag_type="text">igation,” in 2014 IEEE Intelligent Vehicles Symposium Proceedings .</text>
<text top="740" left="334" width="93" height="7" font="font6" id="p14_t159" reading_order_no="158" segment_no="43" tag_type="text">IEEE, 2014, pp. 1378–1383.</text>
</page>
<page number="15" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p15_t1" reading_order_no="0" segment_no="0" tag_type="text">15</text>
<text top="59" left="53" width="247" height="7" font="font6" id="p15_t2" reading_order_no="1" segment_no="1" tag_type="text">[65] M. Angelina Uy and G. Hee Lee, “Pointnetvlad: Deep point cloud</text>
<text top="68" left="71" width="229" height="7" font="font6" id="p15_t3" reading_order_no="2" segment_no="1" tag_type="text">based retrieval for large-scale place recognition,” in Proceedings of the</text>
<text top="77" left="71" width="229" height="7" font="font35" id="p15_t4" reading_order_no="3" segment_no="1" tag_type="text">IEEE Conference on Computer Vision and Pattern Recognition , 2018,</text>
<text top="86" left="71" width="51" height="7" font="font6" id="p15_t5" reading_order_no="4" segment_no="1" tag_type="text">pp. 4470–4479.</text>
<text top="95" left="53" width="247" height="7" font="font6" id="p15_t6" reading_order_no="5" segment_no="3" tag_type="text">[66] L. Yu, A. Jacobson, and M. Milford, “Rhythmic representations:</text>
<text top="104" left="71" width="229" height="7" font="font6" id="p15_t7" reading_order_no="6" segment_no="3" tag_type="text">Learning periodic patterns for scalable place recognition at a sublinear</text>
<text top="113" left="71" width="229" height="7" font="font6" id="p15_t8" reading_order_no="7" segment_no="3" tag_type="text">storage cost,” IEEE Robotics and Automation Letters , vol. 3, no. 2, pp.</text>
<text top="122" left="71" width="51" height="7" font="font6" id="p15_t9" reading_order_no="8" segment_no="3" tag_type="text">811–818, 2018.</text>
<text top="132" left="53" width="247" height="7" font="font6" id="p15_t10" reading_order_no="9" segment_no="5" tag_type="text">[67] D. Doan, Y. Latif, T. Chin, Y. Liu, T. Do, and I. Reid, “Scalable</text>
<text top="141" left="71" width="229" height="7" font="font6" id="p15_t11" reading_order_no="10" segment_no="5" tag_type="text">place recognition under appearance change for autonomous driving,” in</text>
<text top="150" left="71" width="229" height="7" font="font35" id="p15_t12" reading_order_no="11" segment_no="5" tag_type="text">2019 IEEE/CVF International Conference on Computer Vision (ICCV) ,</text>
<text top="159" left="71" width="71" height="7" font="font6" id="p15_t13" reading_order_no="12" segment_no="5" tag_type="text">2019, pp. 9318–9327.</text>
<text top="168" left="53" width="247" height="7" font="font6" id="p15_t14" reading_order_no="13" segment_no="8" tag_type="text">[68] T.-T. Do, T. Hoang, D.-K. Le Tan, A.-D. Doan, and N.-M. Cheung,</text>
<text top="177" left="71" width="229" height="7" font="font6" id="p15_t15" reading_order_no="14" segment_no="8" tag_type="text">“Compact hash code learning with binary deep neural network,” IEEE</text>
<text top="186" left="71" width="213" height="7" font="font35" id="p15_t16" reading_order_no="15" segment_no="8" tag_type="text">Transactions on Multimedia , vol. 22, no. 4, pp. 992–1004, 2019.</text>
<text top="195" left="53" width="247" height="7" font="font6" id="p15_t17" reading_order_no="16" segment_no="9" tag_type="text">[69] Y. Latif, A. D. Doan, T. J. Chin, and I. Reid, “Sprint: Subgraph place</text>
<text top="204" left="71" width="229" height="7" font="font6" id="p15_t18" reading_order_no="17" segment_no="9" tag_type="text">recognition for intelligent transportation,” in 2020 IEEE International</text>
<text top="213" left="71" width="229" height="7" font="font35" id="p15_t19" reading_order_no="18" segment_no="9" tag_type="text">Conference on Robotics and Automation (ICRA) , 2020, pp. 5408–5414.</text>
<text top="223" left="53" width="247" height="7" font="font6" id="p15_t20" reading_order_no="19" segment_no="11" tag_type="text">[70] S. Garg and M. Milford, “Fast, compact and highly scalable visual</text>
<text top="232" left="71" width="229" height="7" font="font6" id="p15_t21" reading_order_no="20" segment_no="11" tag_type="text">place recognition through sequence-based matching of overloaded</text>
<text top="240" left="71" width="229" height="8" font="font6" id="p15_t22" reading_order_no="21" segment_no="11" tag_type="text">representations,” in 2020 IEEE International Conference on Robotics</text>
<text top="249" left="71" width="154" height="8" font="font35" id="p15_t23" reading_order_no="22" segment_no="11" tag_type="text">and Automation (ICRA) , 2020, pp. 3341–3348.</text>
<text top="259" left="53" width="247" height="7" font="font6" id="p15_t24" reading_order_no="23" segment_no="13" tag_type="text">[71] M. Cummins and P. Newman, “Appearance-only slam at large scale</text>
<text top="268" left="71" width="229" height="7" font="font6" id="p15_t25" reading_order_no="24" segment_no="13" tag_type="text">with fab-map 2.0,” The International Journal of Robotics Research ,</text>
<text top="277" left="71" width="120" height="7" font="font6" id="p15_t26" reading_order_no="25" segment_no="13" tag_type="text">vol. 30, no. 9, pp. 1100–1123, 2011.</text>
<text top="286" left="53" width="247" height="7" font="font6" id="p15_t27" reading_order_no="26" segment_no="15" tag_type="text">[72] T. Cieslewski and D. Scaramuzza, “Efficient decentralized visual place</text>
<text top="295" left="71" width="229" height="7" font="font6" id="p15_t28" reading_order_no="27" segment_no="15" tag_type="text">recognition using a distributed inverted index,” IEEE Robotics and</text>
<text top="304" left="71" width="175" height="7" font="font35" id="p15_t29" reading_order_no="28" segment_no="15" tag_type="text">Automation Letters , vol. 2, no. 2, pp. 640–647, 2017.</text>
<text top="314" left="53" width="247" height="7" font="font6" id="p15_t30" reading_order_no="29" segment_no="17" tag_type="text">[73] M. Mohan, D. G´alvez-L´opez, C. Monteleoni, and G. Sibley, “Envi-</text>
<text top="322" left="71" width="229" height="8" font="font6" id="p15_t31" reading_order_no="30" segment_no="17" tag_type="text">ronment selection and hierarchical place recognition,” in 2015 IEEE</text>
<text top="331" left="71" width="229" height="8" font="font35" id="p15_t32" reading_order_no="31" segment_no="17" tag_type="text">International Conference on Robotics and Automation (ICRA) . IEEE,</text>
<text top="340" left="71" width="71" height="7" font="font6" id="p15_t33" reading_order_no="32" segment_no="17" tag_type="text">2015, pp. 5487–5494.</text>
<text top="350" left="53" width="247" height="7" font="font6" id="p15_t34" reading_order_no="33" segment_no="19" tag_type="text">[74] K. MacTavish and T. D. Barfoot, “Towards hierarchical place recog-</text>
<text top="359" left="71" width="229" height="7" font="font6" id="p15_t35" reading_order_no="34" segment_no="19" tag_type="text">nition for long-term autonomy,” in ICRA Workshop on Visual Place</text>
<text top="368" left="71" width="180" height="7" font="font35" id="p15_t36" reading_order_no="35" segment_no="19" tag_type="text">Recognition in Changing Environments , 2014, pp. 1–6.</text>
<text top="377" left="53" width="247" height="7" font="font6" id="p15_t37" reading_order_no="36" segment_no="20" tag_type="text">[75] L. Han and L. Fang, “Mild: Multi-index hashing for appearance based</text>
<text top="386" left="71" width="229" height="7" font="font6" id="p15_t38" reading_order_no="37" segment_no="20" tag_type="text">loop closure detection,” in 2017 IEEE International Conference on</text>
<text top="395" left="71" width="166" height="7" font="font35" id="p15_t39" reading_order_no="38" segment_no="20" tag_type="text">Multimedia and Expo (ICME) , 2017, pp. 139–144.</text>
<text top="404" left="53" width="247" height="7" font="font6" id="p15_t40" reading_order_no="39" segment_no="22" tag_type="text">[76] P. Hansen and B. Browning, “Visual place recognition using hmm</text>
<text top="413" left="71" width="229" height="7" font="font6" id="p15_t41" reading_order_no="40" segment_no="22" tag_type="text">sequence matching,” in 2014 IEEE/RSJ International Conference on</text>
<text top="422" left="71" width="103" height="7" font="font35" id="p15_t42" reading_order_no="41" segment_no="22" tag_type="text">Intelligent Robots and Systems .</text>
<text top="422" left="182" width="93" height="7" font="font6" id="p15_t43" reading_order_no="42" segment_no="22" tag_type="text">IEEE, 2014, pp. 4549–4555.</text>
<text top="432" left="53" width="247" height="7" font="font6" id="p15_t44" reading_order_no="43" segment_no="24" tag_type="text">[77] R. Dub´e, A. Cramariuc, D. Dugas, H. Sommer, M. Dymczyk, J. Nieto,</text>
<text top="441" left="71" width="229" height="7" font="font6" id="p15_t45" reading_order_no="44" segment_no="24" tag_type="text">R. Siegwart, and C. Cadena, “Segmap: Segment-based mapping and</text>
<text top="450" left="71" width="229" height="7" font="font6" id="p15_t46" reading_order_no="45" segment_no="24" tag_type="text">localization using data-driven descriptors,” The International Journal</text>
<text top="459" left="71" width="193" height="7" font="font35" id="p15_t47" reading_order_no="46" segment_no="24" tag_type="text">of Robotics Research , vol. 39, no. 2-3, pp. 339–355, 2020.</text>
<text top="468" left="53" width="247" height="7" font="font6" id="p15_t48" reading_order_no="47" segment_no="26" tag_type="text">[78] D. L. Rizzini, F. Galasso, and S. Caselli, “Geometric relation distri-</text>
<text top="477" left="71" width="229" height="7" font="font6" id="p15_t49" reading_order_no="48" segment_no="26" tag_type="text">bution for place recognition,” IEEE Robotics and Automation Letters ,</text>
<text top="486" left="71" width="108" height="7" font="font6" id="p15_t50" reading_order_no="49" segment_no="26" tag_type="text">vol. 4, no. 2, pp. 523–529, 2019.</text>
<text top="495" left="53" width="247" height="7" font="font6" id="p15_t51" reading_order_no="50" segment_no="28" tag_type="text">[79] C. Premebida, D. R. Faria, and U. Nunes, “Dynamic bayesian network</text>
<text top="504" left="71" width="229" height="7" font="font6" id="p15_t52" reading_order_no="51" segment_no="28" tag_type="text">for semantic place classification in mobile robotics,” Autonomous</text>
<text top="513" left="71" width="147" height="7" font="font35" id="p15_t53" reading_order_no="52" segment_no="28" tag_type="text">Robots , vol. 41, no. 5, pp. 1161–1172, 2017.</text>
<text top="523" left="53" width="247" height="7" font="font6" id="p15_t54" reading_order_no="53" segment_no="30" tag_type="text">[80] F. Cao, Y. Zhuang, H. Zhang, and W. Wang, “Robust place recognition</text>
<text top="532" left="71" width="229" height="7" font="font6" id="p15_t55" reading_order_no="54" segment_no="30" tag_type="text">and loop closing in laser-based slam for ugvs in urban environments,”</text>
<text top="541" left="71" width="201" height="7" font="font35" id="p15_t56" reading_order_no="55" segment_no="30" tag_type="text">IEEE Sensors Journal , vol. 18, no. 10, pp. 4242–4252, 2018.</text>
<text top="550" left="53" width="247" height="7" font="font6" id="p15_t57" reading_order_no="56" segment_no="32" tag_type="text">[81] S¸. S˘aftescu, M. Gadd, D. De Martini, D. Barnes, and P. Newman,</text>
<text top="559" left="71" width="229" height="7" font="font6" id="p15_t58" reading_order_no="57" segment_no="32" tag_type="text">“Kidnapped radar: Topological radar localisation using rotationally-</text>
<text top="568" left="71" width="218" height="7" font="font6" id="p15_t59" reading_order_no="58" segment_no="32" tag_type="text">invariant metric learning,” arXiv preprint arXiv:2001.09438 , 2020.</text>
<text top="577" left="53" width="247" height="7" font="font6" id="p15_t60" reading_order_no="59" segment_no="34" tag_type="text">[82] T. Y. Tang, D. De Martini, D. Barnes, and P. Newman, “Rsl-net:</text>
<text top="586" left="71" width="229" height="7" font="font6" id="p15_t61" reading_order_no="60" segment_no="34" tag_type="text">Localising in satellite images from a radar on the ground,” IEEE</text>
<text top="595" left="71" width="229" height="7" font="font35" id="p15_t62" reading_order_no="61" segment_no="34" tag_type="text">Robotics and Automation Letters , vol. 5, no. 2, pp. 1087–1094, 2020.</text>
<text top="605" left="53" width="247" height="7" font="font6" id="p15_t63" reading_order_no="62" segment_no="35" tag_type="text">[83] T. Cieslewski, E. Stumm, A. Gawel, M. Bosse, S. Lynen, and R. Sieg-</text>
<text top="614" left="71" width="229" height="7" font="font6" id="p15_t64" reading_order_no="63" segment_no="35" tag_type="text">wart, “Point cloud descriptors for place recognition using sparse visual</text>
<text top="623" left="71" width="229" height="7" font="font6" id="p15_t65" reading_order_no="64" segment_no="35" tag_type="text">information,” in 2016 IEEE International Conference on Robotics and</text>
<text top="632" left="71" width="139" height="7" font="font35" id="p15_t66" reading_order_no="65" segment_no="35" tag_type="text">Automation (ICRA) , 2016, pp. 4830–4836.</text>
<text top="641" left="53" width="247" height="7" font="font6" id="p15_t67" reading_order_no="66" segment_no="37" tag_type="text">[84] S. Campbell, N. O’Mahony, L. Krpalcova, D. Riordan, J. Walsh,</text>
<text top="650" left="71" width="229" height="7" font="font6" id="p15_t68" reading_order_no="67" segment_no="37" tag_type="text">A. Murphy, and C. Ryan, “Sensor technology in autonomous vehicles :</text>
<text top="659" left="71" width="229" height="7" font="font6" id="p15_t69" reading_order_no="68" segment_no="37" tag_type="text">A review,” in 2018 29th Irish Signals and Systems Conference (ISSC) ,</text>
<text top="668" left="71" width="47" height="7" font="font6" id="p15_t70" reading_order_no="69" segment_no="37" tag_type="text">2018, pp. 1–4.</text>
<text top="677" left="53" width="247" height="7" font="font6" id="p15_t71" reading_order_no="70" segment_no="39" tag_type="text">[85] A. Broggi, P. Grisleri, and P. Zani, “Sensors technologies for intelligent</text>
<text top="686" left="71" width="229" height="7" font="font6" id="p15_t72" reading_order_no="71" segment_no="39" tag_type="text">vehicles perception systems: A comparison between vision and 3d-</text>
<text top="695" left="71" width="229" height="7" font="font6" id="p15_t73" reading_order_no="72" segment_no="39" tag_type="text">lidar,” in 16th International IEEE Conference on Intelligent Transporta-</text>
<text top="704" left="71" width="152" height="7" font="font35" id="p15_t74" reading_order_no="73" segment_no="39" tag_type="text">tion Systems (ITSC 2013) , 2013, pp. 887–892.</text>
<text top="714" left="53" width="247" height="7" font="font6" id="p15_t75" reading_order_no="74" segment_no="41" tag_type="text">[86] C. Cadena, D. G´alvez-L´opez, F. Ramos, J. D. Tard´os, and J. Neira,</text>
<text top="723" left="71" width="229" height="7" font="font6" id="p15_t76" reading_order_no="75" segment_no="41" tag_type="text">“Robust place recognition with stereo cameras,” in 2010 IEEE/RSJ</text>
<text top="732" left="71" width="200" height="7" font="font35" id="p15_t77" reading_order_no="76" segment_no="41" tag_type="text">International Conference on Intelligent Robots and Systems .</text>
<text top="732" left="281" width="19" height="7" font="font6" id="p15_t78" reading_order_no="77" segment_no="41" tag_type="text">IEEE,</text>
<text top="740" left="71" width="71" height="7" font="font6" id="p15_t79" reading_order_no="78" segment_no="41" tag_type="text">2010, pp. 5182–5189.</text>
<text top="59" left="316" width="247" height="7" font="font6" id="p15_t80" reading_order_no="79" segment_no="2" tag_type="text">[87] T. Morris, F. Dayoub, P. Corke, G. Wyeth, and B. Upcroft, “Multiple</text>
<text top="68" left="334" width="229" height="7" font="font6" id="p15_t81" reading_order_no="80" segment_no="2" tag_type="text">map hypotheses for planning and navigating in non-stationary envi-</text>
<text top="77" left="334" width="229" height="7" font="font6" id="p15_t82" reading_order_no="81" segment_no="2" tag_type="text">ronments,” in 2014 IEEE International Conference on Robotics and</text>
<text top="86" left="334" width="65" height="7" font="font35" id="p15_t83" reading_order_no="82" segment_no="2" tag_type="text">Automation (ICRA) .</text>
<text top="86" left="407" width="93" height="7" font="font6" id="p15_t84" reading_order_no="83" segment_no="2" tag_type="text">IEEE, 2014, pp. 2765–2770.</text>
<text top="96" left="316" width="247" height="7" font="font6" id="p15_t85" reading_order_no="84" segment_no="4" tag_type="text">[88] D. Han, Y. Hwang, N. Kim, and Y. Choi, “Multispectral domain</text>
<text top="105" left="334" width="229" height="7" font="font6" id="p15_t86" reading_order_no="85" segment_no="4" tag_type="text">invariant image for retrieval-based place recognition,” in 2020 IEEE</text>
<text top="114" left="334" width="229" height="7" font="font35" id="p15_t87" reading_order_no="86" segment_no="4" tag_type="text">International Conference on Robotics and Automation (ICRA) , 2020,</text>
<text top="123" left="334" width="51" height="7" font="font6" id="p15_t88" reading_order_no="87" segment_no="4" tag_type="text">pp. 9271–9277.</text>
<text top="133" left="316" width="247" height="7" font="font6" id="p15_t89" reading_order_no="88" segment_no="6" tag_type="text">[89] T. Fischer and M. J. Milford, “Event-based visual place recognition</text>
<text top="142" left="334" width="229" height="7" font="font6" id="p15_t90" reading_order_no="89" segment_no="6" tag_type="text">with ensembles of temporal windows,” IEEE Robotics and Automation</text>
<text top="151" left="334" width="45" height="7" font="font35" id="p15_t91" reading_order_no="90" segment_no="6" tag_type="text">Letters , 2020.</text>
<text top="160" left="316" width="247" height="7" font="font6" id="p15_t92" reading_order_no="91" segment_no="7" tag_type="text">[90] H. Yu, H.-W. Chae, and J.-B. Song, “Place recognition based on surface</text>
<text top="169" left="334" width="229" height="8" font="font6" id="p15_t93" reading_order_no="92" segment_no="7" tag_type="text">graph for a mobile robot,” in 2017 14th International Conference on</text>
<text top="178" left="334" width="229" height="8" font="font35" id="p15_t94" reading_order_no="93" segment_no="7" tag_type="text">Ubiquitous Robots and Ambient Intelligence (URAI) . IEEE, 2017, pp.</text>
<text top="187" left="334" width="30" height="7" font="font6" id="p15_t95" reading_order_no="94" segment_no="7" tag_type="text">342–346.</text>
<text top="197" left="316" width="247" height="7" font="font6" id="p15_t96" reading_order_no="95" segment_no="10" tag_type="text">[91] D. Scaramuzza, F. Fraundorfer, and M. Pollefeys, “Closing the loop</text>
<text top="206" left="334" width="229" height="7" font="font6" id="p15_t97" reading_order_no="96" segment_no="10" tag_type="text">in appearance-guided omnidirectional visual odometry by using vo-</text>
<text top="215" left="334" width="229" height="7" font="font6" id="p15_t98" reading_order_no="97" segment_no="10" tag_type="text">cabulary trees,” Robotics and Autonomous Systems , vol. 58, no. 6, pp.</text>
<text top="224" left="334" width="51" height="7" font="font6" id="p15_t99" reading_order_no="98" segment_no="10" tag_type="text">820–827, 2010.</text>
<text top="234" left="316" width="247" height="7" font="font6" id="p15_t100" reading_order_no="99" segment_no="12" tag_type="text">[92] L. Sless, G. Cohen, B. E. Shlomo, and S. Oron, “Self supervised</text>
<text top="243" left="334" width="229" height="7" font="font6" id="p15_t101" reading_order_no="100" segment_no="12" tag_type="text">occupancy grid learning from sparse radar for autonomous driving,”</text>
<text top="252" left="334" width="131" height="7" font="font35" id="p15_t102" reading_order_no="101" segment_no="12" tag_type="text">arXiv preprint arXiv:1904.00415 , 2019.</text>
<text top="262" left="316" width="247" height="7" font="font6" id="p15_t103" reading_order_no="102" segment_no="14" tag_type="text">[93] J. Dickmann, J. Klappstein, M. Hahn, N. Appenrodt, H.-L. Bloecher,</text>
<text top="271" left="334" width="229" height="7" font="font6" id="p15_t104" reading_order_no="103" segment_no="14" tag_type="text">K. Werber, and A. Sailer, “Automotive radar the key technology for</text>
<text top="280" left="334" width="229" height="7" font="font6" id="p15_t105" reading_order_no="104" segment_no="14" tag_type="text">autonomous driving: From detection and ranging to environmental</text>
<text top="289" left="334" width="229" height="7" font="font6" id="p15_t106" reading_order_no="105" segment_no="14" tag_type="text">understanding,” in 2016 IEEE Radar Conference (RadarConf) . IEEE,</text>
<text top="298" left="334" width="47" height="7" font="font6" id="p15_t107" reading_order_no="106" segment_no="14" tag_type="text">2016, pp. 1–6.</text>
<text top="308" left="316" width="247" height="7" font="font6" id="p15_t108" reading_order_no="107" segment_no="16" tag_type="text">[94] N. Merrill and G. Huang, “Lightweight unsupervised deep loop clo-</text>
<text top="317" left="334" width="152" height="7" font="font6" id="p15_t109" reading_order_no="108" segment_no="16" tag_type="text">sure,” arXiv preprint arXiv:1805.07703 , 2018.</text>
<text top="327" left="316" width="247" height="7" font="font6" id="p15_t110" reading_order_no="109" segment_no="18" tag_type="text">[95] S. Saftescu, M. Gadd, D. D. Martini, D. Barnes, and P. Newman,</text>
<text top="336" left="334" width="229" height="7" font="font6" id="p15_t111" reading_order_no="110" segment_no="18" tag_type="text">“Kidnapped radar: Topological radar localisation using rotationally-</text>
<text top="345" left="334" width="229" height="7" font="font6" id="p15_t112" reading_order_no="111" segment_no="18" tag_type="text">invariant metric learning,” in 2020 IEEE International Conference</text>
<text top="354" left="334" width="229" height="7" font="font35" id="p15_t113" reading_order_no="112" segment_no="18" tag_type="text">on Robotics and Automation, ICRA 2020, Paris, France, May 31 -</text>
<text top="362" left="334" width="57" height="8" font="font35" id="p15_t114" reading_order_no="113" segment_no="18" tag_type="text">August 31, 2020 .</text>
<text top="362" left="399" width="164" height="7" font="font6" id="p15_t115" reading_order_no="114" segment_no="18" tag_type="text">IEEE, 2020, pp. 4358–4364. [Online]. Available:</text>
<text top="371" left="334" width="162" height="7" font="font6" id="p15_t116" reading_order_no="115" segment_no="18" tag_type="text">https://doi.org/10.1109/ICRA40945.2020.9196682</text>
<text top="381" left="316" width="247" height="7" font="font6" id="p15_t117" reading_order_no="116" segment_no="21" tag_type="text">[96] J. Yu, C. Zhu, J. Zhang, Q. Huang, and D. Tao, “Spatial pyramid-</text>
<text top="390" left="334" width="229" height="7" font="font6" id="p15_t118" reading_order_no="117" segment_no="21" tag_type="text">enhanced netvlad with weighted triplet loss for place recognition,”</text>
<text top="399" left="334" width="229" height="7" font="font35" id="p15_t119" reading_order_no="118" segment_no="21" tag_type="text">IEEE transactions on Neural Networks and Learning Systems , vol. 31,</text>
<text top="408" left="334" width="85" height="7" font="font6" id="p15_t120" reading_order_no="119" segment_no="21" tag_type="text">no. 2, pp. 661–674, 2019.</text>
<text top="418" left="316" width="247" height="7" font="font6" id="p15_t121" reading_order_no="120" segment_no="23" tag_type="text">[97] K. Simonyan and A. Zisserman, “Very deep convolutional networks</text>
<text top="427" left="334" width="229" height="7" font="font6" id="p15_t122" reading_order_no="121" segment_no="23" tag_type="text">for large-scale image recognition,” in International Conference on</text>
<text top="436" left="334" width="106" height="7" font="font35" id="p15_t123" reading_order_no="122" segment_no="23" tag_type="text">Learning Representations , 2015.</text>
<text top="446" left="316" width="247" height="7" font="font6" id="p15_t124" reading_order_no="123" segment_no="25" tag_type="text">[98] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov,</text>
<text top="455" left="334" width="229" height="7" font="font6" id="p15_t125" reading_order_no="124" segment_no="25" tag_type="text">D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with</text>
<text top="464" left="334" width="229" height="7" font="font6" id="p15_t126" reading_order_no="125" segment_no="25" tag_type="text">convolutions,” in 2015 IEEE Conference on Computer Vision and</text>
<text top="473" left="334" width="145" height="7" font="font35" id="p15_t127" reading_order_no="126" segment_no="25" tag_type="text">Pattern Recognition (CVPR) , 2015, pp. 1–9.</text>
<text top="483" left="316" width="247" height="7" font="font6" id="p15_t128" reading_order_no="127" segment_no="27" tag_type="text">[99] H. Jegou, F. Perronnin, M. Douze, J. S´anchez, P. Perez, and C. Schmid,</text>
<text top="492" left="334" width="229" height="7" font="font6" id="p15_t129" reading_order_no="128" segment_no="27" tag_type="text">“Aggregating local image descriptors into compact codes,” IEEE Trans-</text>
<text top="501" left="334" width="229" height="7" font="font35" id="p15_t130" reading_order_no="129" segment_no="27" tag_type="text">actions on Pattern Analysis and Machine Intelligence , vol. 34, no. 9,</text>
<text top="510" left="334" width="71" height="7" font="font6" id="p15_t131" reading_order_no="130" segment_no="27" tag_type="text">pp. 1704–1716, 2011.</text>
<text top="520" left="312" width="251" height="7" font="font6" id="p15_t132" reading_order_no="131" segment_no="29" tag_type="text">[100] H. J´egou and O. Chum, “Negative evidences and co-occurences in</text>
<text top="529" left="334" width="229" height="7" font="font6" id="p15_t133" reading_order_no="132" segment_no="29" tag_type="text">image retrieval: The benefit of pca and whitening,” in European</text>
<text top="538" left="334" width="102" height="7" font="font35" id="p15_t134" reading_order_no="133" segment_no="29" tag_type="text">conference on computer vision .</text>
<text top="538" left="444" width="95" height="7" font="font6" id="p15_t135" reading_order_no="134" segment_no="29" tag_type="text">Springer, 2012, pp. 774–787.</text>
<text top="547" left="312" width="251" height="7" font="font6" id="p15_t136" reading_order_no="135" segment_no="31" tag_type="text">[101] H. Jegou, M. Douze, and C. Schmid, “Hamming embedding and</text>
<text top="556" left="334" width="229" height="8" font="font6" id="p15_t137" reading_order_no="136" segment_no="31" tag_type="text">weak geometric consistency for large scale image search,” in European</text>
<text top="565" left="334" width="102" height="7" font="font35" id="p15_t138" reading_order_no="137" segment_no="31" tag_type="text">conference on computer vision .</text>
<text top="565" left="444" width="95" height="7" font="font6" id="p15_t139" reading_order_no="138" segment_no="31" tag_type="text">Springer, 2008, pp. 304–317.</text>
<text top="575" left="312" width="251" height="7" font="font6" id="p15_t140" reading_order_no="139" segment_no="33" tag_type="text">[102] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object</text>
<text top="584" left="334" width="229" height="7" font="font6" id="p15_t141" reading_order_no="140" segment_no="33" tag_type="text">retrieval with large vocabularies and fast spatial matching,” in 2007</text>
<text top="593" left="334" width="202" height="7" font="font35" id="p15_t142" reading_order_no="141" segment_no="33" tag_type="text">IEEE conference on computer vision and pattern recognition .</text>
<text top="593" left="544" width="19" height="7" font="font6" id="p15_t143" reading_order_no="142" segment_no="33" tag_type="text">IEEE,</text>
<text top="602" left="334" width="47" height="7" font="font6" id="p15_t144" reading_order_no="143" segment_no="33" tag_type="text">2007, pp. 1–8.</text>
<text top="612" left="312" width="251" height="7" font="font6" id="p15_t145" reading_order_no="144" segment_no="36" tag_type="text">[103] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Lost in</text>
<text top="621" left="334" width="229" height="7" font="font6" id="p15_t146" reading_order_no="145" segment_no="36" tag_type="text">quantization: Improving particular object retrieval in large scale image</text>
<text top="630" left="334" width="229" height="7" font="font6" id="p15_t147" reading_order_no="146" segment_no="36" tag_type="text">databases,” in 2008 IEEE Conference on Computer Vision and Pattern</text>
<text top="639" left="334" width="91" height="7" font="font35" id="p15_t148" reading_order_no="147" segment_no="36" tag_type="text">Recognition , 2008, pp. 1–8.</text>
<text top="649" left="312" width="251" height="7" font="font6" id="p15_t149" reading_order_no="148" segment_no="38" tag_type="text">[104] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, and E. Romera, “Fusion</text>
<text top="658" left="334" width="229" height="7" font="font6" id="p15_t150" reading_order_no="149" segment_no="38" tag_type="text">and binarization of cnn features for robust topological localization</text>
<text top="667" left="334" width="229" height="7" font="font6" id="p15_t151" reading_order_no="150" segment_no="38" tag_type="text">across seasons,” in 2016 IEEE/RSJ International Conference on In-</text>
<text top="676" left="334" width="195" height="7" font="font35" id="p15_t152" reading_order_no="151" segment_no="38" tag_type="text">telligent Robots and Systems (IROS) , 2016, pp. 4656–4663.</text>
<text top="686" left="312" width="251" height="7" font="font6" id="p15_t153" reading_order_no="152" segment_no="40" tag_type="text">[105] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, “Return</text>
<text top="695" left="334" width="229" height="7" font="font6" id="p15_t154" reading_order_no="153" segment_no="40" tag_type="text">of the devil in the details: Delving deep into convolutional nets,” in</text>
<text top="704" left="334" width="229" height="7" font="font35" id="p15_t155" reading_order_no="154" segment_no="40" tag_type="text">Proceedings of the British Machine Vision Conference . BMVA Press,</text>
<text top="713" left="334" width="18" height="7" font="font6" id="p15_t156" reading_order_no="155" segment_no="40" tag_type="text">2014.</text>
<text top="723" left="312" width="251" height="7" font="font6" id="p15_t157" reading_order_no="156" segment_no="42" tag_type="text">[106] X. Yang and K.-T. T. Cheng, “Local difference binary for ultrafast and</text>
<text top="732" left="334" width="229" height="7" font="font6" id="p15_t158" reading_order_no="157" segment_no="42" tag_type="text">distinctive feature description,” IEEE Transactions on Pattern Analysis</text>
<text top="740" left="334" width="200" height="8" font="font35" id="p15_t159" reading_order_no="158" segment_no="42" tag_type="text">and Machine Intelligence , vol. 36, no. 1, pp. 188–194, 2013.</text>
</page>
<page number="16" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p16_t1" reading_order_no="0" segment_no="0" tag_type="text">16</text>
<text top="59" left="49" width="251" height="7" font="font6" id="p16_t2" reading_order_no="1" segment_no="1" tag_type="text">[107] H. Badino, D. Huber, and T. Kanade, “Real-time topometric local-</text>
<text top="68" left="71" width="229" height="7" font="font6" id="p16_t3" reading_order_no="2" segment_no="1" tag_type="text">ization,” in 2012 IEEE International Conference on Robotics and</text>
<text top="77" left="71" width="39" height="7" font="font35" id="p16_t4" reading_order_no="3" segment_no="1" tag_type="text">Automation .</text>
<text top="77" left="118" width="93" height="7" font="font6" id="p16_t5" reading_order_no="4" segment_no="1" tag_type="text">IEEE, 2012, pp. 1635–1642.</text>
<text top="87" left="49" width="251" height="7" font="font6" id="p16_t6" reading_order_no="5" segment_no="4" tag_type="text">[108] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification</text>
<text top="96" left="71" width="229" height="7" font="font6" id="p16_t7" reading_order_no="6" segment_no="4" tag_type="text">with deep convolutional neural networks,” Communications of the</text>
<text top="105" left="71" width="126" height="7" font="font35" id="p16_t8" reading_order_no="7" segment_no="4" tag_type="text">ACM , vol. 60, no. 6, pp. 84–90, 2017.</text>
<text top="115" left="49" width="251" height="7" font="font6" id="p16_t9" reading_order_no="8" segment_no="6" tag_type="text">[109] A. J. Glover, W. P. Maddern, M. J. Milford, and G. F. Wyeth, “Fab-map</text>
<text top="124" left="71" width="229" height="7" font="font6" id="p16_t10" reading_order_no="9" segment_no="6" tag_type="text">+ ratslam: Appearance-based slam for multiple times of day,” in 2010</text>
<text top="132" left="71" width="229" height="8" font="font35" id="p16_t11" reading_order_no="10" segment_no="6" tag_type="text">IEEE International Conference on Robotics and Automation , 2010, pp.</text>
<text top="141" left="71" width="38" height="7" font="font6" id="p16_t12" reading_order_no="11" segment_no="6" tag_type="text">3507–3512.</text>
<text top="151" left="49" width="251" height="7" font="font6" id="p16_t13" reading_order_no="12" segment_no="8" tag_type="text">[110] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, “Places:</text>
<text top="160" left="71" width="229" height="7" font="font6" id="p16_t14" reading_order_no="13" segment_no="8" tag_type="text">A 10 million image database for scene recognition,” IEEE Transactions</text>
<text top="169" left="71" width="229" height="7" font="font35" id="p16_t15" reading_order_no="14" segment_no="8" tag_type="text">on Pattern Analysis and Machine Intelligence , vol. 40, no. 6, pp. 1452–</text>
<text top="178" left="71" width="39" height="7" font="font6" id="p16_t16" reading_order_no="15" segment_no="8" tag_type="text">1464, 2018.</text>
<text top="188" left="49" width="251" height="7" font="font6" id="p16_t17" reading_order_no="16" segment_no="10" tag_type="text">[111] N. S¨underhauf, S. Shirazi, A. Jacobson, F. Dayoub, E. Pepperell,</text>
<text top="197" left="71" width="229" height="7" font="font6" id="p16_t18" reading_order_no="17" segment_no="10" tag_type="text">B. Upcroft, and M. Milford, “Place recognition with convnet land-</text>
<text top="206" left="71" width="229" height="7" font="font6" id="p16_t19" reading_order_no="18" segment_no="10" tag_type="text">marks: Viewpoint-robust, condition-robust, training-free,” Proceedings</text>
<text top="215" left="71" width="146" height="7" font="font35" id="p16_t20" reading_order_no="19" segment_no="10" tag_type="text">of Robotics: Science and Systems XII , 2015.</text>
<text top="225" left="49" width="251" height="7" font="font6" id="p16_t21" reading_order_no="20" segment_no="12" tag_type="text">[112] S. Dasgupta, “Experiments with random projection,” in Proceedings of</text>
<text top="234" left="71" width="229" height="7" font="font35" id="p16_t22" reading_order_no="21" segment_no="12" tag_type="text">the 16th Conference on Uncertainty in Artificial Intelligence , ser. UAI</text>
<text top="243" left="71" width="13" height="7" font="font6" id="p16_t23" reading_order_no="22" segment_no="12" tag_type="text">’00.</text>
<text top="243" left="94" width="206" height="7" font="font6" id="p16_t24" reading_order_no="23" segment_no="12" tag_type="text">San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.,</text>
<text top="252" left="71" width="59" height="7" font="font6" id="p16_t25" reading_order_no="24" segment_no="12" tag_type="text">2000, p. 143–151.</text>
<text top="261" left="49" width="251" height="7" font="font6" id="p16_t26" reading_order_no="25" segment_no="14" tag_type="text">[113] Y. Kong, W. Liu, and Z. Chen, “Robust convnet landmark-based visual</text>
<text top="270" left="71" width="229" height="7" font="font6" id="p16_t27" reading_order_no="26" segment_no="14" tag_type="text">place recognition by optimizing landmark matching,” IEEE Access ,</text>
<text top="279" left="71" width="106" height="7" font="font6" id="p16_t28" reading_order_no="27" segment_no="14" tag_type="text">vol. 7, pp. 30 754–30 767, 2019.</text>
<text top="289" left="49" width="251" height="7" font="font6" id="p16_t29" reading_order_no="28" segment_no="16" tag_type="text">[114] M. Cheng, Z. Zhang, W. Lin, and P. Torr, “Bing: Binarized normed gra-</text>
<text top="298" left="71" width="229" height="7" font="font6" id="p16_t30" reading_order_no="29" segment_no="16" tag_type="text">dients for objectness estimation at 300fps,” in 2014 IEEE Conference</text>
<text top="307" left="71" width="223" height="7" font="font35" id="p16_t31" reading_order_no="30" segment_no="16" tag_type="text">on Computer Vision and Pattern Recognition , 2014, pp. 3286–3293.</text>
<text top="317" left="49" width="251" height="7" font="font6" id="p16_t32" reading_order_no="31" segment_no="18" tag_type="text">[115] E. Bingham and H. Mannila, “Random projection in dimensionality</text>
<text top="326" left="71" width="229" height="7" font="font6" id="p16_t33" reading_order_no="32" segment_no="18" tag_type="text">reduction: applications to image and text data,” in Proceedings of</text>
<text top="335" left="71" width="229" height="7" font="font35" id="p16_t34" reading_order_no="33" segment_no="18" tag_type="text">the seventh ACM SIGKDD international conference on Knowledge</text>
<text top="344" left="71" width="155" height="7" font="font35" id="p16_t35" reading_order_no="34" segment_no="18" tag_type="text">discovery and data mining , 2001, pp. 245–250.</text>
<text top="354" left="49" width="251" height="7" font="font6" id="p16_t36" reading_order_no="35" segment_no="20" tag_type="text">[116] G. L. Oliveira, W. Burgard, and T. Brox, “Efficient deep models</text>
<text top="363" left="71" width="229" height="7" font="font6" id="p16_t37" reading_order_no="36" segment_no="20" tag_type="text">for monocular road segmentation,” in 2016 IEEE/RSJ International</text>
<text top="372" left="71" width="229" height="7" font="font35" id="p16_t38" reading_order_no="37" segment_no="20" tag_type="text">Conference on Intelligent Robots and Systems (IROS) , 2016, pp. 4885–</text>
<text top="381" left="71" width="18" height="7" font="font6" id="p16_t39" reading_order_no="38" segment_no="20" tag_type="text">4891.</text>
<text top="390" left="49" width="251" height="7" font="font6" id="p16_t40" reading_order_no="39" segment_no="22" tag_type="text">[117] D. Achlioptas, “Database-friendly random projections: Johnson-</text>
<text top="399" left="71" width="229" height="7" font="font6" id="p16_t41" reading_order_no="40" segment_no="22" tag_type="text">lindenstrauss with binary coins,” Journal of computer and System</text>
<text top="408" left="71" width="145" height="7" font="font35" id="p16_t42" reading_order_no="41" segment_no="22" tag_type="text">Sciences , vol. 66, no. 4, pp. 671–687, 2003.</text>
<text top="418" left="49" width="251" height="7" font="font6" id="p16_t43" reading_order_no="42" segment_no="24" tag_type="text">[118] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-</text>
<text top="427" left="71" width="229" height="7" font="font6" id="p16_t44" reading_order_no="43" segment_no="24" tag_type="text">nenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset</text>
<text top="436" left="71" width="229" height="7" font="font6" id="p16_t45" reading_order_no="44" segment_no="24" tag_type="text">for semantic urban scene understanding,” in 2016 IEEE Conference on</text>
<text top="445" left="71" width="229" height="7" font="font35" id="p16_t46" reading_order_no="45" segment_no="24" tag_type="text">Computer Vision and Pattern Recognition (CVPR) , 2016, pp. 3213–</text>
<text top="454" left="71" width="18" height="7" font="font6" id="p16_t47" reading_order_no="46" segment_no="24" tag_type="text">3223.</text>
<text top="464" left="49" width="251" height="7" font="font6" id="p16_t48" reading_order_no="47" segment_no="26" tag_type="text">[119] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, “Virtualworlds as proxy for</text>
<text top="473" left="71" width="229" height="7" font="font6" id="p16_t49" reading_order_no="48" segment_no="26" tag_type="text">multi-object tracking analysis,” in 2016 IEEE Conference on Computer</text>
<text top="482" left="71" width="206" height="7" font="font35" id="p16_t50" reading_order_no="49" segment_no="26" tag_type="text">Vision and Pattern Recognition (CVPR) , 2016, pp. 4340–4349.</text>
<text top="492" left="49" width="251" height="7" font="font6" id="p16_t51" reading_order_no="50" segment_no="28" tag_type="text">[120] K. Simonyan and A. Zisserman, “Very deep convolutional networks for</text>
<text top="501" left="71" width="229" height="7" font="font6" id="p16_t52" reading_order_no="51" segment_no="28" tag_type="text">large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.</text>
<text top="510" left="49" width="251" height="7" font="font6" id="p16_t53" reading_order_no="52" segment_no="30" tag_type="text">[121] J. Sivic and A. Zisserman, “Video google: A text retrieval approach to</text>
<text top="519" left="71" width="116" height="7" font="font6" id="p16_t54" reading_order_no="53" segment_no="30" tag_type="text">object matching in videos,” in null .</text>
<text top="519" left="195" width="69" height="7" font="font6" id="p16_t55" reading_order_no="54" segment_no="30" tag_type="text">IEEE, 2003, p. 1470.</text>
<text top="529" left="49" width="251" height="7" font="font6" id="p16_t56" reading_order_no="55" segment_no="31" tag_type="text">[122] A. Khaliq, S. Ehsan, Z. Chen, M. Milford, and K. McDonald-Maier,</text>
<text top="538" left="71" width="229" height="7" font="font6" id="p16_t57" reading_order_no="56" segment_no="31" tag_type="text">“A holistic visual place recognition approach using lightweight cnns</text>
<text top="547" left="71" width="229" height="7" font="font6" id="p16_t58" reading_order_no="57" segment_no="31" tag_type="text">for significant viewpoint and appearance changes,” IEEE Transactions</text>
<text top="556" left="71" width="156" height="7" font="font35" id="p16_t59" reading_order_no="58" segment_no="31" tag_type="text">on Robotics , vol. 36, no. 2, pp. 561–569, 2020.</text>
<text top="566" left="49" width="251" height="7" font="font6" id="p16_t60" reading_order_no="59" segment_no="33" tag_type="text">[123] Z. Chen, F. Maffra, I. Sa, and M. Chli, “Only look once, mining</text>
<text top="575" left="71" width="229" height="7" font="font6" id="p16_t61" reading_order_no="60" segment_no="33" tag_type="text">distinctive landmarks from convnet for visual place recognition,” in</text>
<text top="584" left="71" width="229" height="7" font="font35" id="p16_t62" reading_order_no="61" segment_no="33" tag_type="text">2017 IEEE/RSJ International Conference on Intelligent Robots and</text>
<text top="593" left="71" width="52" height="7" font="font35" id="p16_t63" reading_order_no="62" segment_no="33" tag_type="text">Systems (IROS) .</text>
<text top="593" left="131" width="73" height="7" font="font6" id="p16_t64" reading_order_no="63" segment_no="33" tag_type="text">IEEE, 2017, pp. 9–16.</text>
<text top="603" left="49" width="251" height="7" font="font6" id="p16_t65" reading_order_no="64" segment_no="35" tag_type="text">[124] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification</text>
<text top="612" left="71" width="229" height="7" font="font6" id="p16_t66" reading_order_no="65" segment_no="35" tag_type="text">with deep convolutional neural networks,” in Advances in neural</text>
<text top="621" left="71" width="178" height="7" font="font35" id="p16_t67" reading_order_no="66" segment_no="35" tag_type="text">information processing systems , 2012, pp. 1097–1105.</text>
<text top="630" left="49" width="251" height="7" font="font6" id="p16_t68" reading_order_no="67" segment_no="37" tag_type="text">[125] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning</text>
<text top="639" left="71" width="229" height="7" font="font6" id="p16_t69" reading_order_no="68" segment_no="37" tag_type="text">deep features for scene recognition using places database,” in Advances</text>
<text top="648" left="71" width="202" height="7" font="font35" id="p16_t70" reading_order_no="69" segment_no="37" tag_type="text">in neural information processing systems , 2014, pp. 487–495.</text>
<text top="658" left="49" width="251" height="7" font="font6" id="p16_t71" reading_order_no="70" segment_no="39" tag_type="text">[126] D. Ravichandran, P. Pantel, and E. Hovy, “Randomized algorithms and</text>
<text top="667" left="71" width="229" height="7" font="font6" id="p16_t72" reading_order_no="71" segment_no="39" tag_type="text">nlp: Using locality sensitive hash functions for high speed noun clus-</text>
<text top="676" left="71" width="229" height="7" font="font6" id="p16_t73" reading_order_no="72" segment_no="39" tag_type="text">tering,” in Proceedings of the 43rd Annual Meeting of the Association</text>
<text top="685" left="71" width="199" height="7" font="font35" id="p16_t74" reading_order_no="73" segment_no="39" tag_type="text">for Computational Linguistics (ACL’05) , 2005, pp. 622–629.</text>
<text top="695" left="49" width="251" height="7" font="font6" id="p16_t75" reading_order_no="74" segment_no="42" tag_type="text">[127] C. L. Zitnick and P. Doll´ar, “Edge boxes: Locating object proposals</text>
<text top="704" left="71" width="191" height="7" font="font6" id="p16_t76" reading_order_no="75" segment_no="42" tag_type="text">from edges,” in European conference on computer vision .</text>
<text top="704" left="271" width="29" height="7" font="font6" id="p16_t77" reading_order_no="76" segment_no="42" tag_type="text">Springer,</text>
<text top="713" left="71" width="63" height="7" font="font6" id="p16_t78" reading_order_no="77" segment_no="42" tag_type="text">2014, pp. 391–405.</text>
<text top="723" left="49" width="251" height="7" font="font6" id="p16_t79" reading_order_no="78" segment_no="44" tag_type="text">[128] A. Torii, J. Sivic, M. Okutomi, and T. Pajdla, “Visual place recognition</text>
<text top="732" left="71" width="229" height="7" font="font6" id="p16_t80" reading_order_no="79" segment_no="44" tag_type="text">with repetitive structures,” in IEEE Transactions on Pattern Analysis</text>
<text top="740" left="71" width="212" height="8" font="font35" id="p16_t81" reading_order_no="80" segment_no="44" tag_type="text">and Machine Intelligence , vol. 37, no. 11, 2015, pp. 2346–2359.</text>
<text top="59" left="312" width="251" height="7" font="font6" id="p16_t82" reading_order_no="81" segment_no="2" tag_type="text">[129] C. Wu, R. Manmatha, A. J. Smola, and P. Kr¨ahenb¨uhl, “Sampling</text>
<text top="68" left="334" width="229" height="7" font="font6" id="p16_t83" reading_order_no="82" segment_no="2" tag_type="text">matters in deep embedding learning,” in 2017 IEEE International</text>
<text top="77" left="334" width="207" height="7" font="font35" id="p16_t84" reading_order_no="83" segment_no="2" tag_type="text">Conference on Computer Vision (ICCV) , 2017, pp. 2859–2867.</text>
<text top="86" left="312" width="251" height="7" font="font6" id="p16_t85" reading_order_no="84" segment_no="3" tag_type="text">[130] A. Torii, R. Arandjelovi´c, J. Sivic, M. Okutomi, and T. Pajdla, “24/7</text>
<text top="95" left="334" width="177" height="7" font="font6" id="p16_t86" reading_order_no="85" segment_no="3" tag_type="text">place recognition by view synthesis,” in CVPR , 2015.</text>
<text top="105" left="312" width="251" height="7" font="font6" id="p16_t87" reading_order_no="86" segment_no="5" tag_type="text">[131] K. Qiu, Y. Ai, B. Tian, B. Wang, and D. Cao, “Siamese-resnet:</text>
<text top="114" left="334" width="229" height="7" font="font6" id="p16_t88" reading_order_no="87" segment_no="5" tag_type="text">implementing loop closure detection based on siamese network,” in</text>
<text top="123" left="334" width="163" height="7" font="font35" id="p16_t89" reading_order_no="88" segment_no="5" tag_type="text">2018 IEEE Intelligent Vehicles Symposium (IV) .</text>
<text top="123" left="508" width="55" height="7" font="font6" id="p16_t90" reading_order_no="89" segment_no="5" tag_type="text">IEEE, 2018, pp.</text>
<text top="132" left="334" width="30" height="7" font="font6" id="p16_t91" reading_order_no="90" segment_no="5" tag_type="text">716–721.</text>
<text top="141" left="312" width="251" height="7" font="font6" id="p16_t92" reading_order_no="91" segment_no="7" tag_type="text">[132] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric</text>
<text top="150" left="334" width="229" height="7" font="font6" id="p16_t93" reading_order_no="92" segment_no="7" tag_type="text">discriminatively, with application to face verification,” in 2005 IEEE</text>
<text top="159" left="334" width="229" height="7" font="font35" id="p16_t94" reading_order_no="93" segment_no="7" tag_type="text">Computer Society Conference on Computer Vision and Pattern Recog-</text>
<text top="168" left="334" width="83" height="7" font="font35" id="p16_t95" reading_order_no="94" segment_no="7" tag_type="text">nition (CVPR’05) , vol. 1.</text>
<text top="168" left="425" width="85" height="7" font="font6" id="p16_t96" reading_order_no="95" segment_no="7" tag_type="text">IEEE, 2005, pp. 539–546.</text>
<text top="177" left="312" width="251" height="7" font="font6" id="p16_t97" reading_order_no="96" segment_no="9" tag_type="text">[133] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, “A</text>
<text top="186" left="334" width="229" height="7" font="font6" id="p16_t98" reading_order_no="97" segment_no="9" tag_type="text">benchmark for the evaluation of rgb-d slam systems,” in 2012 IEEE/RSJ</text>
<text top="195" left="334" width="229" height="7" font="font35" id="p16_t99" reading_order_no="98" segment_no="9" tag_type="text">International Conference on Intelligent Robots and Systems , 2012, pp.</text>
<text top="204" left="334" width="30" height="7" font="font6" id="p16_t100" reading_order_no="99" segment_no="9" tag_type="text">573–580.</text>
<text top="214" left="312" width="251" height="7" font="font6" id="p16_t101" reading_order_no="100" segment_no="11" tag_type="text">[134] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,</text>
<text top="223" left="334" width="229" height="7" font="font6" id="p16_t102" reading_order_no="101" segment_no="11" tag_type="text">T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient convo-</text>
<text top="232" left="334" width="229" height="7" font="font6" id="p16_t103" reading_order_no="102" segment_no="11" tag_type="text">lutional neural networks for mobile vision applications,” arXiv preprint</text>
<text top="240" left="334" width="81" height="8" font="font35" id="p16_t104" reading_order_no="103" segment_no="11" tag_type="text">arXiv:1704.04861 , 2017.</text>
<text top="250" left="312" width="251" height="7" font="font6" id="p16_t105" reading_order_no="104" segment_no="13" tag_type="text">[135] X. Wang, Y. Shi, and K. M. Kitani, “Deep supervised hashing with</text>
<text top="259" left="334" width="188" height="7" font="font6" id="p16_t106" reading_order_no="105" segment_no="13" tag_type="text">triplet labels,” in Asian conference on computer vision .</text>
<text top="259" left="534" width="29" height="7" font="font6" id="p16_t107" reading_order_no="106" segment_no="13" tag_type="text">Springer,</text>
<text top="268" left="334" width="55" height="7" font="font6" id="p16_t108" reading_order_no="107" segment_no="13" tag_type="text">2016, pp. 70–84.</text>
<text top="277" left="312" width="251" height="7" font="font6" id="p16_t109" reading_order_no="108" segment_no="15" tag_type="text">[136] W.-J. Li, S. Wang, and W.-C. Kang, “Feature learning based deep super-</text>
<text top="286" left="334" width="229" height="7" font="font6" id="p16_t110" reading_order_no="109" segment_no="15" tag_type="text">vised hashing with pairwise labels,” arXiv preprint arXiv:1511.03855 ,</text>
<text top="295" left="334" width="18" height="7" font="font6" id="p16_t111" reading_order_no="110" segment_no="15" tag_type="text">2015.</text>
<text top="305" left="312" width="251" height="7" font="font6" id="p16_t112" reading_order_no="111" segment_no="17" tag_type="text">[137] S. Hausler, A. Jacobson, and M. Milford, “Filter early, match late:</text>
<text top="314" left="334" width="229" height="7" font="font6" id="p16_t113" reading_order_no="112" segment_no="17" tag_type="text">Improving network-based visual place recognition,” in 2019 IEEE/RSJ</text>
<text top="322" left="334" width="229" height="8" font="font35" id="p16_t114" reading_order_no="113" segment_no="17" tag_type="text">International Conference on Intelligent Robots and Systems (IROS) ,</text>
<text top="331" left="334" width="71" height="7" font="font6" id="p16_t115" reading_order_no="114" segment_no="17" tag_type="text">2019, pp. 3268–3275.</text>
<text top="341" left="312" width="251" height="7" font="font6" id="p16_t116" reading_order_no="115" segment_no="19" tag_type="text">[138] Z. Chen, A. Jacobson, N. S¨underhauf, B. Upcroft, L. Liu, C. Shen,</text>
<text top="350" left="334" width="229" height="7" font="font6" id="p16_t117" reading_order_no="116" segment_no="19" tag_type="text">I. Reid, and M. Milford, “Deep learning features at scale for visual</text>
<text top="359" left="334" width="229" height="7" font="font6" id="p16_t118" reading_order_no="117" segment_no="19" tag_type="text">place recognition,” in 2017 IEEE International Conference on Robotics</text>
<text top="368" left="334" width="154" height="7" font="font35" id="p16_t119" reading_order_no="118" segment_no="19" tag_type="text">and Automation (ICRA) , 2017, pp. 3223–3230.</text>
<text top="377" left="312" width="251" height="7" font="font6" id="p16_t120" reading_order_no="119" segment_no="21" tag_type="text">[139] S. Appalaraju and V. Chaoji, “Image similarity using deep cnn and</text>
<text top="386" left="334" width="202" height="7" font="font6" id="p16_t121" reading_order_no="120" segment_no="21" tag_type="text">curriculum learning,” arXiv preprint arXiv:1709.08761 , 2017.</text>
<text top="395" left="312" width="251" height="7" font="font6" id="p16_t122" reading_order_no="121" segment_no="23" tag_type="text">[140] E. Hoffer and N. Ailon, “Deep metric learning using triplet network,”</text>
<text top="404" left="334" width="229" height="8" font="font6" id="p16_t123" reading_order_no="122" segment_no="23" tag_type="text">in Similarity-Based Pattern Recognition , A. Feragen, M. Pelillo, and</text>
<text top="413" left="334" width="49" height="7" font="font6" id="p16_t124" reading_order_no="123" segment_no="23" tag_type="text">M. Loog, Eds.</text>
<text top="413" left="393" width="170" height="7" font="font6" id="p16_t125" reading_order_no="124" segment_no="23" tag_type="text">Cham: Springer International Publishing, 2015, pp.</text>
<text top="422" left="334" width="22" height="7" font="font6" id="p16_t126" reading_order_no="125" segment_no="23" tag_type="text">84–92.</text>
<text top="432" left="312" width="251" height="7" font="font6" id="p16_t127" reading_order_no="126" segment_no="25" tag_type="text">[141] N. Carlevaris-Bianco, A. K. Ushani, and R. M. Eustice, “University</text>
<text top="441" left="334" width="229" height="7" font="font6" id="p16_t128" reading_order_no="127" segment_no="25" tag_type="text">of Michigan North Campus long-term vision and LiDAR dataset,”</text>
<text top="450" left="334" width="229" height="7" font="font35" id="p16_t129" reading_order_no="128" segment_no="25" tag_type="text">International Journal of Robotics Research , vol. 35, no. 9, pp. 1023–</text>
<text top="459" left="334" width="39" height="7" font="font6" id="p16_t130" reading_order_no="129" segment_no="25" tag_type="text">1035, 2015.</text>
<text top="468" left="312" width="251" height="7" font="font6" id="p16_t131" reading_order_no="130" segment_no="27" tag_type="text">[142] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous</text>
<text top="477" left="334" width="229" height="7" font="font6" id="p16_t132" reading_order_no="131" segment_no="27" tag_type="text">driving? the kitti vision benchmark suite,” in 2012 IEEE Conference</text>
<text top="486" left="334" width="223" height="7" font="font35" id="p16_t133" reading_order_no="132" segment_no="27" tag_type="text">on Computer Vision and Pattern Recognition , 2012, pp. 3354–3361.</text>
<text top="495" left="312" width="251" height="7" font="font6" id="p16_t134" reading_order_no="133" segment_no="29" tag_type="text">[143] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction</text>
<text top="504" left="334" width="229" height="7" font="font6" id="p16_t135" reading_order_no="134" segment_no="29" tag_type="text">by learning an invariant mapping,” in 2006 IEEE Computer Society</text>
<text top="513" left="334" width="229" height="7" font="font35" id="p16_t136" reading_order_no="135" segment_no="29" tag_type="text">Conference on Computer Vision and Pattern Recognition (CVPR’06) ,</text>
<text top="522" left="334" width="95" height="7" font="font6" id="p16_t137" reading_order_no="136" segment_no="29" tag_type="text">vol. 2, 2006, pp. 1735–1742.</text>
<text top="532" left="312" width="251" height="7" font="font6" id="p16_t138" reading_order_no="137" segment_no="32" tag_type="text">[144] H. Yin, Y. Wang, X. Ding, L. Tang, S. Huang, and R. Xiong, “3d</text>
<text top="541" left="334" width="229" height="7" font="font6" id="p16_t139" reading_order_no="138" segment_no="32" tag_type="text">lidar-based global localization using siamese neural network,” IEEE</text>
<text top="550" left="334" width="229" height="7" font="font35" id="p16_t140" reading_order_no="139" segment_no="32" tag_type="text">Transactions on Intelligent Transportation Systems , vol. 21, no. 4, pp.</text>
<text top="559" left="334" width="59" height="7" font="font6" id="p16_t141" reading_order_no="140" segment_no="32" tag_type="text">1380–1392, 2020.</text>
<text top="568" left="312" width="251" height="7" font="font6" id="p16_t142" reading_order_no="141" segment_no="34" tag_type="text">[145] W. Chen, X. Chen, J. Zhang, and K. Huang, “Beyond triplet loss: a</text>
<text top="577" left="334" width="229" height="7" font="font6" id="p16_t143" reading_order_no="142" segment_no="34" tag_type="text">deep quadruplet network for person re-identification,” in Proceedings</text>
<text top="586" left="334" width="229" height="7" font="font35" id="p16_t144" reading_order_no="143" segment_no="34" tag_type="text">of the IEEE Conference on Computer Vision and Pattern Recognition ,</text>
<text top="595" left="334" width="63" height="7" font="font6" id="p16_t145" reading_order_no="144" segment_no="34" tag_type="text">2017, pp. 403–412.</text>
<text top="604" left="312" width="251" height="7" font="font6" id="p16_t146" reading_order_no="145" segment_no="36" tag_type="text">[146] H. J´egou, M. Douze, C. Schmid, and P. P´erez, “Aggregating local de-</text>
<text top="613" left="334" width="229" height="7" font="font6" id="p16_t147" reading_order_no="146" segment_no="36" tag_type="text">scriptors into a compact image representation,” in 2010 IEEE computer</text>
<text top="622" left="334" width="229" height="7" font="font35" id="p16_t148" reading_order_no="147" segment_no="36" tag_type="text">society conference on computer vision and pattern recognition . IEEE,</text>
<text top="631" left="334" width="71" height="7" font="font6" id="p16_t149" reading_order_no="148" segment_no="36" tag_type="text">2010, pp. 3304–3311.</text>
<text top="641" left="312" width="251" height="7" font="font6" id="p16_t150" reading_order_no="149" segment_no="38" tag_type="text">[147] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image</text>
<text top="650" left="334" width="229" height="7" font="font6" id="p16_t151" reading_order_no="150" segment_no="38" tag_type="text">recognition,” in Proceedings of the IEEE conference on computer vision</text>
<text top="659" left="334" width="145" height="7" font="font35" id="p16_t152" reading_order_no="151" segment_no="38" tag_type="text">and pattern recognition , 2016, pp. 770–778.</text>
<text top="668" left="312" width="251" height="7" font="font6" id="p16_t153" reading_order_no="152" segment_no="40" tag_type="text">[148] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the state</text>
<text top="677" left="334" width="226" height="7" font="font6" id="p16_t154" reading_order_no="153" segment_no="40" tag_type="text">of neural network pruning?” arXiv preprint arXiv:2003.03033 , 2020.</text>
<text top="686" left="312" width="251" height="7" font="font6" id="p16_t155" reading_order_no="154" segment_no="41" tag_type="text">[149] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE</text>
<text top="695" left="334" width="229" height="7" font="font35" id="p16_t156" reading_order_no="155" segment_no="41" tag_type="text">Transactions on Pattern Analysis and Machine Intelligence , vol. 40,</text>
<text top="704" left="334" width="85" height="7" font="font6" id="p16_t157" reading_order_no="156" segment_no="41" tag_type="text">no. 3, pp. 611–625, 2017.</text>
<text top="714" left="312" width="251" height="7" font="font6" id="p16_t158" reading_order_no="157" segment_no="43" tag_type="text">[150] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning</text>
<text top="723" left="334" width="229" height="7" font="font6" id="p16_t159" reading_order_no="158" segment_no="43" tag_type="text">on point sets for 3d classification and segmentation,” in Proceedings</text>
<text top="732" left="334" width="229" height="7" font="font35" id="p16_t160" reading_order_no="159" segment_no="43" tag_type="text">of the IEEE conference on computer vision and pattern recognition ,</text>
<text top="740" left="334" width="63" height="7" font="font6" id="p16_t161" reading_order_no="160" segment_no="43" tag_type="text">2017, pp. 652–660.</text>
</page>
<page number="17" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font36" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
<text top="26" left="556" width="7" height="6" font="font0" id="p17_t1" reading_order_no="0" segment_no="0" tag_type="text">17</text>
<text top="59" left="49" width="251" height="7" font="font6" id="p17_t2" reading_order_no="1" segment_no="1" tag_type="text">[151] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified</text>
<text top="68" left="71" width="229" height="7" font="font6" id="p17_t3" reading_order_no="2" segment_no="1" tag_type="text">embedding for face recognition and clustering,” in Proceedings of the</text>
<text top="77" left="71" width="229" height="7" font="font35" id="p17_t4" reading_order_no="3" segment_no="1" tag_type="text">IEEE conference on computer vision and pattern recognition , 2015,</text>
<text top="86" left="71" width="43" height="7" font="font6" id="p17_t5" reading_order_no="4" segment_no="1" tag_type="text">pp. 815–823.</text>
<text top="95" left="49" width="251" height="7" font="font6" id="p17_t6" reading_order_no="5" segment_no="4" tag_type="text">[152] Y. Latif, R. Garg, M. Milford, and I. Reid, “Addressing challenging</text>
<text top="104" left="71" width="229" height="7" font="font6" id="p17_t7" reading_order_no="6" segment_no="4" tag_type="text">place recognition tasks using generative adversarial networks,” in 2018</text>
<text top="113" left="71" width="229" height="7" font="font35" id="p17_t8" reading_order_no="7" segment_no="4" tag_type="text">IEEE International Conference on Robotics and Automation (ICRA) ,</text>
<text top="122" left="71" width="71" height="7" font="font6" id="p17_t9" reading_order_no="8" segment_no="4" tag_type="text">2018, pp. 2349–2355.</text>
<text top="131" left="49" width="251" height="7" font="font6" id="p17_t10" reading_order_no="9" segment_no="6" tag_type="text">[153] P. Yin, L. Xu, Z. Liu, L. Li, H. Salman, Y. He, W. Xu, H. Wang,</text>
<text top="140" left="71" width="229" height="7" font="font6" id="p17_t11" reading_order_no="10" segment_no="6" tag_type="text">and H. Choset, “Stabilize an unsupervised feature learning for lidar-</text>
<text top="149" left="71" width="229" height="7" font="font6" id="p17_t12" reading_order_no="11" segment_no="6" tag_type="text">based place recognition,” in 2018 IEEE/RSJ International Conference</text>
<text top="158" left="71" width="212" height="7" font="font35" id="p17_t13" reading_order_no="12" segment_no="6" tag_type="text">on Intelligent Robots and Systems (IROS) , 2018, pp. 1162–1167.</text>
<text top="167" left="49" width="251" height="7" font="font6" id="p17_t14" reading_order_no="13" segment_no="8" tag_type="text">[154] J. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image</text>
<text top="176" left="71" width="229" height="7" font="font6" id="p17_t15" reading_order_no="14" segment_no="8" tag_type="text">translation using cycle-consistent adversarial networks,” in 2017 IEEE</text>
<text top="185" left="71" width="229" height="7" font="font35" id="p17_t16" reading_order_no="15" segment_no="8" tag_type="text">International Conference on Computer Vision (ICCV) , 2017, pp. 2242–</text>
<text top="194" left="71" width="18" height="7" font="font6" id="p17_t17" reading_order_no="16" segment_no="8" tag_type="text">2251.</text>
<text top="202" left="49" width="251" height="7" font="font6" id="p17_t18" reading_order_no="17" segment_no="10" tag_type="text">[155] Zhou Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image</text>
<text top="211" left="71" width="229" height="8" font="font6" id="p17_t19" reading_order_no="18" segment_no="10" tag_type="text">quality assessment: from error visibility to structural similarity,” IEEE</text>
<text top="220" left="71" width="229" height="8" font="font35" id="p17_t20" reading_order_no="19" segment_no="10" tag_type="text">Transactions on Image Processing , vol. 13, no. 4, pp. 600–612, 2004.</text>
<text top="229" left="49" width="251" height="7" font="font6" id="p17_t21" reading_order_no="20" segment_no="12" tag_type="text">[156] N. Dalal and B. Triggs, “Histograms of oriented gradients for human</text>
<text top="238" left="71" width="229" height="7" font="font6" id="p17_t22" reading_order_no="21" segment_no="12" tag_type="text">detection,” in 2005 IEEE Computer Society Conference on Computer</text>
<text top="247" left="71" width="229" height="7" font="font35" id="p17_t23" reading_order_no="22" segment_no="12" tag_type="text">Vision and Pattern Recognition (CVPR’05) , vol. 1, 2005, pp. 886–893</text>
<text top="256" left="71" width="21" height="7" font="font6" id="p17_t24" reading_order_no="23" segment_no="12" tag_type="text">vol. 1.</text>
<text top="265" left="49" width="251" height="7" font="font6" id="p17_t25" reading_order_no="24" segment_no="14" tag_type="text">[157] R. Hartley and A. Zisserman, Multiple view geometry in computer</text>
<text top="274" left="71" width="21" height="7" font="font35" id="p17_t26" reading_order_no="25" segment_no="14" tag_type="text">vision .</text>
<text top="274" left="100" width="112" height="7" font="font6" id="p17_t27" reading_order_no="26" segment_no="14" tag_type="text">Cambridge university press, 2003.</text>
<text top="283" left="49" width="251" height="7" font="font6" id="p17_t28" reading_order_no="27" segment_no="15" tag_type="text">[158] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim, “Learning to discover</text>
<text top="292" left="71" width="229" height="7" font="font6" id="p17_t29" reading_order_no="28" segment_no="15" tag_type="text">cross-domain relations with generative adversarial networks,” arXiv</text>
<text top="301" left="71" width="110" height="7" font="font35" id="p17_t30" reading_order_no="29" segment_no="15" tag_type="text">preprint arXiv:1703.05192 , 2017.</text>
<text top="310" left="49" width="251" height="7" font="font6" id="p17_t31" reading_order_no="30" segment_no="17" tag_type="text">[159] S. Hwang, J. Park, N. Kim, Y. Choi, and I. S. Kweon, “Multispectral</text>
<text top="319" left="71" width="229" height="7" font="font6" id="p17_t32" reading_order_no="31" segment_no="17" tag_type="text">pedestrian detection: Benchmark dataset and baseline,” in 2015 IEEE</text>
<text top="328" left="71" width="229" height="7" font="font35" id="p17_t33" reading_order_no="32" segment_no="17" tag_type="text">Conference on Computer Vision and Pattern Recognition (CVPR) ,</text>
<text top="337" left="71" width="71" height="7" font="font6" id="p17_t34" reading_order_no="33" segment_no="17" tag_type="text">2015, pp. 1037–1045.</text>
<text top="346" left="49" width="251" height="7" font="font6" id="p17_t35" reading_order_no="34" segment_no="19" tag_type="text">[160] Z. Wang, J. Li, S. Khademi, and J. van Gemert, “Attention-aware age-</text>
<text top="355" left="71" width="229" height="7" font="font6" id="p17_t36" reading_order_no="35" segment_no="19" tag_type="text">agnostic visual place recognition,” in 2019 IEEE/CVF International</text>
<text top="364" left="71" width="229" height="7" font="font35" id="p17_t37" reading_order_no="36" segment_no="19" tag_type="text">Conference on Computer Vision Workshop (ICCVW) , 2019, pp. 1437–</text>
<text top="373" left="71" width="18" height="7" font="font6" id="p17_t38" reading_order_no="37" segment_no="19" tag_type="text">1446.</text>
<text top="382" left="49" width="251" height="7" font="font6" id="p17_t39" reading_order_no="38" segment_no="21" tag_type="text">[161] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf,</text>
<text top="391" left="71" width="229" height="7" font="font6" id="p17_t40" reading_order_no="39" segment_no="21" tag_type="text">and A. J. Smola, “Integrating structured biological data by kernel</text>
<text top="400" left="71" width="229" height="7" font="font6" id="p17_t41" reading_order_no="40" segment_no="21" tag_type="text">maximum mean discrepancy,” Bioinformatics , vol. 22, no. 14, pp. e49–</text>
<text top="409" left="71" width="34" height="7" font="font6" id="p17_t42" reading_order_no="41" segment_no="21" tag_type="text">e57, 2006.</text>
<text top="418" left="49" width="251" height="7" font="font6" id="p17_t43" reading_order_no="42" segment_no="24" tag_type="text">[162] L. Tang, Y. Wang, Q. Luo, X. Ding, and R. Xiong, “Adversarial feature</text>
<text top="427" left="71" width="229" height="7" font="font6" id="p17_t44" reading_order_no="43" segment_no="24" tag_type="text">disentanglement for place recognition across changing appearance,”</text>
<text top="436" left="71" width="229" height="7" font="font6" id="p17_t45" reading_order_no="44" segment_no="24" tag_type="text">in 2020 IEEE International Conference on Robotics and Automation</text>
<text top="445" left="71" width="99" height="7" font="font35" id="p17_t46" reading_order_no="45" segment_no="24" tag_type="text">(ICRA) , 2020, pp. 1301–1307.</text>
<text top="454" left="49" width="251" height="7" font="font6" id="p17_t47" reading_order_no="46" segment_no="26" tag_type="text">[163] X. Mao, Q. Li, H. Xie, R. Y. K. Lau, Z. Wang, and S. P. Smolley, “Least</text>
<text top="463" left="71" width="229" height="7" font="font6" id="p17_t48" reading_order_no="47" segment_no="26" tag_type="text">squares generative adversarial networks,” in 2017 IEEE International</text>
<text top="471" left="71" width="207" height="8" font="font35" id="p17_t49" reading_order_no="48" segment_no="26" tag_type="text">Conference on Computer Vision (ICCV) , 2017, pp. 2813–2821.</text>
<text top="480" left="49" width="251" height="7" font="font6" id="p17_t50" reading_order_no="49" segment_no="28" tag_type="text">[164] O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,”</text>
<text top="489" left="71" width="138" height="7" font="font35" id="p17_t51" reading_order_no="50" segment_no="28" tag_type="text">British Machine Vision Association , 2015.</text>
<text top="498" left="49" width="251" height="7" font="font6" id="p17_t52" reading_order_no="51" segment_no="29" tag_type="text">[165] A. Brock, T. Lim, J. M. Ritchie, and N. Weston, “Generative and</text>
<text top="507" left="71" width="229" height="7" font="font6" id="p17_t53" reading_order_no="52" segment_no="29" tag_type="text">discriminative voxel modeling with convolutional neural networks,” In:</text>
<text top="516" left="71" width="151" height="7" font="font35" id="p17_t54" reading_order_no="53" segment_no="29" tag_type="text">Workshop on 3D Deep Learning, NIPS , 2016.</text>
<text top="525" left="49" width="251" height="7" font="font6" id="p17_t55" reading_order_no="54" segment_no="31" tag_type="text">[166] S. Hausler, A. Jacobson, and M. Milford, “Multi-process fusion: Visual</text>
<text top="534" left="71" width="229" height="7" font="font6" id="p17_t56" reading_order_no="55" segment_no="31" tag_type="text">place recognition using multiple image processing methods,” IEEE</text>
<text top="543" left="71" width="229" height="7" font="font35" id="p17_t57" reading_order_no="56" segment_no="31" tag_type="text">Robotics and Automation Letters , vol. 4, no. 2, pp. 1924–1931, 2019.</text>
<text top="552" left="49" width="251" height="7" font="font6" id="p17_t58" reading_order_no="57" segment_no="32" tag_type="text">[167] A. Jacobson, Z. Chen, and M. Milford, “Leveraging variable sensor</text>
<text top="561" left="71" width="229" height="7" font="font6" id="p17_t59" reading_order_no="58" segment_no="32" tag_type="text">spatial acuity with a homogeneous, multi-scale place recognition frame-</text>
<text top="570" left="71" width="217" height="7" font="font6" id="p17_t60" reading_order_no="59" segment_no="32" tag_type="text">work,” Biological cybernetics , vol. 112, no. 3, pp. 209–225, 2018.</text>
<text top="579" left="49" width="251" height="7" font="font6" id="p17_t61" reading_order_no="60" segment_no="33" tag_type="text">[168] R. Arroyo, P. F. Alcantarilla, L. M. Bergasa, and E. Romera, “Towards</text>
<text top="588" left="71" width="229" height="7" font="font6" id="p17_t62" reading_order_no="61" segment_no="33" tag_type="text">life-long visual localization using an efficient matching of binary</text>
<text top="597" left="71" width="229" height="7" font="font6" id="p17_t63" reading_order_no="62" segment_no="33" tag_type="text">sequences from images,” in 2015 IEEE International Conference on</text>
<text top="606" left="71" width="184" height="7" font="font35" id="p17_t64" reading_order_no="63" segment_no="33" tag_type="text">Robotics and Automation (ICRA) , 2015, pp. 6328–6335.</text>
<text top="615" left="49" width="251" height="7" font="font6" id="p17_t65" reading_order_no="64" segment_no="34" tag_type="text">[169] Y. Latif, G. Huang, J. Leonard, and J. Neira, “An online sparsity-</text>
<text top="624" left="71" width="229" height="7" font="font6" id="p17_t66" reading_order_no="65" segment_no="34" tag_type="text">cognizant loop-closure algorithm for visual navigation,” in Proceedings</text>
<text top="633" left="71" width="202" height="7" font="font35" id="p17_t67" reading_order_no="66" segment_no="34" tag_type="text">of Robotics: Science and Systems , Berkeley, USA, July 2014.</text>
<text top="642" left="49" width="251" height="7" font="font6" id="p17_t68" reading_order_no="67" segment_no="35" tag_type="text">[170] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-</text>
<text top="651" left="71" width="229" height="7" font="font6" id="p17_t69" reading_order_no="68" segment_no="35" tag_type="text">time object detection with region proposal networks,” in Advances in</text>
<text top="660" left="71" width="185" height="7" font="font35" id="p17_t70" reading_order_no="69" segment_no="35" tag_type="text">neural information processing systems , 2015, pp. 91–99.</text>
<text top="669" left="49" width="251" height="7" font="font6" id="p17_t71" reading_order_no="70" segment_no="37" tag_type="text">[171] R. Mur-Artal, J. M. M. Montiel, and J. D. Tard´os, “Orb-slam: A</text>
<text top="678" left="71" width="229" height="7" font="font6" id="p17_t72" reading_order_no="71" segment_no="37" tag_type="text">versatile and accurate monocular slam system,” IEEE Transactions on</text>
<text top="687" left="71" width="153" height="7" font="font35" id="p17_t73" reading_order_no="72" segment_no="37" tag_type="text">Robotics , vol. 31, no. 5, pp. 1147–1163, 2015.</text>
<text top="696" left="49" width="251" height="7" font="font6" id="p17_t74" reading_order_no="73" segment_no="38" tag_type="text">[172] E. Pepperell, P. I. Corke, and M. J. Milford, “All-environment visual</text>
<text top="705" left="71" width="229" height="7" font="font6" id="p17_t75" reading_order_no="74" segment_no="38" tag_type="text">place recognition with smart,” in 2014 IEEE international conference</text>
<text top="714" left="71" width="118" height="7" font="font35" id="p17_t76" reading_order_no="75" segment_no="38" tag_type="text">on robotics and automation (ICRA) .</text>
<text top="714" left="197" width="93" height="7" font="font6" id="p17_t77" reading_order_no="76" segment_no="38" tag_type="text">IEEE, 2014, pp. 1612–1618.</text>
<text top="723" left="49" width="251" height="7" font="font6" id="p17_t78" reading_order_no="77" segment_no="39" tag_type="text">[173] T. Naseer, W. Burgard, and C. Stachniss, “Robust visual localization</text>
<text top="732" left="71" width="229" height="7" font="font6" id="p17_t79" reading_order_no="78" segment_no="39" tag_type="text">across seasons,” IEEE Transactions on Robotics , vol. 34, no. 2, pp.</text>
<text top="740" left="71" width="51" height="7" font="font6" id="p17_t80" reading_order_no="79" segment_no="39" tag_type="text">289–302, 2018.</text>
<text top="59" left="312" width="251" height="7" font="font6" id="p17_t81" reading_order_no="80" segment_no="2" tag_type="text">[174] C. McManus, B. Upcroft, and P. Newmann, “Scene signatures: Lo-</text>
<text top="68" left="334" width="229" height="7" font="font6" id="p17_t82" reading_order_no="81" segment_no="2" tag_type="text">calised and point-less features for localisation,” in Proceedings of</text>
<text top="77" left="334" width="193" height="7" font="font35" id="p17_t83" reading_order_no="82" segment_no="2" tag_type="text">Robotics: Science and Systems , Berkeley, USA, July 2014.</text>
<text top="86" left="312" width="251" height="7" font="font6" id="p17_t84" reading_order_no="83" segment_no="3" tag_type="text">[175] Z. Chen, L. Liu, I. Sa, Z. Ge, and M. Chli, “Learning context flexible</text>
<text top="95" left="334" width="229" height="7" font="font6" id="p17_t85" reading_order_no="84" segment_no="3" tag_type="text">attention model for long-term visual place recognition,” IEEE Robotics</text>
<text top="104" left="334" width="198" height="7" font="font35" id="p17_t86" reading_order_no="85" segment_no="3" tag_type="text">and Automation Letters , vol. 3, no. 4, pp. 4015–4022, 2018.</text>
<text top="113" left="312" width="251" height="7" font="font6" id="p17_t87" reading_order_no="86" segment_no="5" tag_type="text">[176] S. Hausler and M. Milford, “Hierarchical multi-process fusion for</text>
<text top="122" left="334" width="229" height="7" font="font6" id="p17_t88" reading_order_no="87" segment_no="5" tag_type="text">visual place recognition,” in 2020 IEEE International Conference</text>
<text top="131" left="334" width="229" height="7" font="font35" id="p17_t89" reading_order_no="88" segment_no="5" tag_type="text">on Robotics and Automation, ICRA 2020, Paris, France, May 31 -</text>
<text top="140" left="334" width="57" height="7" font="font35" id="p17_t90" reading_order_no="89" segment_no="5" tag_type="text">August 31, 2020 .</text>
<text top="140" left="399" width="164" height="7" font="font6" id="p17_t91" reading_order_no="90" segment_no="5" tag_type="text">IEEE, 2020, pp. 3327–3333. [Online]. Available:</text>
<text top="149" left="334" width="162" height="7" font="font6" id="p17_t92" reading_order_no="91" segment_no="5" tag_type="text">https://doi.org/10.1109/ICRA40945.2020.9197360</text>
<text top="158" left="312" width="251" height="7" font="font6" id="p17_t93" reading_order_no="92" segment_no="7" tag_type="text">[177] A. Oliva and A. Torralba, “Modeling the shape of the scene: A</text>
<text top="167" left="334" width="229" height="7" font="font6" id="p17_t94" reading_order_no="93" segment_no="7" tag_type="text">holistic representation of the spatial envelope,” International journal</text>
<text top="176" left="334" width="178" height="7" font="font35" id="p17_t95" reading_order_no="94" segment_no="7" tag_type="text">of computer vision , vol. 42, no. 3, pp. 145–175, 2001.</text>
<text top="185" left="312" width="251" height="7" font="font6" id="p17_t96" reading_order_no="95" segment_no="9" tag_type="text">[178] P. F. Alcantarilla, A. Bartoli, and A. J. Davison, “Kaze features,” in</text>
<text top="194" left="334" width="140" height="7" font="font35" id="p17_t97" reading_order_no="96" segment_no="9" tag_type="text">European Conference on Computer Vision .</text>
<text top="194" left="482" width="81" height="7" font="font6" id="p17_t98" reading_order_no="97" segment_no="9" tag_type="text">Springer, 2012, pp. 214–</text>
<text top="202" left="334" width="14" height="7" font="font6" id="p17_t99" reading_order_no="98" segment_no="9" tag_type="text">227.</text>
<text top="211" left="312" width="251" height="7" font="font6" id="p17_t100" reading_order_no="99" segment_no="11" tag_type="text">[179] G. Lin, A. Milan, C. Shen, and I. Reid, “Refinenet: Multi-path</text>
<text top="220" left="334" width="229" height="7" font="font6" id="p17_t101" reading_order_no="100" segment_no="11" tag_type="text">refinement networks for high-resolution semantic segmentation,” in</text>
<text top="229" left="334" width="229" height="7" font="font35" id="p17_t102" reading_order_no="101" segment_no="11" tag_type="text">Proceedings of the IEEE conference on computer vision and pattern</text>
<text top="238" left="334" width="113" height="7" font="font35" id="p17_t103" reading_order_no="102" segment_no="11" tag_type="text">recognition , 2017, pp. 1925–1934.</text>
<text top="247" left="312" width="251" height="7" font="font6" id="p17_t104" reading_order_no="103" segment_no="13" tag_type="text">[180] S. An, G. Che, F. Zhou, X. Liu, X. Ma, and Y. Chen, “Fast and</text>
<text top="256" left="334" width="229" height="7" font="font6" id="p17_t105" reading_order_no="104" segment_no="13" tag_type="text">incremental loop closure detection using proximity graphs,” in 2019</text>
<text top="265" left="334" width="229" height="7" font="font35" id="p17_t106" reading_order_no="105" segment_no="13" tag_type="text">IEEE/RSJ International Conference on Intelligent Robots and Systems</text>
<text top="274" left="334" width="90" height="7" font="font35" id="p17_t107" reading_order_no="106" segment_no="13" tag_type="text">(IROS) , 2019, pp. 378–385.</text>
<text top="283" left="312" width="251" height="7" font="font6" id="p17_t108" reading_order_no="107" segment_no="16" tag_type="text">[181] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen, “Mo-</text>
<text top="292" left="334" width="229" height="7" font="font6" id="p17_t109" reading_order_no="108" segment_no="16" tag_type="text">bilenetv2: Inverted residuals and linear bottlenecks,” in 2018 IEEE/CVF</text>
<text top="301" left="334" width="229" height="7" font="font35" id="p17_t110" reading_order_no="109" segment_no="16" tag_type="text">Conference on Computer Vision and Pattern Recognition , 2018, pp.</text>
<text top="310" left="334" width="38" height="7" font="font6" id="p17_t111" reading_order_no="110" segment_no="16" tag_type="text">4510–4520.</text>
<text top="319" left="312" width="251" height="7" font="font6" id="p17_t112" reading_order_no="111" segment_no="18" tag_type="text">[182] J. Sivic and A. Zisserman, “Efficient visual search of videos cast as</text>
<text top="328" left="334" width="229" height="7" font="font6" id="p17_t113" reading_order_no="112" segment_no="18" tag_type="text">text retrieval,” IEEE Transactions on Pattern Analysis and Machine</text>
<text top="337" left="334" width="155" height="7" font="font35" id="p17_t114" reading_order_no="113" segment_no="18" tag_type="text">Intelligence , vol. 31, no. 4, pp. 591–606, 2008.</text>
<text top="346" left="312" width="251" height="7" font="font6" id="p17_t115" reading_order_no="114" segment_no="20" tag_type="text">[183] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The</text>
<text top="355" left="334" width="229" height="7" font="font6" id="p17_t116" reading_order_no="115" segment_no="20" tag_type="text">synthia dataset: A large collection of synthetic images for semantic</text>
<text top="364" left="334" width="229" height="7" font="font6" id="p17_t117" reading_order_no="116" segment_no="20" tag_type="text">segmentation of urban scenes,” in 2016 IEEE Conference on Computer</text>
<text top="373" left="334" width="206" height="7" font="font35" id="p17_t118" reading_order_no="117" segment_no="20" tag_type="text">Vision and Pattern Recognition (CVPR) , 2016, pp. 3234–3243.</text>
<text top="382" left="312" width="251" height="7" font="font6" id="p17_t119" reading_order_no="118" segment_no="22" tag_type="text">[184] D. G. Lowe, “Distinctive image features from scale-invariant key-</text>
<text top="391" left="334" width="229" height="7" font="font6" id="p17_t120" reading_order_no="119" segment_no="22" tag_type="text">points,” International journal of computer vision , vol. 60, no. 2, pp.</text>
<text top="400" left="334" width="47" height="7" font="font6" id="p17_t121" reading_order_no="120" segment_no="22" tag_type="text">91–110, 2004.</text>
<text top="409" left="312" width="251" height="7" font="font6" id="p17_t122" reading_order_no="121" segment_no="23" tag_type="text">[185] J.-L. Blanco, F.-A. Moreno, and J. Gonzalez, “A collection of outdoor</text>
<text top="418" left="334" width="229" height="7" font="font6" id="p17_t123" reading_order_no="122" segment_no="23" tag_type="text">robotic datasets with centimeter-accuracy ground truth,” Autonomous</text>
<text top="427" left="334" width="119" height="7" font="font35" id="p17_t124" reading_order_no="123" segment_no="23" tag_type="text">Robots , vol. 27, no. 4, p. 327, 2009.</text>
<text top="436" left="312" width="251" height="7" font="font6" id="p17_t125" reading_order_no="124" segment_no="25" tag_type="text">[186] M. Smith, I. Baldwin, W. Churchill, R. Paul, and P. Newman, “The</text>
<text top="445" left="334" width="229" height="7" font="font6" id="p17_t126" reading_order_no="125" segment_no="25" tag_type="text">new college vision and laser data set,” The International Journal of</text>
<text top="454" left="334" width="177" height="7" font="font35" id="p17_t127" reading_order_no="126" segment_no="25" tag_type="text">Robotics Research , vol. 28, no. 5, pp. 595–599, 2009.</text>
<text top="463" left="312" width="251" height="7" font="font6" id="p17_t128" reading_order_no="127" segment_no="27" tag_type="text">[187] S. Niko, P. Neubert, and P. Protzel, “Are we there yet? challenging</text>
<text top="471" left="334" width="229" height="8" font="font6" id="p17_t129" reading_order_no="128" segment_no="27" tag_type="text">SeqSLAM on a 3000 km journey across all four seasons,” in Proc.</text>
<text top="481" left="334" width="229" height="7" font="font35" id="p17_t130" reading_order_no="129" segment_no="27" tag_type="text">of IEEE International Conference on Robotics and Automation Work-</text>
<text top="489" left="334" width="41" height="7" font="font35" id="p17_t131" reading_order_no="130" segment_no="27" tag_type="text">shops , 2013.</text>
<text top="513" left="396" width="167" height="9" font="font36" id="p17_t132" reading_order_no="131" segment_no="30" tag_type="text">Tiago Barros received the Master degree</text>
<text top="525" left="396" width="167" height="9" font="font5" id="p17_t133" reading_order_no="132" segment_no="30" tag_type="text">in Electrical and Computer Engineering</text>
<text top="537" left="396" width="167" height="9" font="font5" id="p17_t134" reading_order_no="133" segment_no="30" tag_type="text">from the University of Coimbra, Portugal,</text>
<text top="548" left="396" width="167" height="9" font="font5" id="p17_t135" reading_order_no="134" segment_no="30" tag_type="text">in 2015. He is currently working towards</text>
<text top="560" left="396" width="167" height="9" font="font5" id="p17_t136" reading_order_no="135" segment_no="30" tag_type="text">the Ph.D. degree in the Institute of</text>
<text top="572" left="396" width="167" height="9" font="font5" id="p17_t137" reading_order_no="136" segment_no="30" tag_type="text">Systems and Robotics, University of</text>
<text top="584" left="396" width="167" height="9" font="font5" id="p17_t138" reading_order_no="137" segment_no="30" tag_type="text">Coimbra, Portugal. His research interests</text>
<text top="596" left="396" width="167" height="9" font="font5" id="p17_t139" reading_order_no="138" segment_no="30" tag_type="text">include deep learning, perception and</text>
<text top="608" left="312" width="49" height="9" font="font5" id="p17_t140" reading_order_no="139" segment_no="30" tag_type="text">localization.</text>
<text top="655" left="396" width="167" height="9" font="font36" id="p17_t141" reading_order_no="140" segment_no="36" tag_type="text">Ricardo Pereira received the Master</text>
<text top="667" left="396" width="26" height="9" font="font5" id="p17_t142" reading_order_no="141" segment_no="36" tag_type="text">degree</text>
<text top="667" left="432" width="8" height="9" font="font5" id="p17_t143" reading_order_no="142" segment_no="36" tag_type="text">in<a href="https://doi.org/10.1109/ICRA40945.2020.9197360">https://doi.org/10.1109/ICRA40945.2020.9197360</a></text>
<text top="667" left="450" width="38" height="9" font="font5" id="p17_t144" reading_order_no="143" segment_no="36" tag_type="text">Electrical</text>
<text top="667" left="499" width="14" height="9" font="font5" id="p17_t145" reading_order_no="144" segment_no="36" tag_type="text">and</text>
<text top="667" left="523" width="40" height="9" font="font5" id="p17_t146" reading_order_no="145" segment_no="36" tag_type="text">Computer</text>
<text top="679" left="396" width="49" height="9" font="font5" id="p17_t147" reading_order_no="146" segment_no="36" tag_type="text">Engineering</text>
<text top="679" left="454" width="19" height="9" font="font5" id="p17_t148" reading_order_no="147" segment_no="36" tag_type="text">from</text>
<text top="679" left="482" width="12" height="9" font="font5" id="p17_t149" reading_order_no="148" segment_no="36" tag_type="text">the</text>
<text top="679" left="504" width="42" height="9" font="font5" id="p17_t150" reading_order_no="149" segment_no="36" tag_type="text">University</text>
<text top="679" left="555" width="8" height="9" font="font5" id="p17_t151" reading_order_no="150" segment_no="36" tag_type="text">of</text>
<text top="691" left="396" width="37" height="9" font="font5" id="p17_t152" reading_order_no="151" segment_no="36" tag_type="text">Coimbra,</text>
<text top="691" left="443" width="36" height="9" font="font5" id="p17_t153" reading_order_no="152" segment_no="36" tag_type="text">Portugal.</text>
<text top="691" left="489" width="12" height="9" font="font5" id="p17_t154" reading_order_no="153" segment_no="36" tag_type="text">He</text>
<text top="691" left="511" width="7" height="9" font="font5" id="p17_t155" reading_order_no="154" segment_no="36" tag_type="text">is</text>
<text top="691" left="527" width="36" height="9" font="font5" id="p17_t156" reading_order_no="155" segment_no="36" tag_type="text">currently</text>
<text top="703" left="396" width="167" height="9" font="font5" id="p17_t157" reading_order_no="156" segment_no="36" tag_type="text">working towards the Ph.D. degree in</text>
<text top="715" left="396" width="167" height="9" font="font5" id="p17_t158" reading_order_no="157" segment_no="36" tag_type="text">the Institute of Systems and Robotics,</text>
<text top="727" left="396" width="167" height="9" font="font5" id="p17_t159" reading_order_no="158" segment_no="36" tag_type="text">University of Coimbra, Portugal. His</text>
<text top="739" left="396" width="167" height="9" font="font5" id="p17_t160" reading_order_no="159" segment_no="36" tag_type="text">research interests include deep learning,</text>
</page>
<page number="18" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p18_t1" reading_order_no="0" segment_no="0" tag_type="text">18</text>
<text top="58" left="133" width="132" height="9" font="font5" id="p18_t2" reading_order_no="1" segment_no="1" tag_type="text">perception, and mobile robotics.</text>
<text top="81" left="133" width="167" height="10" font="font36" id="p18_t3" reading_order_no="2" segment_no="2" tag_type="text">Cristiano Premebida is Assistant Pro-</text>
<text top="94" left="133" width="167" height="9" font="font5" id="p18_t4" reading_order_no="3" segment_no="2" tag_type="text">fessor in the department of electrical and</text>
<text top="106" left="133" width="167" height="9" font="font5" id="p18_t5" reading_order_no="4" segment_no="2" tag_type="text">computer engineering at the University</text>
<text top="117" left="133" width="167" height="9" font="font5" id="p18_t6" reading_order_no="5" segment_no="2" tag_type="text">of Coimbra, Portugal, where he is a</text>
<text top="129" left="133" width="167" height="9" font="font5" id="p18_t7" reading_order_no="6" segment_no="2" tag_type="text">member of the Institute of Systems and</text>
<text top="141" left="133" width="167" height="9" font="font5" id="p18_t8" reading_order_no="7" segment_no="2" tag_type="text">Robotics (ISR-UC). His main research</text>
<text top="153" left="133" width="167" height="9" font="font5" id="p18_t9" reading_order_no="8" segment_no="2" tag_type="text">interests are autonomous vehicles, au-</text>
<text top="165" left="133" width="167" height="9" font="font5" id="p18_t10" reading_order_no="9" segment_no="2" tag_type="text">tonomous robots, robotic perception, co-</text>
<text top="177" left="133" width="167" height="9" font="font5" id="p18_t11" reading_order_no="10" segment_no="2" tag_type="text">operative/connected intelligent transport</text>
<text top="189" left="49" width="251" height="9" font="font5" id="p18_t12" reading_order_no="11" segment_no="2" tag_type="text">systems (CITS), ADAS, machine learning, and sensor fusion.</text>
<text top="201" left="59" width="241" height="9" font="font5" id="p18_t13" reading_order_no="12" segment_no="3" tag_type="text">C. Premebida has collaborated on research projects in</text>
<text top="213" left="49" width="251" height="9" font="font5" id="p18_t14" reading_order_no="13" segment_no="3" tag_type="text">the areas related to C-ITS, autonomous driving, and applied</text>
<text top="225" left="49" width="34" height="9" font="font5" id="p18_t15" reading_order_no="14" segment_no="3" tag_type="text">machine</text>
<text top="225" left="92" width="35" height="9" font="font5" id="p18_t16" reading_order_no="15" segment_no="3" tag_type="text">learning,</text>
<text top="225" left="137" width="38" height="9" font="font5" id="p18_t17" reading_order_no="16" segment_no="3" tag_type="text">including</text>
<text top="225" left="184" width="32" height="9" font="font5" id="p18_t18" reading_order_no="17" segment_no="3" tag_type="text">national</text>
<text top="225" left="226" width="14" height="9" font="font5" id="p18_t19" reading_order_no="18" segment_no="3" tag_type="text">and</text>
<text top="225" left="250" width="50" height="9" font="font5" id="p18_t20" reading_order_no="19" segment_no="3" tag_type="text">international</text>
<text top="237" left="49" width="251" height="9" font="font5" id="p18_t21" reading_order_no="20" segment_no="3" tag_type="text">projects. He is an IEEE-ITS society member, has served</text>
<text top="249" left="49" width="251" height="9" font="font5" id="p18_t22" reading_order_no="21" segment_no="3" tag_type="text">as AE in the flagship conferences ITSC and IVS, and has</text>
<text top="261" left="49" width="251" height="9" font="font5" id="p18_t23" reading_order_no="22" segment_no="3" tag_type="text">regularly organized international workshops on automated</text>
<text top="273" left="49" width="160" height="9" font="font5" id="p18_t24" reading_order_no="23" segment_no="3" tag_type="text">driving, AI/ML perception, and C-ITS.</text>
<text top="297" left="133" width="19" height="9" font="font36" id="p18_t25" reading_order_no="24" segment_no="4" tag_type="text">Lu´ıs</text>
<text top="297" left="163" width="34" height="9" font="font36" id="p18_t26" reading_order_no="25" segment_no="4" tag_type="text">Garrote</text>
<text top="297" left="207" width="33" height="9" font="font5" id="p18_t27" reading_order_no="26" segment_no="4" tag_type="text">received</text>
<text top="297" left="250" width="12" height="9" font="font5" id="p18_t28" reading_order_no="27" segment_no="4" tag_type="text">the</text>
<text top="297" left="272" width="28" height="9" font="font5" id="p18_t29" reading_order_no="28" segment_no="4" tag_type="text">Master</text>
<text top="309" left="133" width="26" height="9" font="font5" id="p18_t30" reading_order_no="29" segment_no="4" tag_type="text">degree</text>
<text top="309" left="169" width="8" height="9" font="font5" id="p18_t31" reading_order_no="30" segment_no="4" tag_type="text">in</text>
<text top="309" left="187" width="38" height="9" font="font5" id="p18_t32" reading_order_no="31" segment_no="4" tag_type="text">Electrical</text>
<text top="309" left="236" width="14" height="9" font="font5" id="p18_t33" reading_order_no="32" segment_no="4" tag_type="text">and</text>
<text top="309" left="260" width="40" height="9" font="font5" id="p18_t34" reading_order_no="33" segment_no="4" tag_type="text">Computer</text>
<text top="321" left="133" width="49" height="9" font="font5" id="p18_t35" reading_order_no="34" segment_no="4" tag_type="text">Engineering</text>
<text top="321" left="191" width="19" height="9" font="font5" id="p18_t36" reading_order_no="35" segment_no="4" tag_type="text">from</text>
<text top="321" left="219" width="12" height="9" font="font5" id="p18_t37" reading_order_no="36" segment_no="4" tag_type="text">the</text>
<text top="321" left="241" width="42" height="9" font="font5" id="p18_t38" reading_order_no="37" segment_no="4" tag_type="text">University</text>
<text top="321" left="292" width="8" height="9" font="font5" id="p18_t39" reading_order_no="38" segment_no="4" tag_type="text">of</text>
<text top="333" left="133" width="37" height="9" font="font5" id="p18_t40" reading_order_no="39" segment_no="4" tag_type="text">Coimbra,</text>
<text top="333" left="180" width="36" height="9" font="font5" id="p18_t41" reading_order_no="40" segment_no="4" tag_type="text">Portugal.</text>
<text top="333" left="226" width="12" height="9" font="font5" id="p18_t42" reading_order_no="41" segment_no="4" tag_type="text">He</text>
<text top="333" left="248" width="7" height="9" font="font5" id="p18_t43" reading_order_no="42" segment_no="4" tag_type="text">is</text>
<text top="333" left="264" width="36" height="9" font="font5" id="p18_t44" reading_order_no="43" segment_no="4" tag_type="text">currently</text>
<text top="345" left="133" width="33" height="9" font="font5" id="p18_t45" reading_order_no="44" segment_no="4" tag_type="text">working</text>
<text top="345" left="176" width="31" height="9" font="font5" id="p18_t46" reading_order_no="45" segment_no="4" tag_type="text">towards</text>
<text top="345" left="218" width="12" height="9" font="font5" id="p18_t47" reading_order_no="46" segment_no="4" tag_type="text">the</text>
<text top="345" left="241" width="23" height="9" font="font5" id="p18_t48" reading_order_no="47" segment_no="4" tag_type="text">Ph.D.</text>
<text top="345" left="274" width="26" height="9" font="font5" id="p18_t49" reading_order_no="48" segment_no="4" tag_type="text">degree</text>
<text top="357" left="133" width="8" height="9" font="font5" id="p18_t50" reading_order_no="49" segment_no="4" tag_type="text">in</text>
<text top="357" left="152" width="12" height="9" font="font5" id="p18_t51" reading_order_no="50" segment_no="4" tag_type="text">the</text>
<text top="357" left="176" width="33" height="9" font="font5" id="p18_t52" reading_order_no="51" segment_no="4" tag_type="text">Institute</text>
<text top="357" left="221" width="8" height="9" font="font5" id="p18_t53" reading_order_no="52" segment_no="4" tag_type="text">of</text>
<text top="357" left="241" width="33" height="9" font="font5" id="p18_t54" reading_order_no="53" segment_no="4" tag_type="text">Systems</text>
<text top="357" left="286" width="14" height="9" font="font5" id="p18_t55" reading_order_no="54" segment_no="4" tag_type="text">and</text>
<text top="369" left="133" width="38" height="9" font="font5" id="p18_t56" reading_order_no="55" segment_no="4" tag_type="text">Robotics,</text>
<text top="369" left="185" width="42" height="9" font="font5" id="p18_t57" reading_order_no="56" segment_no="4" tag_type="text">University</text>
<text top="369" left="240" width="8" height="9" font="font5" id="p18_t58" reading_order_no="57" segment_no="4" tag_type="text">of</text>
<text top="369" left="263" width="37" height="9" font="font5" id="p18_t59" reading_order_no="58" segment_no="4" tag_type="text">Coimbra,</text>
<text top="380" left="133" width="167" height="9" font="font5" id="p18_t60" reading_order_no="59" segment_no="4" tag_type="text">Portugal. His research interests include</text>
<text top="392" left="133" width="167" height="9" font="font5" id="p18_t61" reading_order_no="60" segment_no="4" tag_type="text">deep learning, perception, and mobile</text>
<text top="404" left="133" width="35" height="9" font="font5" id="p18_t62" reading_order_no="61" segment_no="4" tag_type="text">robotics.</text>
<text top="428" left="133" width="167" height="9" font="font36" id="p18_t63" reading_order_no="62" segment_no="5" tag_type="text">Urbano J. Nunes (S’90-M’95-SM’09)</text>
<text top="440" left="133" width="167" height="9" font="font5" id="p18_t64" reading_order_no="63" segment_no="5" tag_type="text">received the Ph.D. in Electrical Engineer-</text>
<text top="452" left="133" width="167" height="9" font="font5" id="p18_t65" reading_order_no="64" segment_no="5" tag_type="text">ing from the University of Coimbra, Por-</text>
<text top="464" left="133" width="167" height="9" font="font5" id="p18_t66" reading_order_no="65" segment_no="5" tag_type="text">tugal, in 1995. He is a Full Professor with</text>
<text top="476" left="133" width="167" height="9" font="font5" id="p18_t67" reading_order_no="66" segment_no="5" tag_type="text">the Electrical and Computer Engineering</text>
<text top="488" left="133" width="167" height="9" font="font5" id="p18_t68" reading_order_no="67" segment_no="5" tag_type="text">Department of Coimbra University, and a</text>
<text top="500" left="133" width="167" height="9" font="font5" id="p18_t69" reading_order_no="68" segment_no="5" tag_type="text">Senior researcher of the Institute for Sys-</text>
<text top="512" left="133" width="167" height="9" font="font5" id="p18_t70" reading_order_no="69" segment_no="5" tag_type="text">tems and Robotics (ISR-UC) where he is</text>
<text top="524" left="133" width="167" height="9" font="font5" id="p18_t71" reading_order_no="70" segment_no="5" tag_type="text">the coordinator of the Human-Centered</text>
<text top="536" left="49" width="251" height="9" font="font5" id="p18_t72" reading_order_no="71" segment_no="5" tag_type="text">Mobile Robotics lab. He has been involved with/responsible</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p18_t73" reading_order_no="72" segment_no="5" tag_type="text">for several funded projects at both national and international</text>
<text top="560" left="49" width="251" height="9" font="font5" id="p18_t74" reading_order_no="73" segment_no="5" tag_type="text">levels in the areas of mobile robotics. He serves as Associate</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p18_t75" reading_order_no="74" segment_no="5" tag_type="text">Editor the IEEE Transactions on Intelligent Vehicles (2015-).</text>
<text top="584" left="49" width="251" height="9" font="font5" id="p18_t76" reading_order_no="75" segment_no="5" tag_type="text">Prof. Nunes was with several international conferences: Gen-</text>
<text top="596" left="49" width="251" height="9" font="font5" id="p18_t77" reading_order_no="76" segment_no="5" tag_type="text">eral co-chair of the 11th IEEE ICAR2003; Program Chair of</text>
<text top="608" left="49" width="251" height="9" font="font5" id="p18_t78" reading_order_no="77" segment_no="5" tag_type="text">IEEE ITSC2006; General Chair of the 13th IEEE ITSC2010;</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p18_t79" reading_order_no="78" segment_no="5" tag_type="text">General Chair for the IEEE/RSJ IROS 2012; and General</text>
<text top="632" left="49" width="137" height="9" font="font5" id="p18_t80" reading_order_no="79" segment_no="5" tag_type="text">Chair of the IEEE ROMAN2017.</text>
</page>
<outline>
<item page="1">I Introduction</item>
<item page="2">II Key concepts of place recognition</item>
<outline>
<item page="2">II-A What is a place?</item>
<item page="2">II-B How are places recognized and remembered?</item>
<outline>
<item page="2">II-B1 Place Modeling</item>
<item page="2">II-B2 Place Mapping</item>
<item page="3">II-B3 Belief Generation</item>
</outline>
<item page="3">II-C What are the major challenges?</item>
<outline>
<item page="3">II-C1 Appearance Change and Perceptual Aliasing</item>
<item page="3">II-C2 Viewpoint Changing </item>
<item page="4">II-C3 Scalability</item>
</outline>
</outline>
<item page="4">III Sensors</item>
<item page="4">IV Supervised place recognition</item>
<outline>
<item page="5">IV-A Pre-trained-based Frameworks</item>
<outline>
<item page="5">IV-A1 Holistic-based</item>
<item page="6">IV-A2 Landmark-based</item>
<item page="6">IV-A3 Region-based</item>
</outline>
<item page="7">IV-B End-to-End Frameworks</item>
<outline>
<item page="7">IV-B1 Loss functions</item>
<item page="8">IV-B2 Camera-based Networks</item>
<item page="9">IV-B3 3D LiDAR-based Network</item>
<item page="9">IV-B4 RADAR-based</item>
</outline>
</outline>
<item page="9">V Unsupervised Place Recognition</item>
<item page="10">VI Semi-supervised Place Recognition</item>
<item page="10">VII Other Frameworks</item>
<outline>
<item page="10">VII-A Parallel Frameworks</item>
<item page="11">VII-B Hierarchical Frameworks</item>
</outline>
<item page="12">VIII Conclusion and discussion</item>
<item page="13">References</item>
</outline>
</pdf2xml>
