<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="7" family="NimbusSanL-Regu" color="#000000"/>
	<fontspec id="font1" size="24" family="NimbusSanL-Regu" color="#000000"/>
	<fontspec id="font2" size="11" family="NimbusSanL-Regu" color="#000000"/>
	<fontspec id="font3" size="8" family="NimbusSanL,Bold" color="#000000"/>
	<fontspec id="font4" size="8" family="NimbusSanL-Regu" color="#000000"/>
	<fontspec id="font5" size="8" family="CMMI8" color="#000000"/>
	<fontspec id="font6" size="11" family="Dingbats" color="#000000"/>
	<fontspec id="font7" size="11" family="NimbusSanL,Bold" color="#000000"/>
	<fontspec id="font8" size="9" family="NimbusSanL,Bold" color="#000000"/>
	<fontspec id="font9" size="27" family="URWPalladioL-Roma" color="#000000"/>
	<fontspec id="font10" size="8" family="URWPalladioL-Roma" color="#000000"/>
	<fontspec id="font11" size="10" family="URWPalladioL-Roma" color="#000000"/>
	<fontspec id="font12" size="8" family="CMSY8" color="#000000"/>
	<fontspec id="font13" size="8" family="URWPalladioL-Ital" color="#000000"/>
	<fontspec id="font14" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font15" size="20" family="Times" color="#7f7f7f"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="text">1</text>
<text top="57" left="84" width="443" height="22" font="font1" id="p1_t3" reading_order_no="3" segment_no="2" tag_type="title">Deep3DPose: Realtime Reconstruction of</text>
<text top="85" left="72" width="467" height="22" font="font1" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="title">Arbitrarily Posed Human Bodies from Single</text>
<text top="113" left="237" width="137" height="22" font="font1" id="p1_t5" reading_order_no="5" segment_no="2" tag_type="title">RGB Images</text>
<text top="151" left="71" width="471" height="10" font="font2" id="p1_t6" reading_order_no="6" segment_no="3" tag_type="text">Liguo Jiang, Miaopeng Li, Jianjie Zhang, Congyi Wang, Juntao Ye, Xinguo Liu, and Jinxiang Chai</text>
<text top="183" left="68" width="472" height="8" font="font3" id="p1_t7" reading_order_no="7" segment_no="4" tag_type="text">Abstract —We introduce an approach that accurately reconstructs 3D human poses and detailed 3D full-body geometric models from<b>Abstract</b></text>
<text top="193" left="68" width="457" height="7" font="font4" id="p1_t8" reading_order_no="8" segment_no="4" tag_type="text">single images in realtime. The key idea of our approach is a novel end-to-end multi-task deep learning framework that uses single</text>
<text top="203" left="68" width="464" height="7" font="font4" id="p1_t9" reading_order_no="9" segment_no="4" tag_type="text">images to predict five outputs simultaneously: foreground segmentation mask, 2D joints positions, semantic body partitions, 3D part</text>
<text top="214" left="68" width="472" height="7" font="font4" id="p1_t10" reading_order_no="10" segment_no="4" tag_type="text">orientations and uv coordinates ( uv map). The multi-task network architecture not only generates more visual cues for reconstruction,</text>
<text top="224" left="68" width="475" height="7" font="font4" id="p1_t11" reading_order_no="11" segment_no="4" tag_type="text">but also makes each individual prediction more accurate. The CNN regressor is further combined with an optimization based algorithm</text>
<text top="234" left="68" width="469" height="7" font="font4" id="p1_t12" reading_order_no="12" segment_no="4" tag_type="text">for accurate kinematic pose reconstruction and full-body shape modeling. We show that the realtime reconstruction reaches accurate</text>
<text top="245" left="68" width="471" height="7" font="font4" id="p1_t13" reading_order_no="13" segment_no="4" tag_type="text">fitting that has not been seen before, especially for wild images. We demonstrate the results of our realtime 3D pose and human body</text>
<text top="255" left="68" width="461" height="7" font="font4" id="p1_t14" reading_order_no="14" segment_no="4" tag_type="text">reconstruction system on various challenging in-the-wild videos. We show the system advances the frontier of 3D human body and</text>
<text top="266" left="68" width="405" height="7" font="font4" id="p1_t15" reading_order_no="15" segment_no="4" tag_type="text">pose reconstruction from single images by quantitative evaluations and comparisons with state-of-the-art methods.</text>
<text top="287" left="68" width="419" height="8" font="font3" id="p1_t16" reading_order_no="16" segment_no="5" tag_type="text">Index Terms —Realtime RGB-based motion capture, multi-task regression, 3D human body and shape reconstruction.</text>
<text top="302" left="302" width="9" height="12" font="font6" id="p1_t17" reading_order_no="17" segment_no="6" tag_type="text">F</text>
<text top="323" left="48" width="6" height="10" font="font7" id="p1_t18" reading_order_no="18" segment_no="7" tag_type="title">1</text>
<text top="323" left="66" width="74" height="10" font="font7" id="p1_t19" reading_order_no="19" segment_no="7" tag_type="title">I NTRODUCTION</text>
<text top="345" left="48" width="19" height="27" font="font9" id="p1_t20" reading_order_no="20" segment_no="8" tag_type="text">C</text>
<text top="345" left="69" width="231" height="9" font="font11" id="p1_t21" reading_order_no="21" segment_no="8" tag_type="text">REATING natural-looking human characters with real-<b>Index Terms</b></text>
<text top="357" left="69" width="231" height="9" font="font11" id="p1_t22" reading_order_no="22" segment_no="8" tag_type="text">istic motions is essential for many applications, includ-</text>
<text top="368" left="48" width="252" height="9" font="font11" id="p1_t23" reading_order_no="23" segment_no="8" tag_type="text">ing movies, video games, robotics, sports training, medical</text>
<text top="380" left="48" width="252" height="9" font="font11" id="p1_t24" reading_order_no="24" segment_no="8" tag_type="text">analytics and social behavior recognition, and so on. Using<b>1</b></text>
<text top="391" left="48" width="252" height="9" font="font11" id="p1_t25" reading_order_no="25" segment_no="8" tag_type="text">expensive and special equipment, such as multi-cameras<b>I</b></text>
<text top="403" left="48" width="252" height="9" font="font11" id="p1_t26" reading_order_no="26" segment_no="8" tag_type="text">and reflective markers based motion capture systems, this<b>NTRODUCTION</b></text>
<text top="414" left="48" width="252" height="9" font="font11" id="p1_t27" reading_order_no="27" segment_no="8" tag_type="text">task can be achieved without too much pain for scenes</text>
<text top="426" left="48" width="252" height="9" font="font11" id="p1_t28" reading_order_no="28" segment_no="8" tag_type="text">that do not impose many restrictions. Yet the inconvenient</text>
<text top="437" left="48" width="252" height="9" font="font11" id="p1_t29" reading_order_no="29" segment_no="8" tag_type="text">accessibility to such equipment has limited the flourishing</text>
<text top="449" left="48" width="177" height="9" font="font11" id="p1_t30" reading_order_no="30" segment_no="8" tag_type="text">of 3D human motion related applications.</text>
<text top="461" left="62" width="238" height="9" font="font11" id="p1_t31" reading_order_no="31" segment_no="11" tag_type="text">The ideal and most convenient way is to use off-the-</text>
<text top="473" left="48" width="252" height="9" font="font11" id="p1_t32" reading_order_no="32" segment_no="11" tag_type="text">shelf RGB cameras to capture live performance and create</text>
<text top="484" left="48" width="252" height="9" font="font11" id="p1_t33" reading_order_no="33" segment_no="11" tag_type="text">3D motion data. The minimal requirement of a single RGB</text>
<text top="496" left="48" width="252" height="9" font="font11" id="p1_t34" reading_order_no="34" segment_no="11" tag_type="text">camera is particularly appealing, as it offers the lowest cost,</text>
<text top="507" left="48" width="252" height="9" font="font11" id="p1_t35" reading_order_no="35" segment_no="11" tag_type="text">easy setup, and the potential of converting huge volume of</text>
<text top="519" left="48" width="252" height="9" font="font11" id="p1_t36" reading_order_no="36" segment_no="11" tag_type="text">Internet videos into a large-scale 3D human body corpus.</text>
<text top="530" left="48" width="252" height="9" font="font11" id="p1_t37" reading_order_no="37" segment_no="11" tag_type="text">Recent years have seen much research efforts being devoted</text>
<text top="542" left="48" width="252" height="9" font="font11" id="p1_t38" reading_order_no="38" segment_no="11" tag_type="text">to estimating not only the skeletal motion but also body</text>
<text top="553" left="48" width="252" height="9" font="font11" id="p1_t39" reading_order_no="39" segment_no="11" tag_type="text">pose and shape. Yet reconstructing 3D pose and shape from</text>
<text top="565" left="48" width="252" height="9" font="font11" id="p1_t40" reading_order_no="40" segment_no="11" tag_type="text">a single RGB camera is still a challenging and undercon-</text>
<text top="576" left="48" width="252" height="9" font="font11" id="p1_t41" reading_order_no="41" segment_no="11" tag_type="text">strained problem with inherent ambiguities, especially in</text>
<text top="588" left="48" width="252" height="9" font="font11" id="p1_t42" reading_order_no="42" segment_no="11" tag_type="text">wild uncontrolled environment and in realtime. Therefore</text>
<text top="599" left="48" width="252" height="9" font="font11" id="p1_t43" reading_order_no="43" segment_no="11" tag_type="text">the state-of-the-art results are often vulnerable to ambigu-</text>
<text top="611" left="48" width="252" height="9" font="font11" id="p1_t44" reading_order_no="44" segment_no="11" tag_type="text">ities in the video (e.g., occlusions, cloth deformation, and</text>
<text top="623" left="48" width="252" height="9" font="font11" id="p1_t45" reading_order_no="45" segment_no="11" tag_type="text">illumination changes), degeneracy in camera motion, and</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p1_t46" reading_order_no="46" segment_no="11" tag_type="text">a lack of discernible features on a human body. Moreover,</text>
<text top="662" left="48" width="4" height="8" font="font12" id="p1_t47" reading_order_no="82" segment_no="14" tag_type="list">•</text>
<text top="662" left="61" width="239" height="8" font="font13" id="p1_t48" reading_order_no="83" segment_no="14" tag_type="list">L. Jiang, J. Ye are with NLPR, Institute of Automation, Chinese Academy</text>
<text top="671" left="61" width="239" height="8" font="font13" id="p1_t49" reading_order_no="84" segment_no="14" tag_type="list">of Sciences, Beijing, China and School of Artificial Intelligence, University</text>
<text top="680" left="61" width="156" height="8" font="font13" id="p1_t50" reading_order_no="85" segment_no="14" tag_type="list">of Chinese Academy of Sciences, Beijing, China.</text>
<text top="689" left="61" width="191" height="8" font="font13" id="p1_t51" reading_order_no="86" segment_no="14" tag_type="list">E-mail: jiangliguo2015@ia.ac.cn and yejuntao@gmail.com.</text>
<text top="698" left="48" width="4" height="8" font="font12" id="p1_t52" reading_order_no="87" segment_no="15" tag_type="list">•</text>
<text top="698" left="61" width="184" height="8" font="font13" id="p1_t53" reading_order_no="88" segment_no="15" tag_type="list">J. Zhang and C. Wang are with Xmov, Shanghai, China.</text>
<text top="707" left="48" width="4" height="8" font="font12" id="p1_t54" reading_order_no="89" segment_no="16" tag_type="list">•</text>
<text top="707" left="61" width="239" height="8" font="font13" id="p1_t55" reading_order_no="90" segment_no="16" tag_type="list">M. Li and X. Liu are with State Key Laboratory of CAD&amp;CG, Zhejiang</text>
<text top="716" left="61" width="99" height="8" font="font13" id="p1_t56" reading_order_no="91" segment_no="16" tag_type="list">University, Hangzhou, China.</text>
<text top="725" left="48" width="4" height="8" font="font12" id="p1_t57" reading_order_no="92" segment_no="17" tag_type="list">•</text>
<text top="725" left="61" width="127" height="8" font="font13" id="p1_t58" reading_order_no="93" segment_no="17" tag_type="list">J. Chai is with Texas A&amp;M University.</text>
<text top="345" left="312" width="252" height="9" font="font11" id="p1_t59" reading_order_no="47" segment_no="9" tag_type="text">methods that achieve realtime, robust as well as accurate</text>
<text top="357" left="312" width="215" height="9" font="font11" id="p1_t60" reading_order_no="48" segment_no="9" tag_type="text">performance have rarely been seen common so far.</text>
<text top="368" left="326" width="238" height="9" font="font11" id="p1_t61" reading_order_no="49" segment_no="10" tag_type="text">We introduce an approach that is capable of obtaining</text>
<text top="380" left="312" width="252" height="9" font="font11" id="p1_t62" reading_order_no="50" segment_no="10" tag_type="text">accurate 3D human poses and body shape from single wild</text>
<text top="391" left="312" width="252" height="9" font="font11" id="p1_t63" reading_order_no="51" segment_no="10" tag_type="text">images in realtime. When applied to video sequences, our</text>
<text top="403" left="312" width="252" height="9" font="font11" id="p1_t64" reading_order_no="52" segment_no="10" tag_type="text">system outputs temporally consistent bodies in motion at</text>
<text top="414" left="312" width="252" height="9" font="font11" id="p1_t65" reading_order_no="53" segment_no="10" tag_type="text">more than 20 Hz on a desktop computer. The power of</text>
<text top="426" left="312" width="252" height="9" font="font11" id="p1_t66" reading_order_no="54" segment_no="10" tag_type="text">our method comes from a convolutional neural network</text>
<text top="437" left="312" width="252" height="9" font="font11" id="p1_t67" reading_order_no="55" segment_no="10" tag_type="text">(CNN) which leverages a multi-task architecture that is able</text>
<text top="449" left="312" width="252" height="9" font="font11" id="p1_t68" reading_order_no="56" segment_no="10" tag_type="text">to outputs five results simultaneously: foreground mask, 2D</text>
<text top="460" left="312" width="252" height="9" font="font11" id="p1_t69" reading_order_no="57" segment_no="10" tag_type="text">joint positions, body partition, 3D part orientation fields</text>
<text top="472" left="312" width="252" height="9" font="font11" id="p1_t70" reading_order_no="58" segment_no="10" tag_type="text">(POFs) and uv coordinates. Body partition index and uv</text>
<text top="483" left="312" width="252" height="10" font="font11" id="p1_t71" reading_order_no="59" segment_no="10" tag_type="text">coordinates indicate part-specific uv coordinates, which is</text>
<text top="495" left="312" width="252" height="9" font="font11" id="p1_t72" reading_order_no="60" segment_no="10" tag_type="text">called IUV [1]. While none of existing networks support so</text>
<text top="507" left="312" width="252" height="9" font="font11" id="p1_t73" reading_order_no="61" segment_no="10" tag_type="text">many tasks at one time, this architecture makes it possible</text>
<text top="518" left="312" width="252" height="9" font="font11" id="p1_t74" reading_order_no="62" segment_no="10" tag_type="text">to refine multiple predictions recurrently. The regressed</text>
<text top="530" left="312" width="252" height="9" font="font11" id="p1_t75" reading_order_no="63" segment_no="10" tag_type="text">results are fed into a kinematic skeleton pose and body</text>
<text top="541" left="312" width="252" height="9" font="font11" id="p1_t76" reading_order_no="64" segment_no="10" tag_type="text">geometry fitting optimizer and outputs a camera-relative</text>
<text top="553" left="312" width="252" height="9" font="font11" id="p1_t77" reading_order_no="65" segment_no="10" tag_type="text">full 3D posed body mesh. The success of our approach</text>
<text top="564" left="312" width="252" height="9" font="font11" id="p1_t78" reading_order_no="66" segment_no="10" tag_type="text">also relies on the expansion of publicly available training</text>
<text top="576" left="312" width="252" height="9" font="font11" id="p1_t79" reading_order_no="67" segment_no="10" tag_type="text">datasets. While it is feasible to annotate a small number of</text>
<text top="587" left="312" width="252" height="9" font="font11" id="p1_t80" reading_order_no="68" segment_no="10" tag_type="text">labels in 2D images, upgrading to a large number of 3D rep-</text>
<text top="599" left="312" width="252" height="9" font="font11" id="p1_t81" reading_order_no="69" segment_no="10" tag_type="text">resentation becomes impractical. The new data is collected</text>
<text top="610" left="312" width="252" height="9" font="font11" id="p1_t82" reading_order_no="70" segment_no="10" tag_type="text">with our in-house cost-efficient, marker-less and scalable</text>
<text top="622" left="312" width="233" height="9" font="font11" id="p1_t83" reading_order_no="71" segment_no="10" tag_type="text">data acquisition system, and is preprocessed efficiently.</text>
<text top="634" left="326" width="238" height="9" font="font11" id="p1_t84" reading_order_no="72" segment_no="13" tag_type="text">The power of our system is demonstrated by recon-</text>
<text top="645" left="312" width="252" height="9" font="font11" id="p1_t85" reading_order_no="73" segment_no="13" tag_type="text">structing 3D human poses and shapes for a wide variety of</text>
<text top="657" left="312" width="252" height="9" font="font11" id="p1_t86" reading_order_no="74" segment_no="13" tag_type="text">subjects from monocular video sequences. We have tested</text>
<text top="668" left="312" width="252" height="9" font="font11" id="p1_t87" reading_order_no="75" segment_no="13" tag_type="text">our realtime system on both live video streams and the</text>
<text top="680" left="312" width="252" height="9" font="font11" id="p1_t88" reading_order_no="76" segment_no="13" tag_type="text">Internet videos, demonstrating its accuracy and robustness</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p1_t89" reading_order_no="77" segment_no="13" tag_type="text">under a variety of uncontrolled illumination conditions and</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p1_t90" reading_order_no="78" segment_no="13" tag_type="text">backgrounds, as well as significant variations on races,</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p1_t91" reading_order_no="79" segment_no="13" tag_type="text">shapes, poses, clothes across individuals. We show that</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p1_t92" reading_order_no="80" segment_no="13" tag_type="text">our system can reconstruct bodies with realistic poses for</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p1_t93" reading_order_no="81" segment_no="13" tag_type="text">highly dynamic motions such as figure skating (Fig. 1), low</text>
<text top="546" left="32" width="0" height="18" font="font15" id="p1_t94" reading_order_no="0" segment_no="12" tag_type="title">arXiv:2106.11536v1  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font16" size="7" family="Verdana" color="#000000"/>
	<fontspec id="font17" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font18" size="10" family="URWPalladioL,Bold" color="#000000"/>
	<fontspec id="font19" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font20" size="10" family="URWPalladioL-Ital" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">2</text>
<text top="127" left="171" width="11" height="9" font="font16" id="p2_t3" reading_order_no="2" segment_no="2" tag_type="figure">(a)</text>
<text top="218" left="171" width="11" height="9" font="font16" id="p2_t4" reading_order_no="5" segment_no="2" tag_type="figure">(d)</text>
<text top="127" left="309" width="11" height="9" font="font16" id="p2_t5" reading_order_no="3" segment_no="2" tag_type="figure">(b)</text>
<text top="127" left="450" width="11" height="9" font="font16" id="p2_t6" reading_order_no="4" segment_no="2" tag_type="figure">(c)</text>
<text top="218" left="310" width="11" height="9" font="font16" id="p2_t7" reading_order_no="6" segment_no="2" tag_type="figure">(e)</text>
<text top="219" left="451" width="10" height="9" font="font16" id="p2_t8" reading_order_no="7" segment_no="2" tag_type="figure">(f)</text>
<text top="219" left="542" width="11" height="9" font="font16" id="p2_t9" reading_order_no="8" segment_no="2" tag_type="figure">(g)</text>
<text top="240" left="48" width="516" height="9" font="font11" id="p2_t10" reading_order_no="9" segment_no="3" tag_type="text">Fig. 1: Given an image (a), our regression network produces five outputs simultaneously: foreground segmentation mask</text>
<text top="251" left="48" width="516" height="9" font="font11" id="p2_t11" reading_order_no="10" segment_no="3" tag_type="text">(b), 2D joints positions (c), body partition (d), and a uv map (e), 3D part orientations (applied to a mean skeleton) (f). These</text>
<text top="263" left="48" width="514" height="9" font="font11" id="p2_t12" reading_order_no="11" segment_no="3" tag_type="text">outputs further guide the generation of a full-body model (g). The whole process runs in realtime on a desktop computer.</text>
<text top="296" left="48" width="252" height="9" font="font11" id="p2_t13" reading_order_no="12" segment_no="4" tag_type="text">energy motions such as walking, and motions with human-</text>
<text top="307" left="48" width="252" height="9" font="font11" id="p2_t14" reading_order_no="13" segment_no="4" tag_type="text">environment interaction such as sitting and standing up. We</text>
<text top="319" left="48" width="252" height="9" font="font11" id="p2_t15" reading_order_no="14" segment_no="4" tag_type="text">evaluate the importance of each key component of our al-</text>
<text top="330" left="48" width="252" height="9" font="font11" id="p2_t16" reading_order_no="15" segment_no="4" tag_type="text">gorithm, by dropping off each component in the reconstruc-</text>
<text top="342" left="48" width="252" height="9" font="font11" id="p2_t17" reading_order_no="16" segment_no="4" tag_type="text">tion. We advance the state-of-the-art realtime reconstruction</text>
<text top="353" left="48" width="252" height="9" font="font11" id="p2_t18" reading_order_no="17" segment_no="4" tag_type="text">of 3D human poses and detailed geometric body meshes</text>
<text top="365" left="48" width="252" height="9" font="font11" id="p2_t19" reading_order_no="18" segment_no="4" tag_type="text">from single images, and offer comparisons with alternative</text>
<text top="376" left="48" width="70" height="9" font="font11" id="p2_t20" reading_order_no="19" segment_no="4" tag_type="text">solutions [2]–[5].</text>
<text top="388" left="62" width="215" height="9" font="font11" id="p2_t21" reading_order_no="20" segment_no="7" tag_type="text">The highlights of our 3D reconstruction system are</text>
<text top="407" left="62" width="4" height="7" font="font17" id="p2_t22" reading_order_no="21" segment_no="8" tag_type="list">•</text>
<text top="405" left="78" width="222" height="9" font="font18" id="p2_t23" reading_order_no="22" segment_no="8" tag_type="list">Realtime. Thanks to our specially designed neural</text>
<text top="417" left="78" width="222" height="9" font="font11" id="p2_t24" reading_order_no="23" segment_no="8" tag_type="list">network, we are able to regress multiple human</text>
<text top="428" left="78" width="222" height="9" font="font11" id="p2_t25" reading_order_no="24" segment_no="8" tag_type="list">structural features from single images in realtime.<b>Realtime.</b></text>
<text top="440" left="78" width="222" height="9" font="font11" id="p2_t26" reading_order_no="25" segment_no="8" tag_type="list">We further feed network outputs to an efficient 3D</text>
<text top="451" left="78" width="222" height="9" font="font11" id="p2_t27" reading_order_no="26" segment_no="8" tag_type="list">human pose and body geometry fitting optimizer,</text>
<text top="463" left="78" width="209" height="9" font="font11" id="p2_t28" reading_order_no="27" segment_no="8" tag_type="list">and achieve realtime reconstruction performance.</text>
<text top="476" left="62" width="4" height="7" font="font17" id="p2_t29" reading_order_no="28" segment_no="9" tag_type="list">•</text>
<text top="475" left="78" width="222" height="9" font="font18" id="p2_t30" reading_order_no="29" segment_no="9" tag_type="list">Fully automatic and robust. With the abundant</text>
<text top="486" left="78" width="222" height="9" font="font11" id="p2_t31" reading_order_no="30" segment_no="9" tag_type="list">regression outputs per-frame, reconstruction can be</text>
<text top="498" left="78" width="222" height="9" font="font11" id="p2_t32" reading_order_no="31" segment_no="9" tag_type="list">achieved from one single image, independent of any</text>
<text top="509" left="78" width="222" height="9" font="font11" id="p2_t33" reading_order_no="32" segment_no="9" tag_type="list">pre-initialized state. This makes reconstruction from<b>Fully automatic and robust.</b></text>
<text top="521" left="78" width="222" height="9" font="font11" id="p2_t34" reading_order_no="33" segment_no="9" tag_type="list">videos no longer suffers from the headache of re-</text>
<text top="532" left="78" width="222" height="9" font="font11" id="p2_t35" reading_order_no="34" segment_no="9" tag_type="list">initialization. Our system is also robust to illumina-</text>
<text top="544" left="78" width="182" height="9" font="font11" id="p2_t36" reading_order_no="35" segment_no="9" tag_type="list">tion variation, as well as clothing diversity.</text>
<text top="557" left="62" width="4" height="7" font="font17" id="p2_t37" reading_order_no="36" segment_no="11" tag_type="list">•</text>
<text top="555" left="78" width="222" height="9" font="font18" id="p2_t38" reading_order_no="37" segment_no="11" tag_type="list">Accuracy. Our realtime system achieves reconstruc-</text>
<text top="567" left="78" width="222" height="9" font="font11" id="p2_t39" reading_order_no="38" segment_no="11" tag_type="list">tion quality that is even more accurate than most</text>
<text top="578" left="78" width="222" height="9" font="font11" id="p2_t40" reading_order_no="39" segment_no="11" tag_type="list">offline or video-based methods in wild images. This</text>
<text top="590" left="78" width="222" height="9" font="font11" id="p2_t41" reading_order_no="40" segment_no="11" tag_type="list">achievement is mainly due to three points: (1) a novel</text>
<text top="601" left="78" width="222" height="9" font="font11" id="p2_t42" reading_order_no="41" segment_no="11" tag_type="list">multi-task deep learning network predicts abundant<b>Accuracy.</b></text>
<text top="613" left="78" width="222" height="9" font="font11" id="p2_t43" reading_order_no="42" segment_no="11" tag_type="list">features, which boosts each other; (2) an efficient</text>
<text top="625" left="78" width="222" height="9" font="font11" id="p2_t44" reading_order_no="43" segment_no="11" tag_type="list">reconstruction process that seamlessly integrates all</text>
<text top="636" left="78" width="222" height="9" font="font11" id="p2_t45" reading_order_no="44" segment_no="11" tag_type="list">the visual features obtained from the deep learning</text>
<text top="648" left="78" width="222" height="9" font="font11" id="p2_t46" reading_order_no="45" segment_no="11" tag_type="list">network. (3) the augmentation to existing training</text>
<text top="659" left="78" width="161" height="9" font="font11" id="p2_t47" reading_order_no="46" segment_no="11" tag_type="list">dataset with our newly collected data.</text>
<text top="687" left="48" width="6" height="10" font="font7" id="p2_t48" reading_order_no="47" segment_no="12" tag_type="title">2</text>
<text top="687" left="66" width="81" height="10" font="font7" id="p2_t49" reading_order_no="48" segment_no="12" tag_type="title">R ELATED W ORK</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p2_t50" reading_order_no="49" segment_no="13" tag_type="text">The research on human body reconstruction from single</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p2_t51" reading_order_no="50" segment_no="13" tag_type="text">RGB images is traced back to skeleton joints estimation,</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p2_t52" reading_order_no="51" segment_no="13" tag_type="text">from 2D to 3D, and has achieved significant advances in</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p2_t53" reading_order_no="52" segment_no="13" tag_type="text">recent years. This line of work has further boosted the<b>2</b></text>
<text top="296" left="312" width="252" height="9" font="font11" id="p2_t54" reading_order_no="53" segment_no="5" tag_type="text">interest for simultaneous pose and shape estimation. We will<b>R</b></text>
<text top="307" left="312" width="252" height="9" font="font11" id="p2_t55" reading_order_no="54" segment_no="5" tag_type="text">focus our review on 2D pose estimation, 3D pose and body<b>ELATED</b></text>
<text top="319" left="312" width="146" height="9" font="font11" id="p2_t56" reading_order_no="55" segment_no="5" tag_type="text">reconstruction from single images.<b>W</b></text>
<text top="332" left="326" width="238" height="9" font="font18" id="p2_t57" reading_order_no="56" segment_no="6" tag_type="text">2D Pose Estimation. Nowadays image based single-<b>ORK</b></text>
<text top="343" left="312" width="252" height="9" font="font11" id="p2_t58" reading_order_no="57" segment_no="6" tag_type="text">person estimation [6]–[8] has achieved great improvement</text>
<text top="355" left="312" width="252" height="9" font="font11" id="p2_t59" reading_order_no="58" segment_no="6" tag_type="text">due to the success of CNN. These methods usually regress</text>
<text top="366" left="312" width="252" height="9" font="font11" id="p2_t60" reading_order_no="59" segment_no="6" tag_type="text">a probability map for each joint, designating the probability</text>
<text top="378" left="312" width="252" height="9" font="font11" id="p2_t61" reading_order_no="60" segment_no="6" tag_type="text">of a joint being located on each image pixel. The image-</text>
<text top="390" left="312" width="252" height="9" font="font11" id="p2_t62" reading_order_no="61" segment_no="6" tag_type="text">to-surface correspondence (IUV), represented by human</text>
<text top="401" left="312" width="252" height="9" font="font11" id="p2_t63" reading_order_no="62" segment_no="6" tag_type="text">part partition and uv coordinates map, was proposed in</text>
<text top="413" left="312" width="252" height="9" font="font11" id="p2_t64" reading_order_no="63" segment_no="6" tag_type="text">Densepose [1], and it is more effective and expressive than</text>
<text top="424" left="312" width="252" height="9" font="font11" id="p2_t65" reading_order_no="64" segment_no="6" tag_type="text">positioning just sparse 2D joints. By predicting the ( u, v )<b>2D Pose Estimation.</b></text>
<text top="436" left="312" width="252" height="9" font="font11" id="p2_t66" reading_order_no="65" segment_no="6" tag_type="text">coordinates and body part index for each pixel, a dense</text>
<text top="447" left="312" width="252" height="9" font="font11" id="p2_t67" reading_order_no="66" segment_no="6" tag_type="text">correspondence between pixels and points on a 3D mesh</text>
<text top="459" left="312" width="252" height="9" font="font11" id="p2_t68" reading_order_no="67" segment_no="6" tag_type="text">is defined. Our goal is different from these 2D or dense</text>
<text top="470" left="312" width="252" height="9" font="font11" id="p2_t69" reading_order_no="68" segment_no="6" tag_type="text">pose estimation methods in that we focus on 3D pose and</text>
<text top="482" left="312" width="135" height="9" font="font11" id="p2_t70" reading_order_no="69" segment_no="6" tag_type="text">geometry model reconstruction.</text>
<text top="495" left="326" width="238" height="9" font="font18" id="p2_t71" reading_order_no="70" segment_no="10" tag_type="text">3D Pose estimation. Other than regressing 2D pose or</text>
<text top="507" left="312" width="252" height="9" font="font11" id="p2_t72" reading_order_no="71" segment_no="10" tag_type="text">dense pose from single images, many people attempt to</text>
<text top="518" left="312" width="252" height="9" font="font11" id="p2_t73" reading_order_no="72" segment_no="10" tag_type="text">estimate 3D pose directly from images. Most recent works</text>
<text top="530" left="312" width="252" height="9" font="font11" id="p2_t74" reading_order_no="73" segment_no="10" tag_type="text">can be divided into two categories: the one-stage method</text>
<text top="541" left="312" width="252" height="9" font="font11" id="p2_t75" reading_order_no="74" segment_no="10" tag_type="text">and the two-stage method . In the two-stage methods [9]–[12],</text>
<text top="553" left="312" width="252" height="9" font="font11" id="p2_t76" reading_order_no="75" segment_no="10" tag_type="text">the task of 3D pose estimation is decoupled into 2D joint</text>
<text top="564" left="312" width="252" height="9" font="font11" id="p2_t77" reading_order_no="76" segment_no="10" tag_type="text">detection and 3D coordinate regression. However, due to</text>
<text top="576" left="312" width="252" height="9" font="font11" id="p2_t78" reading_order_no="77" segment_no="10" tag_type="text">ambiguity of 3D estimation from 2D joints, these methods</text>
<text top="587" left="312" width="252" height="9" font="font11" id="p2_t79" reading_order_no="78" segment_no="10" tag_type="text">not only overlook certain image features having 3D cues,</text>
<text top="599" left="312" width="252" height="9" font="font11" id="p2_t80" reading_order_no="79" segment_no="10" tag_type="text">but also are very sensitive to the results of 2D pose es-</text>
<text top="610" left="312" width="252" height="9" font="font11" id="p2_t81" reading_order_no="80" segment_no="10" tag_type="text">timation. To overcome the ambiguity in lifting 2D to 3D,</text>
<text top="622" left="312" width="252" height="9" font="font11" id="p2_t82" reading_order_no="81" segment_no="10" tag_type="text">priors are introduced in some works. Pavlakos et al. [13]</text>
<text top="634" left="312" width="252" height="9" font="font11" id="p2_t83" reading_order_no="82" segment_no="10" tag_type="text">further annotated the ordinal depth relation in the COCO</text>
<text top="645" left="312" width="252" height="9" font="font11" id="p2_t84" reading_order_no="83" segment_no="10" tag_type="text">dataset [14] and the MPII dataset [15], and proposed to</text>
<text top="657" left="312" width="252" height="9" font="font11" id="p2_t85" reading_order_no="84" segment_no="10" tag_type="text">estimate not only the 2D pose but also the ordinal depth<b>3D Pose estimation.</b></text>
<text top="668" left="312" width="252" height="9" font="font11" id="p2_t86" reading_order_no="85" segment_no="10" tag_type="text">relation as the extra input for lifting 2D to 3D. They achieved</text>
<text top="680" left="312" width="252" height="9" font="font11" id="p2_t87" reading_order_no="86" segment_no="10" tag_type="text">much better results. Different from [13], joint limits and</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p2_t88" reading_order_no="87" segment_no="10" tag_type="text">bone lengths are introduced as constraints in [16]. The one-</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p2_t89" reading_order_no="88" segment_no="10" tag_type="text">stage methods usually use a single cropped image as the</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p2_t90" reading_order_no="89" segment_no="10" tag_type="text">input to a CNN, and directly obtain root-relative 3D joint</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p2_t91" reading_order_no="90" segment_no="10" tag_type="text">positions [18]–[20], parent-relative joint positions [21], or</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p2_t92" reading_order_no="91" segment_no="10" tag_type="text">voxel joint probability map [22], [23]. In the VNect method</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font21" size="9" family="Calibri" color="#000000"/>
	<fontspec id="font22" size="8" family="Calibri" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="text">3</text>
<text top="54" left="411" width="79" height="13" font="font21" id="p3_t3" reading_order_no="10" segment_no="2" tag_type="figure">Full-body shape and</text>
<text top="65" left="414" width="72" height="13" font="font21" id="p3_t4" reading_order_no="11" segment_no="2" tag_type="figure">kinematic 3D pose</text>
<text top="77" left="421" width="56" height="13" font="font21" id="p3_t5" reading_order_no="12" segment_no="2" tag_type="figure">reconstruction</text>
<text top="60" left="275" width="19" height="13" font="font21" id="p3_t6" reading_order_no="4" segment_no="2" tag_type="figure">CNN</text>
<text top="71" left="263" width="41" height="13" font="font21" id="p3_t7" reading_order_no="5" segment_no="2" tag_type="figure">Regression</text>
<text top="60" left="155" width="30" height="13" font="font21" id="p3_t8" reading_order_no="2" segment_no="2" tag_type="figure">Human</text>
<text top="71" left="151" width="37" height="13" font="font21" id="p3_t9" reading_order_no="3" segment_no="2" tag_type="figure">Detection</text>
<text top="49" left="330" width="50" height="13" font="font21" id="p3_t10" reading_order_no="6" segment_no="2" tag_type="figure">a) 2D Joints</text>
<text top="60" left="330" width="36" height="13" font="font21" id="p3_t11" reading_order_no="7" segment_no="2" tag_type="figure">b) Mask</text>
<text top="71" left="330" width="30" height="13" font="font21" id="p3_t12" reading_order_no="8" segment_no="2" tag_type="figure">c) IUV</text>
<text top="83" left="330" width="31" height="13" font="font21" id="p3_t13" reading_order_no="9" segment_no="2" tag_type="figure">d) POF</text>
<text top="168" left="231" width="6" height="10" font="font22" id="p3_t14" reading_order_no="13" segment_no="2" tag_type="figure">a)</text>
<text top="168" left="272" width="6" height="10" font="font22" id="p3_t15" reading_order_no="14" segment_no="2" tag_type="figure">b)</text>
<text top="168" left="357" width="6" height="10" font="font22" id="p3_t16" reading_order_no="15" segment_no="2" tag_type="figure">c)</text>
<text top="168" left="439" width="6" height="10" font="font22" id="p3_t17" reading_order_no="16" segment_no="2" tag_type="figure">d)</text>
<text top="190" left="48" width="516" height="9" font="font11" id="p3_t18" reading_order_no="17" segment_no="3" tag_type="text">Fig. 2: System overview. The CNN outputs 2D joints, foreground mask, IUV (including body partition and u -map and</text>
<text top="201" left="48" width="174" height="9" font="font14" id="p3_t19" reading_order_no="18" segment_no="3" tag_type="text">v -map) and POF (Part Orientation Field).</text>
<text top="234" left="48" width="252" height="9" font="font11" id="p3_t20" reading_order_no="19" segment_no="4" tag_type="text">[3], a fully convolutional network structure is proposed</text>
<text top="246" left="48" width="252" height="9" font="font11" id="p3_t21" reading_order_no="20" segment_no="4" tag_type="text">to directly regress location maps, in order to decrease the</text>
<text top="257" left="48" width="252" height="9" font="font11" id="p3_t22" reading_order_no="21" segment_no="4" tag_type="text">dependency on tight bounding boxes for human. More</text>
<text top="269" left="48" width="252" height="9" font="font11" id="p3_t23" reading_order_no="22" segment_no="4" tag_type="text">importantly, VNect gets global coordinates rather than root-</text>
<text top="281" left="48" width="252" height="9" font="font11" id="p3_t24" reading_order_no="23" segment_no="4" tag_type="text">relative coordinates, and produces real-time performance.</text>
<text top="292" left="48" width="252" height="9" font="font11" id="p3_t25" reading_order_no="24" segment_no="4" tag_type="text">The OriNet [24] decouples bone lengths and bone ori-</text>
<text top="304" left="48" width="252" height="9" font="font11" id="p3_t26" reading_order_no="25" segment_no="4" tag_type="text">entations by representing 3D poses with 3D orientations</text>
<text top="315" left="48" width="252" height="9" font="font11" id="p3_t27" reading_order_no="26" segment_no="4" tag_type="text">of limbs, which are very suitable for motion control. We</text>
<text top="327" left="48" width="252" height="9" font="font11" id="p3_t28" reading_order_no="27" segment_no="4" tag_type="text">also adopt the representation of 3D orientations of limbs.</text>
<text top="338" left="48" width="252" height="9" font="font11" id="p3_t29" reading_order_no="28" segment_no="4" tag_type="text">Yet different from above 3D pose estimation network, we</text>
<text top="350" left="48" width="252" height="9" font="font11" id="p3_t30" reading_order_no="29" segment_no="4" tag_type="text">design an end-to-end network to regress a foreground mask,</text>
<text top="361" left="48" width="252" height="9" font="font11" id="p3_t31" reading_order_no="30" segment_no="4" tag_type="text">2D joint positions, body partition, uv coordinates and 3D</text>
<text top="373" left="48" width="252" height="9" font="font11" id="p3_t32" reading_order_no="31" segment_no="4" tag_type="text">part orientations simultaneously. Please note that existing</text>
<text top="384" left="48" width="252" height="9" font="font11" id="p3_t33" reading_order_no="32" segment_no="4" tag_type="text">works address only one or a subset of the tasks that we</text>
<text top="396" left="48" width="252" height="9" font="font11" id="p3_t34" reading_order_no="33" segment_no="4" tag_type="text">address. Multi-task learning usually boosts the quality of</text>
<text top="407" left="48" width="252" height="9" font="font11" id="p3_t35" reading_order_no="34" segment_no="4" tag_type="text">each individual output due to the correlation among tasks,</text>
<text top="419" left="48" width="252" height="9" font="font11" id="p3_t36" reading_order_no="35" segment_no="4" tag_type="text">and our experiments witness this fact. On the other hand,</text>
<text top="431" left="48" width="252" height="9" font="font11" id="p3_t37" reading_order_no="36" segment_no="4" tag_type="text">while their goal focuses on 3D pose estimation only, we</text>
<text top="442" left="48" width="252" height="9" font="font11" id="p3_t38" reading_order_no="37" segment_no="4" tag_type="text">further reconstruct human body geometry automatically.</text>
<text top="454" left="48" width="252" height="9" font="font11" id="p3_t39" reading_order_no="38" segment_no="4" tag_type="text">With these image features and the geometry model, we are</text>
<text top="465" left="48" width="252" height="9" font="font11" id="p3_t40" reading_order_no="39" segment_no="4" tag_type="text">able to obtain much more accurate pose reconstruction with</text>
<text top="477" left="48" width="98" height="9" font="font11" id="p3_t41" reading_order_no="40" segment_no="4" tag_type="text">strong temporal fitting.</text>
<text top="492" left="62" width="238" height="9" font="font18" id="p3_t42" reading_order_no="41" segment_no="7" tag_type="text">Model-based Pose Estimation. Our method is related</text>
<text top="503" left="48" width="252" height="9" font="font11" id="p3_t43" reading_order_no="42" segment_no="7" tag_type="text">to one set of model-based pose estimation methods. Such</text>
<text top="515" left="48" width="252" height="9" font="font11" id="p3_t44" reading_order_no="43" segment_no="7" tag_type="text">approaches consider a parametric model of the human</text>
<text top="526" left="48" width="252" height="9" font="font11" id="p3_t45" reading_order_no="44" segment_no="7" tag_type="text">body, like SCAPE [25], SMPL [26] and SMPL-X [27], and</text>
<text top="538" left="48" width="252" height="9" font="font11" id="p3_t46" reading_order_no="45" segment_no="7" tag_type="text">the goal is to reconstruct a full 3D body pose and shape.</text>
<text top="549" left="48" width="252" height="9" font="font11" id="p3_t47" reading_order_no="46" segment_no="7" tag_type="text">These approaches can be further divided into model-based<b>Model-based Pose Estimation.</b></text>
<text top="561" left="48" width="160" height="9" font="font20" id="p3_t48" reading_order_no="47" segment_no="7" tag_type="text">optimization and model-based regression .</text>
<text top="576" left="62" width="238" height="9" font="font11" id="p3_t49" reading_order_no="48" segment_no="8" tag_type="text">In the first category, [28] relies on annotated 2D ground</text>
<text top="587" left="48" width="252" height="9" font="font11" id="p3_t50" reading_order_no="49" segment_no="8" tag_type="text">truth, including joint landmarks and body silhouettes, to</text>
<text top="599" left="48" width="252" height="9" font="font11" id="p3_t51" reading_order_no="50" segment_no="8" tag_type="text">optimize the parameters of the SCAPE model through</text>
<text top="610" left="48" width="252" height="9" font="font11" id="p3_t52" reading_order_no="51" segment_no="8" tag_type="text">minimizing errors of the reprojected evidence. With the</text>
<text top="622" left="48" width="252" height="9" font="font11" id="p3_t53" reading_order_no="52" segment_no="8" tag_type="text">SMPLify approach [4], this procedure was made automatic</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p3_t54" reading_order_no="53" segment_no="8" tag_type="text">by replacing annotated 2D joints with 2D pose estimator.</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p3_t55" reading_order_no="54" segment_no="8" tag_type="text">The whole process is then independent of user interference.</text>
<text top="657" left="48" width="252" height="9" font="font11" id="p3_t56" reading_order_no="55" segment_no="8" tag_type="text">Moreover, inter-penetration constraints are introduced to</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p3_t57" reading_order_no="56" segment_no="8" tag_type="text">decrease the depth ambiguity when lifting 2D joints to</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p3_t58" reading_order_no="57" segment_no="8" tag_type="text">3D. The human shape estimation in SMPLify, however,</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p3_t59" reading_order_no="58" segment_no="8" tag_type="text">relies on 2D joints only and does not constrain the body</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p3_t60" reading_order_no="59" segment_no="8" tag_type="text">shape completely. To address this issue, UP3D [29] further</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p3_t61" reading_order_no="60" segment_no="8" tag_type="text">extends the SMPLify method by adding human silhouette</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p3_t62" reading_order_no="61" segment_no="8" tag_type="text">to estimate human shape parameters, with the pipeline</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p3_t63" reading_order_no="62" segment_no="8" tag_type="text">being still automatic. Because of the binary representation</text>
<text top="234" left="312" width="252" height="9" font="font11" id="p3_t64" reading_order_no="63" segment_no="5" tag_type="text">of the human silhouette, as well as the introduction of</text>
<text top="246" left="312" width="252" height="9" font="font11" id="p3_t65" reading_order_no="64" segment_no="5" tag_type="text">cloth intervention in this method, the body shape is still</text>
<text top="257" left="312" width="252" height="9" font="font11" id="p3_t66" reading_order_no="65" segment_no="5" tag_type="text">not sufficiently constrained. To overcome these issues, two</text>
<text top="269" left="312" width="252" height="9" font="font11" id="p3_t67" reading_order_no="66" segment_no="5" tag_type="text">mechanisms have been introduced by our method. The first</text>
<text top="281" left="312" width="252" height="9" font="font11" id="p3_t68" reading_order_no="67" segment_no="5" tag_type="text">one is the IUV, which is albeit more expensive but provides</text>
<text top="292" left="312" width="252" height="9" font="font11" id="p3_t69" reading_order_no="68" segment_no="5" tag_type="text">a dense correspondence between an image and a 3D model.</text>
<text top="304" left="312" width="252" height="9" font="font11" id="p3_t70" reading_order_no="69" segment_no="5" tag_type="text">The second one is the 3D limb orientation, which makes the</text>
<text top="315" left="312" width="231" height="9" font="font11" id="p3_t71" reading_order_no="70" segment_no="5" tag_type="text">reconstruction of human shape and pose more precise.</text>
<text top="330" left="326" width="238" height="9" font="font11" id="p3_t72" reading_order_no="71" segment_no="6" tag_type="text">Among the model-based regression methods, HMR [30]</text>
<text top="342" left="312" width="252" height="9" font="font11" id="p3_t73" reading_order_no="72" segment_no="6" tag_type="text">uses a weakly supervised approach to regress the SMPL</text>
<text top="353" left="312" width="252" height="9" font="font11" id="p3_t74" reading_order_no="73" segment_no="6" tag_type="text">parameters directly from images, relying on 2D keypoints</text>
<text top="365" left="312" width="252" height="9" font="font11" id="p3_t75" reading_order_no="74" segment_no="6" tag_type="text">reprojection and a pose prior learnt in an adversarial man-</text>
<text top="376" left="312" width="252" height="9" font="font11" id="p3_t76" reading_order_no="75" segment_no="6" tag_type="text">ner. Instead of regressing SMPL parameters directly, CMR</text>
<text top="388" left="312" width="252" height="9" font="font11" id="p3_t77" reading_order_no="76" segment_no="6" tag_type="text">[31] builds a structure with Graph-CNN to model the con-</text>
<text top="399" left="312" width="252" height="9" font="font11" id="p3_t78" reading_order_no="77" segment_no="6" tag_type="text">nection of adjacent vertices of a human body mesh, and the</text>
<text top="411" left="312" width="252" height="9" font="font11" id="p3_t79" reading_order_no="78" segment_no="6" tag_type="text">3D coordinates of mesh vertices are directly regressed. EFT</text>
<text top="422" left="312" width="252" height="9" font="font11" id="p3_t80" reading_order_no="79" segment_no="6" tag_type="text">[32], on the other hand, attempts to enrich wild images with</text>
<text top="434" left="312" width="252" height="9" font="font11" id="p3_t81" reading_order_no="80" segment_no="6" tag_type="text">missing SMPL parameters. By fine-tuning the HMR for each</text>
<text top="445" left="312" width="252" height="9" font="font11" id="p3_t82" reading_order_no="81" segment_no="6" tag_type="text">wild image, a few iterations to minimize errors of the 2D</text>
<text top="457" left="312" width="252" height="9" font="font11" id="p3_t83" reading_order_no="82" segment_no="6" tag_type="text">projection, and the current SMPL parameters are obtained.</text>
<text top="469" left="312" width="252" height="9" font="font11" id="p3_t84" reading_order_no="83" segment_no="6" tag_type="text">Treating these parameters as the ground truth for wild</text>
<text top="480" left="312" width="252" height="9" font="font11" id="p3_t85" reading_order_no="84" segment_no="6" tag_type="text">images, the original HMR is fine-tuned for the whole wild</text>
<text top="492" left="312" width="252" height="9" font="font11" id="p3_t86" reading_order_no="85" segment_no="6" tag_type="text">datasets. SPIN [2] adopts a similar idea. Rather than fine-</text>
<text top="503" left="312" width="252" height="9" font="font11" id="p3_t87" reading_order_no="86" segment_no="6" tag_type="text">tuning the HMR to get the ground truth for wild images,</text>
<text top="515" left="312" width="252" height="9" font="font11" id="p3_t88" reading_order_no="87" segment_no="6" tag_type="text">SPIN use the optimization-based method, like SMPLify [4],</text>
<text top="526" left="312" width="252" height="9" font="font11" id="p3_t89" reading_order_no="88" segment_no="6" tag_type="text">to refine the result to be used by HMR as ground truth.</text>
<text top="538" left="312" width="252" height="9" font="font11" id="p3_t90" reading_order_no="89" segment_no="6" tag_type="text">Instead of directly regressing highly non-linear shape and</text>
<text top="549" left="312" width="252" height="9" font="font11" id="p3_t91" reading_order_no="90" segment_no="6" tag_type="text">pose parameters from an image, we regress multiple image</text>
<text top="561" left="312" width="252" height="9" font="font11" id="p3_t92" reading_order_no="91" segment_no="6" tag_type="text">features, and get body shape and pose by a well-designed</text>
<text top="572" left="312" width="252" height="9" font="font11" id="p3_t93" reading_order_no="92" segment_no="6" tag_type="text">optimization formulation. The experiments show that our</text>
<text top="584" left="312" width="252" height="9" font="font11" id="p3_t94" reading_order_no="93" segment_no="6" tag_type="text">reconstruction results are much more accurate, and also</text>
<text top="596" left="312" width="114" height="9" font="font11" id="p3_t95" reading_order_no="94" segment_no="6" tag_type="text">stable on image sequences.</text>
<text top="610" left="326" width="238" height="10" font="font18" id="p3_t96" reading_order_no="95" segment_no="9" tag_type="text">Model-based tracking. Our work is also related to</text>
<text top="622" left="312" width="252" height="9" font="font11" id="p3_t97" reading_order_no="96" segment_no="9" tag_type="text">model-based tracking of 3D human poses using a single</text>
<text top="634" left="312" width="252" height="9" font="font11" id="p3_t98" reading_order_no="97" segment_no="9" tag_type="text">RGB camera. Usually this type of method pre-defines a</text>
<text top="645" left="312" width="252" height="9" font="font11" id="p3_t99" reading_order_no="98" segment_no="9" tag_type="text">human skeleton/body on initialization, and the 3D pose</text>
<text top="657" left="312" width="252" height="9" font="font11" id="p3_t100" reading_order_no="99" segment_no="9" tag_type="text">is updated by minimizing the inconsistency between the</text>
<text top="668" left="312" width="252" height="9" font="font11" id="p3_t101" reading_order_no="100" segment_no="9" tag_type="text">hypothesized poses and observed 2D measurements [33].</text>
<text top="680" left="312" width="252" height="9" font="font11" id="p3_t102" reading_order_no="101" segment_no="9" tag_type="text">This method, on one hand, needs a careful initialization;</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p3_t103" reading_order_no="102" segment_no="9" tag_type="text">on the other hand the optimization is prone to get stuck</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p3_t104" reading_order_no="103" segment_no="9" tag_type="text">in local minima, leading to track failures in the coming</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p3_t105" reading_order_no="104" segment_no="9" tag_type="text">frames. What is different in our method is that the body</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p3_t106" reading_order_no="105" segment_no="9" tag_type="text">model is reconstructed automatically, and is not sensitive to</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p3_t107" reading_order_no="106" segment_no="9" tag_type="text">initialization under abundant constraints. Accurate results</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font23" size="8" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font24" size="5" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font25" size="9" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font26" size="11" family="Calibri,Bold" color="#000000"/>
	<fontspec id="font27" size="11" family="MicrosoftYaHei,Bold" color="#000000"/>
	<fontspec id="font28" size="4" family="TimesNewRomanPSMT" color="#000000"/>
	<fontspec id="font29" size="8" family="TimesNewRomanPS,Italic" color="#000000"/>
	<fontspec id="font30" size="4" family="TimesNewRomanPS,Italic" color="#000000"/>
	<fontspec id="font31" size="8" family="TimesNewRomanPSMT" color="#000000"/>
	<fontspec id="font32" size="8" family="SymbolMT" color="#000000"/>
	<fontspec id="font33" size="5" family="Calibri,Bold" color="#ffffff"/>
	<fontspec id="font34" size="6" family="Calibri" color="#000000"/>
	<fontspec id="font35" size="9" family="Calibri" color="#ffffff"/>
	<fontspec id="font36" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font37" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font38" size="7" family="CMMI7" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="text">4</text>
<text top="126" left="157" width="3" height="11" font="font23" id="p4_t3" reading_order_no="6" segment_no="2" tag_type="figure">F<b>F</b></text>
<text top="101" left="193" width="19" height="7" font="font24" id="p4_t4" reading_order_no="12" segment_no="2" tag_type="figure">Branch 1<b>Branch 1</b></text>
<text top="140" left="193" width="19" height="7" font="font24" id="p4_t5" reading_order_no="24" segment_no="2" tag_type="figure">Branch 2<b>Branch 2</b></text>
<text top="166" left="193" width="19" height="7" font="font24" id="p4_t6" reading_order_no="27" segment_no="2" tag_type="figure">Branch 3<b>Branch 3</b></text>
<text top="166" left="238" width="9" height="7" font="font24" id="p4_t7" reading_order_no="28" segment_no="2" tag_type="figure">UVs<b>UVs</b></text>
<text top="125" left="92" width="2" height="12" font="font25" id="p4_t8" reading_order_no="2" segment_no="2" tag_type="figure">I<b>I</b></text>
<text top="101" left="325" width="19" height="7" font="font24" id="p4_t9" reading_order_no="21" segment_no="2" tag_type="figure">Branch 1<b>Branch 1</b></text>
<text top="140" left="325" width="19" height="7" font="font24" id="p4_t10" reading_order_no="31" segment_no="2" tag_type="figure">Branch 2<b>Branch 2</b></text>
<text top="166" left="325" width="19" height="7" font="font24" id="p4_t11" reading_order_no="34" segment_no="2" tag_type="figure">Branch 3<b>Branch 3</b></text>
<text top="166" left="369" width="9" height="7" font="font24" id="p4_t12" reading_order_no="35" segment_no="2" tag_type="figure">UVs<b>UVs</b></text>
<text top="123" left="415" width="5" height="15" font="font26" id="p4_t13" reading_order_no="37" segment_no="2" tag_type="figure">+<b>+</b></text>
<text top="112" left="449" width="19" height="7" font="font24" id="p4_t14" reading_order_no="42" segment_no="2" tag_type="figure">Branch 2<b>Branch 2</b></text>
<text top="163" left="449" width="19" height="7" font="font24" id="p4_t15" reading_order_no="48" segment_no="2" tag_type="figure">Branch 4<b>Branch 4</b></text>
<text top="163" left="492" width="12" height="7" font="font24" id="p4_t16" reading_order_no="49" segment_no="2" tag_type="figure">Mask<b>Mask</b></text>
<text top="83" left="449" width="19" height="7" font="font24" id="p4_t17" reading_order_no="39" segment_no="2" tag_type="figure">Branch 1<b>Branch 1</b></text>
<text top="189" left="449" width="19" height="7" font="font24" id="p4_t18" reading_order_no="51" segment_no="2" tag_type="figure">Branch 5<b>Branch 5</b></text>
<text top="189" left="492" width="11" height="7" font="font24" id="p4_t19" reading_order_no="52" segment_no="2" tag_type="figure">POFs<b>POFs</b></text>
<text top="75" left="214" width="23" height="11" font="font23" id="p4_t20" reading_order_no="11" segment_no="2" tag_type="figure">stage 1<b>stage 1</b></text>
<text top="60" left="469" width="23" height="11" font="font23" id="p4_t21" reading_order_no="38" segment_no="2" tag_type="figure">stage 6<b>stage 6</b></text>
<text top="97" left="287" width="8" height="14" font="font27" id="p4_t22" reading_order_no="15" segment_no="2" tag_type="figure">+<b>+</b></text>
<text top="149" left="288" width="5" height="15" font="font26" id="p4_t23" reading_order_no="30" segment_no="2" tag_type="figure">+<b>+</b></text>
<text top="93" left="259" width="9" height="10" font="font29" id="p4_t24" reading_order_no="14" segment_no="2" tag_type="figure">H 1</text>
<text top="134" left="260" width="6" height="10" font="font29" id="p4_t25" reading_order_no="26" segment_no="2" tag_type="figure">S 1<i>H</i></text>
<text top="158" left="259" width="8" height="11" font="font29" id="p4_t26" reading_order_no="29" segment_no="2" tag_type="figure">U 1</text>
<text top="160" left="389" width="7" height="10" font="font29" id="p4_t27" reading_order_no="36" segment_no="2" tag_type="figure">U t<i>S</i></text>
<text top="134" left="389" width="6" height="10" font="font29" id="p4_t28" reading_order_no="33" segment_no="2" tag_type="figure">S t</text>
<text top="93" left="389" width="7" height="10" font="font29" id="p4_t29" reading_order_no="23" segment_no="2" tag_type="figure">H t<i>U</i></text>
<text top="77" left="331" width="19" height="11" font="font23" id="p4_t30" reading_order_no="16" segment_no="2" tag_type="figure">stage<i>t</i></text>
<text top="78" left="379" width="4" height="10" font="font31" id="p4_t31" reading_order_no="20" segment_no="2" tag_type="figure">5<i>U</i></text>
<text top="78" left="358" width="4" height="10" font="font31" id="p4_t32" reading_order_no="18" segment_no="2" tag_type="figure">2<i>t</i></text>
<text top="78" left="363" width="14" height="10" font="font32" id="p4_t33" reading_order_no="19" segment_no="2" tag_type="figure"> t <i>S</i></text>
<text top="77" left="350" width="2" height="10" font="font29" id="p4_t34" reading_order_no="17" segment_no="2" tag_type="figure">t<i>t</i></text>
<text top="83" left="513" width="9" height="10" font="font29" id="p4_t35" reading_order_no="41" segment_no="2" tag_type="figure">H 6<i>H</i></text>
<text top="110" left="513" width="7" height="10" font="font29" id="p4_t36" reading_order_no="44" segment_no="2" tag_type="figure">S 6<b>stage </b></text>
<text top="138" left="449" width="19" height="7" font="font24" id="p4_t37" reading_order_no="45" segment_no="2" tag_type="figure">Branch 3</text>
<text top="138" left="493" width="9" height="7" font="font24" id="p4_t38" reading_order_no="46" segment_no="2" tag_type="figure">UVs</text>
<text top="136" left="513" width="9" height="11" font="font29" id="p4_t39" reading_order_no="47" segment_no="2" tag_type="figure">U 6</text>
<text top="162" left="513" width="9" height="10" font="font29" id="p4_t40" reading_order_no="50" segment_no="2" tag_type="figure">M 6</text>
<text top="124" left="116" width="16" height="7" font="font33" id="p4_t41" reading_order_no="4" segment_no="2" tag_type="figure">Mobile<i>t</i></text>
<text top="130" left="116" width="16" height="7" font="font33" id="p4_t42" reading_order_no="5" segment_no="2" tag_type="figure">Net-V2<i>t</i></text>
<text top="195" left="89" width="27" height="10" font="font22" id="p4_t43" reading_order_no="54" segment_no="2" tag_type="figure">Branch *</text>
<text top="215" left="187" width="4" height="10" font="font22" id="p4_t44" reading_order_no="65" segment_no="2" tag_type="figure">C<i>H</i></text>
<text top="215" left="102" width="4" height="10" font="font22" id="p4_t45" reading_order_no="56" segment_no="2" tag_type="figure">C</text>
<text top="223" left="101" width="8" height="8" font="font34" id="p4_t46" reading_order_no="57" segment_no="2" tag_type="figure">3x3<i>S</i></text>
<text top="215" left="131" width="4" height="10" font="font22" id="p4_t47" reading_order_no="59" segment_no="2" tag_type="figure">C<b>Branch 3</b></text>
<text top="223" left="128" width="8" height="8" font="font34" id="p4_t48" reading_order_no="60" segment_no="2" tag_type="figure">3x3<b>UVs</b></text>
<text top="215" left="158" width="4" height="10" font="font22" id="p4_t49" reading_order_no="62" segment_no="2" tag_type="figure">C</text>
<text top="223" left="156" width="8" height="8" font="font34" id="p4_t50" reading_order_no="63" segment_no="2" tag_type="figure">3x3<i>U</i></text>
<text top="215" left="215" width="4" height="10" font="font22" id="p4_t51" reading_order_no="68" segment_no="2" tag_type="figure">C</text>
<text top="224" left="184" width="8" height="8" font="font34" id="p4_t52" reading_order_no="66" segment_no="2" tag_type="figure">3x3<i>M</i></text>
<text top="224" left="213" width="8" height="8" font="font34" id="p4_t53" reading_order_no="69" segment_no="2" tag_type="figure">1x1<b>Mobile</b></text>
<text top="205" left="92" width="4" height="10" font="font32" id="p4_t54" reading_order_no="55" segment_no="2" tag_type="figure"><b>Net-V2</b></text>
<text top="218" left="116" width="5" height="12" font="font35" id="p4_t55" reading_order_no="58" segment_no="2" tag_type="figure">B</text>
<text top="218" left="144" width="5" height="12" font="font35" id="p4_t56" reading_order_no="61" segment_no="2" tag_type="figure">B</text>
<text top="218" left="172" width="5" height="12" font="font35" id="p4_t57" reading_order_no="64" segment_no="2" tag_type="figure">B</text>
<text top="218" left="200" width="5" height="12" font="font35" id="p4_t58" reading_order_no="67" segment_no="2" tag_type="figure">B</text>
<text top="201" left="261" width="49" height="15" font="font22" id="p4_t59" reading_order_no="70" segment_no="2" tag_type="figure">C Convolution</text>
<text top="224" left="260" width="5" height="12" font="font35" id="p4_t60" reading_order_no="71" segment_no="2" tag_type="figure">B</text>
<text top="225" left="272" width="85" height="10" font="font22" id="p4_t61" reading_order_no="72" segment_no="2" tag_type="figure">Batch Normalization + ReLU</text>
<text top="203" left="322" width="5" height="15" font="font26" id="p4_t62" reading_order_no="73" segment_no="2" tag_type="figure">+</text>
<text top="206" left="336" width="39" height="10" font="font22" id="p4_t63" reading_order_no="74" segment_no="2" tag_type="figure">Concatenate</text>
<text top="100" left="231" width="23" height="7" font="font24" id="p4_t64" reading_order_no="13" segment_no="2" tag_type="figure">Heatmaps</text>
<text top="187" left="513" width="8" height="11" font="font29" id="p4_t65" reading_order_no="53" segment_no="2" tag_type="figure">L 6 3 d</text>
<text top="100" left="363" width="23" height="7" font="font24" id="p4_t66" reading_order_no="22" segment_no="2" tag_type="figure">Heatmaps</text>
<text top="139" left="232" width="22" height="7" font="font24" id="p4_t67" reading_order_no="25" segment_no="2" tag_type="figure">Partitions</text>
<text top="140" left="363" width="22" height="7" font="font24" id="p4_t68" reading_order_no="32" segment_no="2" tag_type="figure">Partitions</text>
<text top="112" left="487" width="22" height="7" font="font24" id="p4_t69" reading_order_no="43" segment_no="2" tag_type="figure">Partitions</text>
<text top="83" left="486" width="23" height="7" font="font24" id="p4_t70" reading_order_no="40" segment_no="2" tag_type="figure">Heatmaps</text>
<text top="142" left="85" width="15" height="10" font="font29" id="p4_t71" reading_order_no="3" segment_no="2" tag_type="figure">w  h</text>
<text top="142" left="164" width="6" height="10" font="font31" id="p4_t72" reading_order_no="10" segment_no="2" tag_type="figure">/8</text>
<text top="142" left="149" width="15" height="11" font="font31" id="p4_t73" reading_order_no="8" segment_no="2" tag_type="figure">/8 h</text>
<text top="143" left="143" width="5" height="10" font="font29" id="p4_t74" reading_order_no="7" segment_no="2" tag_type="figure">w<b>+</b></text>
<text top="143" left="155" width="4" height="9" font="font32" id="p4_t75" reading_order_no="9" segment_no="2" tag_type="figure"></text>
<text top="254" left="209" width="195" height="9" font="font11" id="p4_t76" reading_order_no="75" segment_no="3" tag_type="text">Fig. 3: Architecture of our multi-task network.<b>Heatmaps</b></text>
<text top="287" left="48" width="252" height="9" font="font11" id="p4_t77" reading_order_no="76" segment_no="4" tag_type="text">are obtained from single images, therefore it is also robust</text>
<text top="298" left="48" width="87" height="9" font="font11" id="p4_t78" reading_order_no="77" segment_no="4" tag_type="text">on image sequences.</text>
<text top="310" left="62" width="238" height="9" font="font11" id="p4_t79" reading_order_no="78" segment_no="6" tag_type="text">Several other works combine the power of regression<i>d</i></text>
<text top="322" left="48" width="252" height="9" font="font11" id="p4_t80" reading_order_no="79" segment_no="6" tag_type="text">and fitting, as we do. The Total Capture method [34] re-<i>L</i></text>
<text top="333" left="48" width="252" height="9" font="font11" id="p4_t81" reading_order_no="80" segment_no="6" tag_type="text">gresses 2D joint positions and 3D limb orientations with a<b>Heatmaps</b></text>
<text top="345" left="48" width="252" height="9" font="font11" id="p4_t82" reading_order_no="81" segment_no="6" tag_type="text">network, and then optimize human face, body, and hands<b>Partitions</b></text>
<text top="356" left="48" width="252" height="9" font="font11" id="p4_t83" reading_order_no="82" segment_no="6" tag_type="text">with a unified model Adam [35]. Our method outputs<b>Partitions</b></text>
<text top="368" left="48" width="252" height="9" font="font11" id="p4_t84" reading_order_no="83" segment_no="6" tag_type="text">more predictions in realtime (e.g. the IUV and foreground<b>Partitions</b></text>
<text top="379" left="48" width="252" height="9" font="font11" id="p4_t85" reading_order_no="84" segment_no="6" tag_type="text">mask), thus gives more accurate reconstruction of body and<b>Heatmaps</b></text>
<text top="391" left="48" width="252" height="9" font="font11" id="p4_t86" reading_order_no="85" segment_no="6" tag_type="text">pose. More importantly, the goal of our system design is<i>h</i></text>
<text top="402" left="48" width="252" height="9" font="font11" id="p4_t87" reading_order_no="86" segment_no="6" tag_type="text">realtime, therefore we want to discuss more about realtime<i>w</i></text>
<text top="414" left="48" width="252" height="9" font="font11" id="p4_t88" reading_order_no="87" segment_no="6" tag_type="text">systems here. VNect [3] is the first system that captures</text>
<text top="425" left="48" width="252" height="9" font="font11" id="p4_t89" reading_order_no="88" segment_no="6" tag_type="text">kinematic skeleton using a single RGB camera. It uses a</text>
<text top="437" left="48" width="252" height="9" font="font11" id="p4_t90" reading_order_no="89" segment_no="6" tag_type="text">fully convolutional network to regress 2D pose and 3D root-</text>
<text top="449" left="48" width="252" height="9" font="font11" id="p4_t91" reading_order_no="90" segment_no="6" tag_type="text">relative joint positions, and then fit for a kinematic skeleton.<i>h</i></text>
<text top="460" left="48" width="252" height="9" font="font11" id="p4_t92" reading_order_no="91" segment_no="6" tag_type="text">PhysCap [36] adds environment constraints for VNect to<i>w</i></text>
<text top="472" left="48" width="252" height="9" font="font11" id="p4_t93" reading_order_no="92" segment_no="6" tag_type="text">ensure the biophysical plausibility of human postures. In</text>
<text top="483" left="48" width="252" height="9" font="font11" id="p4_t94" reading_order_no="93" segment_no="6" tag_type="text">contrast, our multi-task network outputs more features thus</text>
<text top="495" left="48" width="252" height="9" font="font11" id="p4_t95" reading_order_no="94" segment_no="6" tag_type="text">more expressive geometry models can be reconstructed,</text>
<text top="506" left="48" width="252" height="9" font="font11" id="p4_t96" reading_order_no="95" segment_no="6" tag_type="text">which is manifested by the experimental results. Based on</text>
<text top="518" left="48" width="252" height="9" font="font11" id="p4_t97" reading_order_no="96" segment_no="6" tag_type="text">human skeleton pose capture, some methods further cap-</text>
<text top="529" left="48" width="252" height="9" font="font11" id="p4_t98" reading_order_no="97" segment_no="6" tag_type="text">ture non-rigid deformation of clothing using optimization</text>
<text top="541" left="48" width="252" height="9" font="font11" id="p4_t99" reading_order_no="98" segment_no="6" tag_type="text">such as MonoPerfCap [37] and LiveCap [38], or regression</text>
<text top="552" left="48" width="252" height="9" font="font11" id="p4_t100" reading_order_no="99" segment_no="6" tag_type="text">such as DeepCap [39]. However, the dependency on a pre-</text>
<text top="564" left="48" width="252" height="9" font="font11" id="p4_t101" reading_order_no="100" segment_no="6" tag_type="text">scanned, pre-reconstructed and pre-rigged subject-specific</text>
<text top="575" left="48" width="252" height="9" font="font11" id="p4_t102" reading_order_no="101" segment_no="6" tag_type="text">model limits the usability of these methods, while our</text>
<text top="587" left="48" width="108" height="9" font="font11" id="p4_t103" reading_order_no="102" segment_no="6" tag_type="text">system is fully automatic.</text>
<text top="617" left="48" width="6" height="10" font="font7" id="p4_t104" reading_order_no="103" segment_no="10" tag_type="title">3</text>
<text top="617" left="66" width="98" height="10" font="font7" id="p4_t105" reading_order_no="104" segment_no="10" tag_type="title">M ETHOD O VERVIEW</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p4_t106" reading_order_no="105" segment_no="11" tag_type="text">Our method takes as input a single RGB image with a</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p4_t107" reading_order_no="106" segment_no="11" tag_type="text">single person, and outputs a 3D human body whose pose</text>
<text top="657" left="48" width="252" height="9" font="font11" id="p4_t108" reading_order_no="107" segment_no="11" tag_type="text">and shape are in accordance with the person in image.</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p4_t109" reading_order_no="108" segment_no="11" tag_type="text">This method consists of two parts: a neural network that</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p4_t110" reading_order_no="109" segment_no="11" tag_type="text">regress measurements of human anatomical structures, and</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p4_t111" reading_order_no="110" segment_no="11" tag_type="text">an optimization model that utilizes the network outputs</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p4_t112" reading_order_no="111" segment_no="11" tag_type="text">to build a 3D body mesh. For the regression part (§ 4),</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p4_t113" reading_order_no="112" segment_no="11" tag_type="text">a human is first detected from image by YOLO [40], out-</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p4_t114" reading_order_no="113" segment_no="11" tag_type="text">putting a bounding box. Then the cropped image is fed into</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p4_t115" reading_order_no="114" segment_no="11" tag_type="text">a convolutional neural network (CNN) to get five outputs,</text>
<text top="287" left="312" width="252" height="9" font="font11" id="p4_t116" reading_order_no="115" segment_no="5" tag_type="text">namely foreground segmentation mask, 2D joints positions,</text>
<text top="298" left="312" width="252" height="9" font="font11" id="p4_t117" reading_order_no="116" segment_no="5" tag_type="text">body partition, uv coordinates, and 3D part orientations</text>
<text top="310" left="312" width="252" height="9" font="font11" id="p4_t118" reading_order_no="117" segment_no="5" tag_type="text">(which is also encoded as part orientation field [24], [34]).</text>
<text top="321" left="312" width="252" height="9" font="font11" id="p4_t119" reading_order_no="118" segment_no="5" tag_type="text">For the optimization part (§ 5), our method reconstructs</text>
<text top="333" left="312" width="252" height="9" font="font11" id="p4_t120" reading_order_no="119" segment_no="5" tag_type="text">body pose and shape by fitting a deformable human model.</text>
<text top="344" left="312" width="252" height="9" font="font11" id="p4_t121" reading_order_no="120" segment_no="5" tag_type="text">We show that with as many as five features being integrated</text>
<text top="356" left="312" width="252" height="9" font="font11" id="p4_t122" reading_order_no="121" segment_no="5" tag_type="text">into the optimization pipeline, the reconstruction reaches an<b>3</b></text>
<text top="368" left="312" width="252" height="9" font="font11" id="p4_t123" reading_order_no="122" segment_no="5" tag_type="text">accurate fitting that has not been seen before. The whole<b>M</b></text>
<text top="379" left="312" width="252" height="9" font="font11" id="p4_t124" reading_order_no="123" segment_no="5" tag_type="text">pipeline is illustrated in Fig. 2. The success of our approach<b>ETHOD</b></text>
<text top="391" left="312" width="252" height="9" font="font11" id="p4_t125" reading_order_no="124" segment_no="5" tag_type="text">also relies on the enlargement of publicly available training<b>O</b></text>
<text top="402" left="312" width="252" height="9" font="font11" id="p4_t126" reading_order_no="125" segment_no="5" tag_type="text">datasets. We describe how the new data is collected and<b>VERVIEW</b></text>
<text top="414" left="312" width="243" height="9" font="font11" id="p4_t127" reading_order_no="126" segment_no="5" tag_type="text">preprocessed with our in-house acquisition system in § 6.</text>
<text top="443" left="312" width="6" height="10" font="font7" id="p4_t128" reading_order_no="127" segment_no="7" tag_type="title">4</text>
<text top="443" left="330" width="125" height="10" font="font7" id="p4_t129" reading_order_no="128" segment_no="7" tag_type="title">T HE T RAINING N ETWORK</text>
<text top="460" left="312" width="252" height="9" font="font11" id="p4_t130" reading_order_no="129" segment_no="8" tag_type="text">As mentioned, the key of our method is a multi-task CNN</text>
<text top="472" left="312" width="252" height="9" font="font11" id="p4_t131" reading_order_no="130" segment_no="8" tag_type="text">regressor for predicting five human anatomical structures:</text>
<text top="483" left="312" width="252" height="9" font="font11" id="p4_t132" reading_order_no="131" segment_no="8" tag_type="text">foreground segmentation mask, 2D joints positions, body</text>
<text top="495" left="312" width="252" height="9" font="font11" id="p4_t133" reading_order_no="132" segment_no="8" tag_type="text">partition, uv coordinates and 3D part orientations. The</text>
<text top="506" left="312" width="252" height="9" font="font11" id="p4_t134" reading_order_no="133" segment_no="8" tag_type="text">motivation behind this multi-task architecture is that more</text>
<text top="518" left="312" width="252" height="9" font="font11" id="p4_t135" reading_order_no="134" segment_no="8" tag_type="text">outputs gives more visual cues to be used for reconstruc-</text>
<text top="529" left="312" width="252" height="9" font="font11" id="p4_t136" reading_order_no="135" segment_no="8" tag_type="text">tion. Actually this architecture refines multiple predictions</text>
<text top="541" left="312" width="252" height="9" font="font11" id="p4_t137" reading_order_no="136" segment_no="8" tag_type="text">recurrently, as a result each individual prediction turns to be</text>
<text top="552" left="312" width="252" height="9" font="font11" id="p4_t138" reading_order_no="137" segment_no="8" tag_type="text">more accurate. This is not a surprise, as the power of multi-</text>
<text top="564" left="312" width="252" height="9" font="font11" id="p4_t139" reading_order_no="138" segment_no="8" tag_type="text">task learning is that efficiency and prediction accuracy can</text>
<text top="576" left="312" width="252" height="9" font="font11" id="p4_t140" reading_order_no="139" segment_no="8" tag_type="text">be improved by learning multiple objectives from a shared</text>
<text top="587" left="312" width="81" height="9" font="font11" id="p4_t141" reading_order_no="140" segment_no="8" tag_type="text">representation [41].</text>
<text top="599" left="326" width="238" height="9" font="font11" id="p4_t142" reading_order_no="141" segment_no="9" tag_type="text">Our multi-task regressor is a fully convolutional net-</text>
<text top="608" left="312" width="252" height="11" font="font11" id="p4_t143" reading_order_no="142" segment_no="9" tag_type="text">work. More specificlly, given a RGB image I ∈ R 3 × w × h , a</text>
<text top="622" left="312" width="252" height="9" font="font11" id="p4_t144" reading_order_no="143" segment_no="9" tag_type="text">feed-forward network simultaneously predicts a set of 2D</text>
<text top="631" left="312" width="252" height="11" font="font11" id="p4_t145" reading_order_no="144" segment_no="9" tag_type="text">joint confidence maps H ∈ R J × w × h (where J = 18 is the</text>
<text top="645" left="312" width="252" height="9" font="font11" id="p4_t146" reading_order_no="145" segment_no="9" tag_type="text">number of joints to predict), human mask probability map</text>
<text top="654" left="312" width="252" height="11" font="font14" id="p4_t147" reading_order_no="146" segment_no="9" tag_type="text">M ∈ R w × h , human part probability map plus uv map, and</text>
<text top="666" left="312" width="252" height="11" font="font11" id="p4_t148" reading_order_no="147" segment_no="9" tag_type="text">3D Part Orientation Fields L ∈ R 3 O × w × h , where O = 17 is</text>
<text top="680" left="312" width="111" height="9" font="font11" id="p4_t149" reading_order_no="148" segment_no="9" tag_type="text">the number of body parts.</text>
<text top="691" left="326" width="238" height="9" font="font11" id="p4_t150" reading_order_no="149" segment_no="12" tag_type="text">We use the term IUV map to indicate human partition</text>
<text top="701" left="312" width="252" height="11" font="font11" id="p4_t151" reading_order_no="150" segment_no="12" tag_type="text">probability map S ∈ R ( C +1) × w × h (for C = 24 partitions<b>4</b></text>
<text top="712" left="312" width="252" height="11" font="font11" id="p4_t152" reading_order_no="151" segment_no="12" tag_type="text">and one background) and uv coordinates U ∈ R 2 C × w × h , as<b>T</b></text>
<text top="726" left="312" width="252" height="9" font="font11" id="p4_t153" reading_order_no="152" segment_no="12" tag_type="text">did in [1]. To our knowledge, no previous works have ever<b>HE</b></text>
<text top="737" left="312" width="156" height="9" font="font11" id="p4_t154" reading_order_no="153" segment_no="12" tag_type="text">regressed so many outputs as we do.<b>T</b></text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font39" size="10" family="NimbusSanL,Bold" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="text">5</text>
<text top="45" left="48" width="13" height="9" font="font39" id="p5_t3" reading_order_no="2" segment_no="2" tag_type="title">4.1<b>4.1</b></text>
<text top="45" left="71" width="122" height="9" font="font39" id="p5_t4" reading_order_no="3" segment_no="2" tag_type="title">Multi-task CNN Regression<b>Multi-task CNN Regression</b></text>
<text top="60" left="48" width="252" height="9" font="font11" id="p5_t5" reading_order_no="4" segment_no="3" tag_type="text">Fig. 3 illustrates the structure of our multi-task network.</text>
<text top="71" left="48" width="252" height="9" font="font11" id="p5_t6" reading_order_no="5" segment_no="3" tag_type="text">It is inspired by architectures like [42]–[44], which refine</text>
<text top="83" left="48" width="252" height="9" font="font11" id="p5_t7" reading_order_no="6" segment_no="3" tag_type="text">the predictions recurrently. An image is first encoded by</text>
<text top="94" left="48" width="252" height="9" font="font11" id="p5_t8" reading_order_no="7" segment_no="3" tag_type="text">a convolutional network, generating a set of image features</text>
<text top="106" left="48" width="252" height="9" font="font14" id="p5_t9" reading_order_no="8" segment_no="3" tag_type="text">F , which are then passed over to the first estimation for</text>
<text top="117" left="48" width="252" height="9" font="font11" id="p5_t10" reading_order_no="9" segment_no="3" tag_type="text">each individual task at stage 2. We get coarse predictions</text>
<text top="127" left="48" width="252" height="11" font="font11" id="p5_t11" reading_order_no="10" segment_no="3" tag_type="text">for joint confidence maps H 1 , IUV maps (body partition</text>
<text top="139" left="48" width="252" height="10" font="font14" id="p5_t12" reading_order_no="11" segment_no="3" tag_type="text">S 1 and uv coordinates U 1 ) in stage 1. In the successive</text>
<text top="152" left="48" width="252" height="9" font="font11" id="p5_t13" reading_order_no="12" segment_no="3" tag_type="text">stages, the network takes as input the image feature F , the</text>
<text top="163" left="48" width="252" height="9" font="font11" id="p5_t14" reading_order_no="13" segment_no="3" tag_type="text">results of previous stages of the same type. We formulates</text>
<text top="175" left="48" width="106" height="9" font="font11" id="p5_t15" reading_order_no="14" segment_no="3" tag_type="text">the procedure as follows:</text>
<text top="196" left="122" width="34" height="11" font="font14" id="p5_t16" reading_order_no="15" segment_no="4" tag_type="formula">H t = δ t</text>
<text top="196" left="152" width="74" height="13" font="font19" id="p5_t17" reading_order_no="16" segment_no="4" tag_type="formula">H ( Cat ( F, H t − 1 ))</text>
<text top="198" left="289" width="11" height="9" font="font11" id="p5_t18" reading_order_no="17" segment_no="4" tag_type="text">(1)</text>
<text top="214" left="112" width="32" height="11" font="font14" id="p5_t19" reading_order_no="18" segment_no="5" tag_type="formula">S t = δ t</text>
<text top="214" left="140" width="96" height="13" font="font19" id="p5_t20" reading_order_no="19" segment_no="5" tag_type="formula">S ( Cat ( F, S t − 1 , U t − 1 ))</text>
<text top="216" left="289" width="11" height="9" font="font11" id="p5_t21" reading_order_no="20" segment_no="5" tag_type="text">(2)</text>
<text top="232" left="111" width="33" height="11" font="font14" id="p5_t22" reading_order_no="21" segment_no="6" tag_type="formula">U t = δ t</text>
<text top="232" left="140" width="97" height="13" font="font19" id="p5_t23" reading_order_no="22" segment_no="6" tag_type="formula">U ( Cat ( F, S t − 1 , U t − 1 ))</text>
<text top="235" left="289" width="11" height="9" font="font11" id="p5_t24" reading_order_no="23" segment_no="6" tag_type="text">(3)</text>
<text top="252" left="48" width="252" height="10" font="font11" id="p5_t25" reading_order_no="24" segment_no="8" tag_type="text">where 2 ≤ t ≤ 5 is the stage index, and δ ( · ) is the mapping</text>
<text top="264" left="48" width="252" height="9" font="font11" id="p5_t26" reading_order_no="25" segment_no="8" tag_type="text">for Branch *, as defined as four Conv3 × 3-BN-ReLU blocks</text>
<text top="275" left="48" width="252" height="10" font="font11" id="p5_t27" reading_order_no="26" segment_no="8" tag_type="text">and one Conv1 × 1 task-specified regressor. Cat ( · ) is the</text>
<text top="287" left="48" width="252" height="9" font="font11" id="p5_t28" reading_order_no="27" segment_no="8" tag_type="text">concatenation operation. In stage 6, the joint confidence</text>
<text top="299" left="48" width="252" height="9" font="font11" id="p5_t29" reading_order_no="28" segment_no="8" tag_type="text">map and IUV map from the previous stage is concatenated</text>
<text top="310" left="48" width="252" height="9" font="font11" id="p5_t30" reading_order_no="29" segment_no="8" tag_type="text">and treated as input to predict not only the joint and IUV,</text>
<text top="322" left="48" width="252" height="9" font="font11" id="p5_t31" reading_order_no="30" segment_no="8" tag_type="text">but also two additional terms: the mask M and the part</text>
<text top="333" left="48" width="92" height="10" font="font11" id="p5_t32" reading_order_no="31" segment_no="8" tag_type="text">orientation maps L 3 d .</text>
<text top="345" left="62" width="238" height="9" font="font18" id="p5_t33" reading_order_no="32" segment_no="10" tag_type="text">Loss Term. To guide the training of the multi-task</text>
<text top="357" left="48" width="252" height="9" font="font11" id="p5_t34" reading_order_no="33" segment_no="10" tag_type="text">network, we apply losses for predictions at each stage,</text>
<text top="368" left="48" width="252" height="10" font="font11" id="p5_t35" reading_order_no="34" segment_no="10" tag_type="text">specifically L 2 losses for the confidence maps H , POFs and</text>
<text top="380" left="48" width="252" height="9" font="font14" id="p5_t36" reading_order_no="35" segment_no="10" tag_type="text">U V maps. Note for the U V map, we only take into account a</text>
<text top="391" left="48" width="252" height="9" font="font11" id="p5_t37" reading_order_no="36" segment_no="10" tag_type="text">body part if the pixel is located inside it. When training part</text>
<text top="403" left="48" width="252" height="9" font="font11" id="p5_t38" reading_order_no="37" segment_no="10" tag_type="text">partition, a standard multi-class cross-entropy loss is used.</text>
<text top="414" left="48" width="252" height="9" font="font11" id="p5_t39" reading_order_no="38" segment_no="10" tag_type="text">Note that due to the difference of human part areas, we</text>
<text top="426" left="48" width="252" height="9" font="font11" id="p5_t40" reading_order_no="39" segment_no="10" tag_type="text">balance the supervision for part segmentation classification</text>
<text top="437" left="48" width="252" height="10" font="font11" id="p5_t41" reading_order_no="40" segment_no="10" tag_type="text">by the weight w c for each human part c , so that the network</text>
<text top="449" left="48" width="252" height="9" font="font11" id="p5_t42" reading_order_no="41" segment_no="10" tag_type="text">would not over-fit body parts of large area. The balance</text>
<text top="460" left="48" width="252" height="10" font="font11" id="p5_t43" reading_order_no="42" segment_no="10" tag_type="text">weight w c is inversely proportional to the part area, as in</text>
<text top="472" left="48" width="252" height="9" font="font11" id="p5_t44" reading_order_no="43" segment_no="10" tag_type="text">[45]. Our IUV (body partition and U V maps) ground truth</text>
<text top="484" left="48" width="252" height="9" font="font11" id="p5_t45" reading_order_no="44" segment_no="10" tag_type="text">for hands is inaccurate, because we fit a statistic model into</text>
<text top="495" left="48" width="252" height="9" font="font11" id="p5_t46" reading_order_no="45" segment_no="10" tag_type="text">a skeleton without finger joints, so we just ignore the IUV</text>
<text top="507" left="48" width="252" height="9" font="font11" id="p5_t47" reading_order_no="46" segment_no="10" tag_type="text">loss for hands. The segmentation mask is trained by binary</text>
<text top="518" left="48" width="78" height="9" font="font11" id="p5_t48" reading_order_no="47" segment_no="10" tag_type="text">cross-entropy loss.</text>
<text top="530" left="62" width="238" height="9" font="font18" id="p5_t49" reading_order_no="48" segment_no="12" tag_type="text">Implementation. The training of our multi-task network</text>
<text top="541" left="48" width="252" height="9" font="font11" id="p5_t50" reading_order_no="49" segment_no="12" tag_type="text">consists of three phases. (1) First, we pre-train our net-</text>
<text top="553" left="48" width="252" height="9" font="font11" id="p5_t51" reading_order_no="50" segment_no="12" tag_type="text">work for the 2D joint detection task with in-the-wild image</text>
<text top="564" left="48" width="252" height="9" font="font11" id="p5_t52" reading_order_no="51" segment_no="12" tag_type="text">dataset for stage 1 ∼ 5 , ignoring other tasks, which gives</text>
<text top="576" left="48" width="252" height="9" font="font11" id="p5_t53" reading_order_no="52" segment_no="12" tag_type="text">better generalization performance. Our 2D joint detection</text>
<text top="585" left="48" width="252" height="11" font="font11" id="p5_t54" reading_order_no="53" segment_no="12" tag_type="text">task is trained with an initial learning rate of 10 − 3 and is</text>
<text top="599" left="48" width="252" height="9" font="font11" id="p5_t55" reading_order_no="54" segment_no="12" tag_type="text">reduced every 200,000 iters by a factor γ = 0 . 333 , as [46]</text>
<text top="610" left="48" width="252" height="9" font="font11" id="p5_t56" reading_order_no="55" segment_no="12" tag_type="text">does. (2) Second, we combine our newly collected dataset</text>
<text top="622" left="48" width="252" height="9" font="font11" id="p5_t57" reading_order_no="56" segment_no="12" tag_type="text">with 2D joint dataset, and apply a mix-training strategy for</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p5_t58" reading_order_no="57" segment_no="12" tag_type="text">other tasks while freezing the weights of feature extractor</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p5_t59" reading_order_no="58" segment_no="12" tag_type="text">and 2D joint detector for 100,000 iters by a learning rate of</text>
<text top="655" left="48" width="252" height="11" font="font19" id="p5_t60" reading_order_no="59" segment_no="12" tag_type="text">5 × 10 − 4 . Note that our newly collected data has all desired</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p5_t61" reading_order_no="60" segment_no="12" tag_type="text">ground truth for every task. Fig. 4 shows a few images</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p5_t62" reading_order_no="61" segment_no="12" tag_type="text">in it, including the original captures and the augmented</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p5_t63" reading_order_no="62" segment_no="12" tag_type="text">ones through background replacement. Data augmentation</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p5_t64" reading_order_no="63" segment_no="12" tag_type="text">with background replacement greatly increases the gener-</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p5_t65" reading_order_no="64" segment_no="12" tag_type="text">alization of in-the-wild images. (3) Finally, we unfreeze the</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p5_t66" reading_order_no="65" segment_no="12" tag_type="text">weights of well-trained 2D task and feature extractors, but</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p5_t67" reading_order_no="66" segment_no="12" tag_type="text">apply a smaller learning rate (multiplied by 0.1) for these</text>
<text top="243" left="312" width="252" height="9" font="font11" id="p5_t68" reading_order_no="67" segment_no="7" tag_type="text">Fig. 4: Our training dataset contains original captured im-</text>
<text top="254" left="312" width="252" height="9" font="font11" id="p5_t69" reading_order_no="68" segment_no="7" tag_type="text">ages, as well as augmented images with background re-</text>
<text top="266" left="312" width="46" height="9" font="font11" id="p5_t70" reading_order_no="69" segment_no="7" tag_type="text">placement.</text>
<text top="299" left="312" width="252" height="9" font="font11" id="p5_t71" reading_order_no="70" segment_no="9" tag_type="text">weights. Our full-task training takes 1,000,000 iterations</text>
<text top="309" left="312" width="252" height="11" font="font11" id="p5_t72" reading_order_no="71" segment_no="9" tag_type="text">with a learning rate of 5 × 10 − 4 reduced every 200,000 iters</text>
<text top="322" left="312" width="252" height="9" font="font11" id="p5_t73" reading_order_no="72" segment_no="9" tag_type="text">by a factor γ = 0 . 333 . We employ a rotation augmentation</text>
<text top="332" left="312" width="252" height="11" font="font11" id="p5_t74" reading_order_no="73" segment_no="9" tag_type="text">( ± 30 ◦ ), a scaling augmentation (0.75-1.25) and left-right</text>
<text top="345" left="312" width="252" height="9" font="font11" id="p5_t75" reading_order_no="74" segment_no="9" tag_type="text">flipping (only for in-the-wild dataset) for training. We use</text>
<text top="357" left="312" width="252" height="9" font="font11" id="p5_t76" reading_order_no="75" segment_no="9" tag_type="text">the Caffe framework [47] for network training, and use</text>
<text top="368" left="312" width="252" height="9" font="font11" id="p5_t77" reading_order_no="76" segment_no="9" tag_type="text">the Adadelta solver [48]. The performance of each task is</text>
<text top="380" left="312" width="252" height="9" font="font11" id="p5_t78" reading_order_no="77" segment_no="9" tag_type="text">strongly dependent on the relative weighting between the</text>
<text top="392" left="312" width="252" height="9" font="font11" id="p5_t79" reading_order_no="78" segment_no="9" tag_type="text">loss of each task [49]. And in our experiment, we set loss</text>
<text top="403" left="312" width="252" height="9" font="font11" id="p5_t80" reading_order_no="79" segment_no="9" tag_type="text">weights as follows: w = 0 . 5 for U V , w = 0 . 05 for body</text>
<text top="414" left="312" width="252" height="10" font="font11" id="p5_t81" reading_order_no="80" segment_no="9" tag_type="text">partition, w = 0 . 5 for heatmap, w = 1 . 0 for foreground</text>
<text top="426" left="312" width="252" height="9" font="font11" id="p5_t82" reading_order_no="81" segment_no="9" tag_type="text">mask and w = 1 . 0 for POFs. To balance accuracy and</text>
<text top="438" left="312" width="252" height="9" font="font11" id="p5_t83" reading_order_no="82" segment_no="9" tag_type="text">efficiency, we use MobileNet-V2 [50] as our feature extractor.</text>
<text top="449" left="312" width="252" height="9" font="font11" id="p5_t84" reading_order_no="83" segment_no="9" tag_type="text">Note that we removed the downsampling operations in</text>
<text top="461" left="312" width="252" height="9" font="font11" id="p5_t85" reading_order_no="84" segment_no="9" tag_type="text">last two blocks (by replacing stride=2 in downsampling</text>
<text top="472" left="312" width="252" height="9" font="font11" id="p5_t86" reading_order_no="85" segment_no="9" tag_type="text">convolution with stride=1), and maintained the size of the</text>
<text top="484" left="312" width="252" height="9" font="font11" id="p5_t87" reading_order_no="86" segment_no="9" tag_type="text">final feature map to be 1 / 8 of the input image, which is</text>
<text top="495" left="312" width="48" height="9" font="font11" id="p5_t88" reading_order_no="87" segment_no="9" tag_type="text">224-by-224.</text>
<text top="525" left="312" width="6" height="10" font="font7" id="p5_t89" reading_order_no="88" segment_no="11" tag_type="title">5</text>
<text top="525" left="330" width="57" height="10" font="font7" id="p5_t90" reading_order_no="89" segment_no="11" tag_type="title">A UTOMATIC</text>
<text top="525" left="395" width="54" height="10" font="font7" id="p5_t91" reading_order_no="90" segment_no="11" tag_type="title">K INEMATIC</text>
<text top="525" left="456" width="28" height="10" font="font7" id="p5_t92" reading_order_no="91" segment_no="11" tag_type="title">P OSE</text>
<text top="525" left="491" width="73" height="10" font="font7" id="p5_t93" reading_order_no="92" segment_no="11" tag_type="title">R ECONSTRUC -</text>
<text top="539" left="312" width="201" height="10" font="font7" id="p5_t94" reading_order_no="93" segment_no="11" tag_type="title">TION AND F ULL - BODY S HAPE M ODELING</text>
<text top="555" left="312" width="252" height="9" font="font11" id="p5_t95" reading_order_no="94" segment_no="13" tag_type="text">Our work sets a new mark in terms of level-of-detail</text>
<text top="567" left="312" width="252" height="9" font="font11" id="p5_t96" reading_order_no="95" segment_no="13" tag_type="text">that previous work did not reach. This mainly attributes</text>
<text top="578" left="312" width="252" height="9" font="font11" id="p5_t97" reading_order_no="96" segment_no="13" tag_type="text">to our innovative kinematic pose reconstruction and full-</text>
<text top="590" left="312" width="252" height="9" font="font11" id="p5_t98" reading_order_no="97" segment_no="13" tag_type="text">body shape modeling. The network output (as in § 4) is</text>
<text top="601" left="312" width="252" height="9" font="font11" id="p5_t99" reading_order_no="98" segment_no="13" tag_type="text">of low-quality and noisy, and may not be compatible with</text>
<text top="613" left="312" width="252" height="9" font="font11" id="p5_t100" reading_order_no="99" segment_no="13" tag_type="text">input images or with human kinematic constraints. Directly</text>
<text top="625" left="312" width="252" height="9" font="font11" id="p5_t101" reading_order_no="100" segment_no="13" tag_type="text">using such information leads to inaccurate reconstruction of</text>
<text top="636" left="312" width="252" height="9" font="font11" id="p5_t102" reading_order_no="101" segment_no="13" tag_type="text">human motion. To refine the network output, we develop</text>
<text top="648" left="312" width="252" height="9" font="font11" id="p5_t103" reading_order_no="102" segment_no="13" tag_type="text">a novel algorithm that accurately reconstructs the human</text>
<text top="659" left="312" width="246" height="9" font="font11" id="p5_t104" reading_order_no="103" segment_no="13" tag_type="text">motion as well as a subject-specific full-body mesh model.</text>
<text top="688" left="312" width="13" height="9" font="font39" id="p5_t105" reading_order_no="104" segment_no="14" tag_type="title">5.1</text>
<text top="688" left="335" width="149" height="9" font="font39" id="p5_t106" reading_order_no="105" segment_no="14" tag_type="title">Human Full-body Representation</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p5_t107" reading_order_no="106" segment_no="15" tag_type="text">Similar to SMPL, we approximate the human full-body</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p5_t108" reading_order_no="107" segment_no="15" tag_type="text">geometry with a skinned mesh that is driven by an artic-</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p5_t109" reading_order_no="108" segment_no="15" tag_type="text">ulated skeleton model using Linear Blend Skinning (LBS).</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p5_t110" reading_order_no="109" segment_no="15" tag_type="text">Our skeleton has 45 degree of freedoms (DOFs), 6 of which</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font40" size="7" family="URWPalladioL-Roma" color="#000000"/>
	<fontspec id="font41" size="10" family="CMMIB10" color="#000000"/>
	<fontspec id="font42" size="10" family="CMEX10" color="#000000"/>
	<fontspec id="font43" size="9" family="CMR9" color="#000000"/>
	<fontspec id="font44" size="6" family="CMMIB6" color="#000000"/>
	<fontspec id="font45" size="9" family="CMMI9" color="#000000"/>
	<fontspec id="font46" size="6" family="CMMI6" color="#000000"/>
	<fontspec id="font47" size="9" family="URWPalladioL-Roma" color="#000000"/>
	<fontspec id="font48" size="10" family="NimbusSanL-ReguItal" color="#000000"/>
	<fontspec id="font49" size="10" family="CMBX10" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">6</text>
<text top="197" left="48" width="252" height="9" font="font11" id="p6_t3" reading_order_no="2" segment_no="5" tag_type="text">Fig. 5: Human shape variations: (a) bone length variation;</text>
<text top="209" left="48" width="140" height="9" font="font11" id="p6_t4" reading_order_no="3" segment_no="5" tag_type="text">(b) body part thickness variation.</text>
<text top="242" left="48" width="252" height="9" font="font11" id="p6_t5" reading_order_no="4" segment_no="8" tag_type="text">are for global position and orientation, and 39 are for joint</text>
<text top="254" left="48" width="252" height="9" font="font11" id="p6_t6" reading_order_no="5" segment_no="8" tag_type="text">angles (note that a joint may be of 1, 2 or 3 DOFs). We built</text>
<text top="265" left="48" width="252" height="9" font="font11" id="p6_t7" reading_order_no="6" segment_no="8" tag_type="text">a female mesh model of 28,109 vertices (or 56,142 triangu-</text>
<text top="277" left="48" width="252" height="9" font="font11" id="p6_t8" reading_order_no="7" segment_no="8" tag_type="text">lar faces), carrying more geometric details than the SMPL</text>
<text top="288" left="48" width="252" height="9" font="font11" id="p6_t9" reading_order_no="8" segment_no="8" tag_type="text">model of 6 , 890 vertices. The female model is elaborately</text>
<text top="300" left="48" width="252" height="9" font="font11" id="p6_t10" reading_order_no="9" segment_no="8" tag_type="text">rigged and parameterized such that the shape can be easily</text>
<text top="310" left="48" width="51" height="10" font="font11" id="p6_t11" reading_order_no="10" segment_no="8" tag_type="text">controlled. 1</text>
<text top="323" left="62" width="238" height="9" font="font11" id="p6_t12" reading_order_no="11" segment_no="9" tag_type="text">Following a relatively mature process we build a para-</text>
<text top="335" left="48" width="252" height="9" font="font11" id="p6_t13" reading_order_no="12" segment_no="9" tag_type="text">metric human full-body geometry model (Fig. 5). The model</text>
<text top="346" left="48" width="252" height="9" font="font11" id="p6_t14" reading_order_no="13" segment_no="9" tag_type="text">is controllable in two aspects: (1) skeleton scales, which</text>
<text top="358" left="48" width="252" height="9" font="font11" id="p6_t15" reading_order_no="14" segment_no="9" tag_type="text">encode coarse-level variation like the overall and the per-</text>
<text top="369" left="48" width="252" height="9" font="font11" id="p6_t16" reading_order_no="15" segment_no="9" tag_type="text">bone scales, (2) mesh vertex offsets, which encode fine-level</text>
<text top="381" left="48" width="252" height="9" font="font11" id="p6_t17" reading_order_no="16" segment_no="9" tag_type="text">shape variation, such as thickness of a limb. The parametric</text>
<text top="392" left="48" width="197" height="9" font="font11" id="p6_t18" reading_order_no="17" segment_no="9" tag_type="text">human full-body model can be represented as:</text>
<text top="421" left="108" width="59" height="9" font="font18" id="p6_t19" reading_order_no="18" segment_no="12" tag_type="formula">H i ( α , β , θ ) =</text>
<text top="410" left="170" width="15" height="7" font="font38" id="p6_t20" reading_order_no="19" segment_no="12" tag_type="formula">n − 1</text>
<text top="418" left="170" width="14" height="4" font="font42" id="p6_t21" reading_order_no="20" segment_no="12" tag_type="formula">X</text>
<text top="434" left="171" width="13" height="6" font="font38" id="p6_t22" reading_order_no="21" segment_no="12" tag_type="formula">j =0</text>
<text top="418" left="187" width="45" height="12" font="font14" id="p6_t23" reading_order_no="22" segment_no="12" tag_type="formula">w ij T j ( θ )ˆ v 0</text>
<text top="421" left="230" width="10" height="10" font="font14" id="p6_t24" reading_order_no="23" segment_no="12" tag_type="formula">ij ,<b>H</b></text>
<text top="443" left="123" width="7" height="12" font="font19" id="p6_t25" reading_order_no="24" segment_no="12" tag_type="formula">ˆ 0</text>
<text top="445" left="128" width="112" height="11" font="font19" id="p6_t26" reading_order_no="25" segment_no="12" tag_type="formula">ij = S ( α ) ⊗ ( Q ( β ) ⊕ ˆ ij ) ,</text>
<text top="429" left="289" width="11" height="9" font="font11" id="p6_t27" reading_order_no="26" segment_no="12" tag_type="text">(4)</text>
<text top="464" left="48" width="252" height="10" font="font11" id="p6_t28" reading_order_no="27" segment_no="14" tag_type="text">where H i ( · ) is the coordinate of the i -th vertex of the</text>
<text top="474" left="48" width="65" height="12" font="font11" id="p6_t29" reading_order_no="28" segment_no="14" tag_type="text">mesh model, ˆ v 0</text>
<text top="476" left="111" width="189" height="11" font="font11" id="p6_t30" reading_order_no="29" segment_no="14" tag_type="text">ij is the result after applying the scaling and</text>
<text top="488" left="48" width="252" height="9" font="font11" id="p6_t31" reading_order_no="30" segment_no="14" tag_type="text">offsetting to ˆ ij (which is the i -th vertex represented in the</text>
<text top="499" left="48" width="252" height="10" font="font11" id="p6_t32" reading_order_no="31" segment_no="14" tag_type="text">local coordinate frame of the j -th bone), Q ( β ) ⊕ describes</text>
<text top="510" left="48" width="252" height="10" font="font11" id="p6_t33" reading_order_no="32" segment_no="14" tag_type="text">the vertex offsetting, S ( α ) ⊗ describes the bone scaling, the</text>
<text top="521" left="48" width="252" height="11" font="font11" id="p6_t34" reading_order_no="33" segment_no="14" tag_type="text">shape parameters α ∈ R 8 and β ∈ R 26 provide a low-</text>
<text top="534" left="48" width="252" height="9" font="font11" id="p6_t35" reading_order_no="34" segment_no="14" tag_type="text">dimensional representation of human bone scale variances</text>
<text top="546" left="48" width="252" height="9" font="font11" id="p6_t36" reading_order_no="35" segment_no="14" tag_type="text">and vertex offset variances across individuals respectively,</text>
<text top="557" left="48" width="252" height="10" font="font41" id="p6_t37" reading_order_no="36" segment_no="14" tag_type="text">θ is the pose for deformation, T j ( θ ) is the transformation</text>
<text top="569" left="48" width="252" height="9" font="font11" id="p6_t38" reading_order_no="37" segment_no="14" tag_type="text">of the j -th bone for pose θ , w ij is a sparse weight map for</text>
<text top="580" left="48" width="165" height="9" font="font11" id="p6_t39" reading_order_no="38" segment_no="14" tag_type="text">deformation, n is the number of bones.</text>
<text top="609" left="48" width="13" height="9" font="font39" id="p6_t40" reading_order_no="39" segment_no="19" tag_type="title">5.2</text>
<text top="609" left="71" width="143" height="9" font="font39" id="p6_t41" reading_order_no="40" segment_no="19" tag_type="title">Kinematic Pose Reconstruction</text>
<text top="625" left="48" width="252" height="9" font="font11" id="p6_t42" reading_order_no="41" segment_no="21" tag_type="text">Given the subject-specific full-body mesh model (obtained</text>
<text top="636" left="48" width="252" height="9" font="font11" id="p6_t43" reading_order_no="42" segment_no="21" tag_type="text">in § 5.3) and the network observations (2D pose from 2D</text>
<text top="648" left="48" width="252" height="9" font="font11" id="p6_t44" reading_order_no="43" segment_no="21" tag_type="text">joints probability maps, 3D part orientations from the 3D</text>
<text top="659" left="48" width="252" height="9" font="font11" id="p6_t45" reading_order_no="44" segment_no="21" tag_type="text">limb orientation fields, the human mask, the IUV map for</text>
<text top="671" left="48" width="252" height="9" font="font11" id="p6_t46" reading_order_no="45" segment_no="21" tag_type="text">frame image I i ), our goal is to estimate a human pose θ<b>v</b></text>
<text top="683" left="48" width="252" height="9" font="font11" id="p6_t47" reading_order_no="46" segment_no="21" tag_type="text">which best matches the network observations. We estimate</text>
<text top="702" left="56" width="244" height="8" font="font10" id="p6_t48" reading_order_no="47" segment_no="23" tag_type="footnote">1. Yet currently we do not have a decently parameterized male model</text>
<text top="711" left="48" width="252" height="8" font="font10" id="p6_t49" reading_order_no="48" segment_no="23" tag_type="footnote">on par with the female model. For scenes with a male subject, we either</text>
<text top="720" left="48" width="252" height="8" font="font10" id="p6_t50" reading_order_no="49" segment_no="23" tag_type="footnote">use motion-retargeting to drive a male model mesh but without body</text>
<text top="729" left="48" width="252" height="8" font="font10" id="p6_t51" reading_order_no="119" segment_no="23" tag_type="footnote">dimension adjustment (e.g. Fig. 17(b), or just blindly use the female</text>
<text top="738" left="48" width="175" height="8" font="font10" id="p6_t52" reading_order_no="120" segment_no="23" tag_type="footnote">model (e.g. one case in the accompanying video).</text>
<text top="45" left="312" width="252" height="9" font="font11" id="p6_t53" reading_order_no="121" segment_no="2" tag_type="text">the human pose θ by minimizing the following objective</text>
<text top="57" left="312" width="38" height="9" font="font11" id="p6_t54" reading_order_no="50" segment_no="2" tag_type="text">function:<b>S</b></text>
<text top="75" left="318" width="30" height="8" font="font43" id="p6_t55" reading_order_no="51" segment_no="3" tag_type="formula">arg min</text>
<text top="84" left="331" width="4" height="5" font="font44" id="p6_t56" reading_order_no="52" segment_no="3" tag_type="formula">θ</text>
<text top="74" left="353" width="211" height="9" font="font43" id="p6_t57" reading_order_no="53" segment_no="3" tag_type="formula">( w data E data + w prior E prior + w temporal E temporal ) . (5)</text>
<text top="97" left="312" width="252" height="9" font="font11" id="p6_t58" reading_order_no="54" segment_no="4" tag_type="text">where E data is the data term penalizing the registration</text>
<text top="109" left="312" width="252" height="9" font="font11" id="p6_t59" reading_order_no="55" segment_no="4" tag_type="text">error between the synthesized human model and the obser-</text>
<text top="120" left="312" width="252" height="9" font="font11" id="p6_t60" reading_order_no="56" segment_no="4" tag_type="text">vation. E prior is the prior term that penalizes invalid human<b>Q</b></text>
<text top="131" left="312" width="252" height="10" font="font11" id="p6_t61" reading_order_no="57" segment_no="4" tag_type="text">pose configuration, and E temporal is the pose smoothness</text>
<text top="143" left="312" width="252" height="9" font="font11" id="p6_t62" reading_order_no="58" segment_no="4" tag_type="text">term that penalizes the jerkiness in the motion, which is only</text>
<text top="155" left="312" width="252" height="9" font="font11" id="p6_t63" reading_order_no="59" segment_no="4" tag_type="text">used for video application. While searching for the solution</text>
<text top="166" left="312" width="252" height="9" font="font11" id="p6_t64" reading_order_no="60" segment_no="4" tag_type="text">in an iterative manner, it is possible (and recommended) to</text>
<text top="178" left="312" width="252" height="9" font="font11" id="p6_t65" reading_order_no="61" segment_no="4" tag_type="text">use the pose θ prev from the previous frame as the initial</text>
<text top="189" left="312" width="252" height="9" font="font11" id="p6_t66" reading_order_no="62" segment_no="4" tag_type="text">guess. We will describe each term in detail in the following</text>
<text top="201" left="312" width="51" height="9" font="font11" id="p6_t67" reading_order_no="63" segment_no="4" tag_type="text">subsections.</text>
<text top="225" left="312" width="21" height="9" font="font48" id="p6_t68" reading_order_no="64" segment_no="6" tag_type="title">5.2.1</text>
<text top="225" left="343" width="63" height="9" font="font48" id="p6_t69" reading_order_no="65" segment_no="6" tag_type="title">The Data Term</text>
<text top="239" left="312" width="252" height="10" font="font11" id="p6_t70" reading_order_no="66" segment_no="7" tag_type="text">The data term E data evaluates how well the current human</text>
<text top="251" left="312" width="252" height="9" font="font11" id="p6_t71" reading_order_no="67" segment_no="7" tag_type="text">pose θ matches the network observations by the analysis-<b>H</b></text>
<text top="262" left="312" width="252" height="9" font="font11" id="p6_t72" reading_order_no="68" segment_no="7" tag_type="text">by-synthesis strategy. Given the human pose θ , we first</text>
<text top="274" left="312" width="252" height="9" font="font11" id="p6_t73" reading_order_no="69" segment_no="7" tag_type="text">apply skeleton subspace deformation to synthesize a full-body</text>
<text top="285" left="312" width="252" height="9" font="font11" id="p6_t74" reading_order_no="70" segment_no="7" tag_type="text">mesh model. And then we compute the registration error</text>
<text top="297" left="312" width="252" height="9" font="font11" id="p6_t75" reading_order_no="71" segment_no="7" tag_type="text">between the network observations and the synthesized hu-</text>
<text top="308" left="312" width="168" height="9" font="font11" id="p6_t76" reading_order_no="72" segment_no="7" tag_type="text">man model. The data term is defined as</text>
<text top="327" left="317" width="247" height="10" font="font14" id="p6_t77" reading_order_no="73" segment_no="10" tag_type="formula">E data ( θ ) = w 2 d E 2 d + w 3 d E 3 d + w iuv E iuv + w mask E mask . (6)</text>
<text top="346" left="312" width="252" height="10" font="font11" id="p6_t78" reading_order_no="74" segment_no="11" tag_type="text">Here E 2 d and E 3 d are alignment constraints based on</text>
<text top="358" left="312" width="252" height="9" font="font11" id="p6_t79" reading_order_no="75" segment_no="11" tag_type="text">predicted 2D joints positions and 3D limb orientations, re-</text>
<text top="370" left="312" width="252" height="9" font="font11" id="p6_t80" reading_order_no="76" segment_no="11" tag_type="text">spectively. E iuv penalizes the registration error between the</text>
<text top="381" left="312" width="251" height="10" font="font11" id="p6_t81" reading_order_no="77" segment_no="11" tag_type="text">synthesized uv map and the observed uv map, and E mask<b>v</b></text>
<text top="393" left="312" width="252" height="9" font="font11" id="p6_t82" reading_order_no="78" segment_no="11" tag_type="text">penalizes error between the synthesized and the observed</text>
<text top="404" left="312" width="25" height="9" font="font11" id="p6_t83" reading_order_no="79" segment_no="11" tag_type="text">mask.</text>
<text top="416" left="326" width="238" height="9" font="font18" id="p6_t84" reading_order_no="80" segment_no="13" tag_type="text">Sparse 2D alignment. To minimize the discrepancy be-</text>
<text top="426" left="312" width="252" height="12" font="font11" id="p6_t85" reading_order_no="81" segment_no="13" tag_type="text">tween estimated 2D joints ˆ P 2 d and the projections of the 3D</text>
<text top="438" left="312" width="252" height="12" font="font11" id="p6_t86" reading_order_no="82" segment_no="13" tag_type="text">joints from the human body model, we incorporate ˆ P 2 d into</text>
<text top="452" left="312" width="156" height="9" font="font11" id="p6_t87" reading_order_no="83" segment_no="13" tag_type="text">the following reprojection constraint:</text>
<text top="469" left="364" width="57" height="13" font="font14" id="p6_t88" reading_order_no="84" segment_no="15" tag_type="formula">E 2 d ( θ ) = X</text>
<text top="486" left="413" width="3" height="6" font="font38" id="p6_t89" reading_order_no="85" segment_no="15" tag_type="formula">i</text>
<text top="470" left="423" width="27" height="11" font="font36" id="p6_t90" reading_order_no="86" segment_no="15" tag_type="formula">|| Π( J i</text>
<text top="470" left="446" width="49" height="13" font="font19" id="p6_t91" reading_order_no="87" segment_no="15" tag_type="formula">3 d ( θ )) − ˆ P i</text>
<text top="470" left="490" width="19" height="13" font="font36" id="p6_t92" reading_order_no="88" segment_no="15" tag_type="formula">2 d || 2</text>
<text top="472" left="505" width="7" height="11" font="font14" id="p6_t93" reading_order_no="89" segment_no="15" tag_type="formula">2 .</text>
<text top="473" left="553" width="11" height="9" font="font11" id="p6_t94" reading_order_no="90" segment_no="15" tag_type="text">(7)<b>Q</b></text>
<text top="499" left="312" width="33" height="11" font="font11" id="p6_t95" reading_order_no="91" segment_no="16" tag_type="text">Here J i</text>
<text top="501" left="341" width="223" height="11" font="font19" id="p6_t96" reading_order_no="92" segment_no="16" tag_type="text">3 d ( θ ) is the i -th joint in the skeleton, and Π is the 3D-</text>
<text top="513" left="312" width="252" height="9" font="font11" id="p6_t97" reading_order_no="93" segment_no="16" tag_type="text">to-2D projection matrix according to known intrinsic camera</text>
<text top="524" left="312" width="50" height="9" font="font11" id="p6_t98" reading_order_no="94" segment_no="16" tag_type="text">parameters.</text>
<text top="536" left="326" width="238" height="9" font="font18" id="p6_t99" reading_order_no="95" segment_no="17" tag_type="text">Sparse 3D alignment. Since many 3D poses share the</text>
<text top="547" left="312" width="252" height="9" font="font11" id="p6_t100" reading_order_no="96" segment_no="17" tag_type="text">same reprojection of 2D pose in single image, and it is hard</text>
<text top="559" left="312" width="252" height="9" font="font11" id="p6_t101" reading_order_no="97" segment_no="17" tag_type="text">to infer a 3D pose only with above mentioned reprojection<b>S</b></text>
<text top="571" left="312" width="216" height="9" font="font11" id="p6_t102" reading_order_no="98" segment_no="17" tag_type="text">constraint. Therefore we add another 3D constraint</text>
<text top="590" left="317" width="39" height="9" font="font14" id="p6_t103" reading_order_no="99" segment_no="18" tag_type="formula">E 3 d ( θ ) =</text>
<text top="587" left="366" width="14" height="4" font="font42" id="p6_t104" reading_order_no="100" segment_no="18" tag_type="formula">X</text>
<text top="604" left="357" width="32" height="7" font="font37" id="p6_t105" reading_order_no="101" segment_no="18" tag_type="formula">( m,n ) ∈ B</text>
<text top="587" left="391" width="28" height="12" font="font36" id="p6_t106" reading_order_no="102" segment_no="18" tag_type="formula">|| ( || J m</text>
<text top="587" left="411" width="28" height="13" font="font36" id="p6_t107" reading_order_no="103" segment_no="18" tag_type="formula">3 d − J n</text>
<text top="587" left="433" width="71" height="13" font="font36" id="p6_t108" reading_order_no="104" segment_no="18" tag_type="formula">3 d || 2 · ˆ O m,n − ( J m</text>
<text top="587" left="496" width="29" height="13" font="font36" id="p6_t109" reading_order_no="105" segment_no="18" tag_type="formula">3 d − J n</text>
<text top="587" left="519" width="26" height="13" font="font19" id="p6_t110" reading_order_no="106" segment_no="18" tag_type="formula">3 d )) || 2</text>
<text top="590" left="541" width="23" height="10" font="font14" id="p6_t111" reading_order_no="107" segment_no="18" tag_type="formula">2 . (8)</text>
<text top="619" left="312" width="252" height="12" font="font11" id="p6_t112" reading_order_no="108" segment_no="20" tag_type="text">Here ˆ O m,n is the estimated 3D direction of limb ( m, n ) ,</text>
<text top="632" left="312" width="252" height="10" font="font11" id="p6_t113" reading_order_no="109" segment_no="20" tag_type="text">which is the mean value along the segment from joint J m to</text>
<text top="643" left="312" width="14" height="11" font="font14" id="p6_t114" reading_order_no="110" segment_no="20" tag_type="text">J n .</text>
<text top="657" left="326" width="238" height="9" font="font18" id="p6_t115" reading_order_no="111" segment_no="22" tag_type="text">Why do we use 3D direction? Various representations</text>
<text top="668" left="312" width="252" height="9" font="font11" id="p6_t116" reading_order_no="112" segment_no="22" tag_type="text">have been put forward to denote a 3D pose, including 3D</text>
<text top="680" left="312" width="252" height="9" font="font11" id="p6_t117" reading_order_no="113" segment_no="22" tag_type="text">joint positions, 2D joints position plus root-relative depth,</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p6_t118" reading_order_no="114" segment_no="22" tag_type="text">and 3D limb directions and so on. We adopt 3D limb</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p6_t119" reading_order_no="115" segment_no="22" tag_type="text">direction due to its two advantages. First, limb orientation</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p6_t120" reading_order_no="116" segment_no="22" tag_type="text">is scale-invariant and dataset independent, which helps</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p6_t121" reading_order_no="117" segment_no="22" tag_type="text">resolve scale ambiguity and generalizes easily to diversity</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p6_t122" reading_order_no="118" segment_no="22" tag_type="text">data. Second, because an auto-reconstruction of human</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font50" size="10" family="CMTT10" color="#000000"/>
	<fontspec id="font51" size="7" family="CMMIB7" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p7_t2" reading_order_no="1" segment_no="1" tag_type="text">7</text>
<text top="45" left="48" width="252" height="9" font="font11" id="p7_t3" reading_order_no="2" segment_no="3" tag_type="text">shape is done before tracking, there is no need to worry</text>
<text top="57" left="48" width="248" height="9" font="font11" id="p7_t4" reading_order_no="3" segment_no="3" tag_type="text">too much about limb length ratios in the following frames.</text>
<text top="68" left="62" width="238" height="9" font="font18" id="p7_t5" reading_order_no="4" segment_no="4" tag_type="text">Dense IUV alignment. In addition to the coarse level<b>Dense IUV alignment.</b></text>
<text top="80" left="48" width="252" height="9" font="font11" id="p7_t6" reading_order_no="5" segment_no="4" tag_type="text">shape variations caused by bone length and pose change,</text>
<text top="91" left="48" width="252" height="10" font="font11" id="p7_t7" reading_order_no="6" segment_no="4" tag_type="text">another data term E iuv constrains fine-level shape varia-</text>
<text top="103" left="48" width="252" height="9" font="font11" id="p7_t8" reading_order_no="7" segment_no="4" tag_type="text">tions, such as the thickness of limbs. To this end, we con-</text>
<text top="114" left="48" width="252" height="9" font="font11" id="p7_t9" reading_order_no="8" segment_no="4" tag_type="text">struct a dense pixel-to-surface correspondence represented</text>
<text top="126" left="48" width="252" height="9" font="font11" id="p7_t10" reading_order_no="9" segment_no="4" tag_type="text">as an IUV map. Each pixel of an IUV image has a body part</text>
<text top="137" left="48" width="252" height="9" font="font11" id="p7_t11" reading_order_no="10" segment_no="4" tag_type="text">index i , and a ( uv ) coordinate that maps a pixel to a unique</text>
<text top="149" left="48" width="252" height="9" font="font11" id="p7_t12" reading_order_no="11" segment_no="4" tag_type="text">point on the surface of a body model. Given an IUV map</text>
<text top="161" left="48" width="252" height="9" font="font11" id="p7_t13" reading_order_no="12" segment_no="4" tag_type="text">predicted by the neural network, we select some reliable</text>
<text top="170" left="48" width="252" height="11" font="font11" id="p7_t14" reading_order_no="13" segment_no="4" tag_type="text">pixels p which satisfies δ (1) ( p ) = 1 for the function defined</text>
<text top="184" left="48" width="9" height="9" font="font11" id="p7_t15" reading_order_no="14" segment_no="4" tag_type="text">as</text>
<text top="203" left="51" width="37" height="12" font="font14" id="p7_t16" reading_order_no="15" segment_no="9" tag_type="formula">δ (1) ( p ) =</text>
<text top="200" left="102" width="8" height="9" font="font19" id="p7_t17" reading_order_no="16" segment_no="9" tag_type="formula">1 ,</text>
<text top="200" left="119" width="6" height="9" font="font11" id="p7_t18" reading_order_no="17" segment_no="9" tag_type="formula">if</text>
<text top="199" left="135" width="136" height="10" font="font50" id="p7_t19" reading_order_no="18" segment_no="9" tag_type="formula">Max { v ( p ) } − Max2nd { v ( p ) } &gt; φ</text>
<text top="211" left="102" width="8" height="9" font="font19" id="p7_t20" reading_order_no="19" segment_no="9" tag_type="formula">0 ,</text>
<text top="211" left="119" width="41" height="9" font="font11" id="p7_t21" reading_order_no="20" segment_no="9" tag_type="formula">otherwise</text>
<text top="206" left="289" width="11" height="9" font="font11" id="p7_t22" reading_order_no="21" segment_no="9" tag_type="text">(9)</text>
<text top="230" left="48" width="252" height="9" font="font11" id="p7_t23" reading_order_no="22" segment_no="11" tag_type="text">where v ( p ) is the segmentation probability vector of pixel</text>
<text top="241" left="48" width="252" height="10" font="font14" id="p7_t24" reading_order_no="23" segment_no="11" tag_type="text">p . In our experiment, we set threshold φ = 0 . 5 . Interpolat-</text>
<text top="253" left="48" width="252" height="9" font="font11" id="p7_t25" reading_order_no="24" segment_no="11" tag_type="text">ing these image-to-surface points, a more accurate human</text>
<text top="265" left="48" width="141" height="9" font="font11" id="p7_t26" reading_order_no="25" segment_no="11" tag_type="text">model is obtained by minimizing</text>
<text top="282" left="80" width="60" height="13" font="font14" id="p7_t27" reading_order_no="26" segment_no="14" tag_type="formula">E iuv ( θ ) = X</text>
<text top="299" left="132" width="4" height="6" font="font38" id="p7_t28" reading_order_no="27" segment_no="14" tag_type="formula">p</text>
<text top="283" left="142" width="123" height="11" font="font14" id="p7_t29" reading_order_no="28" segment_no="14" tag_type="formula">δ (1) ( p ) || Π( M ( θ , p )) − ˆ I ( p ) || 2</text>
<text top="285" left="261" width="7" height="11" font="font14" id="p7_t30" reading_order_no="29" segment_no="14" tag_type="formula">2 ,</text>
<text top="285" left="284" width="16" height="9" font="font11" id="p7_t31" reading_order_no="30" segment_no="14" tag_type="text">(10)</text>
<text top="314" left="48" width="252" height="9" font="font11" id="p7_t32" reading_order_no="31" segment_no="15" tag_type="text">where M ( θ , p ) is the synthesized mesh vertex correspond-</text>
<text top="325" left="48" width="252" height="9" font="font11" id="p7_t33" reading_order_no="32" segment_no="15" tag_type="text">ing to pixel p , and Π is the 3D-to-2D projection matrix</text>
<text top="337" left="48" width="207" height="9" font="font11" id="p7_t34" reading_order_no="33" segment_no="15" tag_type="text">according to known intrinsic camera parameters.</text>
<text top="349" left="62" width="238" height="9" font="font18" id="p7_t35" reading_order_no="34" segment_no="17" tag_type="text">Mask Term. The foreground segmentation mask term</text>
<text top="360" left="48" width="252" height="9" font="font11" id="p7_t36" reading_order_no="35" segment_no="17" tag_type="text">is to penalize the inconsistency between the mask of syn-</text>
<text top="372" left="48" width="252" height="9" font="font11" id="p7_t37" reading_order_no="36" segment_no="17" tag_type="text">thesized human mesh model and the mask from network</text>
<text top="383" left="48" width="56" height="9" font="font11" id="p7_t38" reading_order_no="37" segment_no="17" tag_type="text">observations.</text>
<text top="399" left="96" width="68" height="13" font="font14" id="p7_t39" reading_order_no="38" segment_no="20" tag_type="formula">E mask ( θ ) = X</text>
<text top="416" left="155" width="4" height="6" font="font38" id="p7_t40" reading_order_no="39" segment_no="20" tag_type="formula">v</text>
<text top="400" left="166" width="86" height="12" font="font14" id="p7_t41" reading_order_no="40" segment_no="20" tag_type="formula">δ (2) ( v ) || Π( v ) − q v || 2</text>
<text top="407" left="248" width="4" height="6" font="font37" id="p7_t42" reading_order_no="41" segment_no="20" tag_type="formula">2</text>
<text top="402" left="284" width="16" height="9" font="font11" id="p7_t43" reading_order_no="42" segment_no="20" tag_type="text">(11)</text>
<text top="429" left="48" width="252" height="9" font="font11" id="p7_t44" reading_order_no="43" segment_no="21" tag_type="text">where v is the mesh vertex, Π is the 3D-to-2D projection</text>
<text top="441" left="48" width="252" height="9" font="font11" id="p7_t45" reading_order_no="44" segment_no="21" tag_type="text">matrix according to known intrinsic camera parameters,</text>
<text top="450" left="48" width="252" height="11" font="font14" id="p7_t46" reading_order_no="45" segment_no="21" tag_type="text">δ (2) ( v ) indicating whether v is outside the observed mask</text>
<text top="464" left="48" width="252" height="9" font="font11" id="p7_t47" reading_order_no="46" segment_no="21" tag_type="text">or not, q v is the corresponding 2D image position for v</text>
<text top="475" left="48" width="230" height="9" font="font11" id="p7_t48" reading_order_no="47" segment_no="21" tag_type="text">obtained from the distance map of the observed mask.</text>
<text top="499" left="48" width="21" height="9" font="font48" id="p7_t49" reading_order_no="48" segment_no="24" tag_type="title">5.2.2</text>
<text top="499" left="79" width="63" height="9" font="font48" id="p7_t50" reading_order_no="49" segment_no="24" tag_type="title">The Prior Term</text>
<text top="513" left="48" width="252" height="9" font="font11" id="p7_t51" reading_order_no="50" segment_no="25" tag_type="text">To make joints of a human skeleton to be physically mean-</text>
<text top="525" left="48" width="252" height="9" font="font11" id="p7_t52" reading_order_no="51" segment_no="25" tag_type="text">ingful, two different priors, the pose space prior and the</text>
<text top="536" left="48" width="232" height="9" font="font11" id="p7_t53" reading_order_no="52" segment_no="25" tag_type="text">joint limit, are defined and used to form the prior term</text>
<text top="555" left="65" width="218" height="10" font="font14" id="p7_t54" reading_order_no="53" segment_no="28" tag_type="formula">E prior = w pose prior E pose prior + w jt limit E jt limit .</text>
<text top="574" left="62" width="238" height="9" font="font18" id="p7_t55" reading_order_no="54" segment_no="29" tag_type="text">Pose Space Prior. We construct individual PCA models</text>
<text top="586" left="48" width="252" height="9" font="font11" id="p7_t56" reading_order_no="55" segment_no="29" tag_type="text">for each body part (e.g., shoulders, arms, spines, legs and</text>
<text top="596" left="48" width="252" height="10" font="font11" id="p7_t57" reading_order_no="56" segment_no="29" tag_type="text">feet) via CMU mocap database 2 . With those part-wise PCA</text>
<text top="609" left="48" width="252" height="9" font="font11" id="p7_t58" reading_order_no="57" segment_no="29" tag_type="text">models, we are able to constraint the solution space into</text>
<text top="620" left="48" width="252" height="9" font="font11" id="p7_t59" reading_order_no="58" segment_no="29" tag_type="text">the physically meaningful area by minimizing the following</text>
<text top="632" left="48" width="77" height="9" font="font11" id="p7_t60" reading_order_no="59" segment_no="29" tag_type="text">objective function:</text>
<text top="649" left="69" width="194" height="12" font="font14" id="p7_t61" reading_order_no="60" segment_no="31" tag_type="formula">E pose prior ( θ ) = || P T k ( P k ( θ − µ )) + µ − θ || 2 2 ,</text>
<text top="651" left="284" width="16" height="9" font="font11" id="p7_t62" reading_order_no="61" segment_no="31" tag_type="text">(12)</text>
<text top="670" left="48" width="252" height="9" font="font11" id="p7_t63" reading_order_no="62" segment_no="33" tag_type="text">where µ is the mean vector of the PCA model, and P k is</text>
<text top="681" left="48" width="251" height="9" font="font11" id="p7_t64" reading_order_no="63" segment_no="33" tag_type="text">the first k principle components of the PCA model. Here k</text>
<text top="693" left="48" width="190" height="9" font="font11" id="p7_t65" reading_order_no="64" segment_no="33" tag_type="text">is chosen to retain 95% of original variations.</text>
<text top="705" left="62" width="238" height="9" font="font18" id="p7_t66" reading_order_no="65" segment_no="34" tag_type="text">Joint Limit. The joint limit term is added to penalize</text>
<text top="716" left="48" width="252" height="9" font="font11" id="p7_t67" reading_order_no="66" segment_no="34" tag_type="text">invalid joint poses that exceed the range of joint movement.</text>
<text top="738" left="56" width="155" height="8" font="font10" id="p7_t68" reading_order_no="132" segment_no="36" tag_type="footnote">2. http://mocap.cs.cmu.edu/resources.php</text>
<text top="43" left="312" width="232" height="11" font="font11" id="p7_t69" reading_order_no="67" segment_no="2" tag_type="text">Every joint angle θ i , i = 7 , 8 ... 45 should stay within [ θ l</text>
<text top="43" left="541" width="18" height="13" font="font14" id="p7_t70" reading_order_no="68" segment_no="2" tag_type="text">i , θ u</text>
<text top="45" left="553" width="11" height="11" font="font19" id="p7_t71" reading_order_no="69" segment_no="2" tag_type="text">i ] .</text>
<text top="56" left="312" width="215" height="10" font="font11" id="p7_t72" reading_order_no="70" segment_no="2" tag_type="text">The joint limit term E jt limit can be represented as</text>
<text top="84" left="349" width="60" height="9" font="font14" id="p7_t73" reading_order_no="71" segment_no="5" tag_type="formula">E jt limit ( θ ) =</text>
<text top="73" left="416" width="8" height="6" font="font37" id="p7_t74" reading_order_no="72" segment_no="5" tag_type="formula">45</text>
<text top="81" left="412" width="14" height="4" font="font42" id="p7_t75" reading_order_no="73" segment_no="5" tag_type="formula">X</text>
<text top="98" left="413" width="13" height="6" font="font38" id="p7_t76" reading_order_no="74" segment_no="5" tag_type="formula">i =7</text>
<text top="82" left="427" width="99" height="12" font="font19" id="p7_t77" reading_order_no="75" segment_no="5" tag_type="formula">( δ (3) ( θ i &lt; θ l i ) || θ i − θ l i || 2</text>
<text top="107" left="412" width="115" height="12" font="font19" id="p7_t78" reading_order_no="76" segment_no="5" tag_type="formula">+ δ (3) ( θ i &gt; θ u i ) || θ i − θ u i || 2 ) ,</text>
<text top="92" left="548" width="16" height="9" font="font11" id="p7_t79" reading_order_no="77" segment_no="5" tag_type="text">(13)</text>
<text top="126" left="326" width="238" height="11" font="font11" id="p7_t80" reading_order_no="78" segment_no="6" tag_type="text">where the binary function δ (3) ( x ) is 1 if x is true, and is</text>
<text top="139" left="312" width="55" height="9" font="font11" id="p7_t81" reading_order_no="79" segment_no="6" tag_type="text">0 if x is false.</text>
<text top="162" left="312" width="21" height="9" font="font48" id="p7_t82" reading_order_no="80" segment_no="7" tag_type="title">5.2.3</text>
<text top="162" left="343" width="118" height="9" font="font48" id="p7_t83" reading_order_no="81" segment_no="7" tag_type="title">Temporal Smoothness Term</text>
<text top="176" left="312" width="252" height="9" font="font11" id="p7_t84" reading_order_no="82" segment_no="8" tag_type="text">We add a smoothness term to penalize the pose jerkiness</text>
<text top="187" left="312" width="252" height="9" font="font11" id="p7_t85" reading_order_no="83" segment_no="8" tag_type="text">between two consecutive frames. This smoothness term is</text>
<text top="199" left="312" width="46" height="9" font="font11" id="p7_t86" reading_order_no="84" segment_no="8" tag_type="text">defined as:</text>
<text top="215" left="376" width="123" height="12" font="font14" id="p7_t87" reading_order_no="85" segment_no="10" tag_type="formula">E temporal ( θ ) = || θ − θ prev || 2 2</text>
<text top="217" left="548" width="16" height="9" font="font11" id="p7_t88" reading_order_no="86" segment_no="10" tag_type="text">(14)</text>
<text top="239" left="312" width="21" height="9" font="font48" id="p7_t89" reading_order_no="87" segment_no="12" tag_type="title">5.2.4</text>
<text top="239" left="343" width="53" height="9" font="font48" id="p7_t90" reading_order_no="88" segment_no="12" tag_type="title">Optimization</text>
<text top="253" left="312" width="252" height="9" font="font11" id="p7_t91" reading_order_no="89" segment_no="13" tag_type="text">As Eq. 5 is represented as a sum of squares, we can</text>
<text top="265" left="312" width="252" height="9" font="font11" id="p7_t92" reading_order_no="90" segment_no="13" tag_type="text">efficiently solve it by Gauss-Newton method. Since every</text>
<text top="276" left="312" width="252" height="9" font="font11" id="p7_t93" reading_order_no="91" segment_no="13" tag_type="text">term is differentiable, we can directly compute the Jacobian</text>
<text top="288" left="312" width="252" height="9" font="font11" id="p7_t94" reading_order_no="92" segment_no="13" tag_type="text">matrix J ( θ ) and then follow the standard Gauss-Newton</text>
<text top="299" left="312" width="159" height="9" font="font11" id="p7_t95" reading_order_no="93" segment_no="13" tag_type="text">step to solve δ θ and update current θ</text>
<text top="315" left="379" width="118" height="11" font="font14" id="p7_t96" reading_order_no="94" segment_no="16" tag_type="formula">J ( θ ) T J ( θ ) δ θ = J ( θ ) T r ( θ ) ,</text>
<text top="332" left="379" width="51" height="9" font="font41" id="p7_t97" reading_order_no="95" segment_no="16" tag_type="formula">θ = θ + δ θ ,</text>
<text top="325" left="548" width="16" height="9" font="font11" id="p7_t98" reading_order_no="96" segment_no="16" tag_type="text">(15)</text>
<text top="349" left="312" width="252" height="10" font="font11" id="p7_t99" reading_order_no="97" segment_no="18" tag_type="text">where r ( θ ) is the residual vector formed by concatenating</text>
<text top="361" left="312" width="44" height="9" font="font11" id="p7_t100" reading_order_no="98" segment_no="18" tag_type="text">each term.</text>
<text top="373" left="326" width="238" height="9" font="font18" id="p7_t101" reading_order_no="99" segment_no="19" tag_type="text">Parameter values. In our implementation, the weights</text>
<text top="384" left="312" width="252" height="10" font="font14" id="p7_t102" reading_order_no="100" segment_no="19" tag_type="text">w data , w prior and w temporal in Eq. 5 are set to 1.0, 0.5 and 5.0</text>
<text top="396" left="312" width="252" height="9" font="font11" id="p7_t103" reading_order_no="101" segment_no="19" tag_type="text">in our experiments. Weight settings in Eq. 6 are w 2 d = 20 . 0 ,</text>
<text top="407" left="312" width="252" height="10" font="font14" id="p7_t104" reading_order_no="102" segment_no="19" tag_type="text">w 3 d = 300 . 0 , w iuv = 2 . 0 and w mask = 1 . 0 . The weights</text>
<text top="419" left="312" width="252" height="9" font="font11" id="p7_t105" reading_order_no="103" segment_no="19" tag_type="text">in E prior are w pose prior = 0 . 002 and w jt limit = 5 . 0 . The</text>
<text top="430" left="312" width="250" height="10" font="font11" id="p7_t106" reading_order_no="104" segment_no="19" tag_type="text">weight for temporal smoothness term is w temporal = 0 . 003 .</text>
<text top="457" left="312" width="13" height="9" font="font39" id="p7_t107" reading_order_no="105" segment_no="22" tag_type="title">5.3</text>
<text top="457" left="335" width="118" height="9" font="font39" id="p7_t108" reading_order_no="106" segment_no="22" tag_type="title">Full-body Shape Modeling</text>
<text top="472" left="312" width="252" height="9" font="font11" id="p7_t109" reading_order_no="107" segment_no="23" tag_type="text">Now we describe how to reconstruct a full-body mesh</text>
<text top="483" left="312" width="252" height="9" font="font11" id="p7_t110" reading_order_no="108" segment_no="23" tag_type="text">model for a subject using the network observations. Note</text>
<text top="495" left="312" width="252" height="9" font="font11" id="p7_t111" reading_order_no="109" segment_no="23" tag_type="text">in the case that the input is a video stream, the shape</text>
<text top="506" left="312" width="219" height="9" font="font11" id="p7_t112" reading_order_no="110" segment_no="23" tag_type="text">reconstruction is done only once, for the first frame.</text>
<text top="529" left="312" width="21" height="9" font="font48" id="p7_t113" reading_order_no="111" segment_no="26" tag_type="title">5.3.1</text>
<text top="529" left="343" width="94" height="9" font="font48" id="p7_t114" reading_order_no="112" segment_no="26" tag_type="title">Shape Reconstruction</text>
<text top="543" left="312" width="252" height="9" font="font11" id="p7_t115" reading_order_no="113" segment_no="27" tag_type="text">Given an image I , our goal is to reconstruct a subject-</text>
<text top="554" left="312" width="252" height="9" font="font11" id="p7_t116" reading_order_no="114" segment_no="27" tag_type="text">specific human body model H ( α , β , θ ) , with the derived</text>
<text top="566" left="312" width="252" height="9" font="font11" id="p7_t117" reading_order_no="115" segment_no="27" tag_type="text">image positions for 2D joints, the segmentation mask, body</text>
<text top="577" left="312" width="252" height="9" font="font11" id="p7_t118" reading_order_no="116" segment_no="27" tag_type="text">partition, 3D part orientation, and the dense correspondence</text>
<text top="589" left="312" width="252" height="9" font="font11" id="p7_t119" reading_order_no="117" segment_no="27" tag_type="text">( uv map) for human parts. We formulate the reconstruction<b>Mask Term.</b></text>
<text top="600" left="312" width="157" height="9" font="font11" id="p7_t120" reading_order_no="118" segment_no="27" tag_type="text">as a non-linear optimization problem</text>
<text top="618" left="331" width="32" height="9" font="font19" id="p7_t121" reading_order_no="119" segment_no="30" tag_type="formula">arg min</text>
<text top="628" left="336" width="20" height="6" font="font51" id="p7_t122" reading_order_no="120" segment_no="30" tag_type="formula">α , β , θ</text>
<text top="618" left="368" width="162" height="9" font="font19" id="p7_t123" reading_order_no="121" segment_no="30" tag_type="formula">( w data E data + w pose prior E pose prior +</text>
<text top="639" left="422" width="108" height="10" font="font14" id="p7_t124" reading_order_no="122" segment_no="30" tag_type="formula">w shape prior E shape prior ) ,</text>
<text top="629" left="548" width="16" height="9" font="font11" id="p7_t125" reading_order_no="123" segment_no="30" tag_type="text">(16)</text>
<text top="656" left="312" width="252" height="10" font="font11" id="p7_t126" reading_order_no="124" segment_no="32" tag_type="text">where E data is the data term that describes the registration</text>
<text top="668" left="312" width="252" height="9" font="font11" id="p7_t127" reading_order_no="125" segment_no="32" tag_type="text">error between the synthesized model and the network ob-</text>
<text top="680" left="312" width="253" height="9" font="font11" id="p7_t128" reading_order_no="126" segment_no="32" tag_type="text">servations that inherits from § 5.2, E pose prior is the pose</text>
<text top="691" left="312" width="252" height="10" font="font11" id="p7_t129" reading_order_no="127" segment_no="32" tag_type="text">prior term that inherits from § 5.2, E shape prior is the shape</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p7_t130" reading_order_no="128" segment_no="32" tag_type="text">prior term that penalizes the deviation of shape parameters</text>
<text top="714" left="312" width="165" height="9" font="font11" id="p7_t131" reading_order_no="129" segment_no="32" tag_type="text">from the human shape in the database.</text>
<text top="726" left="326" width="238" height="9" font="font18" id="p7_t132" reading_order_no="130" segment_no="35" tag_type="text">The shape prior term. We model the shape prior dis-</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p7_t133" reading_order_no="131" segment_no="35" tag_type="text">tribution with multiple single-variable Gaussian models</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
<text top="29" left="48" width="334" height="6" font="font0" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="text">8</text>
<text top="206" left="121" width="107" height="9" font="font11" id="p8_t3" reading_order_no="2" segment_no="4" tag_type="text">Fig. 6: The capture space.</text>
<text top="241" left="48" width="252" height="9" font="font11" id="p8_t4" reading_order_no="3" segment_no="5" tag_type="text">that is learnt from the human shape database. We define</text>
<text top="252" left="48" width="229" height="10" font="font14" id="p8_t5" reading_order_no="4" segment_no="5" tag_type="text">E shape prior as the deviation distance with parameters</text>
<text top="273" left="59" width="82" height="12" font="font14" id="p8_t6" reading_order_no="5" segment_no="6" tag_type="formula">E shape prior = ( X</text>
<text top="289" left="133" width="3" height="6" font="font38" id="p8_t7" reading_order_no="6" segment_no="6" tag_type="formula">i</text>
<text top="273" left="143" width="82" height="13" font="font36" id="p8_t8" reading_order_no="7" segment_no="6" tag_type="formula">| α i − µ i α | /σ i α + X</text>
<text top="289" left="216" width="3" height="6" font="font38" id="p8_t9" reading_order_no="8" segment_no="6" tag_type="formula">j</text>
<text top="273" left="227" width="34" height="12" font="font36" id="p8_t10" reading_order_no="9" segment_no="6" tag_type="formula">| β j − µ j</text>
<text top="273" left="258" width="22" height="14" font="font36" id="p8_t11" reading_order_no="10" segment_no="6" tag_type="formula">β | /σ j</text>
<text top="276" left="277" width="12" height="11" font="font19" id="p8_t12" reading_order_no="11" segment_no="6" tag_type="formula">β ) ,</text>
<text top="307" left="48" width="249" height="9" font="font11" id="p8_t13" reading_order_no="12" segment_no="9" tag_type="text">where µ is the mean value and σ is the standard deviation.</text>
<text top="334" left="48" width="21" height="9" font="font48" id="p8_t14" reading_order_no="13" segment_no="10" tag_type="title">5.3.2</text>
<text top="334" left="79" width="53" height="9" font="font48" id="p8_t15" reading_order_no="14" segment_no="10" tag_type="title">Optimization</text>
<text top="351" left="48" width="252" height="9" font="font11" id="p8_t16" reading_order_no="15" segment_no="11" tag_type="text">Directly optimizing Eq. 16 is not efficient and often falls into</text>
<text top="363" left="48" width="252" height="9" font="font11" id="p8_t17" reading_order_no="16" segment_no="11" tag_type="text">local minima, because the pose and the shape are coupled.</text>
<text top="374" left="48" width="252" height="9" font="font11" id="p8_t18" reading_order_no="17" segment_no="11" tag_type="text">To address this issue, we decouple the optimization into two</text>
<text top="386" left="48" width="252" height="9" font="font11" id="p8_t19" reading_order_no="18" segment_no="11" tag_type="text">sub-optimization problems: pose optimization and shape</text>
<text top="397" left="48" width="252" height="9" font="font11" id="p8_t20" reading_order_no="19" segment_no="11" tag_type="text">optimization. In each iteration, we first fix shape parameters</text>
<text top="409" left="48" width="218" height="9" font="font11" id="p8_t21" reading_order_no="20" segment_no="11" tag_type="text">and optimize pose parameters, and then vice versa.</text>
<text top="421" left="62" width="238" height="9" font="font18" id="p8_t22" reading_order_no="21" segment_no="13" tag_type="text">Pose estimation. In this step, we optimize the pose</text>
<text top="432" left="48" width="252" height="10" font="font11" id="p8_t23" reading_order_no="22" segment_no="13" tag_type="text">parameter θ while fixing the shape parameter α , β . This</text>
<text top="444" left="48" width="145" height="9" font="font11" id="p8_t24" reading_order_no="23" segment_no="13" tag_type="text">process is identical to that in § 5.2.</text>
<text top="456" left="62" width="238" height="10" font="font18" id="p8_t25" reading_order_no="24" segment_no="15" tag_type="text">Shape estimation. In this step, we optimize the shape</text>
<text top="468" left="48" width="252" height="9" font="font11" id="p8_t26" reading_order_no="25" segment_no="15" tag_type="text">parameter α , β while keeping the pose parameter θ fixed.</text>
<text top="480" left="48" width="252" height="9" font="font11" id="p8_t27" reading_order_no="26" segment_no="15" tag_type="text">And therefore the optimization problem can be represented</text>
<text top="491" left="48" width="11" height="9" font="font11" id="p8_t28" reading_order_no="27" segment_no="15" tag_type="text">by</text>
<text top="513" left="65" width="32" height="9" font="font19" id="p8_t29" reading_order_no="28" segment_no="17" tag_type="formula">arg min</text>
<text top="523" left="74" width="13" height="6" font="font51" id="p8_t30" reading_order_no="29" segment_no="17" tag_type="formula">α , β</text>
<text top="513" left="97" width="171" height="9" font="font19" id="p8_t31" reading_order_no="30" segment_no="17" tag_type="formula">( w data E data + w shape prior E shape prior ) .</text>
<text top="513" left="284" width="16" height="9" font="font11" id="p8_t32" reading_order_no="31" segment_no="17" tag_type="text">(17)</text>
<text top="541" left="48" width="252" height="9" font="font11" id="p8_t33" reading_order_no="32" segment_no="18" tag_type="text">Following the optimization method in § 5.2.4, we can solve</text>
<text top="552" left="48" width="131" height="9" font="font11" id="p8_t34" reading_order_no="33" segment_no="18" tag_type="text">the shape parameters α and β .</text>
<text top="565" left="62" width="238" height="9" font="font18" id="p8_t35" reading_order_no="34" segment_no="21" tag_type="text">Parameter values. For shape reconstruction, the energy</text>
<text top="576" left="48" width="252" height="10" font="font11" id="p8_t36" reading_order_no="35" segment_no="21" tag_type="text">terms E data and E pose prior are the same as in § 5.2. The</text>
<text top="588" left="48" width="203" height="9" font="font11" id="p8_t37" reading_order_no="36" segment_no="21" tag_type="text">shape prior weight we use is w shape prior = 0 . 1 .</text>
<text top="624" left="48" width="6" height="10" font="font7" id="p8_t38" reading_order_no="37" segment_no="24" tag_type="title">6</text>
<text top="624" left="66" width="77" height="10" font="font7" id="p8_t39" reading_order_no="38" segment_no="24" tag_type="title">T RAINING D ATA</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p8_t40" reading_order_no="39" segment_no="26" tag_type="text">As one major highlight of our work, we complement ex-</text>
<text top="657" left="48" width="252" height="9" font="font11" id="p8_t41" reading_order_no="40" segment_no="26" tag_type="text">isting datasets by building a dataset with a large number</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p8_t42" reading_order_no="41" segment_no="26" tag_type="text">of actors, everyday clothing appearances, a broad range</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p8_t43" reading_order_no="42" segment_no="26" tag_type="text">of motions. The data capture setup eases the appearance</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p8_t44" reading_order_no="43" segment_no="26" tag_type="text">augmentation and extends the captured variability. This</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p8_t45" reading_order_no="44" segment_no="26" tag_type="text">gives a potential to bring significant boost to accuracy and</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p8_t46" reading_order_no="45" segment_no="26" tag_type="text">generalizability of the learnt models. In this section we</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p8_t47" reading_order_no="46" segment_no="26" tag_type="text">describe the capture environment and the recording process,</text>
<text top="737" left="48" width="195" height="9" font="font11" id="p8_t48" reading_order_no="47" segment_no="26" tag_type="text">as well as the processing of the captured data.</text>
<text top="45" left="312" width="13" height="9" font="font39" id="p8_t49" reading_order_no="48" segment_no="2" tag_type="title">6.1</text>
<text top="45" left="335" width="89" height="9" font="font39" id="p8_t50" reading_order_no="49" segment_no="2" tag_type="title">Experimental Setup</text>
<text top="60" left="312" width="252" height="9" font="font11" id="p8_t51" reading_order_no="50" segment_no="3" tag_type="text">Our laboratory setup is shown in Fig. 6, where data is</text>
<text top="72" left="312" width="252" height="9" font="font11" id="p8_t52" reading_order_no="51" segment_no="3" tag_type="text">captured by four digital video cameras. The designated</text>
<text top="83" left="312" width="252" height="9" font="font11" id="p8_t53" reading_order_no="52" segment_no="3" tag_type="text">laboratory area is about 4 m × 4 m , and within it we obtain<b>Pose estimation.</b></text>
<text top="94" left="312" width="252" height="10" font="font11" id="p8_t54" reading_order_no="53" segment_no="3" tag_type="text">effective capture court of approximately 2 . 5 m × 2 . 5 m , where</text>
<text top="107" left="312" width="252" height="9" font="font11" id="p8_t55" reading_order_no="54" segment_no="3" tag_type="text">each subject is fully visible to all cameras. Four cameras are</text>
<text top="118" left="312" width="252" height="9" font="font11" id="p8_t56" reading_order_no="55" segment_no="3" tag_type="text">placed at four corners of the court. The floor and the wall are</text>
<text top="130" left="312" width="252" height="9" font="font11" id="p8_t57" reading_order_no="56" segment_no="3" tag_type="text">mantled with green curtains, making it easy for automatic</text>
<text top="141" left="312" width="252" height="9" font="font11" id="p8_t58" reading_order_no="57" segment_no="3" tag_type="text">segmentation of the foreground body. The total number of</text>
<text top="153" left="312" width="252" height="9" font="font11" id="p8_t59" reading_order_no="58" segment_no="3" tag_type="text">actors screened are over 300 , covering a broad range of ages,</text>
<text top="164" left="312" width="252" height="9" font="font11" id="p8_t60" reading_order_no="59" segment_no="3" tag_type="text">body shapes and pose extensions. Our dataset has much</text>
<text top="176" left="312" width="252" height="9" font="font11" id="p8_t61" reading_order_no="60" segment_no="3" tag_type="text">more subjects than in any of existing 3D datasets. Each per-</text>
<text top="187" left="312" width="252" height="9" font="font11" id="p8_t62" reading_order_no="61" segment_no="3" tag_type="text">son is asked to do certain daily life motions as well as sport</text>
<text top="199" left="312" width="252" height="9" font="font11" id="p8_t63" reading_order_no="62" segment_no="3" tag_type="text">motions, for about three minutes. To eliminate redundancy<b>Shape estimation.</b></text>
<text top="210" left="312" width="252" height="9" font="font11" id="p8_t64" reading_order_no="63" segment_no="3" tag_type="text">between consecutive video frames, frame images are further</text>
<text top="222" left="312" width="252" height="9" font="font11" id="p8_t65" reading_order_no="64" segment_no="3" tag_type="text">filtered, and only 500 to 1,000 images are sampled for each</text>
<text top="233" left="312" width="252" height="9" font="font11" id="p8_t66" reading_order_no="65" segment_no="3" tag_type="text">actor. The sampling is achieved by clustering frame images</text>
<text top="245" left="312" width="183" height="9" font="font11" id="p8_t67" reading_order_no="66" segment_no="3" tag_type="text">according to the similarity of human poses.</text>
<text top="274" left="312" width="13" height="9" font="font39" id="p8_t68" reading_order_no="67" segment_no="7" tag_type="title">6.2</text>
<text top="274" left="335" width="194" height="9" font="font39" id="p8_t69" reading_order_no="68" segment_no="7" tag_type="title">Shape Reconstruction from Measurements</text>
<text top="289" left="312" width="252" height="9" font="font11" id="p8_t70" reading_order_no="69" segment_no="8" tag_type="text">To build a highly accurate parametric body model for each</text>
<text top="301" left="312" width="252" height="9" font="font11" id="p8_t71" reading_order_no="70" segment_no="8" tag_type="text">actor, we take some body measurements while an actor is</text>
<text top="311" left="312" width="252" height="10" font="font11" id="p8_t72" reading_order_no="71" segment_no="8" tag_type="text">standing still in A-pose. A set of measurements M ∈ R 44</text>
<text top="324" left="312" width="252" height="9" font="font11" id="p8_t73" reading_order_no="72" segment_no="8" tag_type="text">(including but not limited to lengths of limbs, shoulder</text>
<text top="335" left="312" width="252" height="9" font="font11" id="p8_t74" reading_order_no="73" segment_no="8" tag_type="text">and back, girths of chest, wrist, hip and stomach, etc.), are</text>
<text top="347" left="312" width="252" height="9" font="font11" id="p8_t75" reading_order_no="74" segment_no="8" tag_type="text">tailoring measured with a ruler. Our parametric model has</text>
<text top="358" left="312" width="252" height="9" font="font19" id="p8_t76" reading_order_no="75" segment_no="8" tag_type="text">8 + 26 = 34 shape parameters, more sophisticated than the</text>
<text top="370" left="312" width="252" height="9" font="font11" id="p8_t77" reading_order_no="76" segment_no="8" tag_type="text">SMPL model, which has only 10 shape parameters. With</text>
<text top="381" left="312" width="252" height="10" font="font11" id="p8_t78" reading_order_no="77" segment_no="8" tag_type="text">the measurements, parameters α and β are computed by</text>
<text top="393" left="312" width="139" height="9" font="font11" id="p8_t79" reading_order_no="78" segment_no="8" tag_type="text">minimization an energy function</text>
<text top="412" left="352" width="173" height="10" font="font14" id="p8_t80" reading_order_no="79" segment_no="12" tag_type="formula">E ( α , β ) = w 1 E geoDist + w 2 E shape prior ,</text>
<text top="412" left="548" width="16" height="9" font="font11" id="p8_t81" reading_order_no="80" segment_no="12" tag_type="text">(18)</text>
<text top="432" left="312" width="67" height="10" font="font11" id="p8_t82" reading_order_no="81" segment_no="14" tag_type="text">where E geoDist</text>
<text top="429" left="387" width="33" height="12" font="font19" id="p8_t83" reading_order_no="82" segment_no="14" tag_type="text">= P 44</text>
<text top="432" left="412" width="152" height="11" font="font36" id="p8_t84" reading_order_no="83" segment_no="14" tag_type="text">i =1 || f i ( α , β ) − M i || 2 assesses the</text>
<text top="444" left="312" width="252" height="9" font="font11" id="p8_t85" reading_order_no="84" segment_no="14" tag_type="text">error of geodesic distance on the parameterized human</text>
<text top="455" left="312" width="252" height="10" font="font11" id="p8_t86" reading_order_no="85" segment_no="14" tag_type="text">body, E shape prior determines the shape prior error, with</text>
<text top="467" left="312" width="252" height="9" font="font11" id="p8_t87" reading_order_no="86" segment_no="14" tag_type="text">respect to mixed Gaussian distribution. The above energy</text>
<text top="478" left="312" width="252" height="9" font="font11" id="p8_t88" reading_order_no="87" segment_no="14" tag_type="text">is minimized with the Particle Swarm Optimization method</text>
<text top="490" left="312" width="39" height="9" font="font11" id="p8_t89" reading_order_no="88" segment_no="14" tag_type="text">[51], [52].</text>
<text top="502" left="326" width="238" height="9" font="font18" id="p8_t90" reading_order_no="89" segment_no="16" tag_type="text">Parameter values. We set w 1 = 8 . 0 for E geoDist and</text>
<text top="513" left="312" width="147" height="10" font="font14" id="p8_t91" reading_order_no="90" segment_no="16" tag_type="text">w 2 = 0 . 8 for E shape prior in Eq. 18.</text>
<text top="542" left="312" width="13" height="9" font="font39" id="p8_t92" reading_order_no="91" segment_no="19" tag_type="title">6.3</text>
<text top="542" left="335" width="95" height="9" font="font39" id="p8_t93" reading_order_no="92" segment_no="19" tag_type="title">Pose Reconstruction</text>
<text top="557" left="312" width="252" height="9" font="font11" id="p8_t94" reading_order_no="93" segment_no="20" tag_type="text">With four multi-view images for each frame, a 3D pose is</text>
<text top="569" left="312" width="200" height="9" font="font11" id="p8_t95" reading_order_no="94" segment_no="20" tag_type="text">reconstructed, according to the following steps.</text>
<text top="587" left="326" width="8" height="9" font="font11" id="p8_t96" reading_order_no="95" segment_no="22" tag_type="list">1)</text>
<text top="587" left="346" width="218" height="9" font="font11" id="p8_t97" reading_order_no="96" segment_no="22" tag_type="list">Estimate 18 joints on each image from each camera,<b>Parameter values.</b></text>
<text top="599" left="346" width="166" height="9" font="font11" id="p8_t98" reading_order_no="97" segment_no="22" tag_type="list">with a 2D joint estimation method [43].</text>
<text top="610" left="326" width="8" height="9" font="font11" id="p8_t99" reading_order_no="98" segment_no="23" tag_type="list">2)</text>
<text top="610" left="346" width="218" height="9" font="font11" id="p8_t100" reading_order_no="99" segment_no="23" tag_type="list">Validate the consistency of the estimated 2D joints</text>
<text top="622" left="346" width="209" height="9" font="font11" id="p8_t101" reading_order_no="100" segment_no="23" tag_type="list">from multi-views (see the following explanation).</text>
<text top="634" left="326" width="8" height="9" font="font11" id="p8_t102" reading_order_no="101" segment_no="25" tag_type="list">3)</text>
<text top="634" left="346" width="218" height="9" font="font11" id="p8_t103" reading_order_no="102" segment_no="25" tag_type="list">Segment foreground body from background auto-</text>
<text top="645" left="346" width="141" height="9" font="font11" id="p8_t104" reading_order_no="103" segment_no="25" tag_type="list">matically (green curtains setting).</text>
<text top="657" left="326" width="8" height="9" font="font11" id="p8_t105" reading_order_no="104" segment_no="27" tag_type="list">4)</text>
<text top="657" left="346" width="218" height="9" font="font11" id="p8_t106" reading_order_no="105" segment_no="27" tag_type="list">Solve the human motion by extending Eq. 5 into</text>
<text top="668" left="346" width="218" height="9" font="font11" id="p8_t107" reading_order_no="106" segment_no="27" tag_type="list">multi-views, while dropping off the IUV constraints</text>
<text top="680" left="346" width="218" height="9" font="font11" id="p8_t108" reading_order_no="107" segment_no="27" tag_type="list">and the 3D constraints from Eq. 6 (As both con-</text>
<text top="691" left="346" width="218" height="9" font="font11" id="p8_t109" reading_order_no="108" segment_no="27" tag_type="list">straints are not available when constructing our</text>
<text top="703" left="346" width="40" height="9" font="font11" id="p8_t110" reading_order_no="109" segment_no="27" tag_type="list">datasets).</text>
<text top="714" left="326" width="8" height="9" font="font11" id="p8_t111" reading_order_no="110" segment_no="28" tag_type="list">5)</text>
<text top="714" left="346" width="218" height="9" font="font11" id="p8_t112" reading_order_no="111" segment_no="28" tag_type="list">Recover the 3D body mesh with the shape from</text>
<text top="726" left="346" width="218" height="9" font="font11" id="p8_t113" reading_order_no="112" segment_no="28" tag_type="list">measurements and the pose from multi-view im-<b>6</b></text>
<text top="737" left="346" width="91" height="9" font="font11" id="p8_t114" reading_order_no="113" segment_no="28" tag_type="list">ages, shown in Fig. 7.<b>T</b></text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font52" size="8" family="CMR8" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="560" width="4" height="6" font="font0" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="text">9</text>
<text top="179" left="48" width="252" height="9" font="font11" id="p9_t3" reading_order_no="2" segment_no="3" tag_type="text">Fig. 7: We reconstruct body mesh from multi-views as</text>
<text top="191" left="48" width="57" height="9" font="font11" id="p9_t4" reading_order_no="3" segment_no="3" tag_type="text">ground truth.</text>
<text top="225" left="48" width="252" height="9" font="font11" id="p9_t5" reading_order_no="4" segment_no="5" tag_type="text">In Step 2, it is very common that in a single-view image</text>
<text top="237" left="48" width="252" height="9" font="font11" id="p9_t6" reading_order_no="5" segment_no="5" tag_type="text">some joints could be invisible due to occlusion. Therefore we</text>
<text top="248" left="48" width="252" height="9" font="font11" id="p9_t7" reading_order_no="6" segment_no="5" tag_type="text">propose a cross validation scheme based on multiple views.</text>
<text top="260" left="48" width="252" height="9" font="font11" id="p9_t8" reading_order_no="7" segment_no="5" tag_type="text">For a certain joint, we choose any pair of cameras and use</text>
<text top="271" left="48" width="252" height="9" font="font11" id="p9_t9" reading_order_no="8" segment_no="5" tag_type="text">the two 2D estimations to build the 3D joint position. The</text>
<text top="283" left="48" width="252" height="9" font="font11" id="p9_t10" reading_order_no="9" segment_no="5" tag_type="text">3D joint is then re-projected with respect to the view-ports</text>
<text top="294" left="48" width="252" height="9" font="font11" id="p9_t11" reading_order_no="10" segment_no="5" tag_type="text">of the other two cameras, creating two projected joints. Each</text>
<text top="306" left="48" width="252" height="9" font="font11" id="p9_t12" reading_order_no="11" segment_no="5" tag_type="text">projected joint is compared against the estimated 2D joint</text>
<text top="318" left="48" width="252" height="9" font="font11" id="p9_t13" reading_order_no="12" segment_no="5" tag_type="text">under the same view-port, and their Eucleadian distance</text>
<text top="329" left="48" width="252" height="9" font="font11" id="p9_t14" reading_order_no="13" segment_no="5" tag_type="text">is calculated. Only if the distances in two view-ports are</text>
<text top="341" left="48" width="252" height="9" font="font11" id="p9_t15" reading_order_no="14" segment_no="5" tag_type="text">both below a certain threshold (18 pixels in our experiment),</text>
<text top="352" left="48" width="252" height="9" font="font11" id="p9_t16" reading_order_no="15" segment_no="5" tag_type="text">this set of four 2D estimations for this joint is considered to</text>
<text top="364" left="48" width="252" height="9" font="font11" id="p9_t17" reading_order_no="16" segment_no="5" tag_type="text">be consistent and reliable. If four view-ports fail to reach</text>
<text top="375" left="48" width="252" height="9" font="font11" id="p9_t18" reading_order_no="17" segment_no="5" tag_type="text">consistency, we check if any three of them do. If that</text>
<text top="387" left="48" width="252" height="9" font="font11" id="p9_t19" reading_order_no="18" segment_no="5" tag_type="text">happens, the 2D joint estimation in the three view-ports</text>
<text top="398" left="48" width="252" height="9" font="font11" id="p9_t20" reading_order_no="19" segment_no="5" tag_type="text">are treated as reliable. If such three view-ports can not be</text>
<text top="410" left="48" width="252" height="9" font="font11" id="p9_t21" reading_order_no="20" segment_no="5" tag_type="text">found, we have to ask for help from the previous frame.</text>
<text top="421" left="48" width="252" height="9" font="font11" id="p9_t22" reading_order_no="21" segment_no="5" tag_type="text">The 3D joint from the previous frame is compared with the</text>
<text top="433" left="48" width="252" height="9" font="font11" id="p9_t23" reading_order_no="22" segment_no="5" tag_type="text">reconstructed 3D joint from any pair of cameras, and the</text>
<text top="444" left="48" width="252" height="9" font="font11" id="p9_t24" reading_order_no="23" segment_no="5" tag_type="text">distances are calculated. The minimal distance designates</text>
<text top="456" left="48" width="252" height="9" font="font11" id="p9_t25" reading_order_no="24" segment_no="5" tag_type="text">the pair of cameras and their estimations are treated as</text>
<text top="468" left="48" width="33" height="9" font="font11" id="p9_t26" reading_order_no="25" segment_no="5" tag_type="text">reliable.</text>
<text top="500" left="48" width="13" height="9" font="font39" id="p9_t27" reading_order_no="26" segment_no="10" tag_type="title">6.4<b>6.4</b></text>
<text top="500" left="71" width="43" height="9" font="font39" id="p9_t28" reading_order_no="27" segment_no="10" tag_type="title">IUV Maps<b>IUV Maps</b></text>
<text top="517" left="48" width="252" height="9" font="font11" id="p9_t29" reading_order_no="28" segment_no="12" tag_type="text">The image-to-surface correspondence (IUV map) proposed</text>
<text top="529" left="48" width="252" height="9" font="font11" id="p9_t30" reading_order_no="29" segment_no="12" tag_type="text">in Densepose [1] for human body is an essential mechanism</text>
<text top="541" left="48" width="252" height="9" font="font11" id="p9_t31" reading_order_no="30" segment_no="12" tag_type="text">for mapping a 2D image into a 3D geometry. We improved</text>
<text top="552" left="48" width="252" height="9" font="font11" id="p9_t32" reading_order_no="31" segment_no="12" tag_type="text">this idea with a more solid implementation, and it works</text>
<text top="564" left="48" width="252" height="9" font="font11" id="p9_t33" reading_order_no="32" segment_no="12" tag_type="text">well for persons with loose clothes. In Densepose certain</text>
<text top="575" left="48" width="252" height="9" font="font11" id="p9_t34" reading_order_no="33" segment_no="12" tag_type="text">pixels are manually sampled on each human part, and their</text>
<text top="587" left="48" width="252" height="9" font="font11" id="p9_t35" reading_order_no="34" segment_no="12" tag_type="text">corresponding points on the meshed surface are manually</text>
<text top="598" left="48" width="252" height="9" font="font11" id="p9_t36" reading_order_no="35" segment_no="12" tag_type="text">marked as well. Annotators are asked to determine the</text>
<text top="610" left="48" width="252" height="9" font="font11" id="p9_t37" reading_order_no="36" segment_no="12" tag_type="text">body silhouette if it is covered by clothes, and mark around</text>
<text top="621" left="48" width="252" height="9" font="font11" id="p9_t38" reading_order_no="37" segment_no="12" tag_type="text">100 points for each human. In this situation the burden is</text>
<text top="633" left="48" width="252" height="9" font="font11" id="p9_t39" reading_order_no="38" segment_no="12" tag_type="text">heavy and errors prone to happen, especially when a human</text>
<text top="644" left="48" width="145" height="9" font="font11" id="p9_t40" reading_order_no="39" segment_no="12" tag_type="text">instance wears a large/loose skirt.</text>
<text top="657" left="62" width="238" height="9" font="font11" id="p9_t41" reading_order_no="40" segment_no="15" tag_type="text">We adopt a quite different scheme for computing the</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p9_t42" reading_order_no="41" segment_no="15" tag_type="text">part label and the ( u, v ) coordinate for each pixel, requiring</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p9_t43" reading_order_no="42" segment_no="15" tag_type="text">much less human intervention. The reconstructed 3D mesh</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p9_t44" reading_order_no="43" segment_no="15" tag_type="text">by measurements is re-projected according to the view-</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p9_t45" reading_order_no="44" segment_no="15" tag_type="text">ports of the cameras, creating four images. As each mesh</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p9_t46" reading_order_no="45" segment_no="15" tag_type="text">vertex has a ( u, v ) coordinate and a part label, it is trivial to</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p9_t47" reading_order_no="46" segment_no="15" tag_type="text">compute ( u, v ) and label for each pixel of these images. Due</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p9_t48" reading_order_no="47" segment_no="15" tag_type="text">to the topological complexity of human meshes, we follow</text>
<text top="168" left="348" width="179" height="9" font="font11" id="p9_t49" reading_order_no="48" segment_no="2" tag_type="text">Fig. 8: The IUV maps of the 24 body parts.</text>
<text top="190" left="393" width="42" height="8" font="font10" id="p9_t50" reading_order_no="49" segment_no="4" tag_type="table">Component</text>
<text top="190" left="484" width="36" height="8" font="font10" id="p9_t51" reading_order_no="50" segment_no="4" tag_type="table">Time (ms)</text>
<text top="199" left="368" width="92" height="8" font="font10" id="p9_t52" reading_order_no="51" segment_no="4" tag_type="table">Human detection (YOLO)</text>
<text top="199" left="493" width="18" height="8" font="font10" id="p9_t53" reading_order_no="52" segment_no="4" tag_type="table">8.554</text>
<text top="208" left="383" width="62" height="8" font="font10" id="p9_t54" reading_order_no="53" segment_no="4" tag_type="table">Image Preprocess</text>
<text top="208" left="493" width="18" height="8" font="font10" id="p9_t55" reading_order_no="54" segment_no="4" tag_type="table">0.312</text>
<text top="217" left="376" width="75" height="8" font="font10" id="p9_t56" reading_order_no="55" segment_no="4" tag_type="table">Multi-task Prediction</text>
<text top="217" left="491" width="22" height="8" font="font10" id="p9_t57" reading_order_no="56" segment_no="4" tag_type="table">18.269</text>
<text top="226" left="378" width="72" height="8" font="font10" id="p9_t58" reading_order_no="57" segment_no="4" tag_type="table">Pose Reconstruction</text>
<text top="226" left="491" width="22" height="8" font="font10" id="p9_t59" reading_order_no="58" segment_no="4" tag_type="table">10.232</text>
<text top="235" left="402" width="24" height="8" font="font10" id="p9_t60" reading_order_no="59" segment_no="4" tag_type="table">Others</text>
<text top="235" left="491" width="22" height="8" font="font10" id="p9_t61" reading_order_no="60" segment_no="4" tag_type="table">11.678</text>
<text top="245" left="356" width="117" height="8" font="font10" id="p9_t62" reading_order_no="61" segment_no="4" tag_type="table">Shape Recon (only for 1 st frame)</text>
<text top="245" left="489" width="26" height="8" font="font10" id="p9_t63" reading_order_no="62" segment_no="4" tag_type="table">328.227</text>
<text top="264" left="315" width="246" height="9" font="font11" id="p9_t64" reading_order_no="63" segment_no="6" tag_type="title">TABLE 1: Running time of each component in our system.</text>
<text top="297" left="312" width="252" height="9" font="font11" id="p9_t65" reading_order_no="64" segment_no="7" tag_type="text">Densepose and segment a human mesh into 24 parts, and</text>
<text top="309" left="312" width="208" height="9" font="font11" id="p9_t66" reading_order_no="65" segment_no="7" tag_type="text">define a uv -field on each part, as shown in Fig. 8.</text>
<text top="339" left="312" width="6" height="10" font="font7" id="p9_t67" reading_order_no="66" segment_no="8" tag_type="title">7</text>
<text top="339" left="330" width="140" height="10" font="font7" id="p9_t68" reading_order_no="67" segment_no="8" tag_type="title">R ESULTS AND E VALUATIONS</text>
<text top="357" left="312" width="252" height="9" font="font11" id="p9_t69" reading_order_no="68" segment_no="9" tag_type="text">We demonstrate the power and effectiveness of our system,</text>
<text top="368" left="312" width="252" height="9" font="font11" id="p9_t70" reading_order_no="69" segment_no="9" tag_type="text">by reconstructing 3D human bodies from both live streams</text>
<text top="380" left="312" width="252" height="9" font="font11" id="p9_t71" reading_order_no="70" segment_no="9" tag_type="text">and in-the-wild videos of various scenes (§ 7.1). We quan-</text>
<text top="391" left="312" width="252" height="9" font="font11" id="p9_t72" reading_order_no="71" segment_no="9" tag_type="text">titatively compare the accuracy of our results with state-of-</text>
<text top="403" left="312" width="252" height="9" font="font11" id="p9_t73" reading_order_no="72" segment_no="9" tag_type="text">the-art 3D pose and/or mesh reconstruction methods (§ 7.2).</text>
<text top="415" left="312" width="252" height="9" font="font11" id="p9_t74" reading_order_no="73" segment_no="9" tag_type="text">Our method is also qualitatively compared against four</text>
<text top="426" left="312" width="252" height="9" font="font11" id="p9_t75" reading_order_no="74" segment_no="9" tag_type="text">most related state-of-the-art methods (§ 7.3). In § 7.4, we</text>
<text top="438" left="312" width="252" height="9" font="font11" id="p9_t76" reading_order_no="75" segment_no="9" tag_type="text">evaluate different part the key components of our system</text>
<text top="449" left="312" width="252" height="9" font="font11" id="p9_t77" reading_order_no="76" segment_no="9" tag_type="text">by dropping off each term at one time for both multi-task</text>
<text top="461" left="312" width="252" height="9" font="font11" id="p9_t78" reading_order_no="77" segment_no="9" tag_type="text">network and optimization procedure. Our results are best</text>
<text top="472" left="312" width="138" height="9" font="font11" id="p9_t79" reading_order_no="78" segment_no="9" tag_type="text">seen in the accompanying video.</text>
<text top="508" left="326" width="238" height="9" font="font18" id="p9_t80" reading_order_no="79" segment_no="11" tag_type="text">Computational time. Our system runs at 20 fps on a</text>
<text top="519" left="312" width="252" height="9" font="font11" id="p9_t81" reading_order_no="80" segment_no="11" tag_type="text">desktop computer for the current implementation. Table 1</text>
<text top="531" left="312" width="252" height="9" font="font11" id="p9_t82" reading_order_no="81" segment_no="11" tag_type="text">reports the detailed timing for each component in our</text>
<text top="542" left="312" width="252" height="9" font="font11" id="p9_t83" reading_order_no="82" segment_no="11" tag_type="text">processing pipeline. All execution time is collected on a<b>7</b></text>
<text top="554" left="312" width="252" height="9" font="font11" id="p9_t84" reading_order_no="83" segment_no="11" tag_type="text">computer with an Intel i7 CPU and a nvidia Geforce GTX<b>R</b></text>
<text top="565" left="312" width="252" height="9" font="font11" id="p9_t85" reading_order_no="84" segment_no="11" tag_type="text">2080Ti GPU. Apart from the shape reconstruction, which is<b>ESULTS AND</b></text>
<text top="577" left="312" width="252" height="9" font="font11" id="p9_t86" reading_order_no="85" segment_no="11" tag_type="text">done only once in the first frame, the total processing time<b>E</b></text>
<text top="588" left="312" width="121" height="9" font="font11" id="p9_t87" reading_order_no="86" segment_no="11" tag_type="text">for one cycle is under 50 ms.<b>VALUATIONS</b></text>
<text top="618" left="312" width="13" height="9" font="font39" id="p9_t88" reading_order_no="87" segment_no="13" tag_type="title">7.1</text>
<text top="618" left="335" width="193" height="9" font="font39" id="p9_t89" reading_order_no="88" segment_no="13" tag_type="title">Test on live streams and in-the-wild videos</text>
<text top="633" left="312" width="252" height="9" font="font11" id="p9_t90" reading_order_no="89" segment_no="14" tag_type="text">Our system reconstructs 3D human poses and full-body</text>
<text top="645" left="312" width="252" height="9" font="font11" id="p9_t91" reading_order_no="90" segment_no="14" tag_type="text">geometry models from single images in realtime, and the</text>
<text top="656" left="312" width="252" height="9" font="font11" id="p9_t92" reading_order_no="91" segment_no="14" tag_type="text">reconstructed 3D poses is retargeted to animate a character</text>
<text top="668" left="312" width="252" height="9" font="font11" id="p9_t93" reading_order_no="92" segment_no="14" tag_type="text">in realtime (See Fig. 10). Our technology has potentials</text>
<text top="679" left="312" width="252" height="9" font="font11" id="p9_t94" reading_order_no="93" segment_no="14" tag_type="text">in applications such as game character control, embodied</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p9_t95" reading_order_no="94" segment_no="14" tag_type="text">VR, sport motion analysis and reconstruction of community</text>
<text top="703" left="312" width="26" height="9" font="font11" id="p9_t96" reading_order_no="95" segment_no="14" tag_type="text">video.</text>
<text top="714" left="326" width="238" height="9" font="font11" id="p9_t97" reading_order_no="96" segment_no="16" tag_type="text">We also test our method on various in-the-wild videos,</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p9_t98" reading_order_no="97" segment_no="16" tag_type="text">showing its robustness to different actors with different</text>
<text top="737" left="312" width="252" height="9" font="font11" id="p9_t99" reading_order_no="98" segment_no="16" tag_type="text">body shapes and clothes, even under significant occlusion,<b>Computational time.</b></text>
</page>
<page number="10" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font53" size="8" family="URWPalladioL,Bold" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p10_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="556" width="8" height="6" font="font0" id="p10_t2" reading_order_no="1" segment_no="1" tag_type="text">10</text>
<text top="127" left="82" width="9" height="8" font="font10" id="p10_t3" reading_order_no="2" segment_no="3" tag_type="figure">(a)</text>
<text top="127" left="166" width="10" height="8" font="font10" id="p10_t4" reading_order_no="3" segment_no="3" tag_type="figure">(b)</text>
<text top="127" left="250" width="9" height="8" font="font10" id="p10_t5" reading_order_no="4" segment_no="3" tag_type="figure">(c)</text>
<text top="149" left="48" width="252" height="9" font="font11" id="p10_t6" reading_order_no="5" segment_no="4" tag_type="text">Fig. 9: On the left of each subfigure is the input view from</text>
<text top="161" left="48" width="252" height="9" font="font11" id="p10_t7" reading_order_no="6" segment_no="4" tag_type="text">which the 3D model is reconstructed. On the right, the</text>
<text top="172" left="48" width="215" height="9" font="font11" id="p10_t8" reading_order_no="7" segment_no="4" tag_type="text">model is rendered and overlaid in a different view.</text>
<text top="333" left="48" width="252" height="9" font="font11" id="p10_t9" reading_order_no="8" segment_no="8" tag_type="text">Fig. 10: The reconstructed 3D poses from our system can be</text>
<text top="344" left="48" width="189" height="9" font="font11" id="p10_t10" reading_order_no="9" segment_no="8" tag_type="text">retargeted to animate a character in realtime.</text>
<text top="380" left="48" width="252" height="9" font="font11" id="p10_t11" reading_order_no="10" segment_no="9" tag_type="text">lighting and background changes. Fig. 11 shows some ex-</text>
<text top="392" left="48" width="252" height="9" font="font11" id="p10_t12" reading_order_no="11" segment_no="9" tag_type="text">cerpted frames. Besides, we also present other-view overlay</text>
<text top="403" left="48" width="252" height="9" font="font11" id="p10_t13" reading_order_no="12" segment_no="9" tag_type="text">results in Fig. 9 by using our multi-view test dataset, which</text>
<text top="415" left="48" width="252" height="9" font="font11" id="p10_t14" reading_order_no="13" segment_no="9" tag_type="text">reflects the accuracy of human reconstruction achieved by</text>
<text top="426" left="48" width="252" height="9" font="font11" id="p10_t15" reading_order_no="14" segment_no="9" tag_type="text">our method. Please refer to the accompanying video for</text>
<text top="438" left="48" width="79" height="9" font="font11" id="p10_t16" reading_order_no="15" segment_no="9" tag_type="text">more vivid results.</text>
<text top="475" left="48" width="13" height="9" font="font39" id="p10_t17" reading_order_no="16" segment_no="12" tag_type="title">7.2<b>7.2</b></text>
<text top="475" left="71" width="105" height="9" font="font39" id="p10_t18" reading_order_no="17" segment_no="12" tag_type="title">Quantitative Evaluation<b>Quantitative Evaluation</b></text>
<text top="494" left="48" width="252" height="9" font="font11" id="p10_t19" reading_order_no="18" segment_no="13" tag_type="text">We evaluate the performance of our method on two pop-</text>
<text top="505" left="48" width="252" height="9" font="font11" id="p10_t20" reading_order_no="19" segment_no="13" tag_type="text">ular test benchmarks: 3DPW [53] (outdoor scenes) and</text>
<text top="517" left="48" width="252" height="9" font="font11" id="p10_t21" reading_order_no="20" segment_no="13" tag_type="text">Human3.6M [54] (indoor scenes). Following the standard</text>
<text top="528" left="48" width="252" height="9" font="font11" id="p10_t22" reading_order_no="21" segment_no="13" tag_type="text">protocol for 3D pose estimation [22] in Human3.6M, we</text>
<text top="540" left="48" width="252" height="9" font="font11" id="p10_t23" reading_order_no="22" segment_no="13" tag_type="text">use 5 subjects (S1, S5, S6, S7 and S8) for training, and the</text>
<text top="552" left="48" width="252" height="9" font="font11" id="p10_t24" reading_order_no="23" segment_no="13" tag_type="text">rest 2 subjects (S9 and S11) for testing. As for Human3.6M,</text>
<text top="563" left="48" width="252" height="9" font="font11" id="p10_t25" reading_order_no="24" segment_no="13" tag_type="text">we get ground truth parameters for training images using</text>
<text top="575" left="48" width="252" height="9" font="font11" id="p10_t26" reading_order_no="25" segment_no="13" tag_type="text">MoSh [56] from the raw 3D Mocap markers like [30]. As</text>
<text top="586" left="48" width="252" height="9" font="font11" id="p10_t27" reading_order_no="26" segment_no="13" tag_type="text">for 3DPW, we only use its testing dataset for evaluation as</text>
<text top="598" left="48" width="252" height="9" font="font11" id="p10_t28" reading_order_no="27" segment_no="13" tag_type="text">previous works [2]. Note that Human3.6M and 3DPW have</text>
<text top="609" left="48" width="252" height="9" font="font11" id="p10_t29" reading_order_no="28" segment_no="13" tag_type="text">different skeleton configurations from ours, we therefore</text>
<text top="621" left="48" width="252" height="9" font="font11" id="p10_t30" reading_order_no="29" segment_no="13" tag_type="text">learn a linear regressor for a mapping which maps our mesh</text>
<text top="632" left="48" width="252" height="9" font="font11" id="p10_t31" reading_order_no="30" segment_no="13" tag_type="text">vertices to 17 joints defined in Human3.6M, as did in [30].</text>
<text top="644" left="48" width="252" height="9" font="font11" id="p10_t32" reading_order_no="31" segment_no="13" tag_type="text">For evaluation, we adopt averaged skeleton dimensions</text>
<text top="655" left="48" width="252" height="9" font="font11" id="p10_t33" reading_order_no="32" segment_no="13" tag_type="text">computed from the training set to rescale our reconstruction</text>
<text top="667" left="48" width="93" height="9" font="font11" id="p10_t34" reading_order_no="33" segment_no="13" tag_type="text">human, as did in [22].</text>
<text top="680" left="62" width="238" height="9" font="font11" id="p10_t35" reading_order_no="34" segment_no="16" tag_type="text">The results are shown in Table 2. Our single image</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p10_t36" reading_order_no="35" segment_no="16" tag_type="text">based method is even competitive to the video-based VIBE</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p10_t37" reading_order_no="36" segment_no="16" tag_type="text">[5] on Human3.6M and 3DPW. The results also show that</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p10_t38" reading_order_no="37" segment_no="16" tag_type="text">our newly collected data improves the performance further,</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p10_t39" reading_order_no="38" segment_no="16" tag_type="text">especially on 3DPW (wild), with improved wild generaliza-</text>
<text top="737" left="48" width="19" height="9" font="font11" id="p10_t40" reading_order_no="39" segment_no="16" tag_type="text">tion.</text>
<text top="41" left="361" width="28" height="8" font="font10" id="p10_t41" reading_order_no="40" segment_no="2" tag_type="table">Method</text>
<text top="41" left="438" width="124" height="8" font="font10" id="p10_t42" reading_order_no="41" segment_no="2" tag_type="table">H36M-P1 ↓ H36M-PA ↓ 3DPW-PA ↓</text>
<text top="51" left="349" width="52" height="8" font="font10" id="p10_t43" reading_order_no="42" segment_no="2" tag_type="table">Mehta et al. [3]</text>
<text top="51" left="451" width="14" height="8" font="font10" id="p10_t44" reading_order_no="43" segment_no="2" tag_type="table">80.5</text>
<text top="51" left="499" width="3" height="8" font="font10" id="p10_t45" reading_order_no="44" segment_no="2" tag_type="table">-</text>
<text top="51" left="541" width="3" height="8" font="font10" id="p10_t46" reading_order_no="45" segment_no="2" tag_type="table">-</text>
<text top="60" left="342" width="65" height="8" font="font10" id="p10_t47" reading_order_no="46" segment_no="2" tag_type="table">Pavlakos et al. [13]</text>
<text top="60" left="451" width="14" height="8" font="font10" id="p10_t48" reading_order_no="47" segment_no="2" tag_type="table">56.2</text>
<text top="60" left="491" width="18" height="8" font="font10" id="p10_t49" reading_order_no="48" segment_no="2" tag_type="table">41.80</text>
<text top="60" left="541" width="3" height="8" font="font10" id="p10_t50" reading_order_no="49" segment_no="2" tag_type="table">-</text>
<text top="69" left="351" width="47" height="8" font="font10" id="p10_t51" reading_order_no="50" segment_no="2" tag_type="table">Sun et al. [23]</text>
<text top="69" left="451" width="14" height="8" font="font10" id="p10_t52" reading_order_no="51" segment_no="2" tag_type="table">49.6</text>
<text top="69" left="491" width="18" height="8" font="font10" id="p10_t53" reading_order_no="52" segment_no="2" tag_type="table">40.60</text>
<text top="69" left="541" width="3" height="8" font="font10" id="p10_t54" reading_order_no="53" segment_no="2" tag_type="table">-</text>
<text top="78" left="348" width="53" height="8" font="font10" id="p10_t55" reading_order_no="54" segment_no="2" tag_type="table">Zhou et al. [57]</text>
<text top="78" left="451" width="14" height="8" font="font53" id="p10_t56" reading_order_no="55" segment_no="2" tag_type="table">39.9</text>
<text top="78" left="493" width="14" height="8" font="font53" id="p10_t57" reading_order_no="56" segment_no="2" tag_type="table">32.1</text>
<text top="78" left="541" width="3" height="8" font="font10" id="p10_t58" reading_order_no="57" segment_no="2" tag_type="table">-</text>
<text top="87" left="351" width="47" height="8" font="font10" id="p10_t59" reading_order_no="58" segment_no="2" tag_type="table">Bogo et al. [4]</text>
<text top="87" left="451" width="14" height="8" font="font10" id="p10_t60" reading_order_no="59" segment_no="2" tag_type="table">82.3</text>
<text top="87" left="499" width="3" height="8" font="font10" id="p10_t61" reading_order_no="60" segment_no="2" tag_type="table">-</text>
<text top="87" left="541" width="3" height="8" font="font10" id="p10_t62" reading_order_no="61" segment_no="2" tag_type="table">-</text>
<text top="96" left="340" width="70" height="8" font="font10" id="p10_t63" reading_order_no="62" segment_no="2" tag_type="table">Kanazawa et al. [30]</text>
<text top="96" left="449" width="18" height="8" font="font10" id="p10_t64" reading_order_no="63" segment_no="2" tag_type="table">87.97</text>
<text top="96" left="493" width="14" height="8" font="font10" id="p10_t65" reading_order_no="64" segment_no="2" tag_type="table">56.8</text>
<text top="96" left="535" width="14" height="8" font="font10" id="p10_t66" reading_order_no="65" segment_no="2" tag_type="table">76.7</text>
<text top="105" left="348" width="54" height="8" font="font10" id="p10_t67" reading_order_no="66" segment_no="2" tag_type="table">Xiang et al. [34]</text>
<text top="105" left="451" width="14" height="8" font="font10" id="p10_t68" reading_order_no="67" segment_no="2" tag_type="table">58.3</text>
<text top="105" left="499" width="3" height="8" font="font10" id="p10_t69" reading_order_no="68" segment_no="2" tag_type="table">-<b>39.9</b></text>
<text top="105" left="541" width="3" height="8" font="font10" id="p10_t70" reading_order_no="69" segment_no="2" tag_type="table">-<b>32.1</b></text>
<text top="114" left="339" width="71" height="8" font="font10" id="p10_t71" reading_order_no="70" segment_no="2" tag_type="table">kolotouros et al. [31]</text>
<text top="114" left="451" width="14" height="8" font="font10" id="p10_t72" reading_order_no="71" segment_no="2" tag_type="table">74.7</text>
<text top="114" left="493" width="14" height="8" font="font10" id="p10_t73" reading_order_no="72" segment_no="2" tag_type="table">50.1</text>
<text top="114" left="535" width="14" height="8" font="font10" id="p10_t74" reading_order_no="73" segment_no="2" tag_type="table">70.2</text>
<text top="123" left="341" width="67" height="8" font="font10" id="p10_t75" reading_order_no="74" segment_no="2" tag_type="table">kolotouros et al. [2]</text>
<text top="123" left="457" width="3" height="8" font="font10" id="p10_t76" reading_order_no="75" segment_no="2" tag_type="table">-</text>
<text top="123" left="493" width="14" height="8" font="font10" id="p10_t77" reading_order_no="76" segment_no="2" tag_type="table">44.3</text>
<text top="123" left="535" width="14" height="8" font="font10" id="p10_t78" reading_order_no="77" segment_no="2" tag_type="table">59.2</text>
<text top="132" left="352" width="45" height="8" font="font10" id="p10_t79" reading_order_no="78" segment_no="2" tag_type="table">Joo et al. [58]</text>
<text top="132" left="457" width="3" height="8" font="font10" id="p10_t80" reading_order_no="79" segment_no="2" tag_type="table">-</text>
<text top="132" left="493" width="14" height="8" font="font10" id="p10_t81" reading_order_no="80" segment_no="2" tag_type="table">45.2</text>
<text top="132" left="535" width="14" height="8" font="font10" id="p10_t82" reading_order_no="81" segment_no="2" tag_type="table">55.7</text>
<text top="141" left="345" width="59" height="8" font="font10" id="p10_t83" reading_order_no="82" segment_no="2" tag_type="table">Kocabas et al. [5]</text>
<text top="141" left="451" width="14" height="8" font="font10" id="p10_t84" reading_order_no="83" segment_no="2" tag_type="table">65.6</text>
<text top="141" left="493" width="14" height="8" font="font10" id="p10_t85" reading_order_no="84" segment_no="2" tag_type="table">41.4</text>
<text top="141" left="535" width="14" height="8" font="font53" id="p10_t86" reading_order_no="85" segment_no="2" tag_type="table">51.9</text>
<text top="150" left="326" width="97" height="8" font="font10" id="p10_t87" reading_order_no="86" segment_no="2" tag_type="table">Ours (wild image + H36M)</text>
<text top="150" left="451" width="14" height="8" font="font10" id="p10_t88" reading_order_no="87" segment_no="2" tag_type="table">66.3</text>
<text top="150" left="493" width="14" height="8" font="font10" id="p10_t89" reading_order_no="88" segment_no="2" tag_type="table">47.2</text>
<text top="150" left="535" width="14" height="8" font="font10" id="p10_t90" reading_order_no="89" segment_no="2" tag_type="table">64.1</text>
<text top="159" left="314" width="122" height="8" font="font10" id="p10_t91" reading_order_no="90" segment_no="2" tag_type="table">Ours (wild image + H36M + ours)</text>
<text top="159" left="451" width="14" height="8" font="font10" id="p10_t92" reading_order_no="91" segment_no="2" tag_type="table">63.7</text>
<text top="159" left="493" width="14" height="8" font="font10" id="p10_t93" reading_order_no="92" segment_no="2" tag_type="table">41.8</text>
<text top="159" left="535" width="14" height="8" font="font10" id="p10_t94" reading_order_no="93" segment_no="2" tag_type="table">53.2</text>
<text top="178" left="312" width="252" height="9" font="font11" id="p10_t95" reading_order_no="94" segment_no="5" tag_type="text">TABLE 2: Quantitative Evaluation on Human3.6M (indoor)</text>
<text top="190" left="312" width="252" height="9" font="font11" id="p10_t96" reading_order_no="95" segment_no="5" tag_type="text">and 3DPW (outdoor). The number of H36M-P1 is the Mean</text>
<text top="201" left="312" width="252" height="9" font="font11" id="p10_t97" reading_order_no="96" segment_no="5" tag_type="text">Per Joint Position Error (MPJPE) in millimeter on Hu-</text>
<text top="213" left="312" width="252" height="9" font="font11" id="p10_t98" reading_order_no="97" segment_no="5" tag_type="text">man3.6M, while the number of H36m-PA is the MPJPE on</text>
<text top="224" left="312" width="252" height="9" font="font11" id="p10_t99" reading_order_no="98" segment_no="5" tag_type="text">Human3.6M after procrustes alignment (PA). And 3DPW-</text>
<text top="236" left="312" width="252" height="9" font="font11" id="p10_t100" reading_order_no="99" segment_no="5" tag_type="text">PA is the MPJPE on 3DPW after PA. Our method achieves</text>
<text top="247" left="312" width="210" height="9" font="font11" id="p10_t101" reading_order_no="100" segment_no="5" tag_type="text">competitive performance against previous works.</text>
<text top="271" left="367" width="31" height="8" font="font10" id="p10_t102" reading_order_no="101" segment_no="6" tag_type="table">Methods</text>
<text top="270" left="428" width="43" height="9" font="font10" id="p10_t103" reading_order_no="102" segment_no="6" tag_type="table">PCKh@0.5 ↑</text>
<text top="270" left="483" width="43" height="9" font="font10" id="p10_t104" reading_order_no="103" segment_no="6" tag_type="table">MPJPE-P1 ↓</text>
<text top="280" left="378" width="10" height="8" font="font10" id="p10_t105" reading_order_no="104" segment_no="6" tag_type="table">2D</text>
<text top="280" left="443" width="14" height="8" font="font10" id="p10_t106" reading_order_no="105" segment_no="6" tag_type="table">96.3</text>
<text top="280" left="503" width="3" height="8" font="font10" id="p10_t107" reading_order_no="106" segment_no="6" tag_type="table">-</text>
<text top="289" left="364" width="38" height="8" font="font10" id="p10_t108" reading_order_no="107" segment_no="6" tag_type="table">2D + IUVs</text>
<text top="289" left="443" width="14" height="8" font="font10" id="p10_t109" reading_order_no="108" segment_no="6" tag_type="table">97.5</text>
<text top="289" left="503" width="3" height="8" font="font10" id="p10_t110" reading_order_no="109" segment_no="6" tag_type="table">-</text>
<text top="298" left="364" width="38" height="8" font="font10" id="p10_t111" reading_order_no="110" segment_no="6" tag_type="table">2D + POFs</text>
<text top="298" left="443" width="14" height="8" font="font10" id="p10_t112" reading_order_no="111" segment_no="6" tag_type="table">96.2</text>
<text top="298" left="498" width="14" height="8" font="font10" id="p10_t113" reading_order_no="112" segment_no="6" tag_type="table">51.8<b>51.9</b></text>
<text top="307" left="350" width="66" height="8" font="font10" id="p10_t114" reading_order_no="113" segment_no="6" tag_type="table">2D + IUVs + POFs</text>
<text top="307" left="443" width="14" height="8" font="font53" id="p10_t115" reading_order_no="114" segment_no="6" tag_type="table">97.8</text>
<text top="307" left="498" width="14" height="8" font="font53" id="p10_t116" reading_order_no="115" segment_no="6" tag_type="table">49.8</text>
<text top="326" left="312" width="252" height="9" font="font11" id="p10_t117" reading_order_no="116" segment_no="7" tag_type="text">TABLE 3: Ablation experiments on the effect of multi-task</text>
<text top="337" left="312" width="252" height="9" font="font11" id="p10_t118" reading_order_no="117" segment_no="7" tag_type="text">learning. In the experiment, we modify Fig. 3 to set different</text>
<text top="349" left="312" width="252" height="9" font="font11" id="p10_t119" reading_order_no="118" segment_no="7" tag_type="text">task combinations. Our results shows that IUVs is beneficial</text>
<text top="360" left="312" width="226" height="9" font="font11" id="p10_t120" reading_order_no="119" segment_no="7" tag_type="text">to 2D joint detection and part orientation predictions.</text>
<text top="395" left="312" width="13" height="9" font="font39" id="p10_t121" reading_order_no="120" segment_no="10" tag_type="title">7.3</text>
<text top="395" left="335" width="194" height="9" font="font39" id="p10_t122" reading_order_no="121" segment_no="10" tag_type="title">Comparisons with state-of-the-art methods</text>
<text top="413" left="312" width="252" height="9" font="font11" id="p10_t123" reading_order_no="122" segment_no="11" tag_type="text">To show the efficiency of our method, we compare against</text>
<text top="424" left="312" width="252" height="9" font="font11" id="p10_t124" reading_order_no="123" segment_no="11" tag_type="text">two state-of-the-art regression-based methods, one is single</text>
<text top="436" left="312" width="252" height="9" font="font11" id="p10_t125" reading_order_no="124" segment_no="11" tag_type="text">frame method, SPIN [2], the other is video-based method,</text>
<text top="447" left="312" width="252" height="9" font="font11" id="p10_t126" reading_order_no="125" segment_no="11" tag_type="text">VIBE [5]. Fig. 12 shows the result of a side-by-side compar-</text>
<text top="459" left="312" width="252" height="9" font="font11" id="p10_t127" reading_order_no="126" segment_no="11" tag_type="text">ison. It is obvious that our method achieves better image-</text>
<text top="470" left="312" width="252" height="9" font="font11" id="p10_t128" reading_order_no="127" segment_no="11" tag_type="text">model alignments than SPIN and VIBE. Regression-based</text>
<text top="482" left="312" width="252" height="9" font="font11" id="p10_t129" reading_order_no="128" segment_no="11" tag_type="text">methods usually achieves global image-model alignments</text>
<text top="493" left="312" width="252" height="9" font="font11" id="p10_t130" reading_order_no="129" segment_no="11" tag_type="text">quickly, but at the cost of low quality. This type of nonlinear</text>
<text top="505" left="312" width="252" height="9" font="font11" id="p10_t131" reading_order_no="130" segment_no="11" tag_type="text">prediction is also uneasy to control, due to the mutual effect</text>
<text top="517" left="312" width="252" height="9" font="font11" id="p10_t132" reading_order_no="131" segment_no="11" tag_type="text">between human pose and shape. We decouple them, and</text>
<text top="528" left="312" width="252" height="9" font="font11" id="p10_t133" reading_order_no="132" segment_no="11" tag_type="text">since 2D joint positions and dense image-to-surface corre-</text>
<text top="540" left="312" width="252" height="9" font="font11" id="p10_t134" reading_order_no="133" segment_no="11" tag_type="text">spondence (IUV map) offer better image-model alignments,</text>
<text top="551" left="312" width="252" height="9" font="font11" id="p10_t135" reading_order_no="134" segment_no="11" tag_type="text">and 3D part orientation helps to avoid depth ambiguity, our</text>
<text top="563" left="312" width="252" height="9" font="font11" id="p10_t136" reading_order_no="135" segment_no="11" tag_type="text">method produces more accurate body reconstruction than</text>
<text top="574" left="312" width="67" height="9" font="font11" id="p10_t137" reading_order_no="136" segment_no="11" tag_type="text">SPIN and VIBE.</text>
<text top="587" left="326" width="238" height="9" font="font11" id="p10_t138" reading_order_no="137" segment_no="14" tag_type="text">Furthermore, SPIN does not guarantee temporal stability</text>
<text top="598" left="312" width="252" height="9" font="font11" id="p10_t139" reading_order_no="138" segment_no="14" tag_type="text">because it regresses different body shapes from different</text>
<text top="610" left="312" width="252" height="9" font="font11" id="p10_t140" reading_order_no="139" segment_no="14" tag_type="text">images in a sequence, while VIBE fixes it, but it is not in</text>
<text top="621" left="312" width="252" height="9" font="font11" id="p10_t141" reading_order_no="140" segment_no="14" tag_type="text">realtime. Please refer to Fig. 12 and the accompanying video</text>
<text top="633" left="312" width="113" height="9" font="font11" id="p10_t142" reading_order_no="141" segment_no="14" tag_type="text">for the comparison results.</text>
<text top="645" left="326" width="238" height="9" font="font11" id="p10_t143" reading_order_no="142" segment_no="15" tag_type="text">Our method is also compared against a recent realtime</text>
<text top="657" left="312" width="252" height="9" font="font11" id="p10_t144" reading_order_no="143" segment_no="15" tag_type="text">system, VNect [3], though it captures poses only. We com-<b>97.8</b></text>
<text top="668" left="312" width="252" height="9" font="font11" id="p10_t145" reading_order_no="144" segment_no="15" tag_type="text">pare not only the raw network outputs, but also the final<b>49.8</b></text>
<text top="680" left="312" width="252" height="9" font="font11" id="p10_t146" reading_order_no="145" segment_no="15" tag_type="text">fitting results (see Fig. 13). It is obvious that our method</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p10_t147" reading_order_no="146" segment_no="15" tag_type="text">achieves better pose reconstruction than VNect. Two reasons</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p10_t148" reading_order_no="147" segment_no="15" tag_type="text">account for this. Firstly, our multi-task network produces</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p10_t149" reading_order_no="148" segment_no="15" tag_type="text">more accurate 3D joint positions, as shown in Fig. 13(c).</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p10_t150" reading_order_no="149" segment_no="15" tag_type="text">Secondly, VNect initializes bone lengths for forearm and<b>7.3</b></text>
<text top="737" left="312" width="252" height="9" font="font11" id="p10_t151" reading_order_no="150" segment_no="15" tag_type="text">upper arm to an improper ratio, as seen in Fig. 13(b), while<b>Comparisons with state-of-the-art methods</b></text>
</page>
<page number="11" position="absolute" top="0" left="0" height="792" width="612">
<text top="29" left="48" width="334" height="6" font="font0" id="p11_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="556" width="8" height="6" font="font0" id="p11_t2" reading_order_no="1" segment_no="1" tag_type="text">11</text>
<text top="283" left="48" width="516" height="9" font="font11" id="p11_t3" reading_order_no="2" segment_no="2" tag_type="text">Fig. 11: Our reconstructed 3D bodies with different shapes and clothes, under occlusion, lighting and background</text>
<text top="295" left="48" width="44" height="9" font="font11" id="p11_t4" reading_order_no="3" segment_no="2" tag_type="text">variations.</text>
<text top="324" left="96" width="31" height="8" font="font10" id="p11_t5" reading_order_no="4" segment_no="3" tag_type="table">Methods</text>
<text top="323" left="176" width="121" height="9" font="font10" id="p11_t6" reading_order_no="5" segment_no="3" tag_type="table">H36M-P1 ↓ H36M-PA ↓ 3DPW-PA ↓</text>
<text top="333" left="107" width="10" height="8" font="font10" id="p11_t7" reading_order_no="6" segment_no="3" tag_type="table">2D</text>
<text top="333" left="186" width="18" height="8" font="font10" id="p11_t8" reading_order_no="7" segment_no="3" tag_type="table">121.3</text>
<text top="333" left="227" width="18" height="8" font="font10" id="p11_t9" reading_order_no="8" segment_no="3" tag_type="table">103.4</text>
<text top="333" left="269" width="18" height="8" font="font10" id="p11_t10" reading_order_no="9" segment_no="3" tag_type="table">116.8</text>
<text top="342" left="93" width="38" height="8" font="font10" id="p11_t11" reading_order_no="10" segment_no="3" tag_type="table">2D + mask</text>
<text top="342" left="186" width="18" height="8" font="font10" id="p11_t12" reading_order_no="11" segment_no="3" tag_type="table">118.3</text>
<text top="342" left="227" width="18" height="8" font="font10" id="p11_t13" reading_order_no="12" segment_no="3" tag_type="table">105.1</text>
<text top="342" left="269" width="18" height="8" font="font10" id="p11_t14" reading_order_no="13" segment_no="3" tag_type="table">110.9</text>
<text top="351" left="83" width="58" height="8" font="font10" id="p11_t15" reading_order_no="14" segment_no="3" tag_type="table">2D + mask + 3D</text>
<text top="351" left="188" width="14" height="8" font="font10" id="p11_t16" reading_order_no="15" segment_no="3" tag_type="table">64.7</text>
<text top="351" left="229" width="14" height="8" font="font10" id="p11_t17" reading_order_no="16" segment_no="3" tag_type="table">43.4</text>
<text top="351" left="271" width="14" height="8" font="font10" id="p11_t18" reading_order_no="17" segment_no="3" tag_type="table">51.8</text>
<text top="360" left="71" width="82" height="8" font="font10" id="p11_t19" reading_order_no="18" segment_no="3" tag_type="table">2D + mask + 3D + IUV</text>
<text top="360" left="188" width="14" height="8" font="font53" id="p11_t20" reading_order_no="19" segment_no="3" tag_type="table">63.7</text>
<text top="360" left="229" width="14" height="8" font="font53" id="p11_t21" reading_order_no="20" segment_no="3" tag_type="table">41.8</text>
<text top="360" left="271" width="14" height="8" font="font53" id="p11_t22" reading_order_no="21" segment_no="3" tag_type="table">51.1</text>
<text top="369" left="50" width="123" height="8" font="font10" id="p11_t23" reading_order_no="22" segment_no="3" tag_type="table">2D + mask + 3D + IUV + temporal</text>
<text top="369" left="188" width="14" height="8" font="font10" id="p11_t24" reading_order_no="23" segment_no="3" tag_type="table">65.9</text>
<text top="369" left="229" width="14" height="8" font="font10" id="p11_t25" reading_order_no="24" segment_no="3" tag_type="table">42.4<b>63.7</b></text>
<text top="369" left="271" width="14" height="8" font="font10" id="p11_t26" reading_order_no="25" segment_no="3" tag_type="table">53.5<b>41.8</b></text>
<text top="388" left="48" width="252" height="9" font="font11" id="p11_t27" reading_order_no="26" segment_no="4" tag_type="text">TABLE 4: Ablation study on the importance of each energy<b>51.1</b></text>
<text top="399" left="48" width="252" height="9" font="font11" id="p11_t28" reading_order_no="27" segment_no="4" tag_type="text">term in optimization. We report the MPJPE/MPJPE-PA on</text>
<text top="411" left="48" width="252" height="9" font="font11" id="p11_t29" reading_order_no="28" segment_no="4" tag_type="text">Human3.6M (indoor) and 3DPW (outdoor) on 5 experi-</text>
<text top="422" left="48" width="252" height="9" font="font11" id="p11_t30" reading_order_no="29" segment_no="4" tag_type="text">ments. All five experiments share the same network outputs</text>
<text top="434" left="48" width="194" height="9" font="font11" id="p11_t31" reading_order_no="30" segment_no="4" tag_type="text">but differ in the energy terms in optimization.</text>
<text top="471" left="48" width="252" height="9" font="font11" id="p11_t32" reading_order_no="31" segment_no="6" tag_type="text">our initialization matches with person in image very well.</text>
<text top="482" left="48" width="252" height="9" font="font11" id="p11_t33" reading_order_no="32" segment_no="6" tag_type="text">VNect initializes skeleton by averaging 3D joint positions</text>
<text top="494" left="48" width="252" height="9" font="font11" id="p11_t34" reading_order_no="33" segment_no="6" tag_type="text">from the CNN output at the beginning, which is thus very</text>
<text top="505" left="48" width="252" height="9" font="font11" id="p11_t35" reading_order_no="34" segment_no="6" tag_type="text">sensitive to single CNN outputs (3D joint positions). We</text>
<text top="517" left="48" width="252" height="9" font="font11" id="p11_t36" reading_order_no="35" segment_no="6" tag_type="text">instead utilize more image features, including 2D joints,</text>
<text top="528" left="48" width="252" height="9" font="font11" id="p11_t37" reading_order_no="36" segment_no="6" tag_type="text">3D part orientation and IUV maps, to reconstruct human</text>
<text top="540" left="48" width="252" height="9" font="font11" id="p11_t38" reading_order_no="37" segment_no="6" tag_type="text">bodies more accurately and robustly. Please refer to the</text>
<text top="551" left="48" width="212" height="9" font="font11" id="p11_t39" reading_order_no="38" segment_no="6" tag_type="text">accompanying video for more comparison results.</text>
<text top="564" left="62" width="238" height="9" font="font11" id="p11_t40" reading_order_no="39" segment_no="7" tag_type="text">SMPLify [4] is a somewhat hybrid method: it fits the</text>
<text top="576" left="48" width="252" height="9" font="font11" id="p11_t41" reading_order_no="40" segment_no="7" tag_type="text">SMPL model by optimizing regressed 2D joints without user</text>
<text top="587" left="48" width="252" height="9" font="font11" id="p11_t42" reading_order_no="41" segment_no="7" tag_type="text">intervention. Fig. 14 shows the results given by SMPLify</text>
<text top="599" left="48" width="252" height="9" font="font11" id="p11_t43" reading_order_no="42" segment_no="7" tag_type="text">and our method. At least three issues about SMPLify can be</text>
<text top="610" left="48" width="252" height="9" font="font11" id="p11_t44" reading_order_no="43" segment_no="7" tag_type="text">interpreted from the figure. First, SMPLify is more vulner-</text>
<text top="622" left="48" width="252" height="9" font="font11" id="p11_t45" reading_order_no="44" segment_no="7" tag_type="text">able to depth ambiguity than our method, as can be seen</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p11_t46" reading_order_no="45" segment_no="7" tag_type="text">from images in the first row. This is because SMPLify relies</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p11_t47" reading_order_no="46" segment_no="7" tag_type="text">on 2D joint reprojection alone for model fitting, and this is</text>
<text top="657" left="48" width="252" height="9" font="font11" id="p11_t48" reading_order_no="47" segment_no="7" tag_type="text">insufficient to robustly establish a 3D pose. Second, there</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p11_t49" reading_order_no="48" segment_no="7" tag_type="text">is no constraints for foot orientation in SMPLify. Last, it is</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p11_t50" reading_order_no="49" segment_no="7" tag_type="text">easy for SMPLify to fall into a local minima, as shown in</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p11_t51" reading_order_no="50" segment_no="7" tag_type="text">second row. Our method has hardly convergence problem.</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p11_t52" reading_order_no="51" segment_no="7" tag_type="text">For a single image, the initial shape from the average of our</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p11_t53" reading_order_no="52" segment_no="7" tag_type="text">human model. We use it and predicted 2D joint positions</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p11_t54" reading_order_no="53" segment_no="7" tag_type="text">and 3D bone directions to roughly estimate the root position</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p11_t55" reading_order_no="54" segment_no="7" tag_type="text">and joint angles as initial pose. The optimization problem is</text>
<text top="433" left="362" width="151" height="8" font="font10" id="p11_t56" reading_order_no="55" segment_no="5" tag_type="figure">(a) Comparison our method with SPIN [2]</text>
<text top="552" left="362" width="152" height="8" font="font10" id="p11_t57" reading_order_no="56" segment_no="5" tag_type="figure">(b) Comparison our method with VIBE [5]</text>
<text top="575" left="312" width="252" height="9" font="font11" id="p11_t58" reading_order_no="57" segment_no="8" tag_type="text">Fig. 12: From left to right: the input, SPIN/VIBE and our re-</text>
<text top="587" left="312" width="252" height="9" font="font11" id="p11_t59" reading_order_no="58" segment_no="8" tag_type="text">sults. Our method produces better image-model alignments</text>
<text top="598" left="312" width="89" height="9" font="font11" id="p11_t60" reading_order_no="59" segment_no="8" tag_type="text">than SPIN and VIBE.</text>
<text top="629" left="312" width="73" height="9" font="font11" id="p11_t61" reading_order_no="60" segment_no="9" tag_type="text">over-constrained.</text>
<text top="654" left="312" width="13" height="9" font="font39" id="p11_t62" reading_order_no="61" segment_no="10" tag_type="title">7.4</text>
<text top="654" left="335" width="67" height="9" font="font39" id="p11_t63" reading_order_no="62" segment_no="10" tag_type="title">Ablation Study</text>
<text top="668" left="312" width="252" height="9" font="font11" id="p11_t64" reading_order_no="63" segment_no="11" tag_type="text">We have designed a realtime multi-task network that re-</text>
<text top="680" left="312" width="252" height="9" font="font11" id="p11_t65" reading_order_no="64" segment_no="11" tag_type="text">gresses more features than any other deep learning based</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p11_t66" reading_order_no="65" segment_no="11" tag_type="text">methods. We believe that more features offer more visual</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p11_t67" reading_order_no="66" segment_no="11" tag_type="text">cues that facilitate pose and shape reconstruction. In this<b>7.4</b></text>
<text top="714" left="312" width="252" height="9" font="font11" id="p11_t68" reading_order_no="67" segment_no="11" tag_type="text">section we justify this belief with experiments, evaluating<b>Ablation Study</b></text>
<text top="726" left="312" width="252" height="9" font="font11" id="p11_t69" reading_order_no="68" segment_no="11" tag_type="text">the role of each regressed features in both the CNN regres-</text>
<text top="737" left="312" width="176" height="9" font="font11" id="p11_t70" reading_order_no="69" segment_no="11" tag_type="text">sion and the body reconstruction process.</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font54" size="7" family="Calibri" color="#000000"/>
<text top="29" left="48" width="334" height="6" font="font0" id="p12_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="556" width="8" height="6" font="font0" id="p12_t2" reading_order_no="1" segment_no="1" tag_type="text">12</text>
<text top="128" left="67" width="9" height="8" font="font10" id="p12_t3" reading_order_no="2" segment_no="4" tag_type="figure">(a)</text>
<text top="128" left="140" width="10" height="8" font="font10" id="p12_t4" reading_order_no="3" segment_no="4" tag_type="figure">(b)</text>
<text top="128" left="240" width="9" height="8" font="font10" id="p12_t5" reading_order_no="4" segment_no="4" tag_type="figure">(c)</text>
<text top="151" left="48" width="252" height="9" font="font11" id="p12_t6" reading_order_no="5" segment_no="5" tag_type="text">Fig. 13: Comparison of our results with VNect. (a) the input</text>
<text top="162" left="48" width="252" height="9" font="font11" id="p12_t7" reading_order_no="6" segment_no="5" tag_type="text">image; (b) the final results of VNect (left) and our method</text>
<text top="174" left="48" width="252" height="9" font="font11" id="p12_t8" reading_order_no="7" segment_no="5" tag_type="text">(right); (c) the raw network predictions of VNect (left) and</text>
<text top="185" left="48" width="82" height="9" font="font11" id="p12_t9" reading_order_no="8" segment_no="5" tag_type="text">our method (right).</text>
<text top="471" left="48" width="252" height="9" font="font11" id="p12_t10" reading_order_no="9" segment_no="10" tag_type="text">Fig. 14: Comparison of our method (middle) with SMPLify</text>
<text top="482" left="48" width="29" height="9" font="font11" id="p12_t11" reading_order_no="10" segment_no="10" tag_type="text">(right).</text>
<text top="518" left="62" width="238" height="9" font="font18" id="p12_t12" reading_order_no="11" segment_no="11" tag_type="text">Quantitative analysis for multi-task network To eval-<b>Quantitative analysis for multi-task network</b></text>
<text top="530" left="48" width="252" height="9" font="font11" id="p12_t13" reading_order_no="12" segment_no="11" tag_type="text">uate the importance of our multi-task design, we modify</text>
<text top="541" left="48" width="252" height="9" font="font11" id="p12_t14" reading_order_no="13" segment_no="11" tag_type="text">the network in Fig. 3 into 4 structures with different config-</text>
<text top="553" left="48" width="252" height="9" font="font11" id="p12_t15" reading_order_no="14" segment_no="11" tag_type="text">urations: a) 2D joint detection only, b) 2D joint detection</text>
<text top="564" left="48" width="252" height="9" font="font11" id="p12_t16" reading_order_no="15" segment_no="11" tag_type="text">+ IUV branch, c) 2D joint detection + POFs, d) 2D joint</text>
<text top="576" left="48" width="252" height="9" font="font11" id="p12_t17" reading_order_no="16" segment_no="11" tag_type="text">detection + IUV + POFs. All these networks are trained</text>
<text top="587" left="48" width="252" height="9" font="font11" id="p12_t18" reading_order_no="17" segment_no="11" tag_type="text">with the same training dataset, and tested on our validation</text>
<text top="599" left="48" width="252" height="9" font="font11" id="p12_t19" reading_order_no="18" segment_no="11" tag_type="text">dataset, which contains 11 different subjects not in the</text>
<text top="610" left="48" width="252" height="9" font="font11" id="p12_t20" reading_order_no="19" segment_no="11" tag_type="text">training dataset. For metrics of 2D joint positions, we report</text>
<text top="622" left="48" width="252" height="9" font="font11" id="p12_t21" reading_order_no="20" segment_no="11" tag_type="text">PCKh@0.5 [7] (the higher the better). For 3D part orientation,</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p12_t22" reading_order_no="21" segment_no="11" tag_type="text">we scale the predicted 3D part orientation by the ground-</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p12_t23" reading_order_no="22" segment_no="11" tag_type="text">truth limb length to obtain the 3D joint positions, then align</text>
<text top="657" left="48" width="252" height="9" font="font11" id="p12_t24" reading_order_no="23" segment_no="11" tag_type="text">the root joint position and compute the MPJPE (the lower</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p12_t25" reading_order_no="24" segment_no="11" tag_type="text">the better). The results are reported in Table 3, which shows</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p12_t26" reading_order_no="25" segment_no="11" tag_type="text">the power of mutual promotion of multi-task. We found</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p12_t27" reading_order_no="26" segment_no="11" tag_type="text">that IUV information improves the accuracy of 3D POF</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p12_t28" reading_order_no="27" segment_no="11" tag_type="text">orientation. The reason is that IUV maps provide the part</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p12_t29" reading_order_no="28" segment_no="11" tag_type="text">occlusion relationship which conveys some 3D information.</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p12_t30" reading_order_no="29" segment_no="11" tag_type="text">IUV maps are usually more abstract and more powerful</text>
<text top="737" left="48" width="252" height="9" font="font11" id="p12_t31" reading_order_no="30" segment_no="11" tag_type="text">than 2D landmarks in representing a human, and it is used</text>
<text top="45" left="312" width="68" height="9" font="font11" id="p12_t32" reading_order_no="31" segment_no="2" tag_type="text">by [59] as input.</text>
<text top="57" left="326" width="238" height="9" font="font18" id="p12_t33" reading_order_no="32" segment_no="3" tag_type="text">Quantitative analysis for optimization Table 4 shows</text>
<text top="68" left="312" width="252" height="9" font="font11" id="p12_t34" reading_order_no="33" segment_no="3" tag_type="text">the quantitative performance, which reveals the importance<b>Quantitative analysis for optimization</b></text>
<text top="80" left="312" width="252" height="9" font="font11" id="p12_t35" reading_order_no="34" segment_no="3" tag_type="text">of each energy term. We compare results under 5 different</text>
<text top="91" left="312" width="252" height="9" font="font11" id="p12_t36" reading_order_no="35" segment_no="3" tag_type="text">energy term settings: a) 2D position only; b) 2D position</text>
<text top="103" left="312" width="252" height="9" font="font11" id="p12_t37" reading_order_no="36" segment_no="3" tag_type="text">+ mask; c) 2D position + mask + 3D part orientation; d)</text>
<text top="114" left="312" width="252" height="9" font="font11" id="p12_t38" reading_order_no="37" segment_no="3" tag_type="text">2D position + mask + 3D part orientation + IUV; e) 2D</text>
<text top="126" left="312" width="252" height="9" font="font11" id="p12_t39" reading_order_no="38" segment_no="3" tag_type="text">position + mask + 3D part orientation + IUV + temporal. We</text>
<text top="137" left="312" width="252" height="9" font="font11" id="p12_t40" reading_order_no="39" segment_no="3" tag_type="text">report MPJPE on Human3.6M and 3DPW. The results shows</text>
<text top="149" left="312" width="252" height="9" font="font11" id="p12_t41" reading_order_no="40" segment_no="3" tag_type="text">that every energy term in our optimization is beneficial for</text>
<text top="160" left="312" width="252" height="9" font="font11" id="p12_t42" reading_order_no="41" segment_no="3" tag_type="text">human pose reconstruction. The result with temporal term</text>
<text top="172" left="312" width="252" height="9" font="font11" id="p12_t43" reading_order_no="42" segment_no="3" tag_type="text">shows higher errors, because it ensures temporal smooth</text>
<text top="184" left="312" width="244" height="9" font="font11" id="p12_t44" reading_order_no="43" segment_no="3" tag_type="text">rather than the consistency with the network output cues.</text>
<text top="195" left="326" width="238" height="9" font="font18" id="p12_t45" reading_order_no="44" segment_no="6" tag_type="text">The mask term in optimization. We evaluate the impor-</text>
<text top="207" left="312" width="252" height="9" font="font11" id="p12_t46" reading_order_no="45" segment_no="6" tag_type="text">tance of the foreground segmentation mask by comparing</text>
<text top="218" left="312" width="252" height="9" font="font11" id="p12_t47" reading_order_no="46" segment_no="6" tag_type="text">the reconstructed bodies with and without this term. Fig. 15<b>The mask term in optimization.</b></text>
<text top="230" left="312" width="252" height="9" font="font11" id="p12_t48" reading_order_no="47" segment_no="6" tag_type="text">clearly shows the role of the mask when predicted 2D joints</text>
<text top="241" left="312" width="252" height="9" font="font11" id="p12_t49" reading_order_no="48" segment_no="6" tag_type="text">and 3D orientation are inaccurate, especially when some</text>
<text top="253" left="312" width="76" height="9" font="font11" id="p12_t50" reading_order_no="49" segment_no="6" tag_type="text">joints are missing.</text>
<text top="347" left="362" width="7" height="9" font="font54" id="p12_t51" reading_order_no="50" segment_no="7" tag_type="figure">(a)</text>
<text top="347" left="408" width="8" height="9" font="font54" id="p12_t52" reading_order_no="51" segment_no="7" tag_type="figure">(b)</text>
<text top="347" left="452" width="7" height="9" font="font54" id="p12_t53" reading_order_no="52" segment_no="7" tag_type="figure">(c)</text>
<text top="347" left="497" width="8" height="9" font="font54" id="p12_t54" reading_order_no="53" segment_no="7" tag_type="figure">(d)</text>
<text top="347" left="542" width="8" height="9" font="font54" id="p12_t55" reading_order_no="54" segment_no="7" tag_type="figure">(e)</text>
<text top="369" left="312" width="252" height="9" font="font11" id="p12_t56" reading_order_no="55" segment_no="8" tag_type="text">Fig. 15: Importance of the mask term. Given an image (a),</text>
<text top="380" left="312" width="252" height="9" font="font11" id="p12_t57" reading_order_no="56" segment_no="8" tag_type="text">the network predicts a foreground segmentation mask (b)</text>
<text top="392" left="312" width="252" height="9" font="font11" id="p12_t58" reading_order_no="57" segment_no="8" tag_type="text">and 2D joints (c) (note the left-wrist is missing). If the mask</text>
<text top="404" left="312" width="252" height="9" font="font11" id="p12_t59" reading_order_no="58" segment_no="8" tag_type="text">is not used, the reconstructed body is problematic (d); The</text>
<text top="415" left="312" width="149" height="9" font="font11" id="p12_t60" reading_order_no="59" segment_no="8" tag_type="text">mask helps build a correct pose (e).</text>
<text top="438" left="326" width="238" height="9" font="font18" id="p12_t61" reading_order_no="60" segment_no="9" tag_type="text">The 3D part Orientation term in optimization. Fig. 16</text>
<text top="449" left="312" width="252" height="9" font="font11" id="p12_t62" reading_order_no="61" segment_no="9" tag_type="text">shows an example with and without the 3D orientation</text>
<text top="461" left="312" width="252" height="9" font="font11" id="p12_t63" reading_order_no="62" segment_no="9" tag_type="text">term. The use of the 3D orientation term significantly re-</text>
<text top="472" left="312" width="203" height="9" font="font11" id="p12_t64" reading_order_no="63" segment_no="9" tag_type="text">duces the reconstruction ambiguity of 3D poses.<b>The 3D part Orientation term in optimization.</b></text>
<text top="634" left="312" width="252" height="9" font="font11" id="p12_t65" reading_order_no="64" segment_no="12" tag_type="text">Fig. 16: Importance of the 3D part orientation term. (left)</text>
<text top="646" left="312" width="252" height="9" font="font11" id="p12_t66" reading_order_no="65" segment_no="12" tag_type="text">input image; (middle) result with 3D part orientation term;</text>
<text top="657" left="312" width="198" height="9" font="font11" id="p12_t67" reading_order_no="66" segment_no="12" tag_type="text">(right) result without 3D part orientation term.</text>
<text top="680" left="326" width="238" height="9" font="font18" id="p12_t68" reading_order_no="67" segment_no="13" tag_type="text">The IUV term in optimization. An IUV map plays</text>
<text top="691" left="312" width="252" height="9" font="font11" id="p12_t69" reading_order_no="68" segment_no="13" tag_type="text">important roles in both 3D body geometry reconstruction</text>
<text top="703" left="312" width="252" height="9" font="font11" id="p12_t70" reading_order_no="69" segment_no="13" tag_type="text">and pose estimation. We evaluate the importance of IUV by</text>
<text top="714" left="312" width="252" height="9" font="font11" id="p12_t71" reading_order_no="70" segment_no="13" tag_type="text">dropping off this term in shape reconstruction and pose esti-</text>
<text top="726" left="312" width="252" height="9" font="font11" id="p12_t72" reading_order_no="71" segment_no="13" tag_type="text">mation, respectively. Fig. 17(a) shows a side-by-side compar-<b>The IUV term in optimization.</b></text>
<text top="737" left="312" width="252" height="9" font="font11" id="p12_t73" reading_order_no="72" segment_no="13" tag_type="text">ison while reconstructing an over-weighted lady. Using IUV</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="792" width="612">
<text top="29" left="48" width="334" height="6" font="font0" id="p13_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="556" width="8" height="6" font="font0" id="p13_t2" reading_order_no="1" segment_no="1" tag_type="text">13</text>
<text top="182" left="59" width="229" height="8" font="font10" id="p13_t3" reading_order_no="2" segment_no="4" tag_type="text">(a) Shape reconstructed with the IUV term (middle) manifests</text>
<text top="191" left="59" width="221" height="8" font="font10" id="p13_t4" reading_order_no="3" segment_no="4" tag_type="text">the body weight better than that without the IUV term (right).</text>
<text top="339" left="59" width="229" height="8" font="font10" id="p13_t5" reading_order_no="4" segment_no="8" tag_type="text">(b) Pose reconstructed with (middle) and without (right) the IUV</text>
<text top="348" left="59" width="19" height="8" font="font10" id="p13_t6" reading_order_no="5" segment_no="8" tag_type="text">term.</text>
<text top="369" left="48" width="252" height="9" font="font11" id="p13_t7" reading_order_no="6" segment_no="10" tag_type="text">Fig. 17: The importance of IUV term for shape and pose</text>
<text top="381" left="48" width="63" height="9" font="font11" id="p13_t8" reading_order_no="7" segment_no="10" tag_type="text">reconstruction.</text>
<text top="420" left="48" width="252" height="9" font="font11" id="p13_t9" reading_order_no="8" segment_no="12" tag_type="text">term gives more accurate body model, because IUV terms</text>
<text top="432" left="48" width="252" height="9" font="font11" id="p13_t10" reading_order_no="9" segment_no="12" tag_type="text">impose detailed geometry model constraints from dense</text>
<text top="443" left="48" width="73" height="9" font="font11" id="p13_t11" reading_order_no="10" segment_no="12" tag_type="text">correspondences.</text>
<text top="457" left="62" width="238" height="9" font="font11" id="p13_t12" reading_order_no="11" segment_no="14" tag_type="text">For pose reconstruction, Fig. 17(b) shows that recon-</text>
<text top="469" left="48" width="252" height="9" font="font11" id="p13_t13" reading_order_no="12" segment_no="14" tag_type="text">structed examples with and without IUV term. It is obvious</text>
<text top="480" left="48" width="252" height="9" font="font11" id="p13_t14" reading_order_no="13" segment_no="14" tag_type="text">that IUV term helps recover more accurate result, especially</text>
<text top="492" left="48" width="88" height="9" font="font11" id="p13_t15" reading_order_no="14" segment_no="14" tag_type="text">for body orientation.</text>
<text top="541" left="48" width="13" height="9" font="font39" id="p13_t16" reading_order_no="15" segment_no="18" tag_type="title">7.5<b>7.5</b></text>
<text top="541" left="71" width="51" height="9" font="font39" id="p13_t17" reading_order_no="16" segment_no="18" tag_type="title">Limitations<b>Limitations</b></text>
<text top="564" left="48" width="252" height="9" font="font11" id="p13_t18" reading_order_no="17" segment_no="20" tag_type="text">With no exceptions, our method suffers from several limita-</text>
<text top="576" left="48" width="252" height="9" font="font11" id="p13_t19" reading_order_no="18" segment_no="20" tag_type="text">tions. First, we observed failure cases when a significant part</text>
<text top="587" left="48" width="252" height="9" font="font11" id="p13_t20" reading_order_no="19" segment_no="20" tag_type="text">of the target person is either occluded by other objects or</text>
<text top="599" left="48" width="252" height="9" font="font11" id="p13_t21" reading_order_no="20" segment_no="20" tag_type="text">out of image boundary. Occlusion is the biggest issue and it</text>
<text top="610" left="48" width="252" height="9" font="font11" id="p13_t22" reading_order_no="21" segment_no="20" tag_type="text">imposes more challenge for RGB camera than depth camera</text>
<text top="622" left="48" width="252" height="9" font="font11" id="p13_t23" reading_order_no="22" segment_no="20" tag_type="text">based methods. Second, our method also fails for com-</text>
<text top="634" left="48" width="252" height="9" font="font11" id="p13_t24" reading_order_no="23" segment_no="20" tag_type="text">plicated or uncommon poses, particularly those in sports</text>
<text top="645" left="48" width="252" height="9" font="font11" id="p13_t25" reading_order_no="24" segment_no="20" tag_type="text">videos, such as gymnastics and skydiving. The main reason</text>
<text top="657" left="48" width="252" height="9" font="font11" id="p13_t26" reading_order_no="25" segment_no="20" tag_type="text">is that such data is not adequate in training dataset. Third,</text>
<text top="668" left="48" width="252" height="9" font="font11" id="p13_t27" reading_order_no="26" segment_no="20" tag_type="text">our system does not have specific hand pose detector (as</text>
<text top="680" left="48" width="252" height="9" font="font11" id="p13_t28" reading_order_no="27" segment_no="20" tag_type="text">did in [60]) and each hand is associated with only one joint,</text>
<text top="691" left="48" width="252" height="9" font="font11" id="p13_t29" reading_order_no="28" segment_no="20" tag_type="text">therefore the reconstructed hands are sometimes incorrectly</text>
<text top="703" left="48" width="252" height="9" font="font11" id="p13_t30" reading_order_no="29" segment_no="20" tag_type="text">oriented. Finally, our CNN does not handle multiple bodies</text>
<text top="714" left="48" width="252" height="9" font="font11" id="p13_t31" reading_order_no="30" segment_no="20" tag_type="text">at this moment, but can easily extended to support this.</text>
<text top="726" left="48" width="252" height="9" font="font11" id="p13_t32" reading_order_no="31" segment_no="20" tag_type="text">Solving the above mentioned problems points to interesting</text>
<text top="737" left="48" width="72" height="9" font="font11" id="p13_t33" reading_order_no="32" segment_no="20" tag_type="text">future directions.</text>
<text top="44" left="312" width="6" height="10" font="font7" id="p13_t34" reading_order_no="33" segment_no="2" tag_type="title">8<b>8</b></text>
<text top="44" left="330" width="72" height="10" font="font7" id="p13_t35" reading_order_no="34" segment_no="2" tag_type="title">C ONCLUSIONS<b>C</b></text>
<text top="59" left="312" width="252" height="9" font="font11" id="p13_t36" reading_order_no="35" segment_no="3" tag_type="text">We have presented a method for reconstructing the 3D<b>ONCLUSIONS</b></text>
<text top="71" left="312" width="252" height="9" font="font11" id="p13_t37" reading_order_no="36" segment_no="3" tag_type="text">pose and shape of a human, in a stable and consistent</text>
<text top="83" left="312" width="252" height="9" font="font11" id="p13_t38" reading_order_no="37" segment_no="3" tag_type="text">manner, from a single RGB video stream at more than 20</text>
<text top="94" left="312" width="252" height="9" font="font11" id="p13_t39" reading_order_no="38" segment_no="3" tag_type="text">Hz. Our approach employs a multi-task CNN that regresses</text>
<text top="106" left="312" width="252" height="9" font="font11" id="p13_t40" reading_order_no="39" segment_no="3" tag_type="text">five human anatomical features simultaneously, which are</text>
<text top="117" left="312" width="252" height="9" font="font11" id="p13_t41" reading_order_no="40" segment_no="3" tag_type="text">further cooked with a kinematic pose reconstruction and</text>
<text top="129" left="312" width="252" height="9" font="font11" id="p13_t42" reading_order_no="41" segment_no="3" tag_type="text">shape modeling algorithm, producing a temporally stable</text>
<text top="140" left="312" width="252" height="9" font="font11" id="p13_t43" reading_order_no="42" segment_no="3" tag_type="text">3D reconstruction of the full-body. In contrast to most ex-</text>
<text top="152" left="312" width="252" height="9" font="font11" id="p13_t44" reading_order_no="43" segment_no="3" tag_type="text">isting approaches, our approach can operate on any input</text>
<text top="163" left="312" width="252" height="9" font="font11" id="p13_t45" reading_order_no="44" segment_no="3" tag_type="text">image fully automatically, without strict prescribed bound-</text>
<text top="175" left="312" width="252" height="9" font="font11" id="p13_t46" reading_order_no="45" segment_no="3" tag_type="text">ing boxes, and independent of expensive initialization. We</text>
<text top="186" left="312" width="252" height="9" font="font11" id="p13_t47" reading_order_no="46" segment_no="3" tag_type="text">test and evaluate our system in a variety of challenging re-</text>
<text top="198" left="312" width="252" height="9" font="font11" id="p13_t48" reading_order_no="47" segment_no="3" tag_type="text">altime scenarios, including live streaming from commercial</text>
<text top="209" left="312" width="252" height="9" font="font11" id="p13_t49" reading_order_no="48" segment_no="3" tag_type="text">cameras, as well as in community videos. Results demon-</text>
<text top="221" left="312" width="252" height="9" font="font11" id="p13_t50" reading_order_no="49" segment_no="3" tag_type="text">strate that our approach compares to offline state-of-the-</text>
<text top="233" left="312" width="252" height="9" font="font11" id="p13_t51" reading_order_no="50" segment_no="3" tag_type="text">art monocular RGB methods qualitatively and advances the</text>
<text top="244" left="312" width="252" height="9" font="font11" id="p13_t52" reading_order_no="51" segment_no="3" tag_type="text">realtime 3D body reconstruction methods with a significant</text>
<text top="256" left="312" width="20" height="9" font="font11" id="p13_t53" reading_order_no="52" segment_no="3" tag_type="text">step.</text>
<text top="281" left="312" width="67" height="10" font="font7" id="p13_t54" reading_order_no="53" segment_no="5" tag_type="title">R EFERENCES</text>
<text top="297" left="312" width="9" height="8" font="font10" id="p13_t55" reading_order_no="54" segment_no="6" tag_type="text">[1]<b>R</b></text>
<text top="297" left="330" width="234" height="8" font="font10" id="p13_t56" reading_order_no="55" segment_no="6" tag_type="text">R. A. G ¨uler, N. Neverova, and I. Kokkinos, “Densepose: Dense hu-<b>EFERENCES</b></text>
<text top="306" left="330" width="234" height="8" font="font10" id="p13_t57" reading_order_no="56" segment_no="6" tag_type="text">man pose estimation in the wild,” in IEEE Conference on Computer</text>
<text top="315" left="330" width="179" height="8" font="font13" id="p13_t58" reading_order_no="57" segment_no="6" tag_type="text">Vision and Pattern Recognition , 2018, pp. 7297–7306.</text>
<text top="324" left="312" width="9" height="8" font="font10" id="p13_t59" reading_order_no="58" segment_no="7" tag_type="text">[2]</text>
<text top="324" left="330" width="234" height="8" font="font10" id="p13_t60" reading_order_no="59" segment_no="7" tag_type="text">N. Kolotouros, G. Pavlakos, M. J. Black, and K. Daniilidis, “Learn-</text>
<text top="333" left="330" width="234" height="8" font="font10" id="p13_t61" reading_order_no="60" segment_no="7" tag_type="text">ing to reconstruct 3d human pose and shape via model-fitting in</text>
<text top="342" left="330" width="234" height="8" font="font10" id="p13_t62" reading_order_no="61" segment_no="7" tag_type="text">the loop,” in Proceedings of the IEEE/CVF International Conference on</text>
<text top="351" left="330" width="132" height="8" font="font13" id="p13_t63" reading_order_no="62" segment_no="7" tag_type="text">Computer Vision , 2019, pp. 2252–2261.</text>
<text top="360" left="312" width="9" height="8" font="font10" id="p13_t64" reading_order_no="63" segment_no="9" tag_type="text">[3]</text>
<text top="360" left="330" width="234" height="8" font="font10" id="p13_t65" reading_order_no="64" segment_no="9" tag_type="text">D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shafiei, H.-</text>
<text top="369" left="330" width="234" height="8" font="font10" id="p13_t66" reading_order_no="65" segment_no="9" tag_type="text">P. Seidel, W. Xu, D. Casas, and C. Theobalt, “VNect: Real-time</text>
<text top="378" left="330" width="234" height="8" font="font10" id="p13_t67" reading_order_no="66" segment_no="9" tag_type="text">3D human pose estimation with a single RGB camera,” ACM</text>
<text top="387" left="330" width="200" height="8" font="font13" id="p13_t68" reading_order_no="67" segment_no="9" tag_type="text">Transactions on Graphics (TOG) , vol. 36, no. 4, p. 44, 2017.</text>
<text top="396" left="312" width="9" height="8" font="font10" id="p13_t69" reading_order_no="68" segment_no="11" tag_type="text">[4]</text>
<text top="396" left="330" width="234" height="8" font="font10" id="p13_t70" reading_order_no="69" segment_no="11" tag_type="text">F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and</text>
<text top="405" left="330" width="234" height="8" font="font10" id="p13_t71" reading_order_no="70" segment_no="11" tag_type="text">M. J. Black, “Keep it SMPL: Automatic estimation of 3D human</text>
<text top="414" left="330" width="234" height="8" font="font10" id="p13_t72" reading_order_no="71" segment_no="11" tag_type="text">pose and shape from a single image,” in European Conference on</text>
<text top="423" left="330" width="124" height="8" font="font13" id="p13_t73" reading_order_no="72" segment_no="11" tag_type="text">Computer Vision , 2016, pp. 561–578.</text>
<text top="432" left="312" width="9" height="8" font="font10" id="p13_t74" reading_order_no="73" segment_no="13" tag_type="text">[5]</text>
<text top="432" left="330" width="234" height="8" font="font10" id="p13_t75" reading_order_no="74" segment_no="13" tag_type="text">M. Kocabas, N. Athanasiou, and M. J. Black, “Vibe: Video inference</text>
<text top="441" left="330" width="234" height="8" font="font10" id="p13_t76" reading_order_no="75" segment_no="13" tag_type="text">for human body pose and shape estimation,” in Proceedings of the</text>
<text top="450" left="330" width="234" height="8" font="font13" id="p13_t77" reading_order_no="76" segment_no="13" tag_type="text">IEEE/CVF Conference on Computer Vision and Pattern Recognition ,</text>
<text top="459" left="330" width="72" height="8" font="font10" id="p13_t78" reading_order_no="77" segment_no="13" tag_type="text">2020, pp. 5253–5263.</text>
<text top="468" left="312" width="9" height="8" font="font10" id="p13_t79" reading_order_no="78" segment_no="15" tag_type="text">[6]</text>
<text top="468" left="330" width="234" height="8" font="font10" id="p13_t80" reading_order_no="79" segment_no="15" tag_type="text">S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convo-</text>
<text top="477" left="330" width="234" height="8" font="font10" id="p13_t81" reading_order_no="80" segment_no="15" tag_type="text">lutional pose machines,” in Proceedings of the IEEE Conference on</text>
<text top="486" left="330" width="213" height="8" font="font13" id="p13_t82" reading_order_no="81" segment_no="15" tag_type="text">Computer Vision and Pattern Recognition , 2016, pp. 4724–4732.</text>
<text top="495" left="312" width="9" height="8" font="font10" id="p13_t83" reading_order_no="82" segment_no="16" tag_type="text">[7]</text>
<text top="495" left="330" width="234" height="8" font="font10" id="p13_t84" reading_order_no="83" segment_no="16" tag_type="text">A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for</text>
<text top="504" left="330" width="234" height="8" font="font10" id="p13_t85" reading_order_no="84" segment_no="16" tag_type="text">human pose estimation,” in European conference on computer vision ,</text>
<text top="513" left="330" width="64" height="8" font="font10" id="p13_t86" reading_order_no="85" segment_no="16" tag_type="text">2016, pp. 483–499.</text>
<text top="522" left="312" width="9" height="8" font="font10" id="p13_t87" reading_order_no="86" segment_no="17" tag_type="text">[8]</text>
<text top="522" left="330" width="234" height="8" font="font10" id="p13_t88" reading_order_no="87" segment_no="17" tag_type="text">X. Nie, J. Feng, Y. Zuo, and S. Yan, “Human pose estimation with</text>
<text top="531" left="330" width="234" height="8" font="font10" id="p13_t89" reading_order_no="88" segment_no="17" tag_type="text">parsing induced learner,” in Proceedings of the IEEE Conference on</text>
<text top="540" left="330" width="213" height="8" font="font13" id="p13_t90" reading_order_no="89" segment_no="17" tag_type="text">Computer Vision and Pattern Recognition , 2018, pp. 2100–2108.</text>
<text top="549" left="312" width="9" height="8" font="font10" id="p13_t91" reading_order_no="90" segment_no="19" tag_type="text">[9]</text>
<text top="549" left="330" width="234" height="8" font="font10" id="p13_t92" reading_order_no="91" segment_no="19" tag_type="text">J. Martinez, R. Hossain, J. Romero, and J. J. Little, “A simple yet</text>
<text top="558" left="330" width="234" height="8" font="font10" id="p13_t93" reading_order_no="92" segment_no="19" tag_type="text">effective baseline for 3D human pose estimation,” in Proceedings</text>
<text top="567" left="330" width="234" height="8" font="font13" id="p13_t94" reading_order_no="93" segment_no="19" tag_type="text">of the IEEE International Conference on Computer Vision , 2017, pp.</text>
<text top="576" left="330" width="38" height="8" font="font10" id="p13_t95" reading_order_no="94" segment_no="19" tag_type="text">2640–2649.</text>
<text top="585" left="312" width="252" height="8" font="font10" id="p13_t96" reading_order_no="95" segment_no="21" tag_type="text">[10] H. Ci, C. Wang, X. Ma, and Y. Wang, “Optimizing network struc-</text>
<text top="594" left="330" width="234" height="8" font="font10" id="p13_t97" reading_order_no="96" segment_no="21" tag_type="text">ture for 3d human pose estimation,” in Proceedings of the IEEE/CVF</text>
<text top="603" left="330" width="224" height="8" font="font13" id="p13_t98" reading_order_no="97" segment_no="21" tag_type="text">International Conference on Computer Vision , 2019, pp. 2262–2271.</text>
<text top="612" left="312" width="252" height="8" font="font10" id="p13_t99" reading_order_no="98" segment_no="22" tag_type="text">[11] L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. N. Metaxas, “Seman-</text>
<text top="621" left="330" width="234" height="8" font="font10" id="p13_t100" reading_order_no="99" segment_no="22" tag_type="text">tic graph convolutional networks for 3d human pose regression,”</text>
<text top="630" left="330" width="234" height="8" font="font10" id="p13_t101" reading_order_no="100" segment_no="22" tag_type="text">in Proceedings of the IEEE/CVF Conference on Computer Vision and</text>
<text top="639" left="330" width="142" height="8" font="font13" id="p13_t102" reading_order_no="101" segment_no="22" tag_type="text">Pattern Recognition , 2019, pp. 3425–3435.</text>
<text top="648" left="312" width="252" height="8" font="font10" id="p13_t103" reading_order_no="102" segment_no="23" tag_type="text">[12] H.-S. Fang, Y. Xu, W. Wang, X. Liu, and S.-C. Zhu, “Learning</text>
<text top="657" left="330" width="234" height="8" font="font10" id="p13_t104" reading_order_no="103" segment_no="23" tag_type="text">pose grammar to encode human body configuration for 3d pose</text>
<text top="666" left="330" width="234" height="8" font="font10" id="p13_t105" reading_order_no="104" segment_no="23" tag_type="text">estimation,” in Proceedings of the AAAI Conference on Artificial</text>
<text top="675" left="330" width="109" height="8" font="font13" id="p13_t106" reading_order_no="105" segment_no="23" tag_type="text">Intelligence , vol. 32, no. 1, 2018.</text>
<text top="684" left="312" width="252" height="8" font="font10" id="p13_t107" reading_order_no="106" segment_no="24" tag_type="text">[13] G. Pavlakos, X. Zhou, and K. Daniilidis, “Ordinal depth super-</text>
<text top="693" left="330" width="234" height="8" font="font10" id="p13_t108" reading_order_no="107" segment_no="24" tag_type="text">vision for 3D human pose estimation,” in IEEE Conference on</text>
<text top="702" left="330" width="213" height="8" font="font13" id="p13_t109" reading_order_no="108" segment_no="24" tag_type="text">Computer Vision and Pattern Recognition , 2018, pp. 7307–7316.</text>
<text top="711" left="312" width="252" height="8" font="font10" id="p13_t110" reading_order_no="109" segment_no="25" tag_type="text">[14] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,</text>
<text top="720" left="330" width="234" height="8" font="font10" id="p13_t111" reading_order_no="110" segment_no="25" tag_type="text">P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects</text>
<text top="729" left="330" width="234" height="8" font="font10" id="p13_t112" reading_order_no="111" segment_no="25" tag_type="text">in context,” in European Conference on Computer Vision (ECCV) ,</text>
<text top="738" left="330" width="64" height="8" font="font10" id="p13_t113" reading_order_no="112" segment_no="25" tag_type="text">Z ¨urich, 2014, oral.</text>
</page>
<page number="14" position="absolute" top="0" left="0" height="792" width="612">
<text top="29" left="48" width="334" height="6" font="font0" id="p14_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="556" width="8" height="6" font="font0" id="p14_t2" reading_order_no="1" segment_no="1" tag_type="text">14</text>
<text top="46" left="48" width="252" height="8" font="font10" id="p14_t3" reading_order_no="2" segment_no="2" tag_type="text">[15] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele, “2d human</text>
<text top="55" left="66" width="234" height="8" font="font10" id="p14_t4" reading_order_no="3" segment_no="2" tag_type="text">pose estimation: New benchmark and state of the art analysis,” in</text>
<text top="64" left="66" width="234" height="8" font="font13" id="p14_t5" reading_order_no="4" segment_no="2" tag_type="text">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,</text>
<text top="73" left="66" width="36" height="8" font="font10" id="p14_t6" reading_order_no="5" segment_no="2" tag_type="text">June 2014.</text>
<text top="82" left="48" width="252" height="8" font="font10" id="p14_t7" reading_order_no="6" segment_no="4" tag_type="text">[16] X. Zhou, Q. Huang, X. Sun, X. Xue, and Y. Wei, “Towards 3d hu-</text>
<text top="91" left="66" width="234" height="8" font="font10" id="p14_t8" reading_order_no="7" segment_no="4" tag_type="text">man pose estimation in the wild: a weakly-supervised approach,”</text>
<text top="100" left="66" width="234" height="8" font="font10" id="p14_t9" reading_order_no="8" segment_no="4" tag_type="text">in Proceedings of the IEEE International Conference on Computer</text>
<text top="109" left="66" width="89" height="8" font="font13" id="p14_t10" reading_order_no="9" segment_no="4" tag_type="text">Vision , 2017, pp. 398–407.</text>
<text top="119" left="48" width="252" height="8" font="font10" id="p14_t11" reading_order_no="10" segment_no="6" tag_type="text">[17] R. Dabral, A. Mundhada, U. Kusupati, S. Afaque, A. Sharma, and</text>
<text top="128" left="66" width="234" height="8" font="font10" id="p14_t12" reading_order_no="11" segment_no="6" tag_type="text">A. Jain, “Learning 3d human pose from structure and motion,” in</text>
<text top="137" left="66" width="234" height="8" font="font13" id="p14_t13" reading_order_no="12" segment_no="6" tag_type="text">Proceedings of the European Conference on Computer Vision (ECCV) ,</text>
<text top="146" left="66" width="64" height="8" font="font10" id="p14_t14" reading_order_no="13" segment_no="6" tag_type="text">2018, pp. 668–683.</text>
<text top="155" left="48" width="252" height="8" font="font10" id="p14_t15" reading_order_no="14" segment_no="8" tag_type="text">[18] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu,</text>
<text top="164" left="66" width="234" height="8" font="font10" id="p14_t16" reading_order_no="15" segment_no="8" tag_type="text">and C. Theobalt, “Monocular 3D human pose estimation in the</text>
<text top="173" left="66" width="234" height="8" font="font10" id="p14_t17" reading_order_no="16" segment_no="8" tag_type="text">wild using improved CNN supervision,” in 2017 International</text>
<text top="182" left="66" width="173" height="8" font="font13" id="p14_t18" reading_order_no="17" segment_no="8" tag_type="text">Conference on 3D Vision (3DV) , 2017, pp. 506–516.</text>
<text top="192" left="48" width="252" height="8" font="font10" id="p14_t19" reading_order_no="18" segment_no="11" tag_type="text">[19] B. Tekin, P. M´arquez-Neila, M. Salzmann, and P. Fua, “Learning to</text>
<text top="201" left="66" width="234" height="8" font="font10" id="p14_t20" reading_order_no="19" segment_no="11" tag_type="text">fuse 2d and 3d image cues for monocular body pose estimation,”</text>
<text top="210" left="66" width="234" height="8" font="font10" id="p14_t21" reading_order_no="20" segment_no="11" tag_type="text">in Proceedings of the IEEE International Conference on Computer</text>
<text top="219" left="66" width="97" height="8" font="font13" id="p14_t22" reading_order_no="21" segment_no="11" tag_type="text">Vision , 2017, pp. 3941–3950.</text>
<text top="228" left="48" width="252" height="8" font="font10" id="p14_t23" reading_order_no="22" segment_no="13" tag_type="text">[20] I. Habibie, W. Xu, D. Mehta, G. Pons-Moll, and C. Theobalt, “In</text>
<text top="237" left="66" width="234" height="8" font="font10" id="p14_t24" reading_order_no="23" segment_no="13" tag_type="text">the wild human pose estimation using explicit 2d features and</text>
<text top="246" left="66" width="234" height="8" font="font10" id="p14_t25" reading_order_no="24" segment_no="13" tag_type="text">intermediate 3d representations,” in Proceedings of the IEEE/CVF</text>
<text top="255" left="66" width="234" height="8" font="font13" id="p14_t26" reading_order_no="25" segment_no="13" tag_type="text">Conference on Computer Vision and Pattern Recognition , 2019, pp.</text>
<text top="264" left="66" width="49" height="8" font="font10" id="p14_t27" reading_order_no="26" segment_no="13" tag_type="text">10 905–10 914.</text>
<text top="273" left="48" width="252" height="8" font="font10" id="p14_t28" reading_order_no="27" segment_no="16" tag_type="text">[21] X. Sun, J. Shang, S. Liang, and Y. Wei, “Compositional human</text>
<text top="282" left="66" width="234" height="8" font="font10" id="p14_t29" reading_order_no="28" segment_no="16" tag_type="text">pose regression,” in Proceedings of the IEEE International Conference</text>
<text top="291" left="66" width="142" height="8" font="font13" id="p14_t30" reading_order_no="29" segment_no="16" tag_type="text">on Computer Vision , 2017, pp. 2602–2611.</text>
<text top="301" left="48" width="252" height="8" font="font10" id="p14_t31" reading_order_no="30" segment_no="18" tag_type="text">[22] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis, “Coarse-</text>
<text top="310" left="66" width="234" height="8" font="font10" id="p14_t32" reading_order_no="31" segment_no="18" tag_type="text">to-fine volumetric prediction for single-image 3D human pose,” in</text>
<text top="319" left="66" width="234" height="8" font="font13" id="p14_t33" reading_order_no="32" segment_no="18" tag_type="text">IEEE Conference on Computer Vision and Pattern Recognition , 2017,</text>
<text top="328" left="66" width="52" height="8" font="font10" id="p14_t34" reading_order_no="33" segment_no="18" tag_type="text">pp. 7025–7034.</text>
<text top="337" left="48" width="252" height="8" font="font10" id="p14_t35" reading_order_no="34" segment_no="20" tag_type="text">[23] X. Sun, B. Xiao, F. Wei, S. Liang, and Y. Wei, “Integral human pose</text>
<text top="346" left="66" width="234" height="8" font="font10" id="p14_t36" reading_order_no="35" segment_no="20" tag_type="text">regression,” in Proceedings of the European Conference on Computer</text>
<text top="355" left="66" width="118" height="8" font="font13" id="p14_t37" reading_order_no="36" segment_no="20" tag_type="text">Vision (ECCV) , 2018, pp. 529–545.</text>
<text top="365" left="48" width="252" height="8" font="font10" id="p14_t38" reading_order_no="37" segment_no="22" tag_type="text">[24] C. Luo, X. Chu, and A. Yuille, “Orinet: A fully convolu-</text>
<text top="374" left="66" width="234" height="8" font="font10" id="p14_t39" reading_order_no="38" segment_no="22" tag_type="text">tional network for 3d human pose estimation,” arXiv preprint</text>
<text top="383" left="66" width="81" height="8" font="font13" id="p14_t40" reading_order_no="39" segment_no="22" tag_type="text">arXiv:1811.04989 , 2018.</text>
<text top="392" left="48" width="252" height="8" font="font10" id="p14_t41" reading_order_no="40" segment_no="23" tag_type="text">[25] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, and</text>
<text top="401" left="66" width="234" height="8" font="font10" id="p14_t42" reading_order_no="41" segment_no="23" tag_type="text">J. Davis, “SCAPE: Shape completion and animation of people,”</text>
<text top="410" left="66" width="184" height="8" font="font13" id="p14_t43" reading_order_no="42" segment_no="23" tag_type="text">ACM Trans. Graph. , vol. 24, no. 3, pp. 408–416, 2005.</text>
<text top="419" left="48" width="252" height="8" font="font10" id="p14_t44" reading_order_no="43" segment_no="26" tag_type="text">[26] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black,</text>
<text top="428" left="66" width="234" height="8" font="font10" id="p14_t45" reading_order_no="44" segment_no="26" tag_type="text">“Smpl: A skinned multi-person linear model,” ACM transactions</text>
<text top="437" left="66" width="159" height="8" font="font13" id="p14_t46" reading_order_no="45" segment_no="26" tag_type="text">on graphics (TOG) , vol. 34, no. 6, p. 248, 2015.</text>
<text top="447" left="48" width="252" height="8" font="font10" id="p14_t47" reading_order_no="46" segment_no="27" tag_type="text">[27] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. A. Osman,</text>
<text top="456" left="66" width="234" height="8" font="font10" id="p14_t48" reading_order_no="47" segment_no="27" tag_type="text">D. Tzionas, and M. J. Black, “Expressive body capture: 3D hands,</text>
<text top="465" left="66" width="234" height="8" font="font10" id="p14_t49" reading_order_no="48" segment_no="27" tag_type="text">face, and body from a single image,” in IEEE Conf. on Computer</text>
<text top="474" left="66" width="154" height="8" font="font13" id="p14_t50" reading_order_no="49" segment_no="27" tag_type="text">Vision and Pattern Recognition (CVPR) , 2019.</text>
<text top="483" left="48" width="252" height="8" font="font10" id="p14_t51" reading_order_no="50" segment_no="29" tag_type="text">[28] P. Guan, A. Weiss, A. O. B˘alan, and M. J. Black, “Estimating</text>
<text top="492" left="66" width="234" height="8" font="font10" id="p14_t52" reading_order_no="51" segment_no="29" tag_type="text">human shape and pose from a single image,” in IEEE International</text>
<text top="501" left="66" width="180" height="8" font="font13" id="p14_t53" reading_order_no="52" segment_no="29" tag_type="text">Conference on Computer Vision , 2009, pp. 1381–1388.</text>
<text top="511" left="48" width="252" height="8" font="font10" id="p14_t54" reading_order_no="53" segment_no="31" tag_type="text">[29] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, and P. V.</text>
<text top="520" left="66" width="234" height="8" font="font10" id="p14_t55" reading_order_no="54" segment_no="31" tag_type="text">Gehler, “Unite the people: Closing the loop between 3D and 2D</text>
<text top="529" left="66" width="234" height="8" font="font10" id="p14_t56" reading_order_no="55" segment_no="31" tag_type="text">human representations,” in IEEE Conf. on Computer Vision and</text>
<text top="538" left="66" width="170" height="8" font="font13" id="p14_t57" reading_order_no="56" segment_no="31" tag_type="text">Pattern Recognition (CVPR) , 2017, pp. 6050–6059.</text>
<text top="547" left="48" width="252" height="8" font="font10" id="p14_t58" reading_order_no="57" segment_no="33" tag_type="text">[30] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, “End-to-</text>
<text top="556" left="66" width="234" height="8" font="font10" id="p14_t59" reading_order_no="58" segment_no="33" tag_type="text">end recovery of human shape and pose,” in IEEE Conference on</text>
<text top="565" left="66" width="213" height="8" font="font13" id="p14_t60" reading_order_no="59" segment_no="33" tag_type="text">Computer Vision and Pattern Recognition , 2018, pp. 7122–7131.</text>
<text top="574" left="48" width="252" height="8" font="font10" id="p14_t61" reading_order_no="60" segment_no="35" tag_type="text">[31] N. Kolotouros, G. Pavlakos, and K. Daniilidis, “Convolutional</text>
<text top="583" left="66" width="234" height="8" font="font10" id="p14_t62" reading_order_no="61" segment_no="35" tag_type="text">mesh regression for single-image human shape reconstruction,”</text>
<text top="592" left="66" width="234" height="8" font="font10" id="p14_t63" reading_order_no="62" segment_no="35" tag_type="text">in Proceedings of the IEEE Conference on Computer Vision and Pattern</text>
<text top="601" left="66" width="115" height="8" font="font13" id="p14_t64" reading_order_no="63" segment_no="35" tag_type="text">Recognition , 2019, pp. 4501–4510.</text>
<text top="611" left="48" width="252" height="8" font="font10" id="p14_t65" reading_order_no="64" segment_no="37" tag_type="text">[32] H. Joo, N. Neverova, and A. Vedaldi, “Exemplar fine-tuning</text>
<text top="620" left="66" width="234" height="8" font="font10" id="p14_t66" reading_order_no="65" segment_no="37" tag_type="text">for 3d human pose fitting towards in-the-wild 3d human pose</text>
<text top="629" left="66" width="176" height="8" font="font10" id="p14_t67" reading_order_no="66" segment_no="37" tag_type="text">estimation,” arXiv preprint arXiv:2004.03686 , 2020.</text>
<text top="638" left="48" width="252" height="8" font="font10" id="p14_t68" reading_order_no="67" segment_no="39" tag_type="text">[33] C. Bregler, J. Malik, and K. Pullen, “Twist based acquisition and</text>
<text top="647" left="66" width="234" height="8" font="font10" id="p14_t69" reading_order_no="68" segment_no="39" tag_type="text">tracking of animal and human kinematics,” International Journal of</text>
<text top="656" left="66" width="174" height="8" font="font13" id="p14_t70" reading_order_no="69" segment_no="39" tag_type="text">Computer Vision , vol. 56, no. 3, pp. 179–194, 2004.</text>
<text top="666" left="48" width="252" height="8" font="font10" id="p14_t71" reading_order_no="70" segment_no="41" tag_type="text">[34] D. Xiang, H. Joo, and Y. Sheikh, “Monocular total capture: Posing</text>
<text top="675" left="66" width="234" height="8" font="font10" id="p14_t72" reading_order_no="71" segment_no="41" tag_type="text">face, body and hands in the wild,” in Proceedings of the IEEE</text>
<text top="684" left="66" width="234" height="8" font="font13" id="p14_t73" reading_order_no="72" segment_no="41" tag_type="text">Conference on Computer Vision and Pattern Recognition , 2019, pp.</text>
<text top="693" left="66" width="49" height="8" font="font10" id="p14_t74" reading_order_no="73" segment_no="41" tag_type="text">10 965–10 974.</text>
<text top="702" left="48" width="252" height="8" font="font10" id="p14_t75" reading_order_no="74" segment_no="43" tag_type="text">[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-</text>
<text top="711" left="66" width="149" height="8" font="font10" id="p14_t76" reading_order_no="75" segment_no="43" tag_type="text">tion,” arXiv preprint arXiv:1412.6980 , 2014.</text>
<text top="720" left="48" width="252" height="8" font="font10" id="p14_t77" reading_order_no="76" segment_no="44" tag_type="text">[36] S. Shimada, V. Golyanik, W. Xu, and C. Theobalt, “Physcap:</text>
<text top="729" left="66" width="234" height="8" font="font10" id="p14_t78" reading_order_no="77" segment_no="44" tag_type="text">Physically plausible monocular 3d motion capture in real time,”</text>
<text top="738" left="66" width="190" height="8" font="font13" id="p14_t79" reading_order_no="78" segment_no="44" tag_type="text">ACM Transactions on Graphics , vol. 39, no. 6, dec 2020.</text>
<text top="46" left="312" width="252" height="8" font="font10" id="p14_t80" reading_order_no="79" segment_no="3" tag_type="text">[37] W. Xu, A. Chatterjee, M. Zollh ¨ofer, H. Rhodin, D. Mehta, H.-</text>
<text top="55" left="330" width="234" height="8" font="font10" id="p14_t81" reading_order_no="80" segment_no="3" tag_type="text">P. Seidel, and C. Theobalt, “MonoPerfCap: Human performance</text>
<text top="64" left="330" width="234" height="8" font="font10" id="p14_t82" reading_order_no="81" segment_no="3" tag_type="text">capture from monocular video,” ACM Transactions on Graphics</text>
<text top="73" left="330" width="115" height="8" font="font13" id="p14_t83" reading_order_no="82" segment_no="3" tag_type="text">(TOG) , vol. 37, no. 2, p. 27, 2018.</text>
<text top="83" left="312" width="252" height="8" font="font10" id="p14_t84" reading_order_no="83" segment_no="5" tag_type="text">[38] M. Habermann, W. Xu, M. Zollh ¨ofer, G. Pons-Moll, and</text>
<text top="92" left="330" width="234" height="8" font="font10" id="p14_t85" reading_order_no="84" segment_no="5" tag_type="text">C. Theobalt, “LiveCap: Real-time human performance capture</text>
<text top="101" left="330" width="234" height="8" font="font10" id="p14_t86" reading_order_no="85" segment_no="5" tag_type="text">from monocular video,” ACM Trans. Graph. , vol. 38, no. 2, pp.</text>
<text top="110" left="330" width="58" height="8" font="font10" id="p14_t87" reading_order_no="86" segment_no="5" tag_type="text">14:1–14:17, 2019.</text>
<text top="120" left="312" width="252" height="8" font="font10" id="p14_t88" reading_order_no="87" segment_no="7" tag_type="text">[39] M. Habermann, W. Xu, M. Zollhoefer, G. Ponsmoll, and</text>
<text top="129" left="330" width="234" height="8" font="font10" id="p14_t89" reading_order_no="88" segment_no="7" tag_type="text">C. Theobalt, “Deepcap: Monocular human performance capture</text>
<text top="138" left="330" width="234" height="8" font="font10" id="p14_t90" reading_order_no="89" segment_no="7" tag_type="text">using weak supervision,” arXiv: Computer Vision and Pattern Recog-</text>
<text top="147" left="330" width="42" height="8" font="font13" id="p14_t91" reading_order_no="90" segment_no="7" tag_type="text">nition , 2020.</text>
<text top="157" left="312" width="252" height="8" font="font10" id="p14_t92" reading_order_no="91" segment_no="9" tag_type="text">[40] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only</text>
<text top="165" left="330" width="234" height="9" font="font10" id="p14_t93" reading_order_no="92" segment_no="9" tag_type="text">look once: Unified, real-time object detection,” in IEEE conference</text>
<text top="174" left="330" width="209" height="9" font="font13" id="p14_t94" reading_order_no="93" segment_no="9" tag_type="text">on computer vision and pattern recognition , 2016, pp. 779–788.</text>
<text top="184" left="312" width="128" height="8" font="font10" id="p14_t95" reading_order_no="94" segment_no="10" tag_type="text">[41] R. Caruana, Learning to learn .</text>
<text top="184" left="453" width="111" height="8" font="font10" id="p14_t96" reading_order_no="95" segment_no="10" tag_type="text">Springer, 1998, ch. ”Multitask</text>
<text top="193" left="330" width="78" height="8" font="font10" id="p14_t97" reading_order_no="96" segment_no="10" tag_type="text">learning”, pp. 95–133.</text>
<text top="203" left="312" width="252" height="8" font="font10" id="p14_t98" reading_order_no="97" segment_no="12" tag_type="text">[42] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person</text>
<text top="212" left="330" width="234" height="8" font="font10" id="p14_t99" reading_order_no="98" segment_no="12" tag_type="text">2D pose estimation using part affinity fields,” in IEEE Conference</text>
<text top="221" left="330" width="224" height="8" font="font13" id="p14_t100" reading_order_no="99" segment_no="12" tag_type="text">on Computer Vision and Pattern Recognition , 2017, pp. 7291–7299.</text>
<text top="231" left="312" width="252" height="8" font="font10" id="p14_t101" reading_order_no="100" segment_no="14" tag_type="text">[43] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh, “Convo-</text>
<text top="240" left="330" width="234" height="8" font="font10" id="p14_t102" reading_order_no="101" segment_no="14" tag_type="text">lutional pose machines,” IEEE conference on computer vision and</text>
<text top="249" left="330" width="84" height="8" font="font13" id="p14_t103" reading_order_no="102" segment_no="14" tag_type="text">pattern recognition , 2016.</text>
<text top="259" left="312" width="252" height="8" font="font10" id="p14_t104" reading_order_no="103" segment_no="15" tag_type="text">[44] A.-I. Popa, M. Zanfir, and C. Sminchisescu, “Deep multitask</text>
<text top="268" left="330" width="234" height="8" font="font10" id="p14_t105" reading_order_no="104" segment_no="15" tag_type="text">architecture for integrated 2D and 3D human sensing,” in The IEEE</text>
<text top="277" left="330" width="234" height="8" font="font13" id="p14_t106" reading_order_no="105" segment_no="15" tag_type="text">Conference on Computer Vision and Pattern Recognition (CVPR) , July</text>
<text top="286" left="330" width="18" height="8" font="font10" id="p14_t107" reading_order_no="106" segment_no="15" tag_type="text">2017.</text>
<text top="296" left="312" width="252" height="8" font="font10" id="p14_t108" reading_order_no="107" segment_no="17" tag_type="text">[45] P. Yao, Z. Fang, F. Wu, Y. Feng, and J. Li, “Densebody: Directly</text>
<text top="305" left="330" width="234" height="8" font="font10" id="p14_t109" reading_order_no="108" segment_no="17" tag_type="text">regressing dense 3d human pose and shape from a single color</text>
<text top="314" left="330" width="160" height="8" font="font10" id="p14_t110" reading_order_no="109" segment_no="17" tag_type="text">image,” arXiv preprint arXiv:1903.10153 , 2019.</text>
<text top="324" left="312" width="252" height="8" font="font10" id="p14_t111" reading_order_no="110" segment_no="19" tag_type="text">[46] Z. Cao, G. Hidalgo, T. Simon, S.-E. Wei, and Y. Sheikh, “Open-</text>
<text top="333" left="330" width="234" height="8" font="font10" id="p14_t112" reading_order_no="111" segment_no="19" tag_type="text">Pose: realtime multi-person 2D pose estimation using part affinity</text>
<text top="341" left="330" width="234" height="9" font="font10" id="p14_t113" reading_order_no="112" segment_no="19" tag_type="text">fields,” IEEE conference on computer vision and pattern recognition ,</text>
<text top="351" left="330" width="18" height="8" font="font10" id="p14_t114" reading_order_no="113" segment_no="19" tag_type="text">2018.</text>
<text top="360" left="312" width="252" height="8" font="font10" id="p14_t115" reading_order_no="114" segment_no="21" tag_type="text">[47] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,</text>
<text top="369" left="330" width="234" height="8" font="font10" id="p14_t116" reading_order_no="115" segment_no="21" tag_type="text">S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture</text>
<text top="378" left="330" width="234" height="8" font="font10" id="p14_t117" reading_order_no="116" segment_no="21" tag_type="text">for fast feature embedding,” in Proceedings of the 22nd ACM inter-</text>
<text top="387" left="330" width="115" height="8" font="font13" id="p14_t118" reading_order_no="117" segment_no="21" tag_type="text">national conference on Multimedia .</text>
<text top="387" left="453" width="88" height="8" font="font10" id="p14_t119" reading_order_no="118" segment_no="21" tag_type="text">ACM, 2014, pp. 675–678.</text>
<text top="397" left="312" width="252" height="8" font="font10" id="p14_t120" reading_order_no="119" segment_no="24" tag_type="text">[48] M. D. Zeiler, “Adadelta: an adaptive learning rate method,” arXiv</text>
<text top="406" left="330" width="106" height="8" font="font13" id="p14_t121" reading_order_no="120" segment_no="24" tag_type="text">preprint arXiv:1212.5701 , 2012.</text>
<text top="416" left="312" width="252" height="8" font="font10" id="p14_t122" reading_order_no="121" segment_no="25" tag_type="text">[49] A. Kendall, Y. Gal, and R. Cipolla, “Multi-task learning using</text>
<text top="425" left="330" width="234" height="8" font="font10" id="p14_t123" reading_order_no="122" segment_no="25" tag_type="text">uncertainty to weigh losses for scene geometry and semantics,”</text>
<text top="434" left="330" width="234" height="8" font="font10" id="p14_t124" reading_order_no="123" segment_no="25" tag_type="text">in Proceedings of the IEEE Conference on Computer Vision and Pattern</text>
<text top="443" left="330" width="115" height="8" font="font13" id="p14_t125" reading_order_no="124" segment_no="25" tag_type="text">Recognition , 2018, pp. 7482–7491.</text>
<text top="453" left="312" width="252" height="8" font="font10" id="p14_t126" reading_order_no="125" segment_no="28" tag_type="text">[50] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,</text>
<text top="462" left="330" width="234" height="8" font="font10" id="p14_t127" reading_order_no="126" segment_no="28" tag_type="text">“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Pro-</text>
<text top="471" left="330" width="234" height="8" font="font13" id="p14_t128" reading_order_no="127" segment_no="28" tag_type="text">ceedings of the IEEE conference on computer vision and pattern recogni-</text>
<text top="480" left="330" width="89" height="8" font="font13" id="p14_t129" reading_order_no="128" segment_no="28" tag_type="text">tion , 2018, pp. 4510–4520.</text>
<text top="490" left="312" width="252" height="8" font="font10" id="p14_t130" reading_order_no="129" segment_no="30" tag_type="text">[51] Y. Shi and R. C. Eberhart, “Empirical study of particle swarm</text>
<text top="499" left="330" width="234" height="8" font="font10" id="p14_t131" reading_order_no="130" segment_no="30" tag_type="text">optimization,” in Proceedings of the 1999 Congress on Evolutionary</text>
<text top="508" left="330" width="170" height="8" font="font13" id="p14_t132" reading_order_no="131" segment_no="30" tag_type="text">Computation-CEC99 (Cat. No. 99TH8406) , vol. 3.</text>
<text top="508" left="509" width="55" height="8" font="font10" id="p14_t133" reading_order_no="132" segment_no="30" tag_type="text">IEEE, 1999, pp.</text>
<text top="517" left="330" width="38" height="8" font="font10" id="p14_t134" reading_order_no="133" segment_no="30" tag_type="text">1945–1950.</text>
<text top="526" left="312" width="252" height="8" font="font10" id="p14_t135" reading_order_no="134" segment_no="32" tag_type="text">[52] I. Oikonomidis, N. Kyriazis, and A. A. Argyros, “Efficient model-</text>
<text top="535" left="330" width="234" height="8" font="font10" id="p14_t136" reading_order_no="135" segment_no="32" tag_type="text">based 3D tracking of hand articulations using kinect,” in The</text>
<text top="544" left="330" width="201" height="8" font="font13" id="p14_t137" reading_order_no="136" segment_no="32" tag_type="text">British Machine Vision Conference (BMVC) , 2011, pp. 1–11.</text>
<text top="554" left="312" width="252" height="8" font="font10" id="p14_t138" reading_order_no="137" segment_no="34" tag_type="text">[53] T. von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and</text>
<text top="563" left="330" width="234" height="8" font="font10" id="p14_t139" reading_order_no="138" segment_no="34" tag_type="text">G. Pons-Moll, “Recovering accurate 3d human pose in the wild</text>
<text top="572" left="330" width="234" height="8" font="font10" id="p14_t140" reading_order_no="139" segment_no="34" tag_type="text">using imus and a moving camera,” in Proceedings of the European</text>
<text top="581" left="330" width="194" height="8" font="font13" id="p14_t141" reading_order_no="140" segment_no="34" tag_type="text">Conference on Computer Vision (ECCV) , September 2018.</text>
<text top="591" left="312" width="252" height="8" font="font10" id="p14_t142" reading_order_no="141" segment_no="36" tag_type="text">[54] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, “Human3.</text>
<text top="600" left="330" width="234" height="8" font="font10" id="p14_t143" reading_order_no="142" segment_no="36" tag_type="text">6m: Large scale datasets and predictive methods for 3D human</text>
<text top="609" left="330" width="234" height="8" font="font10" id="p14_t144" reading_order_no="143" segment_no="36" tag_type="text">sensing in natural environments,” IEEE Transactions on Pattern</text>
<text top="618" left="330" width="234" height="8" font="font13" id="p14_t145" reading_order_no="144" segment_no="36" tag_type="text">Analysis and Machine Intelligence , vol. 36, no. 7, pp. 1325–1339, 2013.</text>
<text top="628" left="312" width="252" height="8" font="font10" id="p14_t146" reading_order_no="145" segment_no="38" tag_type="text">[55] Y. Cai, L. Ge, J. Cai, and J. Yuan, “Weakly-supervised 3D hand pose</text>
<text top="637" left="330" width="234" height="8" font="font10" id="p14_t147" reading_order_no="146" segment_no="38" tag_type="text">estimation from monocular RGB images,” in European Conference</text>
<text top="646" left="330" width="163" height="8" font="font13" id="p14_t148" reading_order_no="147" segment_no="38" tag_type="text">on Computer Vision (ECCV) , 2018, pp. 666–682.</text>
<text top="656" left="312" width="252" height="8" font="font10" id="p14_t149" reading_order_no="148" segment_no="40" tag_type="text">[56] M. Loper, N. Mahmood, and M. J. Black, “Mosh: Motion and</text>
<text top="665" left="330" width="234" height="8" font="font10" id="p14_t150" reading_order_no="149" segment_no="40" tag_type="text">shape capture from sparse markers,” ACM Transactions on Graphics</text>
<text top="674" left="330" width="128" height="8" font="font13" id="p14_t151" reading_order_no="150" segment_no="40" tag_type="text">(TOG) , vol. 33, no. 6, pp. 1–13, 2014.</text>
<text top="684" left="312" width="252" height="8" font="font10" id="p14_t152" reading_order_no="151" segment_no="42" tag_type="text">[57] K. Zhou, X. Han, N. Jiang, K. Jia, and J. Lu, “Hemlets pose:</text>
<text top="693" left="330" width="234" height="8" font="font10" id="p14_t153" reading_order_no="152" segment_no="42" tag_type="text">Learning part-centric heatmap triplets for accurate 3d human pose</text>
<text top="702" left="330" width="234" height="8" font="font10" id="p14_t154" reading_order_no="153" segment_no="42" tag_type="text">estimation,” in Proceedings of the IEEE/CVF International Conference</text>
<text top="711" left="330" width="142" height="8" font="font13" id="p14_t155" reading_order_no="154" segment_no="42" tag_type="text">on Computer Vision , 2019, pp. 2344–2353.</text>
<text top="720" left="312" width="252" height="8" font="font10" id="p14_t156" reading_order_no="155" segment_no="45" tag_type="text">[58] H. Joo, N. Neverova, and A. Vedaldi, “Exemplar fine-tuning</text>
<text top="729" left="330" width="234" height="8" font="font10" id="p14_t157" reading_order_no="156" segment_no="45" tag_type="text">for 3d human pose fitting towards in-the-wild 3d human pose</text>
<text top="738" left="330" width="176" height="8" font="font10" id="p14_t158" reading_order_no="157" segment_no="45" tag_type="text">estimation,” arXiv preprint arXiv:2004.03686 , 2020.</text>
</page>
<page number="15" position="absolute" top="0" left="0" height="792" width="612">
<text top="29" left="48" width="334" height="6" font="font0" id="p15_t1" reading_order_no="0" segment_no="0" tag_type="title">IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. XX, NO. , AUGUST 2020</text>
<text top="29" left="556" width="8" height="6" font="font0" id="p15_t2" reading_order_no="1" segment_no="1" tag_type="text">15</text>
<text top="46" left="48" width="252" height="8" font="font10" id="p15_t3" reading_order_no="2" segment_no="2" tag_type="text">[59] Y. Xu, S.-C. Zhu, and T. Tung, “Denserac: Joint 3d pose and shape</text>
<text top="55" left="66" width="234" height="8" font="font10" id="p15_t4" reading_order_no="3" segment_no="2" tag_type="text">estimation by dense render-and-compare,” in Proceedings of the</text>
<text top="64" left="66" width="234" height="8" font="font13" id="p15_t5" reading_order_no="4" segment_no="2" tag_type="text">IEEE International Conference on Computer Vision , 2019, pp. 7760–</text>
<text top="73" left="66" width="18" height="8" font="font10" id="p15_t6" reading_order_no="5" segment_no="2" tag_type="text">7770.</text>
<text top="82" left="48" width="252" height="8" font="font10" id="p15_t7" reading_order_no="6" segment_no="4" tag_type="text">[60] H. Joo, T. Simon, and Y. Sheikh, “Total capture: A 3d deformation</text>
<text top="91" left="66" width="234" height="8" font="font10" id="p15_t8" reading_order_no="7" segment_no="4" tag_type="text">model for tracking faces, hands, and bodies,” in Proceedings of the</text>
<text top="100" left="66" width="234" height="8" font="font13" id="p15_t9" reading_order_no="8" segment_no="4" tag_type="text">IEEE conference on computer vision and pattern recognition , 2018, pp.</text>
<text top="109" left="66" width="38" height="8" font="font10" id="p15_t10" reading_order_no="9" segment_no="4" tag_type="text">8320–8329.</text>
<text top="180" left="130" width="170" height="8" font="font3" id="p15_t11" reading_order_no="10" segment_no="5" tag_type="text">Liguo Jiang received the B.Eng degree in soft-</text>
<text top="189" left="130" width="170" height="7" font="font4" id="p15_t12" reading_order_no="11" segment_no="5" tag_type="text">ware engineering from Chongqing University in</text>
<text top="198" left="130" width="170" height="7" font="font4" id="p15_t13" reading_order_no="12" segment_no="5" tag_type="text">2015. He is currently working toward the PhD</text>
<text top="207" left="130" width="170" height="7" font="font4" id="p15_t14" reading_order_no="13" segment_no="5" tag_type="text">degree in National Laboratory of Pattern Recog-</text>
<text top="216" left="130" width="170" height="7" font="font4" id="p15_t15" reading_order_no="14" segment_no="5" tag_type="text">nition, Institute of Automation, Chinese Academy<b>Liguo Jiang</b></text>
<text top="225" left="130" width="170" height="7" font="font4" id="p15_t16" reading_order_no="15" segment_no="5" tag_type="text">of Sciences. His research interests include deep</text>
<text top="234" left="130" width="170" height="7" font="font4" id="p15_t17" reading_order_no="16" segment_no="5" tag_type="text">learning, human motion capture and cloth simu-</text>
<text top="243" left="130" width="21" height="7" font="font4" id="p15_t18" reading_order_no="17" segment_no="5" tag_type="text">lation.</text>
<text top="330" left="130" width="170" height="8" font="font3" id="p15_t19" reading_order_no="18" segment_no="7" tag_type="text">Miaopeng Li is a Ph.D. student at the State Key</text>
<text top="339" left="130" width="170" height="7" font="font4" id="p15_t20" reading_order_no="19" segment_no="7" tag_type="text">Lab of CAD&amp;CG, Zhejiang University, China.</text>
<text top="348" left="130" width="170" height="7" font="font4" id="p15_t21" reading_order_no="20" segment_no="7" tag_type="text">She received her bachelor degree from North-</text>
<text top="357" left="130" width="170" height="7" font="font4" id="p15_t22" reading_order_no="21" segment_no="7" tag_type="text">western Polytechnical University in 2016. Her</text>
<text top="366" left="130" width="170" height="7" font="font4" id="p15_t23" reading_order_no="22" segment_no="7" tag_type="text">research interests include marker-less human</text>
<text top="375" left="130" width="170" height="7" font="font4" id="p15_t24" reading_order_no="23" segment_no="7" tag_type="text">motion capture, human pose estimation, 3D re-<b>Miaopeng Li</b></text>
<text top="384" left="130" width="123" height="7" font="font4" id="p15_t25" reading_order_no="24" segment_no="7" tag_type="text">construction and their applications.</text>
<text top="482" left="130" width="170" height="8" font="font3" id="p15_t26" reading_order_no="25" segment_no="8" tag_type="text">Jianjie Zhang received PhD degree in computer</text>
<text top="491" left="130" width="170" height="7" font="font4" id="p15_t27" reading_order_no="26" segment_no="8" tag_type="text">science from Texas A&amp;M Univeristy (TAMU). He</text>
<text top="500" left="130" width="170" height="7" font="font4" id="p15_t28" reading_order_no="27" segment_no="8" tag_type="text">is currently a R&amp;D director in Xmov ai Inc. His</text>
<text top="509" left="130" width="170" height="7" font="font4" id="p15_t29" reading_order_no="28" segment_no="8" tag_type="text">primary research is in the area of computer</text>
<text top="518" left="130" width="170" height="7" font="font4" id="p15_t30" reading_order_no="29" segment_no="8" tag_type="text">graphics and vision, including human body mod-</text>
<text top="527" left="130" width="170" height="7" font="font4" id="p15_t31" reading_order_no="30" segment_no="8" tag_type="text">eling and tracking, human body dynamics sim-</text>
<text top="536" left="130" width="170" height="7" font="font4" id="p15_t32" reading_order_no="31" segment_no="8" tag_type="text">ulation, human face modeling and tracking and<b>Jianjie Zhang</b></text>
<text top="545" left="130" width="13" height="7" font="font4" id="p15_t33" reading_order_no="32" segment_no="8" tag_type="text">etc.</text>
<text top="632" left="130" width="170" height="8" font="font3" id="p15_t34" reading_order_no="33" segment_no="10" tag_type="text">Congyi Wang received the PhD degree in com-</text>
<text top="641" left="130" width="170" height="7" font="font4" id="p15_t35" reading_order_no="34" segment_no="10" tag_type="text">puter science from Institute of Computing Tech-</text>
<text top="650" left="130" width="170" height="7" font="font4" id="p15_t36" reading_order_no="35" segment_no="10" tag_type="text">nology, Chinese Academy of Sciences in Jan</text>
<text top="659" left="130" width="170" height="7" font="font4" id="p15_t37" reading_order_no="36" segment_no="10" tag_type="text">2017. Since 2018, he has been a research sci-</text>
<text top="668" left="130" width="170" height="7" font="font4" id="p15_t38" reading_order_no="37" segment_no="10" tag_type="text">entist at XMov, a startup company aiming at</text>
<text top="677" left="130" width="170" height="7" font="font4" id="p15_t39" reading_order_no="38" segment_no="10" tag_type="text">AI powered virtual production line. His research</text>
<text top="686" left="130" width="170" height="7" font="font4" id="p15_t40" reading_order_no="39" segment_no="10" tag_type="text">interests include computer animation, computer</text>
<text top="695" left="130" width="170" height="7" font="font4" id="p15_t41" reading_order_no="40" segment_no="10" tag_type="text">graphics, computer vision and speech signal<b>Congyi Wang</b></text>
<text top="704" left="130" width="41" height="7" font="font4" id="p15_t42" reading_order_no="41" segment_no="10" tag_type="text">processing.</text>
<text top="46" left="394" width="170" height="8" font="font3" id="p15_t43" reading_order_no="42" segment_no="3" tag_type="text">Juntao Ye was awarded his B.Eng from Harbin</text>
<text top="55" left="394" width="170" height="7" font="font4" id="p15_t44" reading_order_no="43" segment_no="3" tag_type="text">Engineering University in 1994, MSc from Insti-</text>
<text top="64" left="394" width="170" height="7" font="font4" id="p15_t45" reading_order_no="44" segment_no="3" tag_type="text">tute of Computational Mathematics and Scien-</text>
<text top="73" left="394" width="170" height="7" font="font4" id="p15_t46" reading_order_no="45" segment_no="3" tag_type="text">tific/Engineering Computing, Chinese Academy</text>
<text top="82" left="394" width="170" height="7" font="font4" id="p15_t47" reading_order_no="46" segment_no="3" tag_type="text">of Sciences in 2000, and his PhD in Computer</text>
<text top="91" left="394" width="170" height="7" font="font4" id="p15_t48" reading_order_no="47" segment_no="3" tag_type="text">Science from The University of Western Ontario,</text>
<text top="100" left="394" width="170" height="7" font="font4" id="p15_t49" reading_order_no="48" segment_no="3" tag_type="text">Canada, in 2005. He is currently an associate</text>
<text top="109" left="394" width="170" height="7" font="font4" id="p15_t50" reading_order_no="49" segment_no="3" tag_type="text">professor with National Laboratory of Pattern</text>
<text top="118" left="394" width="170" height="7" font="font4" id="p15_t51" reading_order_no="50" segment_no="3" tag_type="text">Recognition of the Institute of Automation, Chi-<b>Juntao Ye</b></text>
<text top="127" left="394" width="170" height="7" font="font4" id="p15_t52" reading_order_no="51" segment_no="3" tag_type="text">nese Academy of Sciences. His research in-</text>
<text top="136" left="394" width="170" height="7" font="font4" id="p15_t53" reading_order_no="52" segment_no="3" tag_type="text">terests include graphics, particularly physically-</text>
<text top="145" left="312" width="248" height="7" font="font4" id="p15_t54" reading_order_no="53" segment_no="3" tag_type="text">based simulation of cloth and fluid, as well as image/video processing.</text>
<text top="295" left="394" width="170" height="8" font="font3" id="p15_t55" reading_order_no="54" segment_no="6" tag_type="text">Xinguo Liu received the BS and PhD degrees</text>
<text top="304" left="394" width="170" height="7" font="font4" id="p15_t56" reading_order_no="55" segment_no="6" tag_type="text">in applied mathematics from Zhejiang University</text>
<text top="313" left="394" width="170" height="7" font="font4" id="p15_t57" reading_order_no="56" segment_no="6" tag_type="text">in 1995 and 2001, respectively. He is a pro-</text>
<text top="322" left="394" width="170" height="7" font="font4" id="p15_t58" reading_order_no="57" segment_no="6" tag_type="text">fessor at the School of Computer Science and</text>
<text top="331" left="394" width="170" height="7" font="font4" id="p15_t59" reading_order_no="58" segment_no="6" tag_type="text">Technology, Zhejiang University. He was with</text>
<text top="340" left="394" width="170" height="7" font="font4" id="p15_t60" reading_order_no="59" segment_no="6" tag_type="text">Microsoft Research Asia in Beijing during 2001-</text>
<text top="349" left="394" width="170" height="7" font="font4" id="p15_t61" reading_order_no="60" segment_no="6" tag_type="text">2006, and then joined in Zhejiang University. His</text>
<text top="358" left="394" width="170" height="7" font="font4" id="p15_t62" reading_order_no="61" segment_no="6" tag_type="text">main research interests are in graphics and vi-</text>
<text top="367" left="394" width="170" height="7" font="font4" id="p15_t63" reading_order_no="62" segment_no="6" tag_type="text">sion, particularly geometry processing, realistic</text>
<text top="376" left="394" width="170" height="7" font="font4" id="p15_t64" reading_order_no="63" segment_no="6" tag_type="text">and image-based rendering, and 3D reconstruc-<b>Xinguo Liu</b></text>
<text top="385" left="394" width="15" height="7" font="font4" id="p15_t65" reading_order_no="64" segment_no="6" tag_type="text">tion.</text>
<text top="535" left="394" width="170" height="8" font="font3" id="p15_t66" reading_order_no="65" segment_no="9" tag_type="text">Jinxiang Chai received PhD degree in com-</text>
<text top="544" left="394" width="170" height="7" font="font4" id="p15_t67" reading_order_no="66" segment_no="9" tag_type="text">puter science from Carnegie Mellon Univer-</text>
<text top="553" left="394" width="170" height="7" font="font4" id="p15_t68" reading_order_no="67" segment_no="9" tag_type="text">sity(CMU). He is currently an associate profes-</text>
<text top="562" left="394" width="170" height="7" font="font4" id="p15_t69" reading_order_no="68" segment_no="9" tag_type="text">sor in the Department of Computer Science and</text>
<text top="571" left="394" width="170" height="7" font="font4" id="p15_t70" reading_order_no="69" segment_no="9" tag_type="text">Engineering at Texas A&amp;M University. His pri-</text>
<text top="580" left="394" width="170" height="7" font="font4" id="p15_t71" reading_order_no="70" segment_no="9" tag_type="text">mary research is in the area of computer graph-</text>
<text top="589" left="394" width="170" height="7" font="font4" id="p15_t72" reading_order_no="71" segment_no="9" tag_type="text">ics and vision with broad applications in other</text>
<text top="598" left="394" width="170" height="7" font="font4" id="p15_t73" reading_order_no="72" segment_no="9" tag_type="text">disciplines such as virtual and augmented re-</text>
<text top="607" left="394" width="170" height="7" font="font4" id="p15_t74" reading_order_no="73" segment_no="9" tag_type="text">ality, robotics, human computer interaction, and</text>
<text top="616" left="394" width="170" height="7" font="font4" id="p15_t75" reading_order_no="74" segment_no="9" tag_type="text">biomechanics. He received an NSF CAREER</text>
<text top="625" left="394" width="170" height="7" font="font4" id="p15_t76" reading_order_no="75" segment_no="9" tag_type="text">award for his work on theory and practice of<b>Jinxiang Chai</b></text>
<text top="634" left="312" width="96" height="7" font="font4" id="p15_t77" reading_order_no="76" segment_no="9" tag_type="text">Bayesian motion synthesis.</text>
</page>
</pdf2xml>
