<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="7" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="24" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font2" size="11" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="9" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font4" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font5" size="9" family="CMMI9" color="#000000"/>
	<fontspec id="font6" size="6" family="CMR6" color="#000000"/>
	<fontspec id="font7" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font8" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font9" size="29" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font10" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font11" size="20" family="Times" color="#7f7f7f"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="text">1</text>
<text top="61" left="60" width="492" height="21" font="font1" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="title">Detecting Backdoor in Deep Neural Networks via</text>
<text top="89" left="126" width="359" height="21" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="title">Intentional Adversarial Perturbations</text>
<text top="121" left="118" width="376" height="10" font="font2" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="text">Mingfu Xue, Yinghao Wu, Zhiyu Wu, Yushu Zhang, Jian Wang, and Weiqiang Liu</text>
<text top="178" left="59" width="241" height="8" font="font3" id="p1_t5" reading_order_no="5" segment_no="3" tag_type="text">Abstract —Recently, the security of deep learning systems has</text>
<text top="188" left="49" width="251" height="8" font="font4" id="p1_t6" reading_order_no="6" segment_no="3" tag_type="text">attracted a lot of attentions, especially when applied to safety-</text>
<text top="198" left="49" width="251" height="8" font="font4" id="p1_t7" reading_order_no="7" segment_no="3" tag_type="text">critical tasks, such as autonomous driving, face recognition,</text>
<text top="208" left="49" width="251" height="8" font="font4" id="p1_t8" reading_order_no="8" segment_no="3" tag_type="text">malware classification, etc. Recent researches show that deep</text>
<text top="218" left="49" width="251" height="8" font="font4" id="p1_t9" reading_order_no="9" segment_no="3" tag_type="text">learning model is susceptible to backdoor attacks where the</text>
<text top="228" left="49" width="251" height="8" font="font4" id="p1_t10" reading_order_no="10" segment_no="3" tag_type="text">backdoor embedded in the model will be triggered when a back-</text>
<text top="238" left="49" width="251" height="8" font="font4" id="p1_t11" reading_order_no="11" segment_no="3" tag_type="text">door instance arrives. Many defenses against backdoor attacks</text>
<text top="248" left="49" width="251" height="8" font="font4" id="p1_t12" reading_order_no="12" segment_no="3" tag_type="text">have been proposed. However, existing defense works require</text>
<text top="258" left="49" width="251" height="8" font="font4" id="p1_t13" reading_order_no="13" segment_no="3" tag_type="text">high computational overhead or backdoor attack information</text>
<text top="268" left="49" width="251" height="8" font="font4" id="p1_t14" reading_order_no="14" segment_no="3" tag_type="text">such as the trigger size, which is difficult to satisfy in realistic</text>
<text top="278" left="49" width="251" height="8" font="font4" id="p1_t15" reading_order_no="15" segment_no="3" tag_type="text">scenarios. In this paper, a novel backdoor detection method</text>
<text top="288" left="49" width="251" height="8" font="font4" id="p1_t16" reading_order_no="16" segment_no="3" tag_type="text">based on adversarial examples is proposed. The proposed method</text>
<text top="298" left="49" width="251" height="8" font="font4" id="p1_t17" reading_order_no="17" segment_no="3" tag_type="text">leverages intentional adversarial perturbations to detect whether</text>
<text top="308" left="49" width="251" height="8" font="font4" id="p1_t18" reading_order_no="18" segment_no="3" tag_type="text">an image contains a trigger, which can be applied in both the</text>
<text top="318" left="49" width="251" height="8" font="font4" id="p1_t19" reading_order_no="19" segment_no="3" tag_type="text">training stage and the inference stage (sanitize the training set</text>
<text top="328" left="49" width="251" height="8" font="font4" id="p1_t20" reading_order_no="20" segment_no="3" tag_type="text">in training stage and detect the backdoor instances in inference</text>
<text top="338" left="49" width="251" height="8" font="font4" id="p1_t21" reading_order_no="21" segment_no="3" tag_type="text">stage). Specifically, given an untrusted image, the adversarial</text>
<text top="348" left="49" width="251" height="8" font="font4" id="p1_t22" reading_order_no="22" segment_no="3" tag_type="text">perturbation is added to the image intentionally. If the prediction</text>
<text top="358" left="49" width="251" height="8" font="font4" id="p1_t23" reading_order_no="23" segment_no="3" tag_type="text">of the model on the perturbed image is consistent with that on</text>
<text top="368" left="49" width="251" height="8" font="font4" id="p1_t24" reading_order_no="24" segment_no="3" tag_type="text">the unperturbed image, the input image will be considered as a</text>
<text top="378" left="49" width="251" height="8" font="font4" id="p1_t25" reading_order_no="25" segment_no="3" tag_type="text">backdoor instance. Compared with most existing defense works,</text>
<text top="388" left="49" width="251" height="8" font="font4" id="p1_t26" reading_order_no="26" segment_no="3" tag_type="text">the proposed adversarial perturbation based method requires</text>
<text top="398" left="49" width="251" height="8" font="font4" id="p1_t27" reading_order_no="27" segment_no="3" tag_type="text">low computational resources and maintains the visual quality</text>
<text top="408" left="49" width="251" height="8" font="font4" id="p1_t28" reading_order_no="28" segment_no="3" tag_type="text">of the images. Experimental results show that, the backdoor</text>
<text top="418" left="49" width="251" height="8" font="font4" id="p1_t29" reading_order_no="29" segment_no="3" tag_type="text">detection rate of the proposed defense method is 99.63%, 99.76%</text>
<text top="428" left="49" width="251" height="8" font="font4" id="p1_t30" reading_order_no="30" segment_no="3" tag_type="text">and 99.91% on Fashion-MNIST, CIFAR-10 and GTSRB datasets,</text>
<text top="437" left="49" width="251" height="8" font="font4" id="p1_t31" reading_order_no="31" segment_no="3" tag_type="text">respectively. Besides, the proposed method maintains the visual</text>
<text top="447" left="49" width="251" height="8" font="font4" id="p1_t32" reading_order_no="32" segment_no="3" tag_type="text">quality of the image as the ` 2 norm of the added perturbation</text>
<text top="457" left="49" width="251" height="8" font="font4" id="p1_t33" reading_order_no="33" segment_no="3" tag_type="text">are as low as 2.8715, 3.0513 and 2.4362 on Fashion-MNIST,</text>
<text top="467" left="49" width="251" height="8" font="font4" id="p1_t34" reading_order_no="34" segment_no="3" tag_type="text">CIFAR-10 and GTSRB datasets, respectively. In addition, it is</text>
<text top="477" left="49" width="251" height="8" font="font4" id="p1_t35" reading_order_no="35" segment_no="3" tag_type="text">also demonstrated that the proposed method can achieve high</text>
<text top="487" left="49" width="251" height="8" font="font4" id="p1_t36" reading_order_no="36" segment_no="3" tag_type="text">defense performance against backdoor attacks under different</text>
<text top="497" left="49" width="251" height="8" font="font4" id="p1_t37" reading_order_no="37" segment_no="3" tag_type="text">attack settings (trigger transparency, trigger size and trigger</text>
<text top="507" left="49" width="251" height="8" font="font4" id="p1_t38" reading_order_no="38" segment_no="3" tag_type="text">pattern). Compared with the existing defense work (STRIP), the</text>
<text top="517" left="49" width="251" height="8" font="font4" id="p1_t39" reading_order_no="39" segment_no="3" tag_type="text">proposed method has better detection performance on all the</text>
<text top="527" left="49" width="192" height="8" font="font4" id="p1_t40" reading_order_no="40" segment_no="3" tag_type="text">three datasets, and is more efficient than STRIP.</text>
<text top="543" left="59" width="241" height="8" font="font3" id="p1_t41" reading_order_no="41" segment_no="9" tag_type="text">Index Terms —Backdoor attacks, Deep neural networks, Back-</text>
<text top="553" left="49" width="185" height="8" font="font4" id="p1_t42" reading_order_no="42" segment_no="9" tag_type="text">door detection, Defenses, Adversarial examples</text>
<text top="583" left="136" width="77" height="9" font="font7" id="p1_t43" reading_order_no="43" segment_no="11" tag_type="title">I. I NTRODUCTION</text>
<text top="599" left="49" width="21" height="26" font="font9" id="p1_t44" reading_order_no="44" segment_no="12" tag_type="text">R</text>
<text top="600" left="72" width="228" height="9" font="font7" id="p1_t45" reading_order_no="45" segment_no="12" tag_type="text">ECENT studies show that deep learning models are</text>
<text top="612" left="72" width="228" height="9" font="font7" id="p1_t46" reading_order_no="46" segment_no="12" tag_type="text">vulnerable to backdoor attacks [1]–[3]. Adversaries can</text>
<text top="623" left="49" width="251" height="9" font="font7" id="p1_t47" reading_order_no="47" segment_no="12" tag_type="text">embed the backdoor into deep learning model by modifying</text>
<text top="635" left="49" width="251" height="9" font="font7" id="p1_t48" reading_order_no="48" segment_no="12" tag_type="text">the architectures or parameters of the model, or injecting</text>
<text top="647" left="49" width="251" height="9" font="font7" id="p1_t49" reading_order_no="49" segment_no="12" tag_type="text">backdoor instances in the training set to embed the backdoor</text>
<text top="669" left="57" width="243" height="7" font="font8" id="p1_t50" reading_order_no="98" segment_no="13" tag_type="footnote">M. Xue, Y. Wu, Y. Zhang and J. Wang are with the College of</text>
<text top="678" left="49" width="251" height="7" font="font8" id="p1_t51" reading_order_no="99" segment_no="13" tag_type="footnote">Computer Science and Technology, Nanjing University of Aeronautics and</text>
<text top="687" left="49" width="251" height="7" font="font8" id="p1_t52" reading_order_no="100" segment_no="13" tag_type="footnote">Astronautics, Nanjing, 211106, China (e-mail: mingfu.xue@nuaa.edu.cn;<a href="deeplearning_paper26.html#9">[1]–[3]. </a>Adversaries can</text>
<text top="696" left="49" width="213" height="7" font="font8" id="p1_t53" reading_order_no="101" segment_no="13" tag_type="footnote">wyh@nuaa.edu.cn; yushu@nuaa.edu.cn; wangjian@nuaa.edu.cn).</text>
<text top="705" left="57" width="243" height="7" font="font8" id="p1_t54" reading_order_no="102" segment_no="14" tag_type="footnote">Z. Wu is with the College of Science, Nanjing University of Aeronautics</text>
<text top="714" left="49" width="244" height="7" font="font8" id="p1_t55" reading_order_no="103" segment_no="14" tag_type="footnote">and Astronautics, Nanjing, 211106, China (e-mail: wuzhiyu@nuaa.edu.cn)</text>
<text top="723" left="57" width="243" height="7" font="font8" id="p1_t56" reading_order_no="104" segment_no="15" tag_type="footnote">W. Liu is with the College of Electronic and Information Engineering,</text>
<text top="732" left="49" width="251" height="7" font="font8" id="p1_t57" reading_order_no="105" segment_no="15" tag_type="footnote">Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China</text>
<text top="741" left="49" width="116" height="7" font="font8" id="p1_t58" reading_order_no="106" segment_no="15" tag_type="footnote">(e-mail: liuweiqiang@nuaa.edu.cn).</text>
<text top="178" left="312" width="251" height="9" font="font7" id="p1_t59" reading_order_no="50" segment_no="4" tag_type="text">during training [1]–[3]. The backdoored model will behave</text>
<text top="190" left="312" width="251" height="9" font="font7" id="p1_t60" reading_order_no="51" segment_no="4" tag_type="text">normally for the benign inputs, but it will output the target</text>
<text top="202" left="312" width="186" height="9" font="font7" id="p1_t61" reading_order_no="52" segment_no="4" tag_type="text">label for the input image carrying the trigger.</text>
<text top="213" left="322" width="241" height="9" font="font7" id="p1_t62" reading_order_no="53" segment_no="5" tag_type="text">Many defenses against backdoor attacks have been pro-</text>
<text top="225" left="312" width="251" height="9" font="font7" id="p1_t63" reading_order_no="54" segment_no="5" tag_type="text">posed. However, the existing defense works require high com-</text>
<text top="237" left="312" width="251" height="9" font="font7" id="p1_t64" reading_order_no="55" segment_no="5" tag_type="text">putational overhead [4]–[6], a large number of clean images</text>
<text top="249" left="312" width="251" height="9" font="font7" id="p1_t65" reading_order_no="56" segment_no="5" tag_type="text">to retrain the model [7], or backdoor attack information such<a href="deeplearning_paper26.html#9">[1]–[3]. </a>The backdoored model will behave</text>
<text top="261" left="312" width="251" height="9" font="font7" id="p1_t66" reading_order_no="57" segment_no="5" tag_type="text">as the trigger size [5], [8]. In practice, these requirements are</text>
<text top="273" left="312" width="94" height="9" font="font7" id="p1_t67" reading_order_no="58" segment_no="5" tag_type="text">difficult to be satisfied.</text>
<text top="285" left="322" width="241" height="9" font="font7" id="p1_t68" reading_order_no="59" segment_no="6" tag_type="text">In this paper, we propose a novel backdoor detection</text>
<text top="297" left="312" width="251" height="9" font="font7" id="p1_t69" reading_order_no="60" segment_no="6" tag_type="text">method based on adversarial examples, which only requires</text>
<text top="309" left="312" width="251" height="9" font="font7" id="p1_t70" reading_order_no="61" segment_no="6" tag_type="text">low computational overhead. The proposed method can be<a href="deeplearning_paper26.html#9">[4]–[6], </a>a large number of clean images</text>
<text top="321" left="312" width="251" height="9" font="font7" id="p1_t71" reading_order_no="62" segment_no="6" tag_type="text">applied in both the training stage and the inference stage.<a href="deeplearning_paper26.html#9">[7], </a>or backdoor attack information such</text>
<text top="333" left="312" width="251" height="9" font="font7" id="p1_t72" reading_order_no="63" segment_no="6" tag_type="text">In the training stage, the proposed method can detect and<a href="deeplearning_paper26.html#9">[5], [8]. </a>In practice, these requirements are</text>
<text top="345" left="312" width="251" height="9" font="font7" id="p1_t73" reading_order_no="64" segment_no="6" tag_type="text">remove the backdoor instances in the training dataset. In the</text>
<text top="357" left="312" width="251" height="9" font="font7" id="p1_t74" reading_order_no="65" segment_no="6" tag_type="text">inference stage, the proposed method can determine whether</text>
<text top="369" left="312" width="251" height="9" font="font7" id="p1_t75" reading_order_no="66" segment_no="6" tag_type="text">an input image contains a trigger. Specifically, the proposed</text>
<text top="380" left="312" width="251" height="9" font="font7" id="p1_t76" reading_order_no="67" segment_no="6" tag_type="text">method works as follows. First, the adversarial perturbation</text>
<text top="392" left="312" width="251" height="9" font="font7" id="p1_t77" reading_order_no="68" segment_no="6" tag_type="text">is generated based on the untrusted model with a small set</text>
<text top="404" left="312" width="251" height="9" font="font7" id="p1_t78" reading_order_no="69" segment_no="6" tag_type="text">of clean images. Second, for an image (training image in</text>
<text top="416" left="312" width="251" height="9" font="font7" id="p1_t79" reading_order_no="70" segment_no="6" tag_type="text">the training stage or input image in the inference stage), the</text>
<text top="428" left="312" width="251" height="9" font="font7" id="p1_t80" reading_order_no="71" segment_no="6" tag_type="text">adversarial perturbation will be added on it. If the prediction of</text>
<text top="440" left="312" width="251" height="9" font="font7" id="p1_t81" reading_order_no="72" segment_no="6" tag_type="text">the model on the perturbed image is inconsistent with that on</text>
<text top="452" left="312" width="251" height="9" font="font7" id="p1_t82" reading_order_no="73" segment_no="6" tag_type="text">the unperturbed image, the image is considered to be a clean</text>
<text top="464" left="312" width="251" height="9" font="font7" id="p1_t83" reading_order_no="74" segment_no="6" tag_type="text">image. Otherwise, the image is considered to be a backdoor</text>
<text top="476" left="312" width="251" height="9" font="font7" id="p1_t84" reading_order_no="75" segment_no="6" tag_type="text">instance, which also implies that the model is backdoored and</text>
<text top="488" left="312" width="208" height="9" font="font7" id="p1_t85" reading_order_no="76" segment_no="6" tag_type="text">the predicted label of the image is the target label.</text>
<text top="500" left="322" width="241" height="9" font="font7" id="p1_t86" reading_order_no="77" segment_no="7" tag_type="text">The contributions of this paper are summarized as follows:</text>
<text top="513" left="322" width="241" height="9" font="font7" id="p1_t87" reading_order_no="78" segment_no="8" tag_type="text">• This paper proposes a novel backdoor detection method</text>
<text top="525" left="332" width="231" height="9" font="font7" id="p1_t88" reading_order_no="79" segment_no="8" tag_type="text">based on intentional adversarial perturbation. The adver-</text>
<text top="537" left="332" width="231" height="9" font="font7" id="p1_t89" reading_order_no="80" segment_no="8" tag_type="text">sarial perturbation can fool the deep learning model, mak-</text>
<text top="549" left="332" width="231" height="9" font="font7" id="p1_t90" reading_order_no="81" segment_no="8" tag_type="text">ing the model misclassify the perturbed image. However,</text>
<text top="560" left="332" width="231" height="9" font="font7" id="p1_t91" reading_order_no="82" segment_no="8" tag_type="text">for the backdoor instances, the model will always classify</text>
<text top="572" left="332" width="231" height="9" font="font7" id="p1_t92" reading_order_no="83" segment_no="8" tag_type="text">them as the target class even if these backdoor instances</text>
<text top="584" left="332" width="231" height="9" font="font7" id="p1_t93" reading_order_no="84" segment_no="8" tag_type="text">are added with adversarial perturbation. In this way,</text>
<text top="596" left="332" width="231" height="9" font="font7" id="p1_t94" reading_order_no="85" segment_no="8" tag_type="text">the backdoor instances can be detected via intentional</text>
<text top="608" left="332" width="231" height="9" font="font7" id="p1_t95" reading_order_no="86" segment_no="8" tag_type="text">adversarial perturbations. Moreover, the proposed method</text>
<text top="620" left="332" width="231" height="9" font="font7" id="p1_t96" reading_order_no="87" segment_no="8" tag_type="text">can be deployed in both the training stages and the</text>
<text top="632" left="332" width="231" height="9" font="font7" id="p1_t97" reading_order_no="88" segment_no="8" tag_type="text">inference stage. In the training stage, for a training image,</text>
<text top="644" left="332" width="231" height="9" font="font7" id="p1_t98" reading_order_no="89" segment_no="8" tag_type="text">the intentional adversarial perturbation will be added on</text>
<text top="656" left="332" width="231" height="9" font="font7" id="p1_t99" reading_order_no="90" segment_no="8" tag_type="text">it. If the model’s prediction on the perturbed training</text>
<text top="668" left="332" width="231" height="9" font="font7" id="p1_t100" reading_order_no="91" segment_no="8" tag_type="text">image is consistent with the prediction on the unperturbed</text>
<text top="680" left="332" width="231" height="9" font="font7" id="p1_t101" reading_order_no="92" segment_no="8" tag_type="text">training image, the training image will be considered as a</text>
<text top="692" left="332" width="231" height="9" font="font7" id="p1_t102" reading_order_no="93" segment_no="8" tag_type="text">backdoor instance and then be removed from the training</text>
<text top="704" left="332" width="231" height="9" font="font7" id="p1_t103" reading_order_no="94" segment_no="8" tag_type="text">dataset. In the inference stage, for an input image, the</text>
<text top="716" left="332" width="231" height="9" font="font7" id="p1_t104" reading_order_no="95" segment_no="8" tag_type="text">adversarial perturbation is added on it. If the model’s</text>
<text top="728" left="332" width="231" height="9" font="font7" id="p1_t105" reading_order_no="96" segment_no="8" tag_type="text">prediction on the perturbed image is consistent with the</text>
<text top="740" left="332" width="231" height="9" font="font7" id="p1_t106" reading_order_no="97" segment_no="8" tag_type="text">prediction on the unperturbed image, the input image will</text>
<text top="546" left="32" width="0" height="18" font="font11" id="p1_t107" reading_order_no="0" segment_no="10" tag_type="title">arXiv:2105.14259v2  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font12" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="text">2</text>
<text top="58" left="69" width="155" height="9" font="font7" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">be considered as a backdoor instance.</text>
<text top="70" left="59" width="241" height="9" font="font7" id="p2_t3" reading_order_no="2" segment_no="3" tag_type="text">• In comparison with the work [7] which requires a large</text>
<text top="82" left="69" width="231" height="9" font="font7" id="p2_t4" reading_order_no="3" segment_no="3" tag_type="text">number of clean images to retrain the model to remove<a href="deeplearning_paper26.html#9">[7] </a>which requires a large</text>
<text top="94" left="69" width="231" height="9" font="font7" id="p2_t5" reading_order_no="4" segment_no="3" tag_type="text">the backdoor, the proposed method only requires a small</text>
<text top="106" left="69" width="231" height="9" font="font7" id="p2_t6" reading_order_no="5" segment_no="3" tag_type="text">set of clean images to generate adversarial perturbation.</text>
<text top="118" left="69" width="231" height="9" font="font7" id="p2_t7" reading_order_no="6" segment_no="3" tag_type="text">Besides, the existing work [4] requires training a large</text>
<text top="130" left="69" width="231" height="9" font="font7" id="p2_t8" reading_order_no="7" segment_no="3" tag_type="text">number of backdoored models and clean models, which<a href="deeplearning_paper26.html#9">[4] </a>requires training a large</text>
<text top="142" left="69" width="231" height="9" font="font7" id="p2_t9" reading_order_no="8" segment_no="3" tag_type="text">is computationally expensive. In contrast, the proposed</text>
<text top="154" left="69" width="231" height="9" font="font7" id="p2_t10" reading_order_no="9" segment_no="3" tag_type="text">method only needs to generate the adversarial perturba-</text>
<text top="166" left="69" width="231" height="9" font="font7" id="p2_t11" reading_order_no="10" segment_no="3" tag_type="text">tion with negligible computational overhead. Moreover,</text>
<text top="178" left="69" width="231" height="9" font="font7" id="p2_t12" reading_order_no="11" segment_no="3" tag_type="text">the proposed method does not need any backdoor attack</text>
<text top="190" left="69" width="231" height="9" font="font7" id="p2_t13" reading_order_no="12" segment_no="3" tag_type="text">information, which makes the proposed method more</text>
<text top="202" left="69" width="221" height="9" font="font7" id="p2_t14" reading_order_no="13" segment_no="3" tag_type="text">practical and feasible than the existing works [5], [8].</text>
<text top="214" left="59" width="241" height="9" font="font7" id="p2_t15" reading_order_no="14" segment_no="5" tag_type="text">• Experimental results show that the proposed defense<a href="deeplearning_paper26.html#9">[5], [8].</a></text>
<text top="226" left="69" width="231" height="9" font="font7" id="p2_t16" reading_order_no="15" segment_no="5" tag_type="text">method can achieve high backdoor detection rate (99.63%</text>
<text top="238" left="69" width="231" height="9" font="font7" id="p2_t17" reading_order_no="16" segment_no="5" tag_type="text">99.76% and 99.91% on Fashion-MNIST [9], CIFAR-10</text>
<text top="250" left="69" width="231" height="9" font="font7" id="p2_t18" reading_order_no="17" segment_no="5" tag_type="text">[10] and GTSRB [11] datasets, respectively). It is also</text>
<text top="262" left="69" width="231" height="9" font="font7" id="p2_t19" reading_order_no="18" segment_no="5" tag_type="text">demonstrated that, under different attack settings (differ-<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10</text>
<text top="274" left="69" width="231" height="9" font="font7" id="p2_t20" reading_order_no="19" segment_no="5" tag_type="text">ent trigger transparency, different trigger sizes and dif-<a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively). It is also</text>
<text top="285" left="69" width="231" height="9" font="font7" id="p2_t21" reading_order_no="20" segment_no="5" tag_type="text">ferent trigger patterns), the proposed method can achieve</text>
<text top="297" left="69" width="231" height="9" font="font7" id="p2_t22" reading_order_no="21" segment_no="5" tag_type="text">high defense performance, as the backdoor detection rate</text>
<text top="309" left="69" width="231" height="9" font="font7" id="p2_t23" reading_order_no="22" segment_no="5" tag_type="text">of the proposed approach is as high as 98.80%, 99.70%</text>
<text top="321" left="69" width="231" height="9" font="font7" id="p2_t24" reading_order_no="23" segment_no="5" tag_type="text">and 99.96% on Fashion-MNIST, CIFAR-10 and GTSRB</text>
<text top="333" left="69" width="231" height="9" font="font7" id="p2_t25" reading_order_no="24" segment_no="5" tag_type="text">datasets, respectively. Compared with STRIP [12], the</text>
<text top="345" left="69" width="231" height="9" font="font7" id="p2_t26" reading_order_no="25" segment_no="5" tag_type="text">proposed method achieves higher backdoor detection rate</text>
<text top="357" left="69" width="231" height="9" font="font7" id="p2_t27" reading_order_no="26" segment_no="5" tag_type="text">on all the three datasets. The advantages over STRIP [12]<a href="deeplearning_paper26.html#9">[12], </a>the</text>
<text top="369" left="69" width="231" height="9" font="font7" id="p2_t28" reading_order_no="27" segment_no="5" tag_type="text">are that the proposed method will not destroy the trigger</text>
<text top="381" left="69" width="231" height="9" font="font7" id="p2_t29" reading_order_no="28" segment_no="5" tag_type="text">and only needs to predict two images (perturbed image<a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="393" left="69" width="231" height="9" font="font7" id="p2_t30" reading_order_no="29" segment_no="5" tag_type="text">and unperturbed image). As a result, the proposed method</text>
<text top="405" left="69" width="199" height="9" font="font7" id="p2_t31" reading_order_no="30" segment_no="5" tag_type="text">is more effective and more efficient than STRIP.</text>
<text top="418" left="59" width="241" height="9" font="font7" id="p2_t32" reading_order_no="31" segment_no="7" tag_type="text">This paper is organized as follows. Background and related</text>
<text top="430" left="49" width="251" height="9" font="font7" id="p2_t33" reading_order_no="32" segment_no="7" tag_type="text">works are reviewed in Section II. The proposed detection</text>
<text top="442" left="49" width="251" height="9" font="font7" id="p2_t34" reading_order_no="33" segment_no="7" tag_type="text">method is elaborated in Section III. Experimental results are</text>
<text top="454" left="49" width="251" height="9" font="font7" id="p2_t35" reading_order_no="34" segment_no="7" tag_type="text">presented in Section IV. This paper is concluded in Section<a href="deeplearning_paper26.html#2">II. </a>The proposed detection</text>
<text top="466" left="49" width="10" height="9" font="font7" id="p2_t36" reading_order_no="35" segment_no="7" tag_type="text">V.<a href="deeplearning_paper26.html#3">III. </a>Experimental results are</text>
<text top="489" left="89" width="171" height="9" font="font7" id="p2_t37" reading_order_no="36" segment_no="8" tag_type="title">II. B ACKGROUND AND R ELATED W ORK<a href="deeplearning_paper26.html#5">IV. </a>This paper is concluded in Section</text>
<text top="504" left="59" width="241" height="9" font="font7" id="p2_t38" reading_order_no="37" segment_no="9" tag_type="text">In this section, first, we review the universal adversarial<a href="deeplearning_paper26.html#9">V.</a></text>
<text top="516" left="49" width="251" height="9" font="font7" id="p2_t39" reading_order_no="38" segment_no="9" tag_type="text">perturbation [13], which is utilized by the proposed method.</text>
<text top="528" left="49" width="251" height="9" font="font7" id="p2_t40" reading_order_no="39" segment_no="9" tag_type="text">Second, we review the related works on backdoor attacks and</text>
<text top="540" left="49" width="37" height="9" font="font7" id="p2_t41" reading_order_no="40" segment_no="9" tag_type="text">defenses.</text>
<text top="565" left="49" width="194" height="9" font="font12" id="p2_t42" reading_order_no="41" segment_no="10" tag_type="title">A. Universarial Adversarial Perturbations [13]</text>
<text top="580" left="59" width="241" height="9" font="font7" id="p2_t43" reading_order_no="42" segment_no="11" tag_type="text">It is known that deep neural networks (DNNs) are vul-</text>
<text top="592" left="49" width="251" height="9" font="font7" id="p2_t44" reading_order_no="43" segment_no="11" tag_type="text">nerable to well-crafted small adversarial perturbations. When</text>
<text top="604" left="49" width="251" height="9" font="font7" id="p2_t45" reading_order_no="44" segment_no="11" tag_type="text">added with adversarial perturbation, input image will be mis-</text>
<text top="616" left="49" width="251" height="9" font="font7" id="p2_t46" reading_order_no="45" segment_no="11" tag_type="text">classified by the model [14]. Universal adversarial perturbation<a href="deeplearning_paper26.html#9">[13], </a>which is utilized by the proposed method.</text>
<text top="628" left="49" width="251" height="9" font="font7" id="p2_t47" reading_order_no="46" segment_no="11" tag_type="text">(UAP) [13] is a kind of image-agnostic adversarial perturba-</text>
<text top="640" left="49" width="251" height="9" font="font7" id="p2_t48" reading_order_no="47" segment_no="11" tag_type="text">tion. Different from image-specific adversarial perturbation,</text>
<text top="652" left="49" width="251" height="9" font="font7" id="p2_t49" reading_order_no="48" segment_no="11" tag_type="text">which is specifically crafted for each image [14], UAP is<a href="deeplearning_paper26.html#9">[13]</a></text>
<text top="664" left="49" width="251" height="9" font="font7" id="p2_t50" reading_order_no="49" segment_no="11" tag_type="text">generated based on a model with a small set of clean images</text>
<text top="676" left="49" width="251" height="9" font="font7" id="p2_t51" reading_order_no="50" segment_no="11" tag_type="text">[13]. As a result, the model will also misclassify other images</text>
<text top="688" left="49" width="175" height="9" font="font7" id="p2_t52" reading_order_no="51" segment_no="11" tag_type="text">with the universal adversarial perturbation.</text>
<text top="713" left="49" width="85" height="9" font="font12" id="p2_t53" reading_order_no="52" segment_no="12" tag_type="title">B. Backdoor Attacks<a href="deeplearning_paper26.html#9">[14]. </a>Universal adversarial perturbation</text>
<text top="728" left="59" width="241" height="9" font="font7" id="p2_t54" reading_order_no="53" segment_no="13" tag_type="text">Recently, a number of researches [1]–[3] indicate that the<a href="deeplearning_paper26.html#9">[13] </a>is a kind of image-agnostic adversarial perturba-</text>
<text top="740" left="49" width="251" height="9" font="font7" id="p2_t55" reading_order_no="54" segment_no="13" tag_type="text">backdoor can be embeded into DNN models through injecting</text>
<text top="58" left="312" width="251" height="9" font="font7" id="p2_t56" reading_order_no="55" segment_no="2" tag_type="text">well-crafted backdoor instances into the training set. After the<a href="deeplearning_paper26.html#9">[14], </a>UAP is</text>
<text top="70" left="312" width="251" height="9" font="font7" id="p2_t57" reading_order_no="56" segment_no="2" tag_type="text">training process, the model will behave normally on clean</text>
<text top="82" left="312" width="251" height="9" font="font7" id="p2_t58" reading_order_no="57" segment_no="2" tag_type="text">inputs. However, the malicious functionality hidden in the<a href="deeplearning_paper26.html#9">[13]. </a>As a result, the model will also misclassify other images</text>
<text top="94" left="312" width="251" height="9" font="font7" id="p2_t59" reading_order_no="58" segment_no="2" tag_type="text">backdoored model will be triggered by the input images</text>
<text top="106" left="312" width="251" height="9" font="font7" id="p2_t60" reading_order_no="59" segment_no="2" tag_type="text">containing the trigger, and these backdoor instances will be</text>
<text top="118" left="312" width="251" height="9" font="font7" id="p2_t61" reading_order_no="60" segment_no="2" tag_type="text">classified as the target class [2], [15]. Since the performance<a href="deeplearning_paper26.html#9">[1]–[3] </a>indicate that the</text>
<text top="130" left="312" width="251" height="9" font="font7" id="p2_t62" reading_order_no="61" segment_no="2" tag_type="text">of backdoored model is similar to the performance of clean</text>
<text top="142" left="312" width="251" height="9" font="font7" id="p2_t63" reading_order_no="62" segment_no="2" tag_type="text">model on clean inputs, it is difficult for users to perceive the</text>
<text top="154" left="312" width="251" height="9" font="font7" id="p2_t64" reading_order_no="63" segment_no="2" tag_type="text">existence of the backdoor. However, the attacker can trigger</text>
<text top="166" left="312" width="229" height="9" font="font7" id="p2_t65" reading_order_no="64" segment_no="2" tag_type="text">the malicious behavior by inputting backdoor instances.</text>
<text top="197" left="312" width="128" height="9" font="font12" id="p2_t66" reading_order_no="65" segment_no="4" tag_type="title">C. Existing Backdoor Defenses</text>
<text top="214" left="322" width="241" height="9" font="font7" id="p2_t67" reading_order_no="66" segment_no="6" tag_type="text">To date, some defense methods have been proposed to detect</text>
<text top="226" left="312" width="251" height="9" font="font7" id="p2_t68" reading_order_no="67" segment_no="6" tag_type="text">and mitigate the backdoor attacks. Liu et al. [7] adopted a pre-<a href="deeplearning_paper26.html#9">[2], [15]. </a>Since the performance</text>
<text top="238" left="312" width="251" height="9" font="font7" id="p2_t69" reading_order_no="68" segment_no="6" tag_type="text">trained auto-encoder to preprocess the input image in order to</text>
<text top="250" left="312" width="251" height="9" font="font7" id="p2_t70" reading_order_no="69" segment_no="6" tag_type="text">disable the trigger. They also retrain the backdoored model</text>
<text top="262" left="312" width="251" height="9" font="font7" id="p2_t71" reading_order_no="70" segment_no="6" tag_type="text">with clean images so as to remove the hidden backdoor. Xu</text>
<text top="274" left="312" width="251" height="9" font="font12" id="p2_t72" reading_order_no="71" segment_no="6" tag_type="text">et al. [5] generates a set of backdoor instances as the query</text>
<text top="285" left="312" width="251" height="9" font="font7" id="p2_t73" reading_order_no="72" segment_no="6" tag_type="text">set. Then, they inputs the query set into backdoored models</text>
<text top="297" left="312" width="251" height="9" font="font7" id="p2_t74" reading_order_no="73" segment_no="6" tag_type="text">and clean models to extract representation vectors from those</text>
<text top="309" left="312" width="251" height="9" font="font7" id="p2_t75" reading_order_no="74" segment_no="6" tag_type="text">models. They use the resulting vectors as input to train a meta-</text>
<text top="321" left="312" width="251" height="9" font="font7" id="p2_t76" reading_order_no="75" segment_no="6" tag_type="text">classifier which can predict whether a model is backdoored [5].</text>
<text top="333" left="312" width="251" height="9" font="font7" id="p2_t77" reading_order_no="76" segment_no="6" tag_type="text">However, the method needs the knowledge of the trigger size<a href="deeplearning_paper26.html#9">[7] </a>adopted a pre-</text>
<text top="345" left="312" width="251" height="9" font="font7" id="p2_t78" reading_order_no="77" segment_no="6" tag_type="text">to craft those backdoor instances. Liu et al. [16] demonstrated</text>
<text top="357" left="312" width="251" height="9" font="font7" id="p2_t79" reading_order_no="78" segment_no="6" tag_type="text">that the functionality of the backdoor depends on some specific</text>
<text top="369" left="312" width="251" height="9" font="font7" id="p2_t80" reading_order_no="79" segment_no="6" tag_type="text">neurons in the model. These specific neurons are usually</text>
<text top="381" left="312" width="251" height="9" font="font7" id="p2_t81" reading_order_no="80" segment_no="6" tag_type="text">dormant when the model is queried with clean images [16].</text>
<text top="393" left="312" width="251" height="9" font="font7" id="p2_t82" reading_order_no="81" segment_no="6" tag_type="text">Defenders can find these neurons by inputting clean images<a href="deeplearning_paper26.html#9">[5] </a>generates a set of backdoor instances as the query</text>
<text top="405" left="312" width="251" height="9" font="font7" id="p2_t83" reading_order_no="82" segment_no="6" tag_type="text">into the model. Then these malicious neurons can be pruned</text>
<text top="417" left="312" width="251" height="9" font="font7" id="p2_t84" reading_order_no="83" segment_no="6" tag_type="text">so as to remove the backdoor. However, the pruned model</text>
<text top="429" left="312" width="251" height="9" font="font7" id="p2_t85" reading_order_no="84" segment_no="6" tag_type="text">suffers from the degradation in classification accuracy on clean</text>
<text top="441" left="312" width="251" height="9" font="font7" id="p2_t86" reading_order_no="85" segment_no="6" tag_type="text">inputs due to the pruning [16]. Zhang et al. [4] training a large<a href="deeplearning_paper26.html#9">[5].</a></text>
<text top="453" left="312" width="251" height="9" font="font7" id="p2_t87" reading_order_no="86" segment_no="6" tag_type="text">number of backdoored models and clean models to generate</text>
<text top="465" left="312" width="251" height="9" font="font7" id="p2_t88" reading_order_no="87" segment_no="6" tag_type="text">corresponding universal perturbations [13]. Then they use the</text>
<text top="477" left="312" width="251" height="9" font="font7" id="p2_t89" reading_order_no="88" segment_no="6" tag_type="text">UAPs [13] as the input to train a two-class classifier as the</text>
<text top="489" left="312" width="251" height="9" font="font7" id="p2_t90" reading_order_no="89" segment_no="6" tag_type="text">Trojan detector. However, the computational cost to generate<a href="deeplearning_paper26.html#9">[16] </a>demonstrated</text>
<text top="501" left="312" width="251" height="9" font="font7" id="p2_t91" reading_order_no="90" segment_no="6" tag_type="text">those large number of backdoored models and clean models</text>
<text top="513" left="312" width="251" height="9" font="font7" id="p2_t92" reading_order_no="91" segment_no="6" tag_type="text">is high, which is unaffordable to most users. Chen et al. [17]</text>
<text top="525" left="312" width="251" height="9" font="font7" id="p2_t93" reading_order_no="92" segment_no="6" tag_type="text">analyze the neuron activations to the training data to determine<a href="deeplearning_paper26.html#9">[16].</a></text>
<text top="537" left="312" width="251" height="9" font="font7" id="p2_t94" reading_order_no="93" segment_no="6" tag_type="text">whether it has backdoor instances. It separates the activations</text>
<text top="549" left="312" width="251" height="9" font="font7" id="p2_t95" reading_order_no="94" segment_no="6" tag_type="text">of all training data into two clusters by applying 2-means</text>
<text top="560" left="312" width="251" height="9" font="font7" id="p2_t96" reading_order_no="95" segment_no="6" tag_type="text">clustering. The high silhouette score means that this cluster</text>
<text top="572" left="312" width="251" height="10" font="font7" id="p2_t97" reading_order_no="96" segment_no="6" tag_type="text">corresponds to the backdoor instances [17]. Gao et al. [12]</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p2_t98" reading_order_no="97" segment_no="6" tag_type="text">add a set of other images from different classes to the input<a href="deeplearning_paper26.html#9">[16]. </a>Zhang</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p2_t99" reading_order_no="98" segment_no="6" tag_type="text">image separately so as to generate a set of blended images.</text>
<text top="608" left="312" width="251" height="9" font="font7" id="p2_t100" reading_order_no="99" segment_no="6" tag_type="text">Then, the entropy of the predicted results on these blended<a href="deeplearning_paper26.html#9">[4] </a>training a large</text>
<text top="620" left="312" width="251" height="9" font="font7" id="p2_t101" reading_order_no="100" segment_no="6" tag_type="text">images is calculated. The lower the entropy, the input image</text>
<text top="632" left="312" width="251" height="9" font="font7" id="p2_t102" reading_order_no="101" segment_no="6" tag_type="text">is more likely to carry a trigger [12]. However, the trigger in<a href="deeplearning_paper26.html#9">[13]. </a>Then they use the</text>
<text top="644" left="312" width="251" height="9" font="font7" id="p2_t103" reading_order_no="102" segment_no="6" tag_type="text">the blended image may be destroyed. As a result, the backdoor<a href="deeplearning_paper26.html#9">[13] </a>as the input to train a two-class classifier as the</text>
<text top="656" left="312" width="251" height="9" font="font7" id="p2_t104" reading_order_no="103" segment_no="6" tag_type="text">instance will be incorrectly considered to be a clean one by</text>
<text top="668" left="312" width="251" height="9" font="font7" id="p2_t105" reading_order_no="104" segment_no="6" tag_type="text">STRIP. Wang et al. [18] proposed a defense method named</text>
<text top="680" left="312" width="251" height="9" font="font7" id="p2_t106" reading_order_no="105" segment_no="6" tag_type="text">Neural Cleanse (NC) to reverse engineer the trigger from the</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p2_t107" reading_order_no="106" segment_no="6" tag_type="text">backoored model. For each class, NC computes the minimized</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p2_t108" reading_order_no="107" segment_no="6" tag_type="text">amount of modification to make the model predict images from<a href="deeplearning_paper26.html#9">[17]</a></text>
<text top="716" left="312" width="251" height="9" font="font7" id="p2_t109" reading_order_no="108" segment_no="6" tag_type="text">different classes as this class. Among these modifications, if</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p2_t110" reading_order_no="109" segment_no="6" tag_type="text">a modification is substantially smaller than the others, NC</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p2_t111" reading_order_no="110" segment_no="6" tag_type="text">will consider it as a trigger [18]. However, this method is</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font13" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font14" size="6" family="TIMES NEW ROMAN" color="#000000"/>
	<fontspec id="font15" size="6" family="TIMES NEW ROMAN" color="#c00000"/>
	<fontspec id="font16" size="6" family="TIMES NEW ROMAN" color="#000000"/>
	<fontspec id="font17" size="6" family="TIMES NEW ROMAN,BoldItalic" color="#000000"/>
	<fontspec id="font18" size="6" family="TIMES NEW ROMAN,Bold" color="#0000ff"/>
	<fontspec id="font19" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font20" size="7" family="CMMI7" color="#000000"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="text">3</text>
<text top="58" left="49" width="251" height="9" font="font7" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="text">computationally expensive considering the reverse-engineering</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p3_t3" reading_order_no="2" segment_no="1" tag_type="text">process, especially when the model has a large number of</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p3_t4" reading_order_no="3" segment_no="1" tag_type="text">output classes. Moreover, the reversed trigger is just similar</text>
<text top="94" left="49" width="251" height="9" font="font7" id="p3_t5" reading_order_no="4" segment_no="1" tag_type="text">to the true trigger. In addition, when the true trigger is big or</text>
<text top="106" left="49" width="251" height="9" font="font7" id="p3_t6" reading_order_no="5" segment_no="1" tag_type="text">discrete, the reversed trigger even will not be similar to the</text>
<text top="118" left="49" width="251" height="9" font="font7" id="p3_t7" reading_order_no="6" segment_no="1" tag_type="text">true trigger. Qiao et al. [8] proposed a max-entropy staircase</text>
<text top="130" left="49" width="251" height="9" font="font7" id="p3_t8" reading_order_no="7" segment_no="1" tag_type="text">approximator (MESA) algorithm to reverse a set of candidate</text>
<text top="142" left="49" width="251" height="9" font="font7" id="p3_t9" reading_order_no="8" segment_no="1" tag_type="text">triggers. Then, backdoor instances are generated by separately<a href="deeplearning_paper26.html#9">[8] </a>proposed a max-entropy staircase</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p3_t10" reading_order_no="9" segment_no="1" tag_type="text">adding these candidate triggers to clean images. The model</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p3_t11" reading_order_no="10" segment_no="1" tag_type="text">is fine-tuned on these backdoor instances with correct labels</text>
<text top="178" left="49" width="251" height="9" font="font7" id="p3_t12" reading_order_no="11" segment_no="1" tag_type="text">to remove the backdoor [8]. However, the MESA algorithm</text>
<text top="190" left="49" width="251" height="9" font="font7" id="p3_t13" reading_order_no="12" segment_no="1" tag_type="text">requires the information of the trigger size, which is difficult to</text>
<text top="202" left="49" width="251" height="9" font="font7" id="p3_t14" reading_order_no="13" segment_no="1" tag_type="text">obtain by the defender in realistic scenarios. Chen et al. [19]<a href="deeplearning_paper26.html#9">[8]. </a>However, the MESA algorithm</text>
<text top="214" left="49" width="251" height="9" font="font7" id="p3_t15" reading_order_no="14" segment_no="1" tag_type="text">proposed a GAN-based defense method called DeepInspect.</text>
<text top="226" left="49" width="251" height="9" font="font7" id="p3_t16" reading_order_no="15" segment_no="1" tag_type="text">DeepInspect reconstructs the potential trigger and generates</text>
<text top="238" left="49" width="251" height="9" font="font7" id="p3_t17" reading_order_no="16" segment_no="1" tag_type="text">the backdoor instances by patching these reconstructed trigger</text>
<text top="250" left="49" width="251" height="9" font="font7" id="p3_t18" reading_order_no="17" segment_no="1" tag_type="text">to the clean images with ground truth labels. Then, the<a href="deeplearning_paper26.html#9">[19]</a></text>
<text top="262" left="49" width="251" height="9" font="font7" id="p3_t19" reading_order_no="18" segment_no="1" tag_type="text">backdoored model is fine-tuned on these generated backdoor</text>
<text top="274" left="49" width="160" height="9" font="font7" id="p3_t20" reading_order_no="19" segment_no="1" tag_type="text">instances to remove the backdoor [19].</text>
<text top="289" left="59" width="241" height="9" font="font7" id="p3_t21" reading_order_no="20" segment_no="7" tag_type="text">The main advantages of the proposed approach over the</text>
<text top="301" left="49" width="184" height="9" font="font7" id="p3_t22" reading_order_no="21" segment_no="7" tag_type="text">existing defenses are summarized as follows.</text>
<text top="321" left="59" width="241" height="9" font="font7" id="p3_t23" reading_order_no="22" segment_no="8" tag_type="list">• Compared with [5], [8], which both need to know the</text>
<text top="333" left="69" width="231" height="9" font="font7" id="p3_t24" reading_order_no="23" segment_no="8" tag_type="list">trigger size, the proposed method does not require any<a href="deeplearning_paper26.html#9">[19].</a></text>
<text top="345" left="69" width="231" height="9" font="font7" id="p3_t25" reading_order_no="24" segment_no="8" tag_type="list">backdoor attack information. Moreover, Liu et al. [7]</text>
<text top="357" left="69" width="231" height="9" font="font7" id="p3_t26" reading_order_no="25" segment_no="8" tag_type="list">requires a large number of trusted images to remove</text>
<text top="368" left="69" width="231" height="10" font="font7" id="p3_t27" reading_order_no="26" segment_no="8" tag_type="list">the backdoor (10,000 ∼ 60,000 images for MNIST). In</text>
<text top="381" left="69" width="231" height="9" font="font7" id="p3_t28" reading_order_no="27" segment_no="8" tag_type="list">comparison, the proposed method only requires a small<a href="deeplearning_paper26.html#9">[5], [8], </a>which both need to know the</text>
<text top="393" left="69" width="231" height="9" font="font7" id="p3_t29" reading_order_no="28" segment_no="8" tag_type="list">set of clean images (300 clean images) to generate the</text>
<text top="405" left="69" width="138" height="9" font="font7" id="p3_t30" reading_order_no="29" segment_no="8" tag_type="list">universal adversarial perturbation.</text>
<text top="417" left="59" width="241" height="9" font="font7" id="p3_t31" reading_order_no="30" segment_no="9" tag_type="list">• The detection process of the work [4] requires training</text>
<text top="429" left="69" width="231" height="9" font="font7" id="p3_t32" reading_order_no="31" segment_no="9" tag_type="list">a large number of shadow models (backdoored mod-<a href="deeplearning_paper26.html#9">[7]</a></text>
<text top="441" left="69" width="231" height="9" font="font7" id="p3_t33" reading_order_no="32" segment_no="9" tag_type="list">els and clean models). Nevertheless, the computational</text>
<text top="453" left="69" width="231" height="9" font="font7" id="p3_t34" reading_order_no="33" segment_no="9" tag_type="list">resources for training such a large number of shadow</text>
<text top="465" left="69" width="231" height="9" font="font7" id="p3_t35" reading_order_no="34" segment_no="9" tag_type="list">models are unaffordable for most of the users. In contrast,</text>
<text top="477" left="69" width="231" height="9" font="font7" id="p3_t36" reading_order_no="35" segment_no="9" tag_type="list">the proposed method only needs to generate one single</text>
<text top="489" left="69" width="231" height="9" font="font7" id="p3_t37" reading_order_no="36" segment_no="9" tag_type="list">universal perturbation and only needs the model to make</text>
<text top="501" left="69" width="231" height="9" font="font7" id="p3_t38" reading_order_no="37" segment_no="9" tag_type="list">predictions on the unperturbed image and the perturbed</text>
<text top="513" left="69" width="211" height="9" font="font7" id="p3_t39" reading_order_no="38" segment_no="9" tag_type="list">image, which requires low computational overhead.</text>
<text top="525" left="59" width="241" height="9" font="font7" id="p3_t40" reading_order_no="39" segment_no="11" tag_type="list">• STRIP [12] directly superimposes a number of images</text>
<text top="537" left="69" width="231" height="9" font="font7" id="p3_t41" reading_order_no="40" segment_no="11" tag_type="list">from different classes to the input image. This will not<a href="deeplearning_paper26.html#9">[4] </a>requires training</text>
<text top="549" left="69" width="231" height="9" font="font7" id="p3_t42" reading_order_no="41" segment_no="11" tag_type="list">only destroy the main content of the input image, but</text>
<text top="560" left="69" width="231" height="9" font="font7" id="p3_t43" reading_order_no="42" segment_no="11" tag_type="list">may also accidentally break the trigger. Once the trigger</text>
<text top="572" left="69" width="231" height="9" font="font7" id="p3_t44" reading_order_no="43" segment_no="11" tag_type="list">is destroyed, the entropy of this backdoor instance will</text>
<text top="584" left="69" width="231" height="9" font="font7" id="p3_t45" reading_order_no="44" segment_no="11" tag_type="list">be similar to the entropy of a clean image. Hence STRIP</text>
<text top="596" left="69" width="231" height="9" font="font7" id="p3_t46" reading_order_no="45" segment_no="11" tag_type="list">[12] will fail to detect this backdoor instance. In contrast,</text>
<text top="608" left="69" width="231" height="9" font="font7" id="p3_t47" reading_order_no="46" segment_no="11" tag_type="list">the proposed method perturbs the untrusted image with</text>
<text top="620" left="69" width="231" height="9" font="font7" id="p3_t48" reading_order_no="47" segment_no="11" tag_type="list">universal adversarial perturbation (UAP) [13]. This will</text>
<text top="632" left="69" width="231" height="9" font="font7" id="p3_t49" reading_order_no="48" segment_no="11" tag_type="list">not destroy the trigger and ensures that the predicted</text>
<text top="644" left="69" width="231" height="9" font="font7" id="p3_t50" reading_order_no="49" segment_no="11" tag_type="list">label of the backdoor instance keeps unchanged even</text>
<text top="656" left="69" width="231" height="9" font="font7" id="p3_t51" reading_order_no="50" segment_no="11" tag_type="list">after perturbation. Moreover, for each input image, STRIP<a href="deeplearning_paper26.html#9">[12] </a>directly superimposes a number of images</text>
<text top="668" left="69" width="231" height="9" font="font7" id="p3_t52" reading_order_no="51" segment_no="11" tag_type="list">[12] needs to predict a set of blended images in order</text>
<text top="680" left="69" width="231" height="9" font="font7" id="p3_t53" reading_order_no="52" segment_no="11" tag_type="list">to estimate the entropy of the predicted labels of those</text>
<text top="692" left="69" width="231" height="9" font="font7" id="p3_t54" reading_order_no="53" segment_no="11" tag_type="list">blended images. In comparison, for each image, the</text>
<text top="704" left="69" width="231" height="9" font="font7" id="p3_t55" reading_order_no="54" segment_no="11" tag_type="list">proposed method only needs to predict two images (the</text>
<text top="716" left="69" width="231" height="9" font="font7" id="p3_t56" reading_order_no="55" segment_no="11" tag_type="list">perturbed image and the unperturbed image). Therefore,</text>
<text top="728" left="69" width="231" height="9" font="font7" id="p3_t57" reading_order_no="56" segment_no="11" tag_type="list">the backdoor detection efficiency of the proposed method<a href="deeplearning_paper26.html#9">[12] </a>will fail to detect this backdoor instance. In contrast,</text>
<text top="740" left="69" width="118" height="9" font="font7" id="p3_t58" reading_order_no="57" segment_no="11" tag_type="list">is higher than that of STRIP.</text>
<text top="58" left="374" width="127" height="9" font="font7" id="p3_t59" reading_order_no="58" segment_no="2" tag_type="title">III. T HE P ROPOSED M ETHOD<a href="deeplearning_paper26.html#9">[13]. </a>This will</text>
<text top="75" left="322" width="241" height="9" font="font7" id="p3_t60" reading_order_no="59" segment_no="3" tag_type="text">In this section, first, the overall procedure of the proposed</text>
<text top="87" left="312" width="251" height="9" font="font7" id="p3_t61" reading_order_no="60" segment_no="3" tag_type="text">backdoor detection method is presented in Section III-A. The</text>
<text top="99" left="312" width="251" height="9" font="font7" id="p3_t62" reading_order_no="61" segment_no="3" tag_type="text">proposed method can be divided into two steps, which are</text>
<text top="111" left="312" width="251" height="9" font="font7" id="p3_t63" reading_order_no="62" segment_no="3" tag_type="text">elaborated in Section III-B and Section III-C, respectively.<a href="deeplearning_paper26.html#9">[12] </a>needs to predict a set of blended images in order</text>
<text top="123" left="312" width="251" height="9" font="font7" id="p3_t64" reading_order_no="63" segment_no="3" tag_type="text">Finally, the reason why choosing universal adversarial pertur-</text>
<text top="135" left="312" width="251" height="9" font="font7" id="p3_t65" reading_order_no="64" segment_no="3" tag_type="text">bation [13] for adversarial perturbation generation is discussed</text>
<text top="147" left="312" width="68" height="9" font="font7" id="p3_t66" reading_order_no="65" segment_no="3" tag_type="text">in Section III-D.</text>
<text top="177" left="312" width="64" height="9" font="font12" id="p3_t67" reading_order_no="66" segment_no="4" tag_type="title">A. Overall flow</text>
<text top="193" left="322" width="241" height="9" font="font7" id="p3_t68" reading_order_no="67" segment_no="5" tag_type="text">As shown in Fig. 1, the proposed defense method consists of</text>
<text top="205" left="312" width="251" height="9" font="font7" id="p3_t69" reading_order_no="68" segment_no="5" tag_type="text">two steps. The first step is to generate the universal adversarial</text>
<text top="217" left="312" width="251" height="9" font="font7" id="p3_t70" reading_order_no="69" segment_no="5" tag_type="text">perturbation [13] from the backdoored model with a small set</text>
<text top="229" left="312" width="67" height="9" font="font7" id="p3_t71" reading_order_no="70" segment_no="5" tag_type="text">of clean images.</text>
<text top="242" left="322" width="241" height="9" font="font7" id="p3_t72" reading_order_no="71" segment_no="6" tag_type="text">The second step is backdoor detection, which is summarized</text>
<text top="254" left="312" width="251" height="9" font="font7" id="p3_t73" reading_order_no="72" segment_no="6" tag_type="text">as follows. As shown in Fig. 1, given an untrusted image, the</text>
<text top="266" left="312" width="251" height="9" font="font7" id="p3_t74" reading_order_no="73" segment_no="6" tag_type="text">universal perturbation generated in previous step is added to</text>
<text top="277" left="312" width="251" height="9" font="font7" id="p3_t75" reading_order_no="74" segment_no="6" tag_type="text">this image. Then, both the perturbed image and corresponding</text>
<text top="289" left="312" width="251" height="9" font="font7" id="p3_t76" reading_order_no="75" segment_no="6" tag_type="text">unperturbed image are input into the untrusted model. If</text>
<text top="301" left="312" width="251" height="9" font="font7" id="p3_t77" reading_order_no="76" segment_no="6" tag_type="text">the untrusted model is backdoored, the backdoor instance<a href="deeplearning_paper26.html#3">III-A. </a>The</text>
<text top="313" left="312" width="251" height="9" font="font7" id="p3_t78" reading_order_no="77" segment_no="6" tag_type="text">without perturbation will be misclassified as the target label.</text>
<text top="325" left="312" width="251" height="9" font="font7" id="p3_t79" reading_order_no="78" segment_no="6" tag_type="text">When added with universal adversarial perturbation [13], the<a href="deeplearning_paper26.html#4">III-B </a>and Section <a href="deeplearning_paper26.html#4">III-C, </a>respectively.</text>
<text top="337" left="312" width="251" height="9" font="font7" id="p3_t80" reading_order_no="79" segment_no="6" tag_type="text">backdoor instance which carries a trigger will still be classified</text>
<text top="349" left="312" width="251" height="9" font="font7" id="p3_t81" reading_order_no="80" segment_no="6" tag_type="text">as the target label. However, given a clean image, its predicted<a href="deeplearning_paper26.html#9">[13] </a>for adversarial perturbation generation is discussed</text>
<text top="361" left="312" width="251" height="9" font="font7" id="p3_t82" reading_order_no="81" segment_no="6" tag_type="text">label will change to another label when added with perturba-<a href="deeplearning_paper26.html#5">III-D.</a></text>
<text top="373" left="312" width="251" height="9" font="font7" id="p3_t83" reading_order_no="82" segment_no="6" tag_type="text">tion. Hence, if the backdoored model always predicts an image</text>
<text top="385" left="312" width="251" height="9" font="font7" id="p3_t84" reading_order_no="83" segment_no="6" tag_type="text">as the same label with or without universal perturbation, the<a href="deeplearning_paper26.html#3">1, </a>the proposed defense method consists of</text>
<text top="397" left="312" width="251" height="9" font="font7" id="p3_t85" reading_order_no="84" segment_no="6" tag_type="text">image is considered to be a backdoor instance. Meanwhile,</text>
<text top="409" left="312" width="251" height="9" font="font7" id="p3_t86" reading_order_no="85" segment_no="6" tag_type="text">the predicted label is considered to be the target label. For<a href="deeplearning_paper26.html#9">[13] </a>from the backdoored model with a small set</text>
<text top="421" left="312" width="252" height="9" font="font7" id="p3_t87" reading_order_no="86" segment_no="6" tag_type="text">instance, the label Stop in Fig. 1 is the target label, and the</text>
<text top="433" left="312" width="155" height="9" font="font7" id="p3_t88" reading_order_no="87" segment_no="6" tag_type="text">corresponding image carries a trigger.</text>
<text top="568" left="491" width="0" height="26" font="font14" id="p3_t89" reading_order_no="104" segment_no="10" tag_type="figure">B ac kd oo<a href="deeplearning_paper26.html#3">1, </a>given an untrusted image, the</text>
<text top="591" left="491" width="0" height="17" font="font14" id="p3_t90" reading_order_no="105" segment_no="10" tag_type="figure">re d M</text>
<text top="606" left="491" width="0" height="16" font="font14" id="p3_t91" reading_order_no="106" segment_no="10" tag_type="figure">od el</text>
<text top="568" left="514" width="40" height="7" font="font15" id="p3_t92" reading_order_no="107" segment_no="10" tag_type="figure">Stop (incorrect)</text>
<text top="586" left="514" width="40" height="7" font="font15" id="p3_t93" reading_order_no="108" segment_no="10" tag_type="figure">Stop (incorrect)</text>
<text top="621" left="514" width="55" height="7" font="font15" id="p3_t94" reading_order_no="110" segment_no="10" tag_type="figure">Pedestrian (incorrect)</text>
<text top="603" left="514" width="43" height="7" font="font16" id="p3_t95" reading_order_no="109" segment_no="10" tag_type="figure">Caution (correct)<a href="deeplearning_paper26.html#9">[13], </a>the</text>
<text top="612" left="338" width="26" height="7" font="font17" id="p3_t96" reading_order_no="94" segment_no="10" tag_type="figure">Untrusted</text>
<text top="619" left="342" width="19" height="7" font="font17" id="p3_t97" reading_order_no="95" segment_no="10" tag_type="figure">Images</text>
<text top="639" left="438" width="44" height="7" font="font17" id="p3_t98" reading_order_no="103" segment_no="10" tag_type="figure">Perturbed Image</text>
<text top="617" left="423" width="11" height="7" font="font16" id="p3_t99" reading_order_no="101" segment_no="10" tag_type="figure">Add</text>
<text top="624" left="412" width="31" height="7" font="font16" id="p3_t100" reading_order_no="102" segment_no="10" tag_type="figure">Perturbation</text>
<text top="551" left="372" width="49" height="7" font="font17" id="p3_t101" reading_order_no="96" segment_no="10" tag_type="figure">Backdoor Instance</text>
<text top="639" left="515" width="17" height="7" font="font17" id="p3_t102" reading_order_no="111" segment_no="10" tag_type="figure">Labels</text>
<text top="565" left="422" width="11" height="7" font="font16" id="p3_t103" reading_order_no="97" segment_no="10" tag_type="figure">Add</text>
<text top="572" left="412" width="31" height="7" font="font16" id="p3_t104" reading_order_no="98" segment_no="10" tag_type="figure">Perturbation</text>
<text top="639" left="379" width="34" height="7" font="font17" id="p3_t105" reading_order_no="100" segment_no="10" tag_type="figure">Clean Image<a href="deeplearning_paper26.html#3">1 </a>is the target label, and the</text>
<text top="551" left="438" width="44" height="7" font="font17" id="p3_t106" reading_order_no="99" segment_no="10" tag_type="figure">Perturbed Image</text>
<text top="491" left="342" width="36" height="7" font="font17" id="p3_t107" reading_order_no="89" segment_no="10" tag_type="figure">Clean Images</text>
<text top="537" left="351" width="16" height="7" font="font17" id="p3_t108" reading_order_no="90" segment_no="10" tag_type="figure">Model</text>
<text top="496" left="467" width="92" height="7" font="font17" id="p3_t109" reading_order_no="92" segment_no="10" tag_type="figure">Universal Adversarial Perturbation</text>
<text top="491" left="405" width="21" height="7" font="font16" id="p3_t110" reading_order_no="91" segment_no="10" tag_type="figure">generate</text>
<text top="496" left="313" width="17" height="7" font="font18" id="p3_t111" reading_order_no="88" segment_no="10" tag_type="figure">Step 1</text>
<text top="595" left="313" width="18" height="7" font="font18" id="p3_t112" reading_order_no="93" segment_no="10" tag_type="figure">Step 2</text>
<text top="660" left="312" width="22" height="7" font="font8" id="p3_t113" reading_order_no="112" segment_no="12" tag_type="text">Fig. 1.</text>
<text top="660" left="343" width="220" height="7" font="font8" id="p3_t114" reading_order_no="113" segment_no="12" tag_type="text">The overall flow of the proposed method: adversarial perturbation</text>
<text top="669" left="312" width="160" height="7" font="font8" id="p3_t115" reading_order_no="114" segment_no="12" tag_type="text">generation (Step 1); backdoor detection (Step 2).</text>
<text top="692" left="322" width="241" height="9" font="font7" id="p3_t116" reading_order_no="115" segment_no="13" tag_type="text">The overall flow of the proposed method is outlined in</text>
<text top="704" left="312" width="167" height="9" font="font7" id="p3_t117" reading_order_no="116" segment_no="13" tag_type="text">Algorithm 1 and is described as follows:</text>
<text top="716" left="322" width="241" height="9" font="font7" id="p3_t118" reading_order_no="117" segment_no="14" tag_type="text">1) Given an untrusted model f unt , the universal adversarial</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p3_t119" reading_order_no="118" segment_no="14" tag_type="text">perturbation [13] η is generated based on the untrusted model<i><b>Untrusted</b></i></text>
<text top="740" left="312" width="248" height="9" font="font19" id="p3_t120" reading_order_no="119" segment_no="14" tag_type="text">f unt with a small set of clean images X (only 300 images).<i><b>Images</b></i></text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font21" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font22" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font23" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font24" size="10" family="MSBM10" color="#000000"/>
	<fontspec id="font25" size="10" family="TIMES NEW ROMAN" color="#000000"/>
	<fontspec id="font26" size="6" family="Times New Roman,Italic" color="#000000"/>
	<fontspec id="font27" size="10" family="Times New Roman,Italic" color="#000000"/>
	<fontspec id="font28" size="10" family="Symbol" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">4</text>
<text top="58" left="59" width="35" height="10" font="font7" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="text">2) D unt</text>
<text top="58" left="102" width="198" height="10" font="font21" id="p4_t3" reading_order_no="2" segment_no="1" tag_type="text">= { d 1 , . . . , d n } denotes the untrusted images</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p4_t4" reading_order_no="3" segment_no="1" tag_type="text">(in the training stage, it represents the training data; in the</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p4_t5" reading_order_no="4" segment_no="1" tag_type="text">inference stage, it represents a single input image with n = 1 ).</text>
<text top="93" left="49" width="251" height="11" font="font7" id="p4_t6" reading_order_no="5" segment_no="1" tag_type="text">The perturbation η is then added to the image d i ∈ D unt to</text>
<text top="103" left="49" width="172" height="13" font="font7" id="p4_t7" reading_order_no="6" segment_no="1" tag_type="text">generate the perturbed image ˆ d i = d i + η .</text>
<text top="116" left="59" width="241" height="12" font="font7" id="p4_t8" reading_order_no="7" segment_no="4" tag_type="text">3) Both unperturbed image d i and perturbed image d ˆ i are</text>
<text top="131" left="49" width="251" height="9" font="font7" id="p4_t9" reading_order_no="8" segment_no="4" tag_type="text">input into the untrusted model. The predictions of the model</text>
<text top="142" left="49" width="251" height="10" font="font7" id="p4_t10" reading_order_no="9" segment_no="4" tag_type="text">on the unperturbed image and the perturbed image are y i =</text>
<text top="154" left="49" width="251" height="10" font="font19" id="p4_t11" reading_order_no="10" segment_no="4" tag_type="text">f unt ( d i ) and y ˆ i = f unt ( ˆ d i ) , respectively. If y i = ˆ y i , the input</text>
<text top="166" left="49" width="251" height="10" font="font7" id="p4_t12" reading_order_no="11" segment_no="4" tag_type="text">image d i will be regarded as a backdoor instance. Otherwise,</text>
<text top="179" left="49" width="199" height="9" font="font7" id="p4_t13" reading_order_no="12" segment_no="4" tag_type="text">the input image will be regarded as a clean one.</text>
<text top="204" left="49" width="231" height="9" font="font23" id="p4_t14" reading_order_no="13" segment_no="6" tag_type="title">Algorithm 1 The Proposed Backdoor Detection Method</text>
<text top="217" left="49" width="251" height="10" font="font23" id="p4_t15" reading_order_no="14" segment_no="7" tag_type="code">Input: a clean image set X , backdoored model f unt , untrusted</text>
<text top="229" left="49" width="129" height="10" font="font7" id="p4_t16" reading_order_no="15" segment_no="7" tag_type="code">image set D unt = { d 1 , . . . , d n }</text>
<text top="241" left="49" width="150" height="10" font="font23" id="p4_t17" reading_order_no="16" segment_no="7" tag_type="code">Output: the backdoor instances D bd</text>
<text top="255" left="55" width="54" height="10" font="font19" id="p4_t18" reading_order_no="17" segment_no="7" tag_type="code">1: D bd ← ∅ ;</text>
<text top="266" left="55" width="99" height="11" font="font19" id="p4_t19" reading_order_no="18" segment_no="7" tag_type="code">2: η ← F U AP ( f unt , X ) ;</text>
<text top="279" left="55" width="91" height="9" font="font23" id="p4_t20" reading_order_no="19" segment_no="7" tag_type="code">3: for i = 1 , . . . , n do</text>
<text top="292" left="55" width="6" height="7" font="font8" id="p4_t21" reading_order_no="20" segment_no="7" tag_type="code">4:</text>
<text top="290" left="76" width="61" height="10" font="font19" id="p4_t22" reading_order_no="21" segment_no="7" tag_type="code">y i ← f unt ( d i ) ;</text>
<text top="304" left="55" width="6" height="7" font="font8" id="p4_t23" reading_order_no="22" segment_no="7" tag_type="code">5:</text>
<text top="300" left="76" width="53" height="13" font="font21" id="p4_t24" reading_order_no="23" segment_no="7" tag_type="code">d ˆ i ← d i + η ;</text>
<text top="316" left="55" width="6" height="7" font="font8" id="p4_t25" reading_order_no="24" segment_no="7" tag_type="code">6:</text>
<text top="314" left="76" width="53" height="10" font="font21" id="p4_t26" reading_order_no="25" segment_no="7" tag_type="code">ˆ y i ← f unt ( ˆ</text>
<text top="315" left="121" width="16" height="9" font="font19" id="p4_t27" reading_order_no="26" segment_no="7" tag_type="code">d i ) ;</text>
<text top="328" left="55" width="6" height="7" font="font8" id="p4_t28" reading_order_no="27" segment_no="7" tag_type="code">7:</text>
<text top="327" left="76" width="62" height="9" font="font23" id="p4_t29" reading_order_no="28" segment_no="7" tag_type="code">if y i = ˆ y i then</text>
<text top="340" left="55" width="6" height="7" font="font8" id="p4_t30" reading_order_no="29" segment_no="7" tag_type="code">8:</text>
<text top="339" left="86" width="56" height="9" font="font19" id="p4_t31" reading_order_no="30" segment_no="7" tag_type="code">add ( d i , D bd ) ;</text>
<text top="352" left="55" width="6" height="7" font="font8" id="p4_t32" reading_order_no="31" segment_no="7" tag_type="code">9:</text>
<text top="351" left="76" width="25" height="9" font="font23" id="p4_t33" reading_order_no="32" segment_no="7" tag_type="code">end if</text>
<text top="363" left="51" width="46" height="9" font="font23" id="p4_t34" reading_order_no="33" segment_no="7" tag_type="code">10: end for</text>
<text top="375" left="51" width="65" height="9" font="font23" id="p4_t35" reading_order_no="34" segment_no="7" tag_type="code">11: return D bd</text>
<text top="407" left="59" width="241" height="9" font="font7" id="p4_t36" reading_order_no="35" segment_no="9" tag_type="text">In the following sections, the perturbation generation pro-</text>
<text top="419" left="49" width="251" height="9" font="font7" id="p4_t37" reading_order_no="36" segment_no="9" tag_type="text">cess and the backdoor detection process of the proposed</text>
<text top="431" left="49" width="146" height="9" font="font7" id="p4_t38" reading_order_no="37" segment_no="9" tag_type="text">method, are elaborated respectively.</text>
<text top="464" left="49" width="113" height="9" font="font12" id="p4_t39" reading_order_no="38" segment_no="11" tag_type="title">B. Perturbation Generation</text>
<text top="481" left="59" width="241" height="9" font="font7" id="p4_t40" reading_order_no="39" segment_no="12" tag_type="text">The adversarial perturbation generation method used in</text>
<text top="493" left="49" width="251" height="9" font="font7" id="p4_t41" reading_order_no="40" segment_no="12" tag_type="text">this paper is universal adversarial perturbation (UAP) [13].</text>
<text top="504" left="49" width="251" height="10" font="font7" id="p4_t42" reading_order_no="41" segment_no="12" tag_type="text">Formally, X = { x 1 , . . . , x 300 } denotes the clean image set</text>
<text top="517" left="49" width="251" height="9" font="font7" id="p4_t43" reading_order_no="42" segment_no="12" tag_type="text">and f unt represents the backdoored model, which outputs the</text>
<text top="528" left="49" width="251" height="10" font="font7" id="p4_t44" reading_order_no="43" segment_no="12" tag_type="text">corresponding label f unt ( x ) for each image x i ∈ X . Different</text>
<text top="541" left="49" width="251" height="9" font="font7" id="p4_t45" reading_order_no="44" segment_no="12" tag_type="text">from the UAP generation method in [13] where the ` 2 norm</text>
<text top="553" left="49" width="251" height="9" font="font7" id="p4_t46" reading_order_no="45" segment_no="12" tag_type="text">is used to constrain the intensity of UAP, in this paper, we use</text>
<text top="564" left="49" width="251" height="11" font="font19" id="p4_t47" reading_order_no="46" segment_no="12" tag_type="text">` ∞ norm to constrain the intensity of the perturbation. The ` ∞</text>
<text top="577" left="49" width="251" height="9" font="font7" id="p4_t48" reading_order_no="47" segment_no="12" tag_type="text">norm represents the maximum value of the perturbation. The</text>
<text top="588" left="49" width="251" height="11" font="font7" id="p4_t49" reading_order_no="48" segment_no="12" tag_type="text">perturbation generated under the constraint of ` ∞ norm is the</text>
<text top="600" left="49" width="251" height="9" font="font7" id="p4_t50" reading_order_no="49" segment_no="12" tag_type="text">minimal necessary perturbation, which is smaller than the one</text>
<text top="612" left="49" width="251" height="10" font="font7" id="p4_t51" reading_order_no="50" segment_no="12" tag_type="text">generated under the constraint of ` 2 norm. In the process of</text>
<text top="624" left="49" width="251" height="9" font="font7" id="p4_t52" reading_order_no="51" segment_no="12" tag_type="text">generating adversarial perturbation, the universal perturbation</text>
<text top="636" left="49" width="251" height="9" font="font19" id="p4_t53" reading_order_no="52" segment_no="12" tag_type="text">η is generated by solving the following optimization problem</text>
<text top="648" left="49" width="19" height="9" font="font7" id="p4_t54" reading_order_no="53" segment_no="12" tag_type="text">[13]:</text>
<text top="679" left="60" width="32" height="9" font="font21" id="p4_t55" reading_order_no="54" segment_no="14" tag_type="formula">arg min</text>
<text top="689" left="74" width="4" height="6" font="font20" id="p4_t56" reading_order_no="55" segment_no="14" tag_type="formula">η</text>
<text top="679" left="92" width="184" height="11" font="font13" id="p4_t57" reading_order_no="56" segment_no="14" tag_type="formula">k η k ∞ s.t. f unt ( x i + η ) 6 = f unt ( x i ) , x i ∈ X</text>
<text top="680" left="288" width="12" height="9" font="font7" id="p4_t58" reading_order_no="57" segment_no="14" tag_type="text">(1)</text>
<text top="704" left="59" width="241" height="9" font="font7" id="p4_t59" reading_order_no="58" segment_no="15" tag_type="text">As shown in Eq. (1), in each iteration, for the clean image</text>
<text top="716" left="49" width="251" height="10" font="font19" id="p4_t60" reading_order_no="59" segment_no="15" tag_type="text">x i from X , the ` ∞ norm of the perturbation η is calculated in</text>
<text top="728" left="49" width="251" height="10" font="font7" id="p4_t61" reading_order_no="60" segment_no="15" tag_type="text">order to find the desired perturbation with minimal ` ∞ norm</text>
<text top="740" left="49" width="19" height="9" font="font7" id="p4_t62" reading_order_no="61" segment_no="15" tag_type="text">[13].</text>
<text top="58" left="312" width="95" height="9" font="font12" id="p4_t63" reading_order_no="62" segment_no="2" tag_type="title">C. Backdoor Detection</text>
<text top="74" left="322" width="241" height="9" font="font7" id="p4_t64" reading_order_no="63" segment_no="3" tag_type="text">The proposed method can be applied in two scenarios,</text>
<text top="86" left="312" width="251" height="9" font="font7" id="p4_t65" reading_order_no="64" segment_no="3" tag_type="text">working in the training stage, and working in the inference</text>
<text top="98" left="312" width="251" height="9" font="font7" id="p4_t66" reading_order_no="65" segment_no="3" tag_type="text">stage. In the training stage, the proposed method aims to detect</text>
<text top="110" left="312" width="251" height="9" font="font7" id="p4_t67" reading_order_no="66" segment_no="3" tag_type="text">whether the training dataset contains backdoor instances and</text>
<text top="122" left="312" width="251" height="9" font="font7" id="p4_t68" reading_order_no="67" segment_no="3" tag_type="text">then remove the backdoor instances. In the inference stage,</text>
<text top="134" left="312" width="251" height="9" font="font7" id="p4_t69" reading_order_no="68" segment_no="3" tag_type="text">the goal of the proposed method is to detect whether an input</text>
<text top="146" left="312" width="251" height="9" font="font7" id="p4_t70" reading_order_no="69" segment_no="3" tag_type="text">image contains a trigger. The backdoor detection procedure is</text>
<text top="158" left="312" width="80" height="9" font="font7" id="p4_t71" reading_order_no="70" segment_no="3" tag_type="text">presented in Fig. 2.</text>
<text top="338" left="469" width="12" height="11" font="font25" id="p4_t72" reading_order_no="94" segment_no="5" tag_type="figure">No</text>
<text top="352" left="406" width="15" height="11" font="font25" id="p4_t73" reading_order_no="95" segment_no="5" tag_type="figure">Yes</text>
<text top="370" left="390" width="72" height="11" font="font25" id="p4_t74" reading_order_no="96" segment_no="5" tag_type="figure">Backdoor Instance</text>
<text top="371" left="481" width="49" height="11" font="font25" id="p4_t75" reading_order_no="97" segment_no="5" tag_type="figure">Clean Image</text>
<text top="252" left="359" width="90" height="11" font="font25" id="p4_t76" reading_order_no="77" segment_no="5" tag_type="figure">Untrusted Model</text>
<text top="251" left="455" width="10" height="12" font="font27" id="p4_t77" reading_order_no="78" segment_no="5" tag_type="figure">f unt</text>
<text top="211" left="441" width="64" height="11" font="font25" id="p4_t78" reading_order_no="73" segment_no="5" tag_type="figure">Perturbed Image</text>
<text top="220" left="455" width="5" height="14" font="font25" id="p4_t79" reading_order_no="74" segment_no="5" tag_type="figure">d ˆ</text>
<text top="223" left="471" width="5" height="11" font="font27" id="p4_t80" reading_order_no="76" segment_no="5" tag_type="figure">d</text>
<text top="221" left="463" width="27" height="13" font="font28" id="p4_t81" reading_order_no="75" segment_no="5" tag_type="figure">  </text>
<text top="182" left="435" width="75" height="13" font="font25" id="p4_t82" reading_order_no="72" segment_no="5" tag_type="figure">Add Perturbation </text>
<text top="184" left="344" width="72" height="11" font="font25" id="p4_t83" reading_order_no="71" segment_no="5" tag_type="figure">Untrusted Image d</text>
<text top="326" left="430" width="9" height="11" font="font25" id="p4_t84" reading_order_no="93" segment_no="5" tag_type="figure">ˆ ?</text>
<text top="326" left="414" width="4" height="11" font="font27" id="p4_t85" reading_order_no="91" segment_no="5" tag_type="figure">y</text>
<text top="325" left="421" width="12" height="12" font="font27" id="p4_t86" reading_order_no="92" segment_no="5" tag_type="figure"> y</text>
<text top="282" left="379" width="22" height="12" font="font25" id="p4_t87" reading_order_no="82" segment_no="5" tag_type="figure">unt ( )</text>
<text top="282" left="360" width="4" height="11" font="font27" id="p4_t88" reading_order_no="79" segment_no="5" tag_type="figure">y</text>
<text top="282" left="376" width="3" height="11" font="font27" id="p4_t89" reading_order_no="81" segment_no="5" tag_type="figure">f</text>
<text top="282" left="391" width="5" height="11" font="font27" id="p4_t90" reading_order_no="83" segment_no="5" tag_type="figure">d</text>
<text top="281" left="367" width="5" height="12" font="font28" id="p4_t91" reading_order_no="80" segment_no="5" tag_type="figure"></text>
<text top="280" left="488" width="3" height="11" font="font25" id="p4_t92" reading_order_no="90" segment_no="5" tag_type="figure">ˆ</text>
<text top="282" left="455" width="3" height="11" font="font25" id="p4_t93" reading_order_no="85" segment_no="5" tag_type="figure">ˆ</text>
<text top="283" left="474" width="21" height="12" font="font25" id="p4_t94" reading_order_no="88" segment_no="5" tag_type="figure">unt ( )</text>
<text top="283" left="454" width="4" height="11" font="font27" id="p4_t95" reading_order_no="84" segment_no="5" tag_type="figure">y</text>
<text top="283" left="471" width="3" height="11" font="font27" id="p4_t96" reading_order_no="87" segment_no="5" tag_type="figure">f</text>
<text top="283" left="486" width="5" height="11" font="font27" id="p4_t97" reading_order_no="89" segment_no="5" tag_type="figure">d</text>
<text top="282" left="461" width="5" height="12" font="font28" id="p4_t98" reading_order_no="86" segment_no="5" tag_type="figure"></text>
<text top="399" left="312" width="22" height="7" font="font8" id="p4_t99" reading_order_no="98" segment_no="8" tag_type="text">Fig. 2.</text>
<text top="399" left="344" width="219" height="7" font="font8" id="p4_t100" reading_order_no="99" segment_no="8" tag_type="text">The workflow of the backdoor detection process of the proposed</text>
<text top="408" left="312" width="24" height="7" font="font8" id="p4_t101" reading_order_no="100" segment_no="8" tag_type="text">method</text>
<text top="429" left="322" width="241" height="9" font="font23" id="p4_t102" reading_order_no="101" segment_no="10" tag_type="text">Backdoor Detection in the Training Stage: In this sce-</text>
<text top="441" left="312" width="251" height="9" font="font7" id="p4_t103" reading_order_no="102" segment_no="10" tag_type="text">nario, the training data is obtained from untrusted sources.</text>
<text top="453" left="312" width="251" height="9" font="font7" id="p4_t104" reading_order_no="103" segment_no="10" tag_type="text">The defender attempts to figure out whether the training</text>
<text top="465" left="312" width="251" height="9" font="font7" id="p4_t105" reading_order_no="104" segment_no="10" tag_type="text">dataset contains backdoor instances. If the training dataset</text>
<text top="477" left="312" width="251" height="9" font="font7" id="p4_t106" reading_order_no="105" segment_no="10" tag_type="text">contains backdoor instances, the defender aims to remove the</text>
<text top="489" left="312" width="251" height="9" font="font7" id="p4_t107" reading_order_no="106" segment_no="10" tag_type="text">backdoor instances injected in the training dataset. For each</text>
<text top="501" left="312" width="251" height="9" font="font7" id="p4_t108" reading_order_no="107" segment_no="10" tag_type="text">image in the training set, it will be added with the universal</text>
<text top="512" left="312" width="251" height="9" font="font7" id="p4_t109" reading_order_no="108" segment_no="10" tag_type="text">perturbation [13], and then input into the untrusted model.</text>
<text top="524" left="312" width="251" height="9" font="font7" id="p4_t110" reading_order_no="109" segment_no="10" tag_type="text">The unperturbed image will also be input into the untrusted</text>
<text top="536" left="312" width="251" height="9" font="font7" id="p4_t111" reading_order_no="110" segment_no="10" tag_type="text">model. If the predictions of the model on the perturbed image</text>
<text top="548" left="312" width="251" height="9" font="font7" id="p4_t112" reading_order_no="111" segment_no="10" tag_type="text">and unperturbed image are consistent, this image will be</text>
<text top="560" left="312" width="251" height="9" font="font7" id="p4_t113" reading_order_no="112" segment_no="10" tag_type="text">considered as a backdoor instance. Meanwhile, the untrusted</text>
<text top="572" left="312" width="251" height="9" font="font7" id="p4_t114" reading_order_no="113" segment_no="10" tag_type="text">model is considered to be backdoored. This backdoor detection</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p4_t115" reading_order_no="114" segment_no="10" tag_type="text">procedure will be applied for each image in the training set.</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p4_t116" reading_order_no="115" segment_no="10" tag_type="text">Once the backdoor instances are removed, a clean model can</text>
<text top="608" left="312" width="177" height="9" font="font7" id="p4_t117" reading_order_no="116" segment_no="10" tag_type="text">be trained on the sanitized training dataset.</text>
<text top="620" left="322" width="241" height="9" font="font23" id="p4_t118" reading_order_no="117" segment_no="13" tag_type="text">Backdoor Detection in the Inference Stage: In the in-</text>
<text top="632" left="312" width="251" height="9" font="font7" id="p4_t119" reading_order_no="118" segment_no="13" tag_type="text">ference stage, the well-trained model is deployed to provide</text>
<text top="644" left="312" width="251" height="9" font="font7" id="p4_t120" reading_order_no="119" segment_no="13" tag_type="text">prediction services. The goal of the defender in this scenario</text>
<text top="656" left="312" width="251" height="9" font="font7" id="p4_t121" reading_order_no="120" segment_no="13" tag_type="text">is to detect whether an input image carries a trigger. Given</text>
<text top="668" left="312" width="252" height="9" font="font7" id="p4_t122" reading_order_no="121" segment_no="13" tag_type="text">an input image d , after being added with perturbation η , the</text>
<text top="677" left="312" width="251" height="12" font="font7" id="p4_t123" reading_order_no="122" segment_no="13" tag_type="text">perturbed image d ˆ and the unperturbed image d will be input</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p4_t124" reading_order_no="123" segment_no="13" tag_type="text">into the model. If the predicted labels of the perturbed image</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p4_t125" reading_order_no="124" segment_no="13" tag_type="text">is consistent with that of the unperturbed image, the input</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p4_t126" reading_order_no="125" segment_no="13" tag_type="text">image is considered to be a backdoor instance. Meanwhile,</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p4_t127" reading_order_no="126" segment_no="13" tag_type="text">the model is considered to be a backdoored model, and the</text>
<text top="740" left="312" width="208" height="9" font="font7" id="p4_t128" reading_order_no="127" segment_no="13" tag_type="text">predicted label is considered to be the target label.</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font29" size="10" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font30" size="9" family="TimesNewRomanPSMT" color="#000000"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">5</text>
<text top="58" left="49" width="251" height="9" font="font12" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="text">D. Why choose UAP [13] for adversarial perturbation gener-<a href="deeplearning_paper26.html#9">[13] </a>for adversarial perturbation gener-</text>
<text top="70" left="49" width="25" height="9" font="font12" id="p5_t3" reading_order_no="2" segment_no="1" tag_type="text">ation?</text>
<text top="88" left="59" width="241" height="9" font="font7" id="p5_t4" reading_order_no="3" segment_no="4" tag_type="text">In this paper, we exploit adversarial perturbation to perturb</text>
<text top="100" left="49" width="251" height="9" font="font7" id="p5_t5" reading_order_no="4" segment_no="4" tag_type="text">the main content of the backdoor instance other than the</text>
<text top="112" left="49" width="251" height="9" font="font7" id="p5_t6" reading_order_no="5" segment_no="4" tag_type="text">trigger. However, not all kinds of adversarial perturbation</text>
<text top="124" left="49" width="251" height="9" font="font7" id="p5_t7" reading_order_no="6" segment_no="4" tag_type="text">generation methods are suitable to use in the proposed method.</text>
<text top="136" left="49" width="251" height="9" font="font7" id="p5_t8" reading_order_no="7" segment_no="4" tag_type="text">We evaluate four different adversarial perturbation generation</text>
<text top="148" left="49" width="251" height="9" font="font7" id="p5_t9" reading_order_no="8" segment_no="4" tag_type="text">methods [13], [20]–[22] in Section IV-D. The experimental<a href="deeplearning_paper26.html#9">[13], [20]–[22] </a>in Section <a href="deeplearning_paper26.html#7">IV-D. </a>The experimental</text>
<text top="160" left="49" width="251" height="9" font="font7" id="p5_t10" reading_order_no="9" segment_no="4" tag_type="text">results show that when the image is perturbed by universal</text>
<text top="172" left="49" width="251" height="9" font="font7" id="p5_t11" reading_order_no="10" segment_no="4" tag_type="text">adversarial perturbation [13], the detection performance of the<a href="deeplearning_paper26.html#9">[13], </a>the detection performance of the</text>
<text top="184" left="49" width="251" height="9" font="font7" id="p5_t12" reading_order_no="11" segment_no="4" tag_type="text">proposed method is the highest among the four adversarial</text>
<text top="196" left="49" width="134" height="9" font="font7" id="p5_t13" reading_order_no="12" segment_no="4" tag_type="text">perturbation generation methods.</text>
<text top="209" left="59" width="241" height="9" font="font7" id="p5_t14" reading_order_no="13" segment_no="7" tag_type="text">The reason is as follows. The existing adversarial example</text>
<text top="221" left="49" width="251" height="9" font="font7" id="p5_t15" reading_order_no="14" segment_no="7" tag_type="text">attacks can be divided into two categories, image-specific</text>
<text top="232" left="49" width="251" height="9" font="font7" id="p5_t16" reading_order_no="15" segment_no="7" tag_type="text">adversarial attack and image-agnostic adversarial attack [23].<a href="deeplearning_paper26.html#9">[23].</a></text>
<text top="244" left="49" width="251" height="9" font="font7" id="p5_t17" reading_order_no="16" segment_no="7" tag_type="text">For image-specific adversarial attack, one perturbation can</text>
<text top="256" left="49" width="251" height="9" font="font7" id="p5_t18" reading_order_no="17" segment_no="7" tag_type="text">only fool the model for one specific image [24]. The ground-<a href="deeplearning_paper26.html#9">[24]. </a>The ground-</text>
<text top="268" left="49" width="251" height="9" font="font7" id="p5_t19" reading_order_no="18" segment_no="7" tag_type="text">truth label of the specific image is required in order to generate</text>
<text top="280" left="49" width="251" height="9" font="font7" id="p5_t20" reading_order_no="19" segment_no="7" tag_type="text">the image-specific perturbation which can cause the perturbed</text>
<text top="292" left="49" width="251" height="9" font="font7" id="p5_t21" reading_order_no="20" segment_no="7" tag_type="text">image to be misclassified from its ground-truth label to other</text>
<text top="304" left="49" width="251" height="9" font="font7" id="p5_t22" reading_order_no="21" segment_no="7" tag_type="text">label [24]. However, for backdoor instance, the label used<a href="deeplearning_paper26.html#9">[24]. </a>However, for backdoor instance, the label used</text>
<text top="316" left="49" width="251" height="9" font="font7" id="p5_t23" reading_order_no="22" segment_no="7" tag_type="text">to generate the image-specific perturbation is the target label</text>
<text top="328" left="49" width="251" height="9" font="font7" id="p5_t24" reading_order_no="23" segment_no="7" tag_type="text">rather than the ground-truth label. In other words, for backdoor</text>
<text top="340" left="49" width="251" height="9" font="font7" id="p5_t25" reading_order_no="24" segment_no="7" tag_type="text">instances, the image-specific perturbation is generated in order</text>
<text top="352" left="49" width="251" height="9" font="font7" id="p5_t26" reading_order_no="25" segment_no="7" tag_type="text">to change the predicted result of perturbed backdoor instance</text>
<text top="364" left="49" width="251" height="9" font="font7" id="p5_t27" reading_order_no="26" segment_no="7" tag_type="text">from the target label to other one. Under this circumstance,</text>
<text top="376" left="49" width="251" height="9" font="font7" id="p5_t28" reading_order_no="27" segment_no="7" tag_type="text">the generated image-specific perturbation will strongly affect</text>
<text top="388" left="49" width="251" height="9" font="font7" id="p5_t29" reading_order_no="28" segment_no="7" tag_type="text">the trigger, as the trigger contributes heavily to the predicted</text>
<text top="400" left="49" width="251" height="9" font="font7" id="p5_t30" reading_order_no="29" segment_no="7" tag_type="text">result and the predicted result is the target label. Once the</text>
<text top="412" left="49" width="251" height="9" font="font7" id="p5_t31" reading_order_no="30" segment_no="7" tag_type="text">trigger is strongly affected by the image-specific perturbation,</text>
<text top="424" left="49" width="251" height="9" font="font7" id="p5_t32" reading_order_no="31" segment_no="7" tag_type="text">the predicted label of the backdoor instance after perturba-</text>
<text top="436" left="49" width="251" height="9" font="font7" id="p5_t33" reading_order_no="32" segment_no="7" tag_type="text">tion will change. Then the detection method will incorrectly</text>
<text top="448" left="49" width="251" height="9" font="font7" id="p5_t34" reading_order_no="33" segment_no="7" tag_type="text">consider this backdoor instance as a clean one. For image-</text>
<text top="460" left="49" width="251" height="9" font="font7" id="p5_t35" reading_order_no="34" segment_no="7" tag_type="text">agnostic adversarial attack, it only needs to generate one</text>
<text top="472" left="49" width="251" height="9" font="font7" id="p5_t36" reading_order_no="35" segment_no="7" tag_type="text">single perturbation, which can cause misclassification for all</text>
<text top="484" left="49" width="251" height="9" font="font7" id="p5_t37" reading_order_no="36" segment_no="7" tag_type="text">images when the perturbation is added to those images [13].<a href="deeplearning_paper26.html#9">[13].</a></text>
<text top="495" left="49" width="251" height="9" font="font7" id="p5_t38" reading_order_no="37" segment_no="7" tag_type="text">This single perturbation is generated based on a small set of</text>
<text top="507" left="49" width="251" height="9" font="font7" id="p5_t39" reading_order_no="38" segment_no="7" tag_type="text">clean images [13]. Therefore, the trigger stamped in backdoor<a href="deeplearning_paper26.html#9">[13]. </a>Therefore, the trigger stamped in backdoor</text>
<text top="519" left="49" width="251" height="9" font="font7" id="p5_t40" reading_order_no="39" segment_no="7" tag_type="text">instance will only be slightly affected by the generated image-</text>
<text top="531" left="49" width="88" height="9" font="font7" id="p5_t41" reading_order_no="40" segment_no="7" tag_type="text">agnostic perturbation.</text>
<text top="544" left="59" width="241" height="9" font="font7" id="p5_t42" reading_order_no="41" segment_no="13" tag_type="text">In summary, UAP [13], as a kind of image-agnostic per-<a href="deeplearning_paper26.html#9">[13], </a>as a kind of image-agnostic per-</text>
<text top="556" left="49" width="251" height="9" font="font7" id="p5_t43" reading_order_no="42" segment_no="13" tag_type="text">turbation, has much less influence on the trigger than the</text>
<text top="568" left="49" width="251" height="9" font="font7" id="p5_t44" reading_order_no="43" segment_no="13" tag_type="text">image-specific perturbation, so the label of backdoor instance</text>
<text top="580" left="49" width="251" height="9" font="font7" id="p5_t45" reading_order_no="44" segment_no="13" tag_type="text">will keep unchanged even after being perturbed by UAP [13].<a href="deeplearning_paper26.html#9">[13].</a></text>
<text top="592" left="49" width="251" height="9" font="font7" id="p5_t46" reading_order_no="45" segment_no="13" tag_type="text">Therefore, we choose UAP [13] as the perturbation generation<a href="deeplearning_paper26.html#9">[13] </a>as the perturbation generation</text>
<text top="604" left="49" width="154" height="9" font="font7" id="p5_t47" reading_order_no="46" segment_no="13" tag_type="text">method used in the proposed method.</text>
<text top="636" left="111" width="127" height="9" font="font7" id="p5_t48" reading_order_no="47" segment_no="16" tag_type="title">IV. E XPERIMENTAL R ESULTS</text>
<text top="656" left="59" width="241" height="9" font="font7" id="p5_t49" reading_order_no="48" segment_no="17" tag_type="text">In this section, first, we introduce the datasets, the cor-</text>
<text top="668" left="49" width="251" height="9" font="font7" id="p5_t50" reading_order_no="49" segment_no="17" tag_type="text">responding DNN models, and the metrics used to evaluate</text>
<text top="680" left="49" width="251" height="9" font="font7" id="p5_t51" reading_order_no="50" segment_no="17" tag_type="text">the proposed approach. Second, the experimental results are</text>
<text top="692" left="49" width="251" height="9" font="font7" id="p5_t52" reading_order_no="51" segment_no="17" tag_type="text">analyzed. Third, we evaluate the defense performance of</text>
<text top="704" left="49" width="251" height="9" font="font7" id="p5_t53" reading_order_no="52" segment_no="17" tag_type="text">the proposed method against backdoor attacks with different</text>
<text top="716" left="49" width="251" height="9" font="font7" id="p5_t54" reading_order_no="53" segment_no="17" tag_type="text">settings (trigger transparency, trigger size and trigger pattern).</text>
<text top="728" left="49" width="251" height="9" font="font7" id="p5_t55" reading_order_no="54" segment_no="17" tag_type="text">Last, performance comparisons between the proposed method</text>
<text top="740" left="49" width="242" height="9" font="font7" id="p5_t56" reading_order_no="55" segment_no="17" tag_type="text">and the existing backdoor detection technique is presented.</text>
<text top="58" left="312" width="93" height="9" font="font12" id="p5_t57" reading_order_no="56" segment_no="2" tag_type="title">A. Experimental Setup</text>
<text top="73" left="322" width="241" height="9" font="font12" id="p5_t58" reading_order_no="57" segment_no="3" tag_type="text">1) Datasets : We evaluate the proposed method on three</text>
<text top="85" left="312" width="251" height="9" font="font7" id="p5_t59" reading_order_no="58" segment_no="3" tag_type="text">benchmark datasets: Fashion-MNIST [9], CIFAR-10 [10] and</text>
<text top="97" left="312" width="90" height="9" font="font7" id="p5_t60" reading_order_no="59" segment_no="3" tag_type="text">GTSRB [11] datasets.</text>
<text top="111" left="322" width="241" height="9" font="font23" id="p5_t61" reading_order_no="60" segment_no="5" tag_type="list">• Fashion-MNIST [9] is a dataset consists of a training set</text>
<text top="123" left="332" width="231" height="9" font="font7" id="p5_t62" reading_order_no="61" segment_no="5" tag_type="list">with 60,000 images and a test set with 10,000 images.</text>
<text top="134" left="332" width="231" height="10" font="font7" id="p5_t63" reading_order_no="62" segment_no="5" tag_type="list">Each image is a 28 × 28 grayscale image, assigned with</text>
<text top="147" left="332" width="231" height="9" font="font7" id="p5_t64" reading_order_no="63" segment_no="5" tag_type="list">a label from 10 classes [9]. In the experiment, the model</text>
<text top="159" left="332" width="166" height="9" font="font7" id="p5_t65" reading_order_no="64" segment_no="5" tag_type="list">trained on this dataset is DenseNet [25].<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and</text>
<text top="171" left="322" width="241" height="9" font="font23" id="p5_t66" reading_order_no="65" segment_no="6" tag_type="list">• CIFAR-10 [10] consists of a training set with 50,000<a href="deeplearning_paper26.html#9">[11] </a>datasets.</text>
<text top="183" left="332" width="231" height="9" font="font7" id="p5_t67" reading_order_no="66" segment_no="6" tag_type="list">images and a test set with 10,000 images. Each image is</text>
<text top="194" left="332" width="231" height="10" font="font7" id="p5_t68" reading_order_no="67" segment_no="6" tag_type="list">a 32 × 32 colored image, belonging to one of 10 classes</text>
<text top="207" left="332" width="231" height="9" font="font7" id="p5_t69" reading_order_no="68" segment_no="6" tag_type="list">[10]. In the experiment, the model trained on this dataset<a href="deeplearning_paper26.html#9">[9] </a>is a dataset consists of a training set</text>
<text top="219" left="332" width="62" height="9" font="font7" id="p5_t70" reading_order_no="69" segment_no="6" tag_type="list">is ResNet [26].</text>
<text top="231" left="322" width="241" height="9" font="font23" id="p5_t71" reading_order_no="70" segment_no="8" tag_type="list">• GTSRB [11] is a dataset containing 39,209 labeled</text>
<text top="243" left="332" width="231" height="9" font="font7" id="p5_t72" reading_order_no="71" segment_no="8" tag_type="list">images, which are categorized into 43 classes. GTSRB</text>
<text top="255" left="332" width="231" height="9" font="font7" id="p5_t73" reading_order_no="72" segment_no="8" tag_type="list">has 35,209 training images, 4,000 validation images and</text>
<text top="267" left="332" width="231" height="9" font="font7" id="p5_t74" reading_order_no="73" segment_no="8" tag_type="list">12,630 test images [11]. In the experiment, the model</text>
<text top="279" left="332" width="160" height="9" font="font7" id="p5_t75" reading_order_no="74" segment_no="8" tag_type="list">trained on this dataset is AlexNet [27].</text>
<text top="292" left="322" width="241" height="9" font="font12" id="p5_t76" reading_order_no="75" segment_no="9" tag_type="text">2) Experimental Settings of Backdoor Attack : The trigger<a href="deeplearning_paper26.html#9">[9]. </a>In the experiment, the model</text>
<text top="304" left="312" width="251" height="9" font="font7" id="p5_t77" reading_order_no="76" segment_no="9" tag_type="text">used in Fashion-MNIST [9] images is four 1 × 10 rectangles<a href="deeplearning_paper26.html#9">[25].</a></text>
<text top="316" left="312" width="251" height="9" font="font7" id="p5_t78" reading_order_no="77" segment_no="9" tag_type="text">placed at the corners (four corners in total) of the image,</text>
<text top="328" left="312" width="251" height="9" font="font7" id="p5_t79" reading_order_no="78" segment_no="9" tag_type="text">and the intensity of the trigger is 0.15. The trigger used in</text>
<text top="339" left="312" width="251" height="10" font="font7" id="p5_t80" reading_order_no="79" segment_no="9" tag_type="text">CIFAR-10 [10] and GTSRB [11] images is a 4 × 4 square.<a href="deeplearning_paper26.html#9">[10] </a>consists of a training set with 50,000</text>
<text top="352" left="312" width="251" height="9" font="font7" id="p5_t81" reading_order_no="80" segment_no="9" tag_type="text">The intensities of triggers in CIFAR-10 and GTSRB are set</text>
<text top="364" left="312" width="251" height="9" font="font7" id="p5_t82" reading_order_no="81" segment_no="9" tag_type="text">to be 0.5 and 0.2 respectively. Some backdoor instances used</text>
<text top="376" left="312" width="176" height="9" font="font7" id="p5_t83" reading_order_no="82" segment_no="9" tag_type="text">in the experiments are illustrated in Fig. 3.</text>
<text top="466" left="340" width="10" height="8" font="font30" id="p5_t84" reading_order_no="83" segment_no="10" tag_type="figure">(a)</text>
<text top="466" left="523" width="10" height="8" font="font30" id="p5_t85" reading_order_no="85" segment_no="10" tag_type="figure">(c)</text>
<text top="466" left="430" width="11" height="8" font="font30" id="p5_t86" reading_order_no="84" segment_no="10" tag_type="figure">(b)</text>
<text top="488" left="312" width="22" height="7" font="font8" id="p5_t87" reading_order_no="86" segment_no="11" tag_type="text">Fig. 3.<a href="deeplearning_paper26.html#9">[10]. </a>In the experiment, the model trained on this dataset</text>
<text top="488" left="344" width="219" height="7" font="font8" id="p5_t88" reading_order_no="87" segment_no="11" tag_type="text">Examples of backdoor instances: (a) Fashion-MNIST images; (b)<a href="deeplearning_paper26.html#9">[26].</a></text>
<text top="497" left="312" width="129" height="7" font="font8" id="p5_t89" reading_order_no="88" segment_no="11" tag_type="text">CIFAR-10 images; (c) GTSRB images.</text>
<text top="518" left="322" width="241" height="10" font="font12" id="p5_t90" reading_order_no="89" segment_no="12" tag_type="text">3) Metrics : Backdoor Attack Success Rate (BASR).</text>
<text top="530" left="312" width="251" height="9" font="font7" id="p5_t91" reading_order_no="90" segment_no="12" tag_type="text">Backdoor Attack Success Rate is defined as the percentage<a href="deeplearning_paper26.html#9">[11] </a>is a dataset containing 39,209 labeled</text>
<text top="542" left="312" width="251" height="9" font="font7" id="p5_t92" reading_order_no="91" segment_no="12" tag_type="text">of backdoor instances that are successfully classified as the</text>
<text top="554" left="312" width="188" height="9" font="font7" id="p5_t93" reading_order_no="92" segment_no="12" tag_type="text">target class among all backdoor instances [1].</text>
<text top="566" left="322" width="241" height="9" font="font23" id="p5_t94" reading_order_no="93" segment_no="14" tag_type="text">Backdoor Detection Rate (BDR). Backdoor Detection<a href="deeplearning_paper26.html#9">[11]. </a>In the experiment, the model</text>
<text top="578" left="312" width="251" height="9" font="font7" id="p5_t95" reading_order_no="94" segment_no="14" tag_type="text">Rate is defined as the percentage of backdoor instances that<a href="deeplearning_paper26.html#9">[27].</a></text>
<text top="590" left="312" width="251" height="9" font="font7" id="p5_t96" reading_order_no="95" segment_no="14" tag_type="text">are successfully detected by the proposed method among all</text>
<text top="602" left="312" width="80" height="9" font="font7" id="p5_t97" reading_order_no="96" segment_no="14" tag_type="text">backdoor instances.</text>
<text top="614" left="322" width="241" height="9" font="font23" id="p5_t98" reading_order_no="97" segment_no="15" tag_type="text">Clean Image Identification Rate (CIIR). Clean Image</text>
<text top="626" left="312" width="251" height="9" font="font7" id="p5_t99" reading_order_no="98" segment_no="15" tag_type="text">Identification Rate is defined as the percentage of clean images</text>
<text top="638" left="312" width="251" height="9" font="font7" id="p5_t100" reading_order_no="99" segment_no="15" tag_type="text">that are correctly classified as clean ones among all clean<a href="deeplearning_paper26.html#9">[9] </a>images is four</text>
<text top="650" left="312" width="31" height="9" font="font7" id="p5_t101" reading_order_no="100" segment_no="15" tag_type="text">images.</text>
<text top="677" left="312" width="167" height="9" font="font12" id="p5_t102" reading_order_no="101" segment_no="18" tag_type="title">B. Effectiveness of the Proposed Method</text>
<text top="692" left="322" width="241" height="9" font="font7" id="p5_t103" reading_order_no="102" segment_no="19" tag_type="text">In this section, the defense performances of the proposed</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p5_t104" reading_order_no="103" segment_no="19" tag_type="text">method on Fashion-MNIST [9], CIFAR-10 [10] and GTSRB</text>
<text top="716" left="312" width="111" height="9" font="font7" id="p5_t105" reading_order_no="104" segment_no="19" tag_type="text">[11] datasets are presented.</text>
<text top="728" left="322" width="241" height="9" font="font7" id="p5_t106" reading_order_no="105" segment_no="20" tag_type="text">Table I shows the backdoor attack success rate of the back-</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p5_t107" reading_order_no="106" segment_no="20" tag_type="text">door attack on the Fashion-MNIST, CIFAR-10 and GTSRB<a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>images is a</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font31" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="text">6</text>
<text top="58" left="49" width="251" height="9" font="font7" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">datasets and the corresponding classification accuracy of the</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p6_t3" reading_order_no="2" segment_no="1" tag_type="text">backdoored model. For each dataset, all test images are in-</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p6_t4" reading_order_no="3" segment_no="1" tag_type="text">jected with trigger to evaluate the backdoor attack success rate.</text>
<text top="94" left="49" width="251" height="9" font="font7" id="p6_t5" reading_order_no="4" segment_no="1" tag_type="text">The classification accuracy is evaluated on all the clean test</text>
<text top="106" left="49" width="251" height="9" font="font7" id="p6_t6" reading_order_no="5" segment_no="1" tag_type="text">images for each dataset. As shown in Table I, the classification<a href="deeplearning_paper26.html#6">I, </a>the classification</text>
<text top="118" left="49" width="251" height="9" font="font7" id="p6_t7" reading_order_no="6" segment_no="1" tag_type="text">accuracy on clean images of the backdoored model is 92.19%,</text>
<text top="130" left="49" width="251" height="9" font="font7" id="p6_t8" reading_order_no="7" segment_no="1" tag_type="text">92.77% and 95.16% on Fashion-MNIST [9], CIFAR-10 [10]<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="142" left="49" width="251" height="9" font="font7" id="p6_t9" reading_order_no="8" segment_no="1" tag_type="text">and GTSRB [11] respectively. Without the proposed defense<a href="deeplearning_paper26.html#9">[11] </a>respectively. Without the proposed defense</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p6_t10" reading_order_no="9" segment_no="1" tag_type="text">method, the backdoor attack success rate (BASR) is 99.47%,</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p6_t11" reading_order_no="10" segment_no="1" tag_type="text">99.77% and 97.89% on Fashion-MNIST [9], CIFAR-10 [10]<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="178" left="49" width="123" height="9" font="font7" id="p6_t12" reading_order_no="11" segment_no="1" tag_type="text">and GTSRB [11] respectively.<a href="deeplearning_paper26.html#9">[11] </a>respectively.</text>
<text top="197" left="159" width="30" height="7" font="font8" id="p6_t13" reading_order_no="12" segment_no="4" tag_type="title">TABLE I</text>
<text top="206" left="51" width="247" height="7" font="font8" id="p6_t14" reading_order_no="13" segment_no="5" tag_type="text">C LASSIFICATION ACCURACY AND BACKDOOR ATTACK SUCCESS RATE ON</text>
<text top="216" left="59" width="231" height="6" font="font31" id="p6_t15" reading_order_no="14" segment_no="5" tag_type="text">THREE DIFFERENT CLASSIFICATION TASKS WITHOUT THE PROPOSED</text>
<text top="225" left="156" width="36" height="6" font="font31" id="p6_t16" reading_order_no="15" segment_no="5" tag_type="text">APPROACH</text>
<text top="247" left="90" width="77" height="9" font="font7" id="p6_t17" reading_order_no="16" segment_no="6" tag_type="table">Benchmark dataset</text>
<text top="247" left="198" width="38" height="9" font="font7" id="p6_t18" reading_order_no="17" segment_no="6" tag_type="table">Accuracy</text>
<text top="247" left="250" width="26" height="9" font="font7" id="p6_t19" reading_order_no="18" segment_no="6" tag_type="table">BASR</text>
<text top="264" left="71" width="115" height="9" font="font7" id="p6_t20" reading_order_no="19" segment_no="6" tag_type="table">Fashion-MNIST (DenseNet)</text>
<text top="264" left="201" width="31" height="9" font="font7" id="p6_t21" reading_order_no="20" segment_no="6" tag_type="table">92.19%</text>
<text top="264" left="248" width="31" height="9" font="font7" id="p6_t22" reading_order_no="21" segment_no="6" tag_type="table">99.47%</text>
<text top="276" left="88" width="81" height="9" font="font7" id="p6_t23" reading_order_no="22" segment_no="6" tag_type="table">CIFAR-10 (ResNet)</text>
<text top="276" left="201" width="31" height="9" font="font7" id="p6_t24" reading_order_no="23" segment_no="6" tag_type="table">92.77%</text>
<text top="276" left="248" width="31" height="9" font="font7" id="p6_t25" reading_order_no="24" segment_no="6" tag_type="table">99.77%</text>
<text top="288" left="90" width="76" height="9" font="font7" id="p6_t26" reading_order_no="25" segment_no="6" tag_type="table">GTSRB (AlexNet)</text>
<text top="288" left="201" width="31" height="9" font="font7" id="p6_t27" reading_order_no="26" segment_no="6" tag_type="table">95.16%</text>
<text top="288" left="248" width="31" height="9" font="font7" id="p6_t28" reading_order_no="27" segment_no="6" tag_type="table">97.89%</text>
<text top="314" left="59" width="241" height="9" font="font7" id="p6_t29" reading_order_no="28" segment_no="7" tag_type="text">Table II shows the clean image identification rate, the</text>
<text top="326" left="49" width="251" height="9" font="font7" id="p6_t30" reading_order_no="29" segment_no="7" tag_type="text">backdoor detection rate and the intensity of universal pertur-<a href="deeplearning_paper26.html#6">II </a>shows the clean image identification rate, the</text>
<text top="338" left="49" width="251" height="9" font="font7" id="p6_t31" reading_order_no="30" segment_no="7" tag_type="text">bation on three datasets after the proposed method is applied.</text>
<text top="350" left="49" width="251" height="9" font="font7" id="p6_t32" reading_order_no="31" segment_no="7" tag_type="text">In this paper, after the proposed method is deployed, the</text>
<text top="362" left="49" width="251" height="9" font="font7" id="p6_t33" reading_order_no="32" segment_no="7" tag_type="text">clean image identification rate is calculated on a set of 2,000</text>
<text top="374" left="49" width="251" height="9" font="font7" id="p6_t34" reading_order_no="33" segment_no="7" tag_type="text">clean images randomly selected from the test images for each</text>
<text top="386" left="49" width="251" height="9" font="font7" id="p6_t35" reading_order_no="34" segment_no="7" tag_type="text">dataset. Similarly, in this paper, the backdoor detection rate</text>
<text top="398" left="49" width="251" height="9" font="font7" id="p6_t36" reading_order_no="35" segment_no="7" tag_type="text">is calculated on a set of 2,000 backdoor instances generated</text>
<text top="410" left="49" width="251" height="9" font="font7" id="p6_t37" reading_order_no="36" segment_no="7" tag_type="text">by adding trigger to 2,000 images randomly selected from the</text>
<text top="422" left="49" width="251" height="9" font="font7" id="p6_t38" reading_order_no="37" segment_no="7" tag_type="text">test images for each dataset. As shown in Table II, after the</text>
<text top="434" left="49" width="251" height="9" font="font7" id="p6_t39" reading_order_no="38" segment_no="7" tag_type="text">proposed defense method is deployed, the backdoor detection<a href="deeplearning_paper26.html#6">II, </a>after the</text>
<text top="446" left="49" width="251" height="9" font="font7" id="p6_t40" reading_order_no="39" segment_no="7" tag_type="text">rate is 99.63%, 99.76% and 99.91% on Fashion-MNIST [9],</text>
<text top="458" left="49" width="251" height="9" font="font7" id="p6_t41" reading_order_no="40" segment_no="7" tag_type="text">CIFAR-10 [10] and GTSRB [11] respectively. Meanwhile,<a href="deeplearning_paper26.html#9">[9],</a></text>
<text top="470" left="49" width="251" height="9" font="font7" id="p6_t42" reading_order_no="41" segment_no="7" tag_type="text">the clean image identification rate (CIIR) of the proposed<a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively. Meanwhile,</text>
<text top="482" left="49" width="251" height="9" font="font7" id="p6_t43" reading_order_no="42" segment_no="7" tag_type="text">method is 90.66%, 89.82% and 98.85% on Fashion-MNIST</text>
<text top="494" left="49" width="206" height="9" font="font7" id="p6_t44" reading_order_no="43" segment_no="7" tag_type="text">[9], CIFAR-10 [10] and GTSRB [11] respectively.</text>
<text top="513" left="158" width="33" height="7" font="font8" id="p6_t45" reading_order_no="44" segment_no="11" tag_type="title">TABLE II<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively.</text>
<text top="522" left="54" width="240" height="7" font="font8" id="p6_t46" reading_order_no="45" segment_no="12" tag_type="text">T HE BACKDOOR DETECTION RATE , THE CLEAN IMAGE IDENTIFICATION</text>
<text top="532" left="55" width="239" height="6" font="font31" id="p6_t47" reading_order_no="46" segment_no="12" tag_type="text">RATE AND THE PERTURBATION INTENSITY ON THREE DATASETS AFTER</text>
<text top="541" left="97" width="154" height="6" font="font31" id="p6_t48" reading_order_no="47" segment_no="12" tag_type="text">THE PROPOSED DEFENSE METHOD IS APPLIED</text>
<text top="563" left="93" width="30" height="9" font="font7" id="p6_t49" reading_order_no="48" segment_no="14" tag_type="table">Dataset</text>
<text top="563" left="166" width="20" height="9" font="font7" id="p6_t50" reading_order_no="49" segment_no="14" tag_type="table">CIIR</text>
<text top="563" left="209" width="20" height="9" font="font7" id="p6_t51" reading_order_no="50" segment_no="14" tag_type="table">BDR</text>
<text top="563" left="246" width="35" height="9" font="font7" id="p6_t52" reading_order_no="51" segment_no="14" tag_type="table">Intensity</text>
<text top="580" left="68" width="81" height="9" font="font7" id="p6_t53" reading_order_no="52" segment_no="14" tag_type="table">Fashion-MNIST [9]</text>
<text top="580" left="161" width="31" height="9" font="font7" id="p6_t54" reading_order_no="53" segment_no="14" tag_type="table">90.66%</text>
<text top="580" left="203" width="31" height="9" font="font7" id="p6_t55" reading_order_no="54" segment_no="14" tag_type="table">99.63%</text>
<text top="580" left="250" width="27" height="9" font="font7" id="p6_t56" reading_order_no="55" segment_no="14" tag_type="table">2.8715</text>
<text top="592" left="77" width="62" height="9" font="font7" id="p6_t57" reading_order_no="56" segment_no="14" tag_type="table">CIFAR-10 [10]<a href="deeplearning_paper26.html#9">[9]</a></text>
<text top="592" left="161" width="31" height="9" font="font7" id="p6_t58" reading_order_no="57" segment_no="14" tag_type="table">89.82%</text>
<text top="592" left="203" width="31" height="9" font="font7" id="p6_t59" reading_order_no="58" segment_no="14" tag_type="table">99.76%</text>
<text top="592" left="250" width="27" height="9" font="font7" id="p6_t60" reading_order_no="59" segment_no="14" tag_type="table">3.0513</text>
<text top="604" left="82" width="52" height="9" font="font7" id="p6_t61" reading_order_no="60" segment_no="14" tag_type="table">GTSRB [11]<a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="604" left="161" width="31" height="9" font="font7" id="p6_t62" reading_order_no="61" segment_no="14" tag_type="table">98.85%</text>
<text top="604" left="203" width="31" height="9" font="font7" id="p6_t63" reading_order_no="62" segment_no="14" tag_type="table">99.91%</text>
<text top="604" left="250" width="27" height="9" font="font7" id="p6_t64" reading_order_no="63" segment_no="14" tag_type="table">2.4362</text>
<text top="630" left="59" width="241" height="9" font="font7" id="p6_t65" reading_order_no="64" segment_no="15" tag_type="text">Overall, experimental results show that the proposed defense<a href="deeplearning_paper26.html#9">[11]</a></text>
<text top="642" left="49" width="251" height="9" font="font7" id="p6_t66" reading_order_no="65" segment_no="15" tag_type="text">method can effectively detect backdoor attacks on different</text>
<text top="654" left="49" width="251" height="9" font="font7" id="p6_t67" reading_order_no="66" segment_no="15" tag_type="text">datasets and DNN architectures. In the three datasets, the</text>
<text top="666" left="49" width="251" height="9" font="font7" id="p6_t68" reading_order_no="67" segment_no="15" tag_type="text">proposed method can achieve high backdoor detection rate</text>
<text top="678" left="49" width="165" height="9" font="font7" id="p6_t69" reading_order_no="68" segment_no="15" tag_type="text">and high clean image identification rate.</text>
<text top="701" left="49" width="251" height="9" font="font12" id="p6_t70" reading_order_no="69" segment_no="17" tag_type="title">C. Defense Performance of the Proposed Method under Dif-</text>
<text top="713" left="49" width="87" height="9" font="font12" id="p6_t71" reading_order_no="70" segment_no="17" tag_type="title">ferent Attack Settings</text>
<text top="728" left="59" width="241" height="9" font="font7" id="p6_t72" reading_order_no="71" segment_no="18" tag_type="text">In this section, we evaluate the performance of the proposed</text>
<text top="740" left="49" width="251" height="9" font="font7" id="p6_t73" reading_order_no="72" segment_no="18" tag_type="text">method under different trigger settings (trigger transparency</text>
<text top="58" left="312" width="153" height="9" font="font7" id="p6_t74" reading_order_no="73" segment_no="2" tag_type="text">[12], trigger size and trigger pattern).</text>
<text top="71" left="322" width="241" height="9" font="font12" id="p6_t75" reading_order_no="74" segment_no="3" tag_type="text">1) Trigger Transparency : In the experiment, we evaluate</text>
<text top="83" left="312" width="251" height="9" font="font7" id="p6_t76" reading_order_no="75" segment_no="3" tag_type="text">the performance of the proposed method against backdoor</text>
<text top="95" left="312" width="251" height="9" font="font7" id="p6_t77" reading_order_no="76" segment_no="3" tag_type="text">attacks with different trigger transparency settings [12]. The</text>
<text top="107" left="312" width="251" height="9" font="font7" id="p6_t78" reading_order_no="77" segment_no="3" tag_type="text">values of the trigger transparency in the experiment are set<a href="deeplearning_paper26.html#9">[12], </a>trigger size and trigger pattern).</text>
<text top="119" left="312" width="251" height="9" font="font7" id="p6_t79" reading_order_no="78" segment_no="3" tag_type="text">to be 50%, 60%, 70% and 80%, respectively. As shown</text>
<text top="131" left="312" width="251" height="9" font="font7" id="p6_t80" reading_order_no="79" segment_no="3" tag_type="text">in Table III, for the backdoor attacks with different trigger</text>
<text top="143" left="312" width="251" height="9" font="font7" id="p6_t81" reading_order_no="80" segment_no="3" tag_type="text">transparency settings, the backdoor detection rates are all at</text>
<text top="155" left="312" width="251" height="9" font="font7" id="p6_t82" reading_order_no="81" segment_no="3" tag_type="text">a high level on the three datasets. Specifically, when the</text>
<text top="167" left="312" width="251" height="9" font="font7" id="p6_t83" reading_order_no="82" segment_no="3" tag_type="text">trigger transparency is 50%, the backdoor detection rate is</text>
<text top="179" left="312" width="251" height="9" font="font7" id="p6_t84" reading_order_no="83" segment_no="3" tag_type="text">98.80%, 99.70% and 99.96% on Fashion-MNIST [9], CIFAR-<a href="deeplearning_paper26.html#9">[12]. </a>The</text>
<text top="191" left="312" width="251" height="9" font="font7" id="p6_t85" reading_order_no="84" segment_no="3" tag_type="text">10 [10] and GTSRB [11] datasets respectively. When the</text>
<text top="202" left="312" width="251" height="9" font="font7" id="p6_t86" reading_order_no="85" segment_no="3" tag_type="text">trigger transparency increases to 80%, after the proposed</text>
<text top="214" left="312" width="251" height="9" font="font7" id="p6_t87" reading_order_no="86" segment_no="3" tag_type="text">method is applied, the backdoor detection rate is still at a<a href="deeplearning_paper26.html#6">III, </a>for the backdoor attacks with different trigger</text>
<text top="226" left="312" width="251" height="9" font="font7" id="p6_t88" reading_order_no="87" segment_no="3" tag_type="text">high level (99.37%, 96.30% and 99.07% on Fashion-MNIST,</text>
<text top="238" left="312" width="251" height="9" font="font7" id="p6_t89" reading_order_no="88" segment_no="3" tag_type="text">CIFAR-10 and GTSRB datasets respectively). The experimen-</text>
<text top="250" left="312" width="251" height="9" font="font7" id="p6_t90" reading_order_no="89" segment_no="3" tag_type="text">tal results indicate that, the proposed defense method can</text>
<text top="262" left="312" width="251" height="9" font="font7" id="p6_t91" reading_order_no="90" segment_no="3" tag_type="text">effectively detect the backdoor instances with different trigger<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-</text>
<text top="274" left="312" width="251" height="9" font="font7" id="p6_t92" reading_order_no="91" segment_no="3" tag_type="text">transparency settings. The reason is as follows. When the<a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets respectively. When the</text>
<text top="286" left="312" width="251" height="9" font="font7" id="p6_t93" reading_order_no="92" segment_no="3" tag_type="text">trigger transparency is set to be 0%, the trigger is opaque.</text>
<text top="298" left="312" width="251" height="9" font="font7" id="p6_t94" reading_order_no="93" segment_no="3" tag_type="text">When the trigger transparency is set to be 90%, the trigger</text>
<text top="310" left="312" width="251" height="9" font="font7" id="p6_t95" reading_order_no="94" segment_no="3" tag_type="text">is almost invisible. Generally, the higher the transparency of</text>
<text top="322" left="312" width="251" height="9" font="font7" id="p6_t96" reading_order_no="95" segment_no="3" tag_type="text">the trigger, the trigger is more susceptible to the perturbation.</text>
<text top="334" left="312" width="251" height="9" font="font7" id="p6_t97" reading_order_no="96" segment_no="3" tag_type="text">However, UAP [13] is a kind of image-agnostic perturbation,</text>
<text top="346" left="312" width="251" height="9" font="font7" id="p6_t98" reading_order_no="97" segment_no="3" tag_type="text">which is generated based on clean images. Therefore, when</text>
<text top="358" left="312" width="251" height="9" font="font7" id="p6_t99" reading_order_no="98" segment_no="3" tag_type="text">UAP is added to a backdoor instance, the trigger in the</text>
<text top="370" left="312" width="251" height="9" font="font7" id="p6_t100" reading_order_no="99" segment_no="3" tag_type="text">backdoor instance will only be slightly affected. As a result,</text>
<text top="381" left="312" width="251" height="10" font="font7" id="p6_t101" reading_order_no="100" segment_no="3" tag_type="text">even the transparency of trigger is high (50% ∼ 80%), the</text>
<text top="394" left="312" width="251" height="9" font="font7" id="p6_t102" reading_order_no="101" segment_no="3" tag_type="text">proposed method can still achieve high backdoor detection</text>
<text top="406" left="312" width="17" height="9" font="font7" id="p6_t103" reading_order_no="102" segment_no="3" tag_type="text">rate.</text>
<text top="426" left="420" width="36" height="7" font="font8" id="p6_t104" reading_order_no="103" segment_no="8" tag_type="title">TABLE III<a href="deeplearning_paper26.html#9">[13] </a>is a kind of image-agnostic perturbation,</text>
<text top="435" left="316" width="243" height="7" font="font8" id="p6_t105" reading_order_no="104" segment_no="9" tag_type="text">T HE BACKDOOR DETECTION RATE OF THE PROPOSED METHOD AGAINST</text>
<text top="445" left="328" width="220" height="6" font="font31" id="p6_t106" reading_order_no="105" segment_no="9" tag_type="text">BACKDOOR ATTACKS WITH DIFFERENT TRIGGER TRANSPARENCY</text>
<text top="454" left="377" width="121" height="6" font="font31" id="p6_t107" reading_order_no="106" segment_no="9" tag_type="text">SETTINGS ON THE THREE DATASETS</text>
<text top="475" left="323" width="54" height="9" font="font7" id="p6_t108" reading_order_no="107" segment_no="10" tag_type="table">Transparency</text>
<text top="475" left="389" width="66" height="9" font="font7" id="p6_t109" reading_order_no="108" segment_no="10" tag_type="table">Fashion-MNIST</text>
<text top="475" left="466" width="42" height="9" font="font7" id="p6_t110" reading_order_no="109" segment_no="10" tag_type="table">CIFAR-10</text>
<text top="475" left="520" width="32" height="9" font="font7" id="p6_t111" reading_order_no="110" segment_no="10" tag_type="table">GTSRB</text>
<text top="493" left="341" width="18" height="9" font="font7" id="p6_t112" reading_order_no="111" segment_no="10" tag_type="table">80%</text>
<text top="493" left="406" width="31" height="9" font="font7" id="p6_t113" reading_order_no="112" segment_no="10" tag_type="table">99.37%</text>
<text top="493" left="472" width="31" height="9" font="font7" id="p6_t114" reading_order_no="113" segment_no="10" tag_type="table">96.30%</text>
<text top="493" left="521" width="31" height="9" font="font7" id="p6_t115" reading_order_no="114" segment_no="10" tag_type="table">99.07%</text>
<text top="504" left="341" width="18" height="9" font="font7" id="p6_t116" reading_order_no="115" segment_no="10" tag_type="table">70%</text>
<text top="504" left="406" width="31" height="9" font="font7" id="p6_t117" reading_order_no="116" segment_no="10" tag_type="table">99.72%</text>
<text top="504" left="472" width="31" height="9" font="font7" id="p6_t118" reading_order_no="117" segment_no="10" tag_type="table">98.75%</text>
<text top="504" left="521" width="31" height="9" font="font7" id="p6_t119" reading_order_no="118" segment_no="10" tag_type="table">98.62%</text>
<text top="516" left="341" width="18" height="9" font="font7" id="p6_t120" reading_order_no="119" segment_no="10" tag_type="table">60%</text>
<text top="516" left="406" width="31" height="9" font="font7" id="p6_t121" reading_order_no="120" segment_no="10" tag_type="table">95.40%</text>
<text top="516" left="472" width="31" height="9" font="font7" id="p6_t122" reading_order_no="121" segment_no="10" tag_type="table">98.62%</text>
<text top="516" left="521" width="31" height="9" font="font7" id="p6_t123" reading_order_no="122" segment_no="10" tag_type="table">99.90%</text>
<text top="528" left="341" width="18" height="9" font="font7" id="p6_t124" reading_order_no="123" segment_no="10" tag_type="table">50%</text>
<text top="528" left="406" width="31" height="9" font="font7" id="p6_t125" reading_order_no="124" segment_no="10" tag_type="table">98.80%</text>
<text top="528" left="472" width="31" height="9" font="font7" id="p6_t126" reading_order_no="125" segment_no="10" tag_type="table">99.70%</text>
<text top="528" left="521" width="31" height="9" font="font7" id="p6_t127" reading_order_no="126" segment_no="10" tag_type="table">99.96%</text>
<text top="560" left="322" width="241" height="9" font="font12" id="p6_t128" reading_order_no="127" segment_no="13" tag_type="text">2) Trigger Size : In this section, we evaluate the perfor-</text>
<text top="572" left="312" width="251" height="9" font="font7" id="p6_t129" reading_order_no="128" segment_no="13" tag_type="text">mance of the proposed method against backdoor attacks with</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p6_t130" reading_order_no="129" segment_no="13" tag_type="text">three different trigger sizes. The experiment results are shown</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p6_t131" reading_order_no="130" segment_no="13" tag_type="text">in Table IV. As shown in Table IV, for different trigger sizes,</text>
<text top="608" left="312" width="251" height="9" font="font7" id="p6_t132" reading_order_no="131" segment_no="13" tag_type="text">the proposed method can achieve very high backdoor detection</text>
<text top="620" left="312" width="251" height="9" font="font7" id="p6_t133" reading_order_no="132" segment_no="13" tag_type="text">rates (over 99% mostly). Even if the trigger is small, such as</text>
<text top="631" left="312" width="251" height="10" font="font21" id="p6_t134" reading_order_no="133" segment_no="13" tag_type="text">1 × 4 , 2 × 2 , 2 × 2 in Fashion-MNIST [9], CIFAR-10 [10] and</text>
<text top="643" left="312" width="251" height="9" font="font7" id="p6_t135" reading_order_no="134" segment_no="13" tag_type="text">GTSRB [11] datasets, respectively, the proposed method can</text>
<text top="655" left="312" width="251" height="9" font="font7" id="p6_t136" reading_order_no="135" segment_no="13" tag_type="text">still achieve high backdoor detection rates (99.65%, 99.07%,</text>
<text top="667" left="312" width="88" height="9" font="font7" id="p6_t137" reading_order_no="136" segment_no="13" tag_type="text">98.25% respectively).</text>
<text top="680" left="322" width="241" height="9" font="font12" id="p6_t138" reading_order_no="137" segment_no="16" tag_type="text">3) Trigger Pattern : The performance of the proposed</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p6_t139" reading_order_no="138" segment_no="16" tag_type="text">method against backdoor attacks with different trigger patterns</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p6_t140" reading_order_no="139" segment_no="16" tag_type="text">is also evaluated. In the experiment, the square trigger and</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p6_t141" reading_order_no="140" segment_no="16" tag_type="text">cross pattern trigger (referred to as trigger A and trigger</text>
<text top="728" left="312" width="251" height="9" font="font12" id="p6_t142" reading_order_no="141" segment_no="16" tag_type="text">B respectively) are used to evaluate the proposed method.</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p6_t143" reading_order_no="142" segment_no="16" tag_type="text">The square trigger and the cross pattern trigger for the three</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font32" size="10" family="TimesNewRomanPSMT" color="#000000"/>
	<fontspec id="font33" size="10" family="TimesNewRomanPS,Italic" color="#000000"/>
	<fontspec id="font34" size="8" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font35" size="9" family="TimesNewRomanPS,Italic" color="#000000"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="text">7</text>
<text top="53" left="288" width="36" height="7" font="font8" id="p7_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE IV</text>
<text top="62" left="132" width="348" height="7" font="font8" id="p7_t3" reading_order_no="2" segment_no="2" tag_type="text">B ACKDOOR DETECTION RATES OF THE BACKDOOR ATTACKS WITH DIFFERENT TRIGGER SIZE SETTINGS</text>
<text top="81" left="96" width="30" height="9" font="font7" id="p7_t4" reading_order_no="3" segment_no="3" tag_type="table">Dataset</text>
<text top="81" left="156" width="81" height="9" font="font7" id="p7_t5" reading_order_no="4" segment_no="3" tag_type="table">Fashion-MNIST [9]</text>
<text top="81" left="294" width="62" height="9" font="font7" id="p7_t6" reading_order_no="5" segment_no="3" tag_type="table">CIFAR-10 [10]<a href="deeplearning_paper26.html#9">[9]</a></text>
<text top="81" left="428" width="52" height="9" font="font7" id="p7_t7" reading_order_no="6" segment_no="3" tag_type="table">GTSRB [11]<a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="94" left="102" width="17" height="9" font="font7" id="p7_t8" reading_order_no="7" segment_no="3" tag_type="table">Size<a href="deeplearning_paper26.html#9">[11]</a></text>
<text top="93" left="142" width="22" height="10" font="font21" id="p7_t9" reading_order_no="8" segment_no="3" tag_type="table">1 × 4</text>
<text top="93" left="185" width="22" height="10" font="font21" id="p7_t10" reading_order_no="9" segment_no="3" tag_type="table">1 × 6</text>
<text top="93" left="228" width="22" height="10" font="font21" id="p7_t11" reading_order_no="10" segment_no="3" tag_type="table">1 × 8</text>
<text top="93" left="271" width="22" height="10" font="font21" id="p7_t12" reading_order_no="11" segment_no="3" tag_type="table">2 × 2</text>
<text top="93" left="314" width="22" height="10" font="font21" id="p7_t13" reading_order_no="12" segment_no="3" tag_type="table">4 × 4</text>
<text top="93" left="357" width="23" height="10" font="font21" id="p7_t14" reading_order_no="13" segment_no="3" tag_type="table">6 × 6</text>
<text top="93" left="400" width="23" height="10" font="font21" id="p7_t15" reading_order_no="14" segment_no="3" tag_type="table">2 × 2</text>
<text top="93" left="444" width="22" height="10" font="font21" id="p7_t16" reading_order_no="15" segment_no="3" tag_type="table">4 × 4</text>
<text top="93" left="487" width="22" height="10" font="font21" id="p7_t17" reading_order_no="16" segment_no="3" tag_type="table">6 × 6</text>
<text top="106" left="100" width="20" height="9" font="font7" id="p7_t18" reading_order_no="17" segment_no="3" tag_type="table">BDR</text>
<text top="106" left="138" width="31" height="9" font="font7" id="p7_t19" reading_order_no="18" segment_no="3" tag_type="table">99.65%</text>
<text top="106" left="181" width="31" height="9" font="font7" id="p7_t20" reading_order_no="19" segment_no="3" tag_type="table">99.67%</text>
<text top="106" left="224" width="31" height="9" font="font7" id="p7_t21" reading_order_no="20" segment_no="3" tag_type="table">99.75%</text>
<text top="106" left="267" width="31" height="9" font="font7" id="p7_t22" reading_order_no="21" segment_no="3" tag_type="table">99.07%</text>
<text top="106" left="310" width="31" height="9" font="font7" id="p7_t23" reading_order_no="22" segment_no="3" tag_type="table">99.70%</text>
<text top="106" left="353" width="31" height="9" font="font7" id="p7_t24" reading_order_no="23" segment_no="3" tag_type="table">99.75%</text>
<text top="106" left="396" width="31" height="9" font="font7" id="p7_t25" reading_order_no="24" segment_no="3" tag_type="table">98.25%</text>
<text top="106" left="439" width="31" height="9" font="font7" id="p7_t26" reading_order_no="25" segment_no="3" tag_type="table">99.07%</text>
<text top="106" left="482" width="31" height="9" font="font7" id="p7_t27" reading_order_no="26" segment_no="3" tag_type="table">99.60%</text>
<text top="142" left="49" width="251" height="9" font="font7" id="p7_t28" reading_order_no="27" segment_no="4" tag_type="text">datasets are shown in Fig. 4. There are 16 pixels and 7 pixels</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p7_t29" reading_order_no="28" segment_no="4" tag_type="text">contained in the square trigger and the cross pattern trigger,</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p7_t30" reading_order_no="29" segment_no="4" tag_type="text">respectively. As shown in Fig. 5, for all the three datasets, the</text>
<text top="178" left="49" width="251" height="9" font="font7" id="p7_t31" reading_order_no="30" segment_no="4" tag_type="text">proposed method can achieve high backdoor detection rates</text>
<text top="190" left="49" width="251" height="9" font="font7" id="p7_t32" reading_order_no="31" segment_no="4" tag_type="text">against the backdoor attacks with trigger A and trigger B ,</text>
<text top="202" left="49" width="251" height="9" font="font7" id="p7_t33" reading_order_no="32" segment_no="4" tag_type="text">respectively. For the trigger A , after the proposed method is</text>
<text top="214" left="49" width="251" height="9" font="font7" id="p7_t34" reading_order_no="33" segment_no="4" tag_type="text">applied, the backdoor detection rates are 99.30%, 98.57%, and</text>
<text top="226" left="49" width="251" height="9" font="font7" id="p7_t35" reading_order_no="34" segment_no="4" tag_type="text">99.47% on Fashion-MNIST [9], CIFAR-10 [10] and GTSRB</text>
<text top="238" left="49" width="251" height="9" font="font7" id="p7_t36" reading_order_no="35" segment_no="4" tag_type="text">[11], respectively. For the trigger B , after the proposed method</text>
<text top="250" left="49" width="251" height="9" font="font7" id="p7_t37" reading_order_no="36" segment_no="4" tag_type="text">is applied, the backdoor detection rates are 98.20%, 98.45%,</text>
<text top="262" left="49" width="251" height="9" font="font7" id="p7_t38" reading_order_no="37" segment_no="4" tag_type="text">99.22% on Fashion-MNIST [9], CIFAR-10 [10] and GTSRB</text>
<text top="274" left="49" width="107" height="9" font="font7" id="p7_t39" reading_order_no="38" segment_no="4" tag_type="text">[11] datasets, respectively.</text>
<text top="427" left="266" width="11" height="9" font="font32" id="p7_t40" reading_order_no="39" segment_no="7" tag_type="figure">(c)</text>
<text top="427" left="190" width="11" height="9" font="font32" id="p7_t41" reading_order_no="40" segment_no="7" tag_type="figure">(b)</text>
<text top="427" left="112" width="11" height="9" font="font32" id="p7_t42" reading_order_no="41" segment_no="7" tag_type="figure">(a)</text>
<text top="322" left="52" width="35" height="9" font="font33" id="p7_t43" reading_order_no="42" segment_no="7" tag_type="figure">trigger A</text>
<text top="392" left="52" width="35" height="9" font="font33" id="p7_t44" reading_order_no="43" segment_no="7" tag_type="figure">trigger B</text>
<text top="449" left="49" width="22" height="7" font="font8" id="p7_t45" reading_order_no="44" segment_no="9" tag_type="text">Fig. 4.</text>
<text top="449" left="82" width="218" height="7" font="font8" id="p7_t46" reading_order_no="45" segment_no="9" tag_type="text">Examples of backdoor instances with different triggers. The first</text>
<text top="458" left="49" width="251" height="7" font="font8" id="p7_t47" reading_order_no="46" segment_no="9" tag_type="text">row is examples of backdoor instances with trigger A . The second row is<a href="deeplearning_paper26.html#7">4. </a>There are 16 pixels and 7 pixels</text>
<text top="467" left="49" width="251" height="7" font="font8" id="p7_t48" reading_order_no="47" segment_no="9" tag_type="text">examples of backdoor instances with trigger B . (a) Fashion-MNIST images.</text>
<text top="476" left="49" width="140" height="7" font="font8" id="p7_t49" reading_order_no="48" segment_no="9" tag_type="text">(b) CIFAR-10 images. (c) GTSRB images.<a href="deeplearning_paper26.html#7">5, </a>for all the three datasets, the</text>
<text top="681" left="129" width="25" height="8" font="font35" id="p7_t50" reading_order_no="57" segment_no="10" tag_type="figure">t r i g g e r   A</text>
<text top="681" left="216" width="25" height="8" font="font35" id="p7_t51" reading_order_no="58" segment_no="10" tag_type="figure">t r i g g e r   B</text>
<text top="672" left="94" width="0" height="8" font="font30" id="p7_t52" reading_order_no="56" segment_no="10" tag_type="figure">0</text>
<text top="647" left="89" width="4" height="8" font="font30" id="p7_t53" reading_order_no="55" segment_no="10" tag_type="figure">2 0</text>
<text top="621" left="89" width="4" height="8" font="font30" id="p7_t54" reading_order_no="54" segment_no="10" tag_type="figure">4 0</text>
<text top="596" left="89" width="4" height="8" font="font30" id="p7_t55" reading_order_no="53" segment_no="10" tag_type="figure">6 0</text>
<text top="570" left="89" width="4" height="8" font="font30" id="p7_t56" reading_order_no="52" segment_no="10" tag_type="figure">8 0</text>
<text top="545" left="84" width="8" height="8" font="font30" id="p7_t57" reading_order_no="51" segment_no="10" tag_type="figure">1 0 0</text>
<text top="568" left="81" width="0" height="95" font="font30" id="p7_t58" reading_order_no="50" segment_no="10" tag_type="figure">(% e at R n o ti ec et D r o o d k ac B</text>
<text top="558" left="81" width="0" height="8" font="font30" id="p7_t59" reading_order_no="49" segment_no="10" tag_type="figure">)</text>
<text top="694" left="157" width="57" height="8" font="font30" id="p7_t60" reading_order_no="59" segment_no="10" tag_type="figure">P a t t e r n   o f   T r i g g e r<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB</text>
<text top="518" left="215" width="48" height="8" font="font30" id="p7_t61" reading_order_no="60" segment_no="10" tag_type="figure">F a s h i o n - M N I S T<a href="deeplearning_paper26.html#9">[11], </a>respectively. For the</text>
<text top="528" left="215" width="31" height="8" font="font30" id="p7_t62" reading_order_no="61" segment_no="10" tag_type="figure">C I F A R - 1 0</text>
<text top="539" left="215" width="22" height="8" font="font30" id="p7_t63" reading_order_no="62" segment_no="10" tag_type="figure">G T S R B</text>
<text top="716" left="49" width="22" height="7" font="font8" id="p7_t64" reading_order_no="63" segment_no="13" tag_type="text">Fig. 5.</text>
<text top="716" left="80" width="220" height="7" font="font8" id="p7_t65" reading_order_no="64" segment_no="13" tag_type="text">Backdoor detection rates with different settings of trigger patterns<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB</text>
<text top="725" left="49" width="149" height="7" font="font8" id="p7_t66" reading_order_no="65" segment_no="13" tag_type="text">after the proposed defense method is applied.<a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively.</text>
<text top="143" left="312" width="251" height="9" font="font12" id="p7_t67" reading_order_no="66" segment_no="5" tag_type="text">D. Experiment Results of the Proposed Method with Four</text>
<text top="155" left="312" width="229" height="9" font="font12" id="p7_t68" reading_order_no="67" segment_no="5" tag_type="text">Different Adversarial Perturbation Generation Methods.</text>
<text top="170" left="322" width="241" height="9" font="font7" id="p7_t69" reading_order_no="68" segment_no="6" tag_type="text">In this section, the effectiveness of different adversarial</text>
<text top="182" left="312" width="251" height="9" font="font7" id="p7_t70" reading_order_no="69" segment_no="6" tag_type="text">perturbation generation methods is evaluated. The four dif-<i>trigger A</i></text>
<text top="194" left="312" width="251" height="9" font="font7" id="p7_t71" reading_order_no="70" segment_no="6" tag_type="text">ferent adversarial perturbation generation methods evaluated<i>trigger B</i></text>
<text top="206" left="312" width="251" height="9" font="font7" id="p7_t72" reading_order_no="71" segment_no="6" tag_type="text">in the experiment are C&amp;W [20], DeepFool [21], PGD [22]</text>
<text top="218" left="312" width="251" height="9" font="font7" id="p7_t73" reading_order_no="72" segment_no="6" tag_type="text">and UAP [13]. These four adversarial perturbation generation</text>
<text top="230" left="312" width="251" height="9" font="font7" id="p7_t74" reading_order_no="73" segment_no="6" tag_type="text">methods are separately used to generate the perturbation</text>
<text top="242" left="312" width="251" height="9" font="font7" id="p7_t75" reading_order_no="74" segment_no="6" tag_type="text">which is later utilized in the proposed method. As shown</text>
<text top="254" left="312" width="251" height="9" font="font7" id="p7_t76" reading_order_no="75" segment_no="6" tag_type="text">in Fig. 6 and Fig. 7, for the three datsets, using UAP [13]</text>
<text top="266" left="312" width="251" height="9" font="font7" id="p7_t77" reading_order_no="76" segment_no="6" tag_type="text">in the proposed method obtains the highest BDR (99.63%,</text>
<text top="278" left="312" width="251" height="9" font="font7" id="p7_t78" reading_order_no="77" segment_no="6" tag_type="text">99.76% and 99.91% in Fashion-MNIST [9], CIFAR-10 [10]</text>
<text top="290" left="312" width="251" height="9" font="font7" id="p7_t79" reading_order_no="78" segment_no="6" tag_type="text">and GTSRB [11] datasets, respectively), and the highest CIIR</text>
<text top="302" left="312" width="251" height="9" font="font7" id="p7_t80" reading_order_no="79" segment_no="6" tag_type="text">(90.66%, 89.82% and 98.85% in Fashion-MNIST [9], CIFAR-</text>
<text top="314" left="312" width="251" height="9" font="font7" id="p7_t81" reading_order_no="80" segment_no="6" tag_type="text">10 [10] and GTSRB [11] datasets, respectively) among all<i>t r i g g e r   A</i></text>
<text top="326" left="312" width="251" height="9" font="font7" id="p7_t82" reading_order_no="81" segment_no="6" tag_type="text">the four adversarial perturbation generation methods. With the<i>t r i g g e r   B</i></text>
<text top="337" left="312" width="251" height="9" font="font7" id="p7_t83" reading_order_no="82" segment_no="6" tag_type="text">adversarial perturbation generated by C&amp;W [20], DeepFool</text>
<text top="349" left="312" width="251" height="9" font="font7" id="p7_t84" reading_order_no="83" segment_no="6" tag_type="text">[21] and PGD [22], the performance of the proposed method</text>
<text top="361" left="312" width="251" height="9" font="font7" id="p7_t85" reading_order_no="84" segment_no="6" tag_type="text">is less effective, as the backdoor detection rates are as low as</text>
<text top="373" left="312" width="251" height="9" font="font7" id="p7_t86" reading_order_no="85" segment_no="6" tag_type="text">74.42%, 65.22% and 91.52% by using C&amp;W [20], DeepFool</text>
<text top="385" left="312" width="251" height="9" font="font7" id="p7_t87" reading_order_no="86" segment_no="6" tag_type="text">[21] and PGD [22], respectively (as shown in Fig. 6), and the</text>
<text top="397" left="312" width="251" height="9" font="font7" id="p7_t88" reading_order_no="87" segment_no="6" tag_type="text">clean image identification rates are as low as 41.92%, 47.85%</text>
<text top="409" left="312" width="251" height="9" font="font7" id="p7_t89" reading_order_no="88" segment_no="6" tag_type="text">and 67.77% by using C&amp;W [20], DeepFool [21] and PGD</text>
<text top="421" left="312" width="159" height="9" font="font7" id="p7_t90" reading_order_no="89" segment_no="6" tag_type="text">[22], respectively (as shown in Fig. 7).</text>
<text top="602" left="371" width="27" height="8" font="font30" id="p7_t91" reading_order_no="100" segment_no="8" tag_type="figure">C W   [ 2 0 ]</text>
<text top="602" left="416" width="24" height="8" font="font30" id="p7_t92" reading_order_no="101" segment_no="8" tag_type="figure">D F   [ 2 1 ]</text>
<text top="602" left="456" width="30" height="8" font="font30" id="p7_t93" reading_order_no="102" segment_no="8" tag_type="figure">P G D   [ 2 2 ]</text>
<text top="602" left="499" width="32" height="8" font="font30" id="p7_t94" reading_order_no="103" segment_no="8" tag_type="figure">U A P     [ 1 3 ]</text>
<text top="593" left="356" width="0" height="8" font="font30" id="p7_t95" reading_order_no="99" segment_no="8" tag_type="figure">0</text>
<text top="567" left="351" width="4" height="8" font="font30" id="p7_t96" reading_order_no="98" segment_no="8" tag_type="figure">2 0</text>
<text top="542" left="351" width="4" height="8" font="font30" id="p7_t97" reading_order_no="97" segment_no="8" tag_type="figure">4 0</text>
<text top="516" left="351" width="4" height="8" font="font30" id="p7_t98" reading_order_no="96" segment_no="8" tag_type="figure">6 0</text>
<text top="491" left="351" width="4" height="8" font="font30" id="p7_t99" reading_order_no="95" segment_no="8" tag_type="figure">8 0</text>
<text top="465" left="347" width="8" height="8" font="font30" id="p7_t100" reading_order_no="94" segment_no="8" tag_type="figure">1 0 0</text>
<text top="570" left="344" width="0" height="13" font="font30" id="p7_t101" reading_order_no="93" segment_no="8" tag_type="figure">ac B</text>
<text top="508" left="344" width="0" height="62" font="font30" id="p7_t102" reading_order_no="92" segment_no="8" tag_type="figure">R n o ti ec et D r o o d k</text>
<text top="488" left="344" width="0" height="20" font="font30" id="p7_t103" reading_order_no="91" segment_no="8" tag_type="figure">(% e at</text>
<text top="478" left="344" width="0" height="8" font="font30" id="p7_t104" reading_order_no="90" segment_no="8" tag_type="figure">)</text>
<text top="615" left="369" width="147" height="8" font="font30" id="p7_t105" reading_order_no="104" segment_no="8" tag_type="figure">A d v e r s a r i a l   P e r t u r b a t i o n   G e n e r a t i o n   M e t h o d</text>
<text top="446" left="388" width="48" height="8" font="font30" id="p7_t106" reading_order_no="105" segment_no="8" tag_type="figure">F a s h i o n - M N I S T</text>
<text top="457" left="388" width="31" height="8" font="font30" id="p7_t107" reading_order_no="106" segment_no="8" tag_type="figure">C I F A R - 1 0</text>
<text top="468" left="388" width="22" height="8" font="font30" id="p7_t108" reading_order_no="107" segment_no="8" tag_type="figure">G T S R B</text>
<text top="636" left="312" width="251" height="7" font="font8" id="p7_t109" reading_order_no="108" segment_no="11" tag_type="text">Fig. 6. The backdoor detection rate of the proposed method by using different</text>
<text top="645" left="312" width="145" height="7" font="font8" id="p7_t110" reading_order_no="109" segment_no="11" tag_type="text">adversarial perturbation generation methods.</text>
<text top="668" left="322" width="241" height="9" font="font7" id="p7_t111" reading_order_no="110" segment_no="12" tag_type="text">As mentioned in Section III-D, in the perturbation gener-</text>
<text top="680" left="312" width="251" height="9" font="font7" id="p7_t112" reading_order_no="111" segment_no="12" tag_type="text">ation process, image-specific adversarial attack requires the</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p7_t113" reading_order_no="112" segment_no="12" tag_type="text">label of each specific image to generate corresponding image-</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p7_t114" reading_order_no="113" segment_no="12" tag_type="text">specific perturbation. If the image is a backdoor instance,</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p7_t115" reading_order_no="114" segment_no="12" tag_type="text">its label will be the target label, then the image-specific</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p7_t116" reading_order_no="115" segment_no="12" tag_type="text">perturbation will be generated based on the target label instead</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p7_t117" reading_order_no="116" segment_no="12" tag_type="text">of its ground-truth label. Hence, the generated image-specific</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font36" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="text">8</text>
<text top="208" left="108" width="27" height="8" font="font30" id="p8_t2" reading_order_no="14" segment_no="1" tag_type="figure">C W   [ 2 0 ]</text>
<text top="208" left="153" width="24" height="8" font="font30" id="p8_t3" reading_order_no="15" segment_no="1" tag_type="figure">D F   [ 2 1 ]</text>
<text top="208" left="193" width="30" height="8" font="font30" id="p8_t4" reading_order_no="16" segment_no="1" tag_type="figure">P G D   [ 2 2 ]</text>
<text top="208" left="237" width="30" height="8" font="font30" id="p8_t5" reading_order_no="17" segment_no="1" tag_type="figure">U A P   [ 1 3 ]</text>
<text top="199" left="94" width="0" height="8" font="font30" id="p8_t6" reading_order_no="13" segment_no="1" tag_type="figure">0</text>
<text top="174" left="89" width="4" height="8" font="font30" id="p8_t7" reading_order_no="12" segment_no="1" tag_type="figure">2 0</text>
<text top="148" left="89" width="4" height="8" font="font30" id="p8_t8" reading_order_no="11" segment_no="1" tag_type="figure">4 0</text>
<text top="123" left="89" width="4" height="8" font="font30" id="p8_t9" reading_order_no="10" segment_no="1" tag_type="figure">6 0</text>
<text top="97" left="89" width="4" height="8" font="font30" id="p8_t10" reading_order_no="9" segment_no="1" tag_type="figure">8 0</text>
<text top="72" left="84" width="8" height="8" font="font30" id="p8_t11" reading_order_no="8" segment_no="1" tag_type="figure">1 0 0</text>
<text top="182" left="81" width="0" height="20" font="font30" id="p8_t12" reading_order_no="7" segment_no="1" tag_type="figure">an le C</text>
<text top="163" left="81" width="0" height="19" font="font30" id="p8_t13" reading_order_no="6" segment_no="1" tag_type="figure">ag m I</text>
<text top="142" left="81" width="0" height="21" font="font30" id="p8_t14" reading_order_no="5" segment_no="1" tag_type="figure">en Id e</text>
<text top="125" left="81" width="0" height="17" font="font30" id="p8_t15" reading_order_no="4" segment_no="1" tag_type="figure">ca fi ti</text>
<text top="104" left="81" width="0" height="21" font="font30" id="p8_t16" reading_order_no="3" segment_no="1" tag_type="figure">R n o ti</text>
<text top="85" left="81" width="0" height="19" font="font30" id="p8_t17" reading_order_no="2" segment_no="1" tag_type="figure">(% e at</text>
<text top="75" left="81" width="0" height="8" font="font30" id="p8_t18" reading_order_no="1" segment_no="1" tag_type="figure">)</text>
<text top="218" left="106" width="146" height="8" font="font30" id="p8_t19" reading_order_no="18" segment_no="1" tag_type="figure">A d v e r s a r i a l   P e r t u r b a t i o n   G e n e r a t i o n   M e t h o d</text>
<text top="57" left="126" width="48" height="8" font="font30" id="p8_t20" reading_order_no="19" segment_no="1" tag_type="figure">F a s h i o n - M N I S T</text>
<text top="68" left="126" width="31" height="8" font="font30" id="p8_t21" reading_order_no="20" segment_no="1" tag_type="figure">C I F A R - 1 0</text>
<text top="78" left="126" width="22" height="8" font="font30" id="p8_t22" reading_order_no="21" segment_no="1" tag_type="figure">G T S R B</text>
<text top="240" left="49" width="251" height="7" font="font8" id="p8_t23" reading_order_no="22" segment_no="4" tag_type="text">Fig. 7. The clean image identification rates of the proposed method by using</text>
<text top="249" left="49" width="175" height="7" font="font8" id="p8_t24" reading_order_no="23" segment_no="4" tag_type="text">different adversarial perturbation generation methods.</text>
<text top="280" left="49" width="251" height="9" font="font7" id="p8_t25" reading_order_no="24" segment_no="5" tag_type="text">perturbation will strongly affect the backdoor trigger. Once</text>
<text top="292" left="49" width="251" height="9" font="font7" id="p8_t26" reading_order_no="25" segment_no="5" tag_type="text">the backdoor trigger is strongly affected, it cannot trigger the</text>
<text top="304" left="49" width="251" height="9" font="font7" id="p8_t27" reading_order_no="26" segment_no="5" tag_type="text">backdoor, leading to the inconsistence of predicted labels of</text>
<text top="316" left="49" width="251" height="9" font="font7" id="p8_t28" reading_order_no="27" segment_no="5" tag_type="text">backdoor instance before and after perturbation. As a result,</text>
<text top="328" left="49" width="251" height="9" font="font7" id="p8_t29" reading_order_no="28" segment_no="5" tag_type="text">this backdoor instance will be incorrectly considered as a</text>
<text top="340" left="49" width="251" height="9" font="font7" id="p8_t30" reading_order_no="29" segment_no="5" tag_type="text">benign image. Therefore, the performance of the proposed</text>
<text top="352" left="49" width="251" height="9" font="font7" id="p8_t31" reading_order_no="30" segment_no="5" tag_type="text">method with image-specific perturbation is low. UAP [13]</text>
<text top="364" left="49" width="251" height="9" font="font7" id="p8_t32" reading_order_no="31" segment_no="5" tag_type="text">is a kind of image-agnostic perturbation, which is generated</text>
<text top="376" left="49" width="251" height="9" font="font7" id="p8_t33" reading_order_no="32" segment_no="5" tag_type="text">based on a small set of clean images. The influence of UAP is</text>
<text top="388" left="49" width="251" height="9" font="font7" id="p8_t34" reading_order_no="33" segment_no="5" tag_type="text">mostly on the salient regions of the backdoor instance instead</text>
<text top="400" left="49" width="251" height="9" font="font7" id="p8_t35" reading_order_no="34" segment_no="5" tag_type="text">of the trigger. Therefore, the predicted labels of the backdoor</text>
<text top="412" left="49" width="251" height="9" font="font7" id="p8_t36" reading_order_no="35" segment_no="5" tag_type="text">instance before and after perturbation are consistent, which</text>
<text top="424" left="49" width="251" height="9" font="font7" id="p8_t37" reading_order_no="36" segment_no="5" tag_type="text">makes the image be correctly detected as a backdoor instance.</text>
<text top="436" left="49" width="251" height="9" font="font7" id="p8_t38" reading_order_no="37" segment_no="5" tag_type="text">In summary, among these four adversarial perturbation gen-</text>
<text top="448" left="49" width="251" height="9" font="font7" id="p8_t39" reading_order_no="38" segment_no="5" tag_type="text">eration methods, UAP [13] is most suitable for use in the</text>
<text top="460" left="49" width="72" height="9" font="font7" id="p8_t40" reading_order_no="39" segment_no="5" tag_type="text">proposed method.</text>
<text top="486" left="49" width="142" height="9" font="font12" id="p8_t41" reading_order_no="40" segment_no="9" tag_type="title">E. Comparison with Related Work</text>
<text top="501" left="59" width="241" height="9" font="font7" id="p8_t42" reading_order_no="41" segment_no="10" tag_type="text">In this section, the proposed method is compared with</text>
<text top="513" left="49" width="251" height="9" font="font7" id="p8_t43" reading_order_no="42" segment_no="10" tag_type="text">STRIP [12]. In the detection process of STRIP [12], a set</text>
<text top="525" left="49" width="251" height="9" font="font7" id="p8_t44" reading_order_no="43" segment_no="10" tag_type="text">of other images from different classes are added to the input<a href="deeplearning_paper26.html#9">[13]</a></text>
<text top="537" left="49" width="251" height="9" font="font7" id="p8_t45" reading_order_no="44" segment_no="10" tag_type="text">image separately in order to generate a set of blended images</text>
<text top="549" left="49" width="251" height="9" font="font7" id="p8_t46" reading_order_no="45" segment_no="10" tag_type="text">[12]. Then, STRIP utilizes entropy to measure the randomness</text>
<text top="561" left="49" width="251" height="9" font="font7" id="p8_t47" reading_order_no="46" segment_no="10" tag_type="text">of the predicted labels of all the blended images [12]. The</text>
<text top="573" left="49" width="251" height="9" font="font7" id="p8_t48" reading_order_no="47" segment_no="10" tag_type="text">entropy of clean images is significantly lower than the entropy</text>
<text top="584" left="49" width="251" height="9" font="font7" id="p8_t49" reading_order_no="48" segment_no="10" tag_type="text">of backdoor instances. As a result, the smaller the entropy, the</text>
<text top="596" left="49" width="213" height="9" font="font7" id="p8_t50" reading_order_no="49" segment_no="10" tag_type="text">input image is more likely to contain a trigger [12].</text>
<text top="608" left="59" width="241" height="9" font="font7" id="p8_t51" reading_order_no="50" segment_no="12" tag_type="text">The advantages of the proposed method over STRIP [12]</text>
<text top="620" left="49" width="251" height="9" font="font7" id="p8_t52" reading_order_no="51" segment_no="12" tag_type="text">are as follows. (i) The proposed method perturbs the image<a href="deeplearning_paper26.html#9">[13] </a>is most suitable for use in the</text>
<text top="632" left="49" width="251" height="9" font="font7" id="p8_t53" reading_order_no="52" segment_no="12" tag_type="text">with universal perturbation [13] rather than other images from</text>
<text top="644" left="49" width="251" height="10" font="font7" id="p8_t54" reading_order_no="53" segment_no="12" tag_type="text">different classes. Since the ` ∞ norm of UAP [13] is very</text>
<text top="656" left="49" width="251" height="9" font="font7" id="p8_t55" reading_order_no="54" segment_no="12" tag_type="text">low, the perturbation is very small. In addition, because UAP</text>
<text top="668" left="49" width="251" height="9" font="font7" id="p8_t56" reading_order_no="55" segment_no="12" tag_type="text">is generated from clean images, the generated UAP mainly<a href="deeplearning_paper26.html#9">[12]. </a>In the detection process of STRIP <a href="deeplearning_paper26.html#9">[12], </a>a set</text>
<text top="680" left="49" width="251" height="9" font="font7" id="p8_t57" reading_order_no="56" segment_no="12" tag_type="text">focuses on perturbing the salient regions of an image rather</text>
<text top="692" left="49" width="251" height="9" font="font7" id="p8_t58" reading_order_no="57" segment_no="12" tag_type="text">than the trigger. Therefore, the trigger is almost unaffected.</text>
<text top="704" left="49" width="251" height="9" font="font7" id="p8_t59" reading_order_no="58" segment_no="12" tag_type="text">However, in STRIP, other images are directly added to the<a href="deeplearning_paper26.html#9">[12]. </a>Then, STRIP utilizes entropy to measure the randomness</text>
<text top="716" left="49" width="251" height="9" font="font7" id="p8_t60" reading_order_no="59" segment_no="12" tag_type="text">input image, so the input image is globally affected [12]. It<a href="deeplearning_paper26.html#9">[12]. </a>The</text>
<text top="728" left="49" width="251" height="9" font="font7" id="p8_t61" reading_order_no="60" segment_no="12" tag_type="text">will not only destroy the main content of the input image, but</text>
<text top="740" left="49" width="251" height="9" font="font7" id="p8_t62" reading_order_no="61" segment_no="12" tag_type="text">also may break the trigger. (ii) For each image, the proposed</text>
<text top="58" left="312" width="251" height="9" font="font7" id="p8_t63" reading_order_no="62" segment_no="2" tag_type="text">method only needs to generate one extra perturbed image<a href="deeplearning_paper26.html#9">[12].</a></text>
<text top="70" left="312" width="251" height="9" font="font7" id="p8_t64" reading_order_no="63" segment_no="2" tag_type="text">and predict two images (perturbed and unperturbed image).<a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="82" left="312" width="251" height="9" font="font7" id="p8_t65" reading_order_no="64" segment_no="2" tag_type="text">However, for each input image, STRIP needs to generate a set</text>
<text top="94" left="312" width="251" height="9" font="font7" id="p8_t66" reading_order_no="65" segment_no="2" tag_type="text">of blended images and input them to the model in order to<a href="deeplearning_paper26.html#9">[13] </a>rather than other images from</text>
<text top="106" left="312" width="251" height="9" font="font7" id="p8_t67" reading_order_no="66" segment_no="2" tag_type="text">estimate the entropy of the predicted labels of these blended</text>
<text top="118" left="312" width="251" height="9" font="font7" id="p8_t68" reading_order_no="67" segment_no="2" tag_type="text">images [12]. Therefore, the detection process of STRIP [12]</text>
<text top="130" left="312" width="251" height="9" font="font7" id="p8_t69" reading_order_no="68" segment_no="2" tag_type="text">is more complex than the proposed method, thus the proposed</text>
<text top="142" left="312" width="153" height="9" font="font7" id="p8_t70" reading_order_no="69" segment_no="2" tag_type="text">method is more efficient than STRIP.<a href="deeplearning_paper26.html#9">[13] </a>is very</text>
<text top="155" left="322" width="241" height="9" font="font7" id="p8_t71" reading_order_no="70" segment_no="3" tag_type="text">The experiment is also conducted to compare the proposed</text>
<text top="167" left="312" width="251" height="9" font="font7" id="p8_t72" reading_order_no="71" segment_no="3" tag_type="text">method with STRIP [12], and the experimental results are</text>
<text top="179" left="312" width="251" height="9" font="font7" id="p8_t73" reading_order_no="72" segment_no="3" tag_type="text">shown in Table V. We reproduce STRIP by following the</text>
<text top="191" left="312" width="251" height="9" font="font7" id="p8_t74" reading_order_no="73" segment_no="3" tag_type="text">method proposed in [12] for comparision. As shown in Table</text>
<text top="203" left="312" width="251" height="9" font="font7" id="p8_t75" reading_order_no="74" segment_no="3" tag_type="text">V, the performance of the proposed method is significantly</text>
<text top="215" left="312" width="251" height="9" font="font7" id="p8_t76" reading_order_no="75" segment_no="3" tag_type="text">better than that of STRIP [12] on all the three datasets.<a href="deeplearning_paper26.html#9">[12]. </a>It</text>
<text top="227" left="312" width="251" height="9" font="font7" id="p8_t77" reading_order_no="76" segment_no="3" tag_type="text">Specifically, for STRIP, the backdoor detection rate is 63.40%,</text>
<text top="238" left="312" width="251" height="9" font="font7" id="p8_t78" reading_order_no="77" segment_no="3" tag_type="text">96.32% and 73.95% on Fashion-MNIST [9], CIFAR-10 [10]</text>
<text top="250" left="312" width="251" height="9" font="font7" id="p8_t79" reading_order_no="78" segment_no="3" tag_type="text">and GTSRB [11] respectively. For the proposed approach,</text>
<text top="262" left="312" width="251" height="9" font="font7" id="p8_t80" reading_order_no="79" segment_no="3" tag_type="text">the backdoor detection rate is 99.63%, 99.76% and 99.91%</text>
<text top="274" left="312" width="251" height="9" font="font7" id="p8_t81" reading_order_no="80" segment_no="3" tag_type="text">on Fashion-MNIST [9], CIFAR-10 [10] and GTSRB [11]</text>
<text top="286" left="312" width="251" height="9" font="font7" id="p8_t82" reading_order_no="81" segment_no="3" tag_type="text">datasets, respectively. For the clean image identification rate,</text>
<text top="298" left="312" width="251" height="9" font="font7" id="p8_t83" reading_order_no="82" segment_no="3" tag_type="text">the proposed method also has better performance than STRIP</text>
<text top="310" left="312" width="251" height="9" font="font7" id="p8_t84" reading_order_no="83" segment_no="3" tag_type="text">[12]. Specifically, for STRIP, the clean image identification<a href="deeplearning_paper26.html#9">[12]. </a>Therefore, the detection process of STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="322" left="312" width="251" height="9" font="font7" id="p8_t85" reading_order_no="84" segment_no="3" tag_type="text">rate is 67.40%, 89.57% and 88.80% on Fashion-MNIST [9],</text>
<text top="334" left="312" width="251" height="9" font="font7" id="p8_t86" reading_order_no="85" segment_no="3" tag_type="text">CIFAR-10 [10] and GTSRB [11] datasets, respectively. For</text>
<text top="346" left="312" width="251" height="9" font="font7" id="p8_t87" reading_order_no="86" segment_no="3" tag_type="text">the proposed method, the clean image identification rate is</text>
<text top="358" left="312" width="251" height="9" font="font7" id="p8_t88" reading_order_no="87" segment_no="3" tag_type="text">90.66%, 89.82% and 98.85% on Fashion-MNIST [9], CIFAR-<a href="deeplearning_paper26.html#9">[12], </a>and the experimental results are</text>
<text top="370" left="312" width="194" height="9" font="font7" id="p8_t89" reading_order_no="88" segment_no="3" tag_type="text">10 [10] and GTSRB [11] datasets, respectively.<a href="deeplearning_paper26.html#8">V. </a>We reproduce STRIP by following the</text>
<text top="393" left="421" width="33" height="7" font="font8" id="p8_t90" reading_order_no="89" segment_no="6" tag_type="title">TABLE V<a href="deeplearning_paper26.html#9">[12] </a>for comparision. As shown in Table</text>
<text top="402" left="320" width="235" height="7" font="font8" id="p8_t91" reading_order_no="90" segment_no="7" tag_type="text">P ERFORMANCE COMPARISON BETWEEN THE PROPOSED METHOD AND<a href="deeplearning_paper26.html#8">V, </a>the performance of the proposed method is significantly</text>
<text top="411" left="417" width="40" height="7" font="font8" id="p8_t92" reading_order_no="91" segment_no="7" tag_type="text">STRIP [12]<a href="deeplearning_paper26.html#9">[12] </a>on all the three datasets.</text>
<text top="450" left="336" width="26" height="8" font="font36" id="p8_t93" reading_order_no="92" segment_no="8" tag_type="table">Dataset</text>
<text top="434" left="414" width="29" height="8" font="font36" id="p8_t94" reading_order_no="93" segment_no="8" tag_type="table">CIIR on<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="445" left="405" width="46" height="8" font="font36" id="p8_t95" reading_order_no="94" segment_no="8" tag_type="table">clean images<a href="deeplearning_paper26.html#9">[11] </a>respectively. For the proposed approach,</text>
<text top="434" left="503" width="30" height="8" font="font36" id="p8_t96" reading_order_no="95" segment_no="8" tag_type="table">BDR on</text>
<text top="445" left="484" width="68" height="8" font="font36" id="p8_t97" reading_order_no="96" segment_no="8" tag_type="table">backdoor instances<a href="deeplearning_paper26.html#9">[9]</a>, CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11]</a></text>
<text top="460" left="389" width="42" height="8" font="font36" id="p8_t98" reading_order_no="97" segment_no="8" tag_type="table">STRIP [12]</text>
<text top="460" left="446" width="17" height="8" font="font36" id="p8_t99" reading_order_no="98" segment_no="8" tag_type="table">Ours</text>
<text top="460" left="479" width="42" height="8" font="font36" id="p8_t100" reading_order_no="99" segment_no="8" tag_type="table">STRIP [12]<a href="deeplearning_paper26.html#9">[12]. </a>Specifically, for STRIP, the clean image identification</text>
<text top="460" left="536" width="17" height="8" font="font36" id="p8_t101" reading_order_no="100" segment_no="8" tag_type="table">Ours<a href="deeplearning_paper26.html#9">[9],</a></text>
<text top="475" left="320" width="58" height="8" font="font36" id="p8_t102" reading_order_no="101" segment_no="8" tag_type="table">Fashion-MNIST<a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively. For</text>
<text top="475" left="396" width="27" height="8" font="font36" id="p8_t103" reading_order_no="102" segment_no="8" tag_type="table">67.40%</text>
<text top="475" left="441" width="27" height="8" font="font36" id="p8_t104" reading_order_no="103" segment_no="8" tag_type="table">90.66%<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-</text>
<text top="475" left="486" width="27" height="8" font="font36" id="p8_t105" reading_order_no="104" segment_no="8" tag_type="table">63.40%<a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively.</text>
<text top="475" left="531" width="27" height="8" font="font36" id="p8_t106" reading_order_no="105" segment_no="8" tag_type="table">99.63%</text>
<text top="485" left="331" width="37" height="8" font="font36" id="p8_t107" reading_order_no="106" segment_no="8" tag_type="table">CIFAR-10</text>
<text top="485" left="396" width="27" height="8" font="font36" id="p8_t108" reading_order_no="107" segment_no="8" tag_type="table">89.57%</text>
<text top="485" left="441" width="27" height="8" font="font36" id="p8_t109" reading_order_no="108" segment_no="8" tag_type="table">89.82%<a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="485" left="486" width="27" height="8" font="font36" id="p8_t110" reading_order_no="109" segment_no="8" tag_type="table">96.32%</text>
<text top="485" left="531" width="27" height="8" font="font36" id="p8_t111" reading_order_no="110" segment_no="8" tag_type="table">99.76%</text>
<text top="496" left="335" width="28" height="8" font="font36" id="p8_t112" reading_order_no="111" segment_no="8" tag_type="table">GTSRB</text>
<text top="496" left="396" width="27" height="8" font="font36" id="p8_t113" reading_order_no="112" segment_no="8" tag_type="table">88.80%</text>
<text top="496" left="441" width="27" height="8" font="font36" id="p8_t114" reading_order_no="113" segment_no="8" tag_type="table">98.85%</text>
<text top="496" left="486" width="27" height="8" font="font36" id="p8_t115" reading_order_no="114" segment_no="8" tag_type="table">73.95%<a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="496" left="531" width="27" height="8" font="font36" id="p8_t116" reading_order_no="115" segment_no="8" tag_type="table">99.91%</text>
<text top="525" left="322" width="241" height="9" font="font7" id="p8_t117" reading_order_no="116" segment_no="11" tag_type="text">Note that, the intensity of trigger in this experiment is at a<a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="537" left="312" width="251" height="9" font="font7" id="p8_t118" reading_order_no="117" segment_no="11" tag_type="text">low level (0.15, 0.5, 0.2 for Fashion-MNIST [9], CIFAR-10</text>
<text top="549" left="312" width="251" height="9" font="font7" id="p8_t119" reading_order_no="118" segment_no="11" tag_type="text">[10] and GTSRB [11] datasets, respectively). For STRIP [12],</text>
<text top="560" left="312" width="251" height="9" font="font7" id="p8_t120" reading_order_no="119" segment_no="11" tag_type="text">the backdoor instance is totally superimposed with other image</text>
<text top="572" left="312" width="251" height="9" font="font7" id="p8_t121" reading_order_no="120" segment_no="11" tag_type="text">from different classes, so the trigger is inevitably blended</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p8_t122" reading_order_no="121" segment_no="11" tag_type="text">with other pixels. Generally, when the intensity of trigger is</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p8_t123" reading_order_no="122" segment_no="11" tag_type="text">normal, the trigger in the blended image may still activate the</text>
<text top="608" left="312" width="251" height="9" font="font7" id="p8_t124" reading_order_no="123" segment_no="11" tag_type="text">backdoor. However, the intensity of trigger in this experiment</text>
<text top="620" left="312" width="251" height="9" font="font7" id="p8_t125" reading_order_no="124" segment_no="11" tag_type="text">is low, so the trigger in the blended backdoor instance is</text>
<text top="632" left="312" width="251" height="9" font="font7" id="p8_t126" reading_order_no="125" segment_no="11" tag_type="text">destroyed and will be ignored by the model. Therefore, the</text>
<text top="644" left="312" width="251" height="9" font="font7" id="p8_t127" reading_order_no="126" segment_no="11" tag_type="text">entropy of this backdoor instance is similar to the entropy of</text>
<text top="656" left="312" width="251" height="9" font="font7" id="p8_t128" reading_order_no="127" segment_no="11" tag_type="text">clean images. As a result, STRIP [12] will incorrectly consider</text>
<text top="668" left="312" width="251" height="9" font="font7" id="p8_t129" reading_order_no="128" segment_no="11" tag_type="text">the backdoor instance as a clean one. In comparison, the</text>
<text top="680" left="312" width="251" height="9" font="font7" id="p8_t130" reading_order_no="129" segment_no="11" tag_type="text">proposed method uses the universal adversarial perturbation</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p8_t131" reading_order_no="130" segment_no="11" tag_type="text">(UAP [13]) to perturb the input image. First, unlike STRIP</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p8_t132" reading_order_no="131" segment_no="11" tag_type="text">[12], adversarial perturbation will not globally perturb the</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p8_t133" reading_order_no="132" segment_no="11" tag_type="text">input image. The perturbation will only modify limited number</text>
<text top="728" left="312" width="251" height="10" font="font7" id="p8_t134" reading_order_no="133" segment_no="11" tag_type="text">of pixels as the ` ∞ norm of UAP is very low. Second, UAP</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p8_t135" reading_order_no="134" segment_no="11" tag_type="text">[13] is generated from a small set of clean images. Therefore,<a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
<text top="30" left="563" width="3" height="6" font="font0" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="text">9</text>
<text top="58" left="49" width="251" height="9" font="font7" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="text">even if the backdoor instance is perturbed, the trigger in</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p9_t3" reading_order_no="2" segment_no="1" tag_type="text">the backdoor instance will only be slightly affected. As a</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p9_t4" reading_order_no="3" segment_no="1" tag_type="text">result, the proposed method can successfully detect backdoor</text>
<text top="94" left="49" width="251" height="9" font="font7" id="p9_t5" reading_order_no="4" segment_no="1" tag_type="text">instances carrying the trigger with low intensity. However,</text>
<text top="106" left="49" width="251" height="9" font="font7" id="p9_t6" reading_order_no="5" segment_no="1" tag_type="text">STRIP [12] fails to detect some backdoor instances in this<a href="deeplearning_paper26.html#9">[12] </a>fails to detect some backdoor instances in this</text>
<text top="118" left="49" width="242" height="9" font="font7" id="p9_t7" reading_order_no="6" segment_no="1" tag_type="text">experiment, where the intensity of trigger is at a low level.</text>
<text top="130" left="59" width="241" height="9" font="font7" id="p9_t8" reading_order_no="7" segment_no="5" tag_type="text">In summary, there are two advantages of the proposed</text>
<text top="142" left="49" width="251" height="9" font="font7" id="p9_t9" reading_order_no="8" segment_no="5" tag_type="text">method over STRIP [12]. First, the the proposed method<a href="deeplearning_paper26.html#9">[12]. </a>First, the the proposed method</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p9_t10" reading_order_no="9" segment_no="5" tag_type="text">is more effective than STRIP, as the proposed method will</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p9_t11" reading_order_no="10" segment_no="5" tag_type="text">not destroy the trigger while STRIP may destroy the trigger.</text>
<text top="178" left="49" width="251" height="9" font="font7" id="p9_t12" reading_order_no="11" segment_no="5" tag_type="text">Second, the proposed method is more efficient than STRIP,</text>
<text top="190" left="49" width="251" height="9" font="font7" id="p9_t13" reading_order_no="12" segment_no="5" tag_type="text">as the proposed method only needs to predict two images</text>
<text top="202" left="49" width="251" height="9" font="font7" id="p9_t14" reading_order_no="13" segment_no="5" tag_type="text">(perturbed and unperturbed image) for each untrusted image</text>
<text top="214" left="49" width="221" height="9" font="font7" id="p9_t15" reading_order_no="14" segment_no="5" tag_type="text">and STRIP needs to predicts a set of blended images.</text>
<text top="242" left="139" width="71" height="9" font="font7" id="p9_t16" reading_order_no="15" segment_no="9" tag_type="title">V. C ONCLUSION</text>
<text top="259" left="59" width="241" height="9" font="font7" id="p9_t17" reading_order_no="16" segment_no="11" tag_type="text">In this paper, we propose a novel backdoor detection method</text>
<text top="271" left="49" width="251" height="9" font="font7" id="p9_t18" reading_order_no="17" segment_no="11" tag_type="text">based on adversarial perturbations. Specifically, the universal</text>
<text top="283" left="49" width="251" height="9" font="font7" id="p9_t19" reading_order_no="18" segment_no="11" tag_type="text">adversarial perturbation [13] is first generated from the model,</text>
<text top="295" left="49" width="251" height="9" font="font7" id="p9_t20" reading_order_no="19" segment_no="11" tag_type="text">then the generated perturbation is added to the image. If<a href="deeplearning_paper26.html#9">[13] </a>is first generated from the model,</text>
<text top="307" left="49" width="251" height="9" font="font7" id="p9_t21" reading_order_no="20" segment_no="11" tag_type="text">the prediction of model on the perturbed image is consistent</text>
<text top="319" left="49" width="251" height="9" font="font7" id="p9_t22" reading_order_no="21" segment_no="11" tag_type="text">with the one on the unperturbed image, the input image is</text>
<text top="331" left="49" width="251" height="9" font="font7" id="p9_t23" reading_order_no="22" segment_no="11" tag_type="text">considered as a backdoor instance. Experimental results show</text>
<text top="343" left="49" width="251" height="9" font="font7" id="p9_t24" reading_order_no="23" segment_no="11" tag_type="text">that, the proposed defense method can achieve high backdoor</text>
<text top="355" left="49" width="251" height="9" font="font7" id="p9_t25" reading_order_no="24" segment_no="11" tag_type="text">detection rate and high clean image identification rate, while</text>
<text top="367" left="49" width="251" height="9" font="font7" id="p9_t26" reading_order_no="25" segment_no="11" tag_type="text">maintaining the visual quality of the image. Besides, the</text>
<text top="379" left="49" width="251" height="9" font="font7" id="p9_t27" reading_order_no="26" segment_no="11" tag_type="text">defense performance of the proposed method against backdoor</text>
<text top="391" left="49" width="251" height="9" font="font7" id="p9_t28" reading_order_no="27" segment_no="11" tag_type="text">attacks under different settings is also demonstrated to be</text>
<text top="403" left="49" width="251" height="9" font="font7" id="p9_t29" reading_order_no="28" segment_no="11" tag_type="text">effective. Our future work will explore the defenses against</text>
<text top="415" left="49" width="200" height="9" font="font7" id="p9_t30" reading_order_no="29" segment_no="11" tag_type="text">physical backdoor attacks in real physical world.</text>
<text top="443" left="147" width="56" height="9" font="font7" id="p9_t31" reading_order_no="30" segment_no="18" tag_type="title">R EFERENCES</text>
<text top="461" left="53" width="247" height="7" font="font8" id="p9_t32" reading_order_no="31" segment_no="20" tag_type="text">[1] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “BadNets: Evaluating</text>
<text top="470" left="67" width="233" height="7" font="font8" id="p9_t33" reading_order_no="32" segment_no="20" tag_type="text">backdooring attacks on deep neural networks,” IEEE Access , vol. 7, pp.</text>
<text top="479" left="67" width="69" height="7" font="font8" id="p9_t34" reading_order_no="33" segment_no="20" tag_type="text">47 230–47 244, 2019.</text>
<text top="488" left="53" width="247" height="7" font="font8" id="p9_t35" reading_order_no="34" segment_no="22" tag_type="text">[2] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks</text>
<text top="497" left="67" width="233" height="7" font="font8" id="p9_t36" reading_order_no="35" segment_no="22" tag_type="text">on deep learning systems using data poisoning,” arXiv:1712.05526 ,</text>
<text top="506" left="67" width="18" height="7" font="font8" id="p9_t37" reading_order_no="36" segment_no="22" tag_type="text">2017.</text>
<text top="515" left="53" width="247" height="7" font="font8" id="p9_t38" reading_order_no="37" segment_no="24" tag_type="text">[3] M. Barni, K. Kallas, and B. Tondi, “A new backdoor attack in CNNS by</text>
<text top="524" left="67" width="233" height="8" font="font8" id="p9_t39" reading_order_no="38" segment_no="24" tag_type="text">training set corruption without label poisoning,” in IEEE International</text>
<text top="533" left="67" width="176" height="7" font="font34" id="p9_t40" reading_order_no="39" segment_no="24" tag_type="text">Conference on Image Processing , 2019, pp. 101–105.</text>
<text top="543" left="53" width="247" height="7" font="font8" id="p9_t41" reading_order_no="40" segment_no="25" tag_type="text">[4] X. Zhang, A. Mian, R. Gupta, N. Rahnavard, and M. Shah, “Cas-</text>
<text top="552" left="67" width="233" height="7" font="font8" id="p9_t42" reading_order_no="41" segment_no="25" tag_type="text">sandra: Detecting trojaned networks from adversarial perturbations,”</text>
<text top="560" left="67" width="81" height="8" font="font34" id="p9_t43" reading_order_no="42" segment_no="25" tag_type="text">arXiv:2007.14433 , 2020.</text>
<text top="570" left="53" width="247" height="7" font="font8" id="p9_t44" reading_order_no="43" segment_no="26" tag_type="text">[5] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “Detecting</text>
<text top="579" left="67" width="212" height="7" font="font8" id="p9_t45" reading_order_no="44" segment_no="26" tag_type="text">AI trojans using meta neural analysis,” arXiv:1910.03137 , 2019.</text>
<text top="588" left="53" width="247" height="7" font="font8" id="p9_t46" reading_order_no="45" segment_no="27" tag_type="text">[6] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “Universal</text>
<text top="597" left="67" width="233" height="7" font="font8" id="p9_t47" reading_order_no="46" segment_no="27" tag_type="text">Litmus Patterns: Revealing backdoor attacks in CNNs,” in IEEE/CVF</text>
<text top="606" left="67" width="233" height="7" font="font34" id="p9_t48" reading_order_no="47" segment_no="27" tag_type="text">Conference on Computer Vision and Pattern Recognition , 2020, pp. 298–</text>
<text top="615" left="67" width="14" height="7" font="font8" id="p9_t49" reading_order_no="48" segment_no="27" tag_type="text">307.</text>
<text top="624" left="53" width="247" height="7" font="font8" id="p9_t50" reading_order_no="49" segment_no="28" tag_type="text">[7] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in IEEE International</text>
<text top="633" left="67" width="167" height="7" font="font34" id="p9_t51" reading_order_no="50" segment_no="28" tag_type="text">Conference on Computer Design , 2017, pp. 45–48.</text>
<text top="642" left="53" width="247" height="7" font="font8" id="p9_t52" reading_order_no="51" segment_no="29" tag_type="text">[8] X. Qiao, Y. Yang, and H. Li, “Defending neural backdoors via generative</text>
<text top="651" left="67" width="233" height="7" font="font8" id="p9_t53" reading_order_no="52" segment_no="29" tag_type="text">distribution modeling,” in Annual Conference on Neural Information</text>
<text top="660" left="67" width="150" height="7" font="font34" id="p9_t54" reading_order_no="53" segment_no="29" tag_type="text">Processing Systems , 2019, pp. 14 004–14 013.</text>
<text top="669" left="53" width="247" height="7" font="font8" id="p9_t55" reading_order_no="54" segment_no="30" tag_type="text">[9] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-MNIST: a novel</text>
<text top="678" left="67" width="19" height="7" font="font8" id="p9_t56" reading_order_no="55" segment_no="30" tag_type="text">image</text>
<text top="678" left="94" width="22" height="7" font="font8" id="p9_t57" reading_order_no="56" segment_no="30" tag_type="text">dataset</text>
<text top="678" left="124" width="9" height="7" font="font8" id="p9_t58" reading_order_no="57" segment_no="30" tag_type="text">for</text>
<text top="678" left="140" width="46" height="7" font="font8" id="p9_t59" reading_order_no="58" segment_no="30" tag_type="text">benchmarking</text>
<text top="678" left="193" width="27" height="7" font="font8" id="p9_t60" reading_order_no="59" segment_no="30" tag_type="text">machine</text>
<text top="678" left="227" width="26" height="7" font="font8" id="p9_t61" reading_order_no="60" segment_no="30" tag_type="text">learning</text>
<text top="678" left="261" width="39" height="7" font="font8" id="p9_t62" reading_order_no="61" segment_no="30" tag_type="text">algorithms,”</text>
<text top="687" left="67" width="110" height="7" font="font34" id="p9_t63" reading_order_no="62" segment_no="30" tag_type="text">arXiv:1708.07747 , pp. 1–6, 2017.</text>
<text top="696" left="49" width="251" height="7" font="font8" id="p9_t64" reading_order_no="63" segment_no="31" tag_type="text">[10] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features</text>
<text top="705" left="67" width="115" height="7" font="font8" id="p9_t65" reading_order_no="64" segment_no="31" tag_type="text">from tiny images,” pp. 1–60, 2009.</text>
<text top="714" left="49" width="251" height="7" font="font8" id="p9_t66" reading_order_no="65" segment_no="32" tag_type="text">[11] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “The german traffic</text>
<text top="723" left="67" width="233" height="7" font="font8" id="p9_t67" reading_order_no="66" segment_no="32" tag_type="text">sign recognition benchmark: A multi-class classification competition,”</text>
<text top="732" left="67" width="233" height="7" font="font8" id="p9_t68" reading_order_no="67" segment_no="32" tag_type="text">in International Joint Conference on Neural Networks , 2011, pp. 1453–</text>
<text top="741" left="67" width="18" height="7" font="font8" id="p9_t69" reading_order_no="68" segment_no="32" tag_type="text">1460.</text>
<text top="60" left="312" width="251" height="7" font="font8" id="p9_t70" reading_order_no="69" segment_no="2" tag_type="text">[12] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal,</text>
<text top="69" left="330" width="233" height="7" font="font8" id="p9_t71" reading_order_no="70" segment_no="2" tag_type="text">“STRIP: A defence against trojan attacks on deep neural networks,”</text>
<text top="78" left="330" width="233" height="7" font="font8" id="p9_t72" reading_order_no="71" segment_no="2" tag_type="text">in Proceedings of the 35th Annual Computer Security Applications</text>
<text top="87" left="330" width="105" height="7" font="font34" id="p9_t73" reading_order_no="72" segment_no="2" tag_type="text">Conference , 2019, pp. 113–125.</text>
<text top="96" left="312" width="251" height="7" font="font8" id="p9_t74" reading_order_no="73" segment_no="3" tag_type="text">[13] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal</text>
<text top="105" left="330" width="233" height="7" font="font8" id="p9_t75" reading_order_no="74" segment_no="3" tag_type="text">adversarial perturbations,” in IEEE Conference on Computer Vision and</text>
<text top="113" left="330" width="125" height="8" font="font34" id="p9_t76" reading_order_no="75" segment_no="3" tag_type="text">Pattern Recognition , 2017, pp. 86–94.</text>
<text top="122" left="312" width="251" height="7" font="font8" id="p9_t77" reading_order_no="76" segment_no="4" tag_type="text">[14] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing</text>
<text top="131" left="330" width="233" height="7" font="font8" id="p9_t78" reading_order_no="77" segment_no="4" tag_type="text">adversarial examples,” in 3rd International Conference on Learning</text>
<text top="140" left="330" width="107" height="7" font="font34" id="p9_t79" reading_order_no="78" segment_no="4" tag_type="text">Representations , 2015, pp. 1–11.</text>
<text top="149" left="312" width="251" height="7" font="font8" id="p9_t80" reading_order_no="79" segment_no="6" tag_type="text">[15] M. Xue, C. He, J. Wang, and W. Liu, “One-to-N &amp; N-to-One: Two</text>
<text top="158" left="330" width="233" height="7" font="font8" id="p9_t81" reading_order_no="80" segment_no="6" tag_type="text">advanced backdoor attacks against deep learning models,” IEEE Trans-</text>
<text top="167" left="330" width="222" height="7" font="font34" id="p9_t82" reading_order_no="81" segment_no="6" tag_type="text">actions on Dependable and Secure Computing , 2020, Early Access.</text>
<text top="176" left="312" width="251" height="7" font="font8" id="p9_t83" reading_order_no="82" segment_no="7" tag_type="text">[16] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-Pruning: Defending against</text>
<text top="185" left="330" width="233" height="7" font="font8" id="p9_t84" reading_order_no="83" segment_no="7" tag_type="text">backdooring attacks on deep neural networks,” in Proceedings of 21st</text>
<text top="194" left="330" width="233" height="7" font="font34" id="p9_t85" reading_order_no="84" segment_no="7" tag_type="text">International Symposium on Attacks, Intrusions, and Defenses , 2018, pp.</text>
<text top="203" left="330" width="30" height="7" font="font8" id="p9_t86" reading_order_no="85" segment_no="7" tag_type="text">273–294.</text>
<text top="212" left="312" width="251" height="7" font="font8" id="p9_t87" reading_order_no="86" segment_no="8" tag_type="text">[17] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee,</text>
<text top="221" left="330" width="233" height="7" font="font8" id="p9_t88" reading_order_no="87" segment_no="8" tag_type="text">I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural</text>
<text top="230" left="330" width="233" height="7" font="font8" id="p9_t89" reading_order_no="88" segment_no="8" tag_type="text">networks by activation clustering,” in the 33th AAAI Conference on</text>
<text top="239" left="330" width="161" height="7" font="font34" id="p9_t90" reading_order_no="89" segment_no="8" tag_type="text">Artificial Intelligence , vol. 2301, 2019, pp. 1–10.</text>
<text top="248" left="312" width="251" height="7" font="font8" id="p9_t91" reading_order_no="90" segment_no="10" tag_type="text">[18] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.</text>
<text top="257" left="330" width="233" height="7" font="font8" id="p9_t92" reading_order_no="91" segment_no="10" tag_type="text">Zhao, “Neural Cleanse: Identifying and mitigating backdoor attacks in</text>
<text top="266" left="330" width="233" height="7" font="font8" id="p9_t93" reading_order_no="92" segment_no="10" tag_type="text">neural networks,” in IEEE Symposium on Security and Privacy , 2019,</text>
<text top="275" left="330" width="43" height="7" font="font8" id="p9_t94" reading_order_no="93" segment_no="10" tag_type="text">pp. 707–723.</text>
<text top="284" left="312" width="251" height="7" font="font8" id="p9_t95" reading_order_no="94" segment_no="12" tag_type="text">[19] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “DeepInspect: A black-box</text>
<text top="293" left="330" width="233" height="7" font="font8" id="p9_t96" reading_order_no="95" segment_no="12" tag_type="text">trojan detection and mitigation framework for deep neural networks,”</text>
<text top="302" left="330" width="233" height="7" font="font8" id="p9_t97" reading_order_no="96" segment_no="12" tag_type="text">in Proceedings of the 28th International Joint Conference on Artificial</text>
<text top="311" left="330" width="114" height="7" font="font34" id="p9_t98" reading_order_no="97" segment_no="12" tag_type="text">Intelligence , 2019, pp. 4658–4664.</text>
<text top="320" left="312" width="251" height="7" font="font8" id="p9_t99" reading_order_no="98" segment_no="13" tag_type="text">[20] N. Carlini and D. A. Wagner, “Towards evaluating the robustness of</text>
<text top="329" left="330" width="233" height="7" font="font8" id="p9_t100" reading_order_no="99" segment_no="13" tag_type="text">neural networks,” in IEEE Symposium on Security and Privacy , 2017,</text>
<text top="338" left="330" width="35" height="7" font="font8" id="p9_t101" reading_order_no="100" segment_no="13" tag_type="text">pp. 39–57.</text>
<text top="347" left="312" width="251" height="7" font="font8" id="p9_t102" reading_order_no="101" segment_no="14" tag_type="text">[21] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool: A simple</text>
<text top="356" left="330" width="233" height="7" font="font8" id="p9_t103" reading_order_no="102" segment_no="14" tag_type="text">and accurate method to fool deep neural networks,” in IEEE Conference</text>
<text top="365" left="330" width="223" height="7" font="font34" id="p9_t104" reading_order_no="103" segment_no="14" tag_type="text">on Computer Vision and Pattern Recognition , 2016, pp. 2574–2582.</text>
<text top="374" left="312" width="251" height="7" font="font8" id="p9_t105" reading_order_no="104" segment_no="15" tag_type="text">[22] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards</text>
<text top="382" left="330" width="233" height="8" font="font8" id="p9_t106" reading_order_no="105" segment_no="15" tag_type="text">deep learning models resistant to adversarial attacks,” in 6th Interna-</text>
<text top="391" left="330" width="211" height="8" font="font34" id="p9_t107" reading_order_no="106" segment_no="15" tag_type="text">tional Conference on Learning Representations , 2018, pp. 1–28.</text>
<text top="400" left="312" width="251" height="7" font="font8" id="p9_t108" reading_order_no="107" segment_no="16" tag_type="text">[23] N. Akhtar and A. S. Mian, “Threat of adversarial attacks on deep</text>
<text top="409" left="330" width="233" height="7" font="font8" id="p9_t109" reading_order_no="108" segment_no="16" tag_type="text">learning in computer vision: A survey,” IEEE Access , vol. 6, pp. 14 410–</text>
<text top="418" left="330" width="44" height="7" font="font8" id="p9_t110" reading_order_no="109" segment_no="16" tag_type="text">14 430, 2018.</text>
<text top="427" left="312" width="251" height="7" font="font8" id="p9_t111" reading_order_no="110" segment_no="17" tag_type="text">[24] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.</text>
<text top="436" left="330" width="233" height="7" font="font8" id="p9_t112" reading_order_no="111" segment_no="17" tag_type="text">Goodfellow, and R. Fergus, “Intriguing properties of neural networks,”</text>
<text top="445" left="330" width="77" height="7" font="font34" id="p9_t113" reading_order_no="112" segment_no="17" tag_type="text">arXiv:1312.6199 , 2013.</text>
<text top="454" left="312" width="251" height="7" font="font8" id="p9_t114" reading_order_no="113" segment_no="19" tag_type="text">[25] G. Huang, Z. Liu, and K. Q. Weinberger, “Densely connected convolu-</text>
<text top="463" left="330" width="139" height="7" font="font8" id="p9_t115" reading_order_no="114" segment_no="19" tag_type="text">tional networks,” arXiv:1608.06993 , 2016.</text>
<text top="472" left="312" width="251" height="7" font="font8" id="p9_t116" reading_order_no="115" segment_no="21" tag_type="text">[26] S. Targ, D. Almeida, and K. Lyman, “ResNet in ResNet: Generalizing</text>
<text top="481" left="330" width="158" height="7" font="font8" id="p9_t117" reading_order_no="116" segment_no="21" tag_type="text">residual architectures,” arXiv:1603.08029 , 2016.</text>
<text top="490" left="312" width="251" height="7" font="font8" id="p9_t118" reading_order_no="117" segment_no="23" tag_type="text">[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification</text>
<text top="499" left="330" width="233" height="7" font="font8" id="p9_t119" reading_order_no="118" segment_no="23" tag_type="text">with deep convolutional neural networks,” Advances in neural informa-</text>
<text top="508" left="330" width="181" height="7" font="font34" id="p9_t120" reading_order_no="119" segment_no="23" tag_type="text">tion processing systems , vol. 25, pp. 1097–1105, 2012.</text>
</page>
<outline>
<item page="1">I Introduction</item>
<item page="2">II Background and Related Work</item>
<outline>
<item page="2">II-A Universarial Adversarial Perturbations Moosavi-Dezfooli17</item>
<item page="2">II-B Backdoor Attacks</item>
<item page="2">II-C Existing Backdoor Defenses</item>
</outline>
<item page="3">III The Proposed Method</item>
<outline>
<item page="3">III-A Overall flow</item>
<item page="4">III-B Perturbation Generation</item>
<item page="4">III-C Backdoor Detection</item>
<item page="5">III-D Why choose UAP Moosavi-Dezfooli17 for adversarial perturbation generation?</item>
</outline>
<item page="5">IV Experimental Results</item>
<outline>
<item page="5">IV-A Experimental Setup</item>
<outline>
<item page="5">IV-A1 Datasets</item>
<item page="5">IV-A2 Experimental Settings of Backdoor Attack</item>
<item page="5">IV-A3 Metrics</item>
</outline>
<item page="5">IV-B Effectiveness of the Proposed Method</item>
<item page="6">IV-C Defense Performance of the Proposed Method under Different Attack Settings</item>
<outline>
<item page="6">IV-C1 Trigger Transparency</item>
<item page="6">IV-C2 Trigger Size</item>
<item page="6">IV-C3 Trigger Pattern</item>
</outline>
<item page="7">IV-D Experiment Results of the Proposed Method with Four Different Adversarial Perturbation Generation Methods.</item>
<item page="8">IV-E Comparison with Related Work</item>
</outline>
<item page="9">V Conclusion</item>
<item page="9">References</item>
</outline>
</pdf2xml>
