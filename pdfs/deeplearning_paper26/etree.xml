<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="7" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="24" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font2" size="11" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="9" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font4" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font5" size="9" family="CMMI9" color="#000000"/>
	<fontspec id="font6" size="6" family="CMR6" color="#000000"/>
	<fontspec id="font7" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font8" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font9" size="29" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font10" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font11" size="20" family="Times" color="#7f7f7f"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="text">1</text>
<text top="61" left="60" width="492" height="21" font="font1" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="title">Detecting Backdoor in Deep Neural Networks via</text>
<text top="89" left="126" width="359" height="21" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="title">Intentional Adversarial Perturbations</text>
<text top="121" left="118" width="376" height="10" font="font2" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="text">Mingfu Xue, Yinghao Wu, Zhiyu Wu, Yushu Zhang, Jian Wang, and Weiqiang Liu</text>
<text top="178" left="59" width="31" height="8" font="font3" id="p1_t5" reading_order_no="5" segment_no="3" tag_type="text">Abstract</text>
<text top="178" left="90" width="210" height="8" font="font4" id="p1_t6" reading_order_no="6" segment_no="3" tag_type="text">—Recently, the security of deep learning systems has</text>
<text top="188" left="49" width="251" height="8" font="font4" id="p1_t7" reading_order_no="7" segment_no="3" tag_type="text">attracted a lot of attentions, especially when applied to safety-</text>
<text top="198" left="49" width="251" height="8" font="font4" id="p1_t8" reading_order_no="8" segment_no="3" tag_type="text">critical tasks, such as autonomous driving, face recognition,</text>
<text top="208" left="49" width="251" height="8" font="font4" id="p1_t9" reading_order_no="9" segment_no="3" tag_type="text">malware classification, etc. Recent researches show that deep</text>
<text top="218" left="49" width="251" height="8" font="font4" id="p1_t10" reading_order_no="10" segment_no="3" tag_type="text">learning model is susceptible to backdoor attacks where the</text>
<text top="228" left="49" width="251" height="8" font="font4" id="p1_t11" reading_order_no="11" segment_no="3" tag_type="text">backdoor embedded in the model will be triggered when a back-</text>
<text top="238" left="49" width="251" height="8" font="font4" id="p1_t12" reading_order_no="12" segment_no="3" tag_type="text">door instance arrives. Many defenses against backdoor attacks</text>
<text top="248" left="49" width="251" height="8" font="font4" id="p1_t13" reading_order_no="13" segment_no="3" tag_type="text">have been proposed. However, existing defense works require</text>
<text top="258" left="49" width="251" height="8" font="font4" id="p1_t14" reading_order_no="14" segment_no="3" tag_type="text">high computational overhead or backdoor attack information</text>
<text top="268" left="49" width="251" height="8" font="font4" id="p1_t15" reading_order_no="15" segment_no="3" tag_type="text">such as the trigger size, which is difficult to satisfy in realistic</text>
<text top="278" left="49" width="251" height="8" font="font4" id="p1_t16" reading_order_no="16" segment_no="3" tag_type="text">scenarios. In this paper, a novel backdoor detection method</text>
<text top="288" left="49" width="251" height="8" font="font4" id="p1_t17" reading_order_no="17" segment_no="3" tag_type="text">based on adversarial examples is proposed. The proposed method</text>
<text top="298" left="49" width="251" height="8" font="font4" id="p1_t18" reading_order_no="18" segment_no="3" tag_type="text">leverages intentional adversarial perturbations to detect whether</text>
<text top="308" left="49" width="251" height="8" font="font4" id="p1_t19" reading_order_no="19" segment_no="3" tag_type="text">an image contains a trigger, which can be applied in both the</text>
<text top="318" left="49" width="251" height="8" font="font4" id="p1_t20" reading_order_no="20" segment_no="3" tag_type="text">training stage and the inference stage (sanitize the training set</text>
<text top="328" left="49" width="251" height="8" font="font4" id="p1_t21" reading_order_no="21" segment_no="3" tag_type="text">in training stage and detect the backdoor instances in inference</text>
<text top="338" left="49" width="251" height="8" font="font4" id="p1_t22" reading_order_no="22" segment_no="3" tag_type="text">stage). Specifically, given an untrusted image, the adversarial</text>
<text top="348" left="49" width="251" height="8" font="font4" id="p1_t23" reading_order_no="23" segment_no="3" tag_type="text">perturbation is added to the image intentionally. If the prediction</text>
<text top="358" left="49" width="251" height="8" font="font4" id="p1_t24" reading_order_no="24" segment_no="3" tag_type="text">of the model on the perturbed image is consistent with that on</text>
<text top="368" left="49" width="251" height="8" font="font4" id="p1_t25" reading_order_no="25" segment_no="3" tag_type="text">the unperturbed image, the input image will be considered as a</text>
<text top="378" left="49" width="251" height="8" font="font4" id="p1_t26" reading_order_no="26" segment_no="3" tag_type="text">backdoor instance. Compared with most existing defense works,</text>
<text top="388" left="49" width="251" height="8" font="font4" id="p1_t27" reading_order_no="27" segment_no="3" tag_type="text">the proposed adversarial perturbation based method requires</text>
<text top="398" left="49" width="251" height="8" font="font4" id="p1_t28" reading_order_no="28" segment_no="3" tag_type="text">low computational resources and maintains the visual quality</text>
<text top="408" left="49" width="251" height="8" font="font4" id="p1_t29" reading_order_no="29" segment_no="3" tag_type="text">of the images. Experimental results show that, the backdoor</text>
<text top="418" left="49" width="251" height="8" font="font4" id="p1_t30" reading_order_no="30" segment_no="3" tag_type="text">detection rate of the proposed defense method is 99.63%, 99.76%</text>
<text top="428" left="49" width="251" height="8" font="font4" id="p1_t31" reading_order_no="31" segment_no="3" tag_type="text">and 99.91% on Fashion-MNIST, CIFAR-10 and GTSRB datasets,</text>
<text top="437" left="49" width="251" height="8" font="font4" id="p1_t32" reading_order_no="32" segment_no="3" tag_type="text">respectively. Besides, the proposed method maintains the visual</text>
<text top="447" left="49" width="108" height="8" font="font4" id="p1_t33" reading_order_no="33" segment_no="3" tag_type="text">quality of the image as the</text>
<text top="447" left="160" width="4" height="8" font="font5" id="p1_t34" reading_order_no="34" segment_no="3" tag_type="text">`</text>
<text top="450" left="164" width="4" height="5" font="font6" id="p1_t35" reading_order_no="35" segment_no="3" tag_type="text">2</text>
<text top="447" left="172" width="128" height="8" font="font4" id="p1_t36" reading_order_no="36" segment_no="3" tag_type="text">norm of the added perturbation</text>
<text top="457" left="49" width="251" height="8" font="font4" id="p1_t37" reading_order_no="37" segment_no="3" tag_type="text">are as low as 2.8715, 3.0513 and 2.4362 on Fashion-MNIST,</text>
<text top="467" left="49" width="251" height="8" font="font4" id="p1_t38" reading_order_no="38" segment_no="3" tag_type="text">CIFAR-10 and GTSRB datasets, respectively. In addition, it is</text>
<text top="477" left="49" width="251" height="8" font="font4" id="p1_t39" reading_order_no="39" segment_no="3" tag_type="text">also demonstrated that the proposed method can achieve high</text>
<text top="487" left="49" width="251" height="8" font="font4" id="p1_t40" reading_order_no="40" segment_no="3" tag_type="text">defense performance against backdoor attacks under different</text>
<text top="497" left="49" width="251" height="8" font="font4" id="p1_t41" reading_order_no="41" segment_no="3" tag_type="text">attack settings (trigger transparency, trigger size and trigger</text>
<text top="507" left="49" width="251" height="8" font="font4" id="p1_t42" reading_order_no="42" segment_no="3" tag_type="text">pattern). Compared with the existing defense work (STRIP), the</text>
<text top="517" left="49" width="251" height="8" font="font4" id="p1_t43" reading_order_no="43" segment_no="3" tag_type="text">proposed method has better detection performance on all the</text>
<text top="527" left="49" width="192" height="8" font="font4" id="p1_t44" reading_order_no="44" segment_no="3" tag_type="text">three datasets, and is more efficient than STRIP.</text>
<text top="543" left="59" width="47" height="8" font="font3" id="p1_t45" reading_order_no="45" segment_no="9" tag_type="text">Index Terms</text>
<text top="543" left="106" width="194" height="8" font="font4" id="p1_t46" reading_order_no="46" segment_no="9" tag_type="text">—Backdoor attacks, Deep neural networks, Back-</text>
<text top="553" left="49" width="185" height="8" font="font4" id="p1_t47" reading_order_no="47" segment_no="9" tag_type="text">door detection, Defenses, Adversarial examples</text>
<text top="583" left="136" width="15" height="9" font="font7" id="p1_t48" reading_order_no="48" segment_no="11" tag_type="title">I. I</text>
<text top="584" left="151" width="62" height="7" font="font8" id="p1_t49" reading_order_no="49" segment_no="11" tag_type="title">NTRODUCTION</text>
<text top="599" left="49" width="21" height="26" font="font9" id="p1_t50" reading_order_no="50" segment_no="12" tag_type="text">R</text>
<text top="600" left="72" width="228" height="9" font="font7" id="p1_t51" reading_order_no="51" segment_no="12" tag_type="text">ECENT studies show that deep learning models are</text>
<text top="612" left="72" width="228" height="9" font="font7" id="p1_t52" reading_order_no="52" segment_no="12" tag_type="text">vulnerable to backdoor attacks <a href="deeplearning_paper26.html#9">[1]–[3]. </a>Adversaries can</text>
<text top="623" left="49" width="251" height="9" font="font7" id="p1_t53" reading_order_no="53" segment_no="12" tag_type="text">embed the backdoor into deep learning model by modifying</text>
<text top="635" left="49" width="251" height="9" font="font7" id="p1_t54" reading_order_no="54" segment_no="12" tag_type="text">the architectures or parameters of the model, or injecting</text>
<text top="647" left="49" width="251" height="9" font="font7" id="p1_t55" reading_order_no="55" segment_no="12" tag_type="text">backdoor instances in the training set to embed the backdoor</text>
<text top="669" left="57" width="243" height="7" font="font8" id="p1_t56" reading_order_no="105" segment_no="13" tag_type="footnote">M. Xue, Y. Wu, Y. Zhang and J. Wang are with the College of</text>
<text top="678" left="49" width="251" height="7" font="font8" id="p1_t57" reading_order_no="106" segment_no="13" tag_type="footnote">Computer Science and Technology, Nanjing University of Aeronautics and</text>
<text top="687" left="49" width="251" height="7" font="font8" id="p1_t58" reading_order_no="107" segment_no="13" tag_type="footnote">Astronautics, Nanjing, 211106, China (e-mail: mingfu.xue@nuaa.edu.cn;</text>
<text top="696" left="49" width="213" height="7" font="font8" id="p1_t59" reading_order_no="108" segment_no="13" tag_type="footnote">wyh@nuaa.edu.cn; yushu@nuaa.edu.cn; wangjian@nuaa.edu.cn).</text>
<text top="705" left="57" width="243" height="7" font="font8" id="p1_t60" reading_order_no="109" segment_no="14" tag_type="footnote">Z. Wu is with the College of Science, Nanjing University of Aeronautics</text>
<text top="714" left="49" width="244" height="7" font="font8" id="p1_t61" reading_order_no="110" segment_no="14" tag_type="footnote">and Astronautics, Nanjing, 211106, China (e-mail: wuzhiyu@nuaa.edu.cn)</text>
<text top="723" left="57" width="243" height="7" font="font8" id="p1_t62" reading_order_no="111" segment_no="15" tag_type="footnote">W. Liu is with the College of Electronic and Information Engineering,</text>
<text top="732" left="49" width="251" height="7" font="font8" id="p1_t63" reading_order_no="112" segment_no="15" tag_type="footnote">Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China</text>
<text top="741" left="49" width="116" height="7" font="font8" id="p1_t64" reading_order_no="113" segment_no="15" tag_type="footnote">(e-mail: liuweiqiang@nuaa.edu.cn).</text>
<text top="178" left="312" width="251" height="9" font="font7" id="p1_t65" reading_order_no="56" segment_no="4" tag_type="text">during training <a href="deeplearning_paper26.html#9">[1]–[3]. </a>The backdoored model will behave</text>
<text top="190" left="312" width="251" height="9" font="font7" id="p1_t66" reading_order_no="57" segment_no="4" tag_type="text">normally for the benign inputs, but it will output the target</text>
<text top="202" left="312" width="186" height="9" font="font7" id="p1_t67" reading_order_no="58" segment_no="4" tag_type="text">label for the input image carrying the trigger.</text>
<text top="213" left="322" width="241" height="9" font="font7" id="p1_t68" reading_order_no="59" segment_no="5" tag_type="text">Many defenses against backdoor attacks have been pro-</text>
<text top="225" left="312" width="251" height="9" font="font7" id="p1_t69" reading_order_no="60" segment_no="5" tag_type="text">posed. However, the existing defense works require high com-</text>
<text top="237" left="312" width="251" height="9" font="font7" id="p1_t70" reading_order_no="61" segment_no="5" tag_type="text">putational overhead <a href="deeplearning_paper26.html#9">[4]–[6], </a>a large number of clean images</text>
<text top="249" left="312" width="251" height="9" font="font7" id="p1_t71" reading_order_no="62" segment_no="5" tag_type="text">to retrain the model <a href="deeplearning_paper26.html#9">[7], </a>or backdoor attack information such</text>
<text top="261" left="312" width="251" height="9" font="font7" id="p1_t72" reading_order_no="63" segment_no="5" tag_type="text">as the trigger size <a href="deeplearning_paper26.html#9">[5], [8]. </a>In practice, these requirements are</text>
<text top="273" left="312" width="94" height="9" font="font7" id="p1_t73" reading_order_no="64" segment_no="5" tag_type="text">difficult to be satisfied.</text>
<text top="285" left="322" width="241" height="9" font="font7" id="p1_t74" reading_order_no="65" segment_no="6" tag_type="text">In this paper, we propose a novel backdoor detection</text>
<text top="297" left="312" width="251" height="9" font="font7" id="p1_t75" reading_order_no="66" segment_no="6" tag_type="text">method based on adversarial examples, which only requires</text>
<text top="309" left="312" width="251" height="9" font="font7" id="p1_t76" reading_order_no="67" segment_no="6" tag_type="text">low computational overhead. The proposed method can be</text>
<text top="321" left="312" width="251" height="9" font="font7" id="p1_t77" reading_order_no="68" segment_no="6" tag_type="text">applied in both the training stage and the inference stage.</text>
<text top="333" left="312" width="251" height="9" font="font7" id="p1_t78" reading_order_no="69" segment_no="6" tag_type="text">In the training stage, the proposed method can detect and</text>
<text top="345" left="312" width="251" height="9" font="font7" id="p1_t79" reading_order_no="70" segment_no="6" tag_type="text">remove the backdoor instances in the training dataset. In the</text>
<text top="357" left="312" width="251" height="9" font="font7" id="p1_t80" reading_order_no="71" segment_no="6" tag_type="text">inference stage, the proposed method can determine whether</text>
<text top="369" left="312" width="251" height="9" font="font7" id="p1_t81" reading_order_no="72" segment_no="6" tag_type="text">an input image contains a trigger. Specifically, the proposed</text>
<text top="380" left="312" width="251" height="9" font="font7" id="p1_t82" reading_order_no="73" segment_no="6" tag_type="text">method works as follows. First, the adversarial perturbation</text>
<text top="392" left="312" width="251" height="9" font="font7" id="p1_t83" reading_order_no="74" segment_no="6" tag_type="text">is generated based on the untrusted model with a small set</text>
<text top="404" left="312" width="251" height="9" font="font7" id="p1_t84" reading_order_no="75" segment_no="6" tag_type="text">of clean images. Second, for an image (training image in</text>
<text top="416" left="312" width="251" height="9" font="font7" id="p1_t85" reading_order_no="76" segment_no="6" tag_type="text">the training stage or input image in the inference stage), the</text>
<text top="428" left="312" width="251" height="9" font="font7" id="p1_t86" reading_order_no="77" segment_no="6" tag_type="text">adversarial perturbation will be added on it. If the prediction of</text>
<text top="440" left="312" width="251" height="9" font="font7" id="p1_t87" reading_order_no="78" segment_no="6" tag_type="text">the model on the perturbed image is inconsistent with that on</text>
<text top="452" left="312" width="251" height="9" font="font7" id="p1_t88" reading_order_no="79" segment_no="6" tag_type="text">the unperturbed image, the image is considered to be a clean</text>
<text top="464" left="312" width="251" height="9" font="font7" id="p1_t89" reading_order_no="80" segment_no="6" tag_type="text">image. Otherwise, the image is considered to be a backdoor</text>
<text top="476" left="312" width="251" height="9" font="font7" id="p1_t90" reading_order_no="81" segment_no="6" tag_type="text">instance, which also implies that the model is backdoored and</text>
<text top="488" left="312" width="208" height="9" font="font7" id="p1_t91" reading_order_no="82" segment_no="6" tag_type="text">the predicted label of the image is the target label.</text>
<text top="500" left="322" width="241" height="9" font="font7" id="p1_t92" reading_order_no="83" segment_no="7" tag_type="text">The contributions of this paper are summarized as follows:</text>
<text top="514" left="322" width="4" height="7" font="font10" id="p1_t93" reading_order_no="84" segment_no="8" tag_type="text">•</text>
<text top="513" left="332" width="231" height="9" font="font7" id="p1_t94" reading_order_no="85" segment_no="8" tag_type="text">This paper proposes a novel backdoor detection method</text>
<text top="525" left="332" width="231" height="9" font="font7" id="p1_t95" reading_order_no="86" segment_no="8" tag_type="text">based on intentional adversarial perturbation. The adver-</text>
<text top="537" left="332" width="231" height="9" font="font7" id="p1_t96" reading_order_no="87" segment_no="8" tag_type="text">sarial perturbation can fool the deep learning model, mak-</text>
<text top="549" left="332" width="231" height="9" font="font7" id="p1_t97" reading_order_no="88" segment_no="8" tag_type="text">ing the model misclassify the perturbed image. However,</text>
<text top="560" left="332" width="231" height="9" font="font7" id="p1_t98" reading_order_no="89" segment_no="8" tag_type="text">for the backdoor instances, the model will always classify</text>
<text top="572" left="332" width="231" height="9" font="font7" id="p1_t99" reading_order_no="90" segment_no="8" tag_type="text">them as the target class even if these backdoor instances</text>
<text top="584" left="332" width="231" height="9" font="font7" id="p1_t100" reading_order_no="91" segment_no="8" tag_type="text">are added with adversarial perturbation. In this way,</text>
<text top="596" left="332" width="231" height="9" font="font7" id="p1_t101" reading_order_no="92" segment_no="8" tag_type="text">the backdoor instances can be detected via intentional</text>
<text top="608" left="332" width="231" height="9" font="font7" id="p1_t102" reading_order_no="93" segment_no="8" tag_type="text">adversarial perturbations. Moreover, the proposed method</text>
<text top="620" left="332" width="231" height="9" font="font7" id="p1_t103" reading_order_no="94" segment_no="8" tag_type="text">can be deployed in both the training stages and the</text>
<text top="632" left="332" width="231" height="9" font="font7" id="p1_t104" reading_order_no="95" segment_no="8" tag_type="text">inference stage. In the training stage, for a training image,</text>
<text top="644" left="332" width="231" height="9" font="font7" id="p1_t105" reading_order_no="96" segment_no="8" tag_type="text">the intentional adversarial perturbation will be added on</text>
<text top="656" left="332" width="231" height="9" font="font7" id="p1_t106" reading_order_no="97" segment_no="8" tag_type="text">it. If the model’s prediction on the perturbed training</text>
<text top="668" left="332" width="231" height="9" font="font7" id="p1_t107" reading_order_no="98" segment_no="8" tag_type="text">image is consistent with the prediction on the unperturbed</text>
<text top="680" left="332" width="231" height="9" font="font7" id="p1_t108" reading_order_no="99" segment_no="8" tag_type="text">training image, the training image will be considered as a</text>
<text top="692" left="332" width="231" height="9" font="font7" id="p1_t109" reading_order_no="100" segment_no="8" tag_type="text">backdoor instance and then be removed from the training</text>
<text top="704" left="332" width="231" height="9" font="font7" id="p1_t110" reading_order_no="101" segment_no="8" tag_type="text">dataset. In the inference stage, for an input image, the</text>
<text top="716" left="332" width="231" height="9" font="font7" id="p1_t111" reading_order_no="102" segment_no="8" tag_type="text">adversarial perturbation is added on it. If the model’s</text>
<text top="728" left="332" width="231" height="9" font="font7" id="p1_t112" reading_order_no="103" segment_no="8" tag_type="text">prediction on the perturbed image is consistent with the</text>
<text top="740" left="332" width="231" height="9" font="font7" id="p1_t113" reading_order_no="104" segment_no="8" tag_type="text">prediction on the unperturbed image, the input image will</text>
<text top="546" left="32" width="0" height="18" font="font11" id="p1_t114" reading_order_no="0" segment_no="10" tag_type="title">arXiv:2105.14259v2  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font12" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="text">2</text>
<text top="58" left="69" width="155" height="9" font="font7" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">be considered as a backdoor instance.</text>
<text top="72" left="59" width="4" height="7" font="font10" id="p2_t3" reading_order_no="2" segment_no="3" tag_type="text">•</text>
<text top="70" left="69" width="231" height="9" font="font7" id="p2_t4" reading_order_no="3" segment_no="3" tag_type="text">In comparison with the work <a href="deeplearning_paper26.html#9">[7] </a>which requires a large</text>
<text top="82" left="69" width="231" height="9" font="font7" id="p2_t5" reading_order_no="4" segment_no="3" tag_type="text">number of clean images to retrain the model to remove</text>
<text top="94" left="69" width="231" height="9" font="font7" id="p2_t6" reading_order_no="5" segment_no="3" tag_type="text">the backdoor, the proposed method only requires a small</text>
<text top="106" left="69" width="231" height="9" font="font7" id="p2_t7" reading_order_no="6" segment_no="3" tag_type="text">set of clean images to generate adversarial perturbation.</text>
<text top="118" left="69" width="231" height="9" font="font7" id="p2_t8" reading_order_no="7" segment_no="3" tag_type="text">Besides, the existing work <a href="deeplearning_paper26.html#9">[4] </a>requires training a large</text>
<text top="130" left="69" width="231" height="9" font="font7" id="p2_t9" reading_order_no="8" segment_no="3" tag_type="text">number of backdoored models and clean models, which</text>
<text top="142" left="69" width="231" height="9" font="font7" id="p2_t10" reading_order_no="9" segment_no="3" tag_type="text">is computationally expensive. In contrast, the proposed</text>
<text top="154" left="69" width="231" height="9" font="font7" id="p2_t11" reading_order_no="10" segment_no="3" tag_type="text">method only needs to generate the adversarial perturba-</text>
<text top="166" left="69" width="231" height="9" font="font7" id="p2_t12" reading_order_no="11" segment_no="3" tag_type="text">tion with negligible computational overhead. Moreover,</text>
<text top="178" left="69" width="231" height="9" font="font7" id="p2_t13" reading_order_no="12" segment_no="3" tag_type="text">the proposed method does not need any backdoor attack</text>
<text top="190" left="69" width="231" height="9" font="font7" id="p2_t14" reading_order_no="13" segment_no="3" tag_type="text">information, which makes the proposed method more</text>
<text top="202" left="69" width="221" height="9" font="font7" id="p2_t15" reading_order_no="14" segment_no="3" tag_type="text">practical and feasible than the existing works <a href="deeplearning_paper26.html#9">[5], [8].</a></text>
<text top="215" left="59" width="4" height="7" font="font10" id="p2_t16" reading_order_no="15" segment_no="5" tag_type="text">•</text>
<text top="214" left="69" width="231" height="9" font="font7" id="p2_t17" reading_order_no="16" segment_no="5" tag_type="text">Experimental results show that the proposed defense</text>
<text top="226" left="69" width="231" height="9" font="font7" id="p2_t18" reading_order_no="17" segment_no="5" tag_type="text">method can achieve high backdoor detection rate (99.63%</text>
<text top="238" left="69" width="231" height="9" font="font7" id="p2_t19" reading_order_no="18" segment_no="5" tag_type="text">99.76% and 99.91% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10</text>
<text top="250" left="69" width="231" height="9" font="font7" id="p2_t20" reading_order_no="19" segment_no="5" tag_type="text"><a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively). It is also</text>
<text top="262" left="69" width="231" height="9" font="font7" id="p2_t21" reading_order_no="20" segment_no="5" tag_type="text">demonstrated that, under different attack settings (differ-</text>
<text top="274" left="69" width="231" height="9" font="font7" id="p2_t22" reading_order_no="21" segment_no="5" tag_type="text">ent trigger transparency, different trigger sizes and dif-</text>
<text top="285" left="69" width="231" height="9" font="font7" id="p2_t23" reading_order_no="22" segment_no="5" tag_type="text">ferent trigger patterns), the proposed method can achieve</text>
<text top="297" left="69" width="231" height="9" font="font7" id="p2_t24" reading_order_no="23" segment_no="5" tag_type="text">high defense performance, as the backdoor detection rate</text>
<text top="309" left="69" width="231" height="9" font="font7" id="p2_t25" reading_order_no="24" segment_no="5" tag_type="text">of the proposed approach is as high as 98.80%, 99.70%</text>
<text top="321" left="69" width="231" height="9" font="font7" id="p2_t26" reading_order_no="25" segment_no="5" tag_type="text">and 99.96% on Fashion-MNIST, CIFAR-10 and GTSRB</text>
<text top="333" left="69" width="231" height="9" font="font7" id="p2_t27" reading_order_no="26" segment_no="5" tag_type="text">datasets, respectively. Compared with STRIP <a href="deeplearning_paper26.html#9">[12], </a>the</text>
<text top="345" left="69" width="231" height="9" font="font7" id="p2_t28" reading_order_no="27" segment_no="5" tag_type="text">proposed method achieves higher backdoor detection rate</text>
<text top="357" left="69" width="231" height="9" font="font7" id="p2_t29" reading_order_no="28" segment_no="5" tag_type="text">on all the three datasets. The advantages over STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="369" left="69" width="231" height="9" font="font7" id="p2_t30" reading_order_no="29" segment_no="5" tag_type="text">are that the proposed method will not destroy the trigger</text>
<text top="381" left="69" width="231" height="9" font="font7" id="p2_t31" reading_order_no="30" segment_no="5" tag_type="text">and only needs to predict two images (perturbed image</text>
<text top="393" left="69" width="231" height="9" font="font7" id="p2_t32" reading_order_no="31" segment_no="5" tag_type="text">and unperturbed image). As a result, the proposed method</text>
<text top="405" left="69" width="199" height="9" font="font7" id="p2_t33" reading_order_no="32" segment_no="5" tag_type="text">is more effective and more efficient than STRIP.</text>
<text top="418" left="59" width="241" height="9" font="font7" id="p2_t34" reading_order_no="33" segment_no="7" tag_type="text">This paper is organized as follows. Background and related</text>
<text top="430" left="49" width="251" height="9" font="font7" id="p2_t35" reading_order_no="34" segment_no="7" tag_type="text">works are reviewed in Section <a href="deeplearning_paper26.html#2">II. </a>The proposed detection</text>
<text top="442" left="49" width="251" height="9" font="font7" id="p2_t36" reading_order_no="35" segment_no="7" tag_type="text">method is elaborated in Section <a href="deeplearning_paper26.html#3">III. </a>Experimental results are</text>
<text top="454" left="49" width="251" height="9" font="font7" id="p2_t37" reading_order_no="36" segment_no="7" tag_type="text">presented in Section <a href="deeplearning_paper26.html#5">IV. </a>This paper is concluded in Section</text>
<text top="466" left="49" width="10" height="9" font="font7" id="p2_t38" reading_order_no="37" segment_no="7" tag_type="text"><a href="deeplearning_paper26.html#9">V.</a></text>
<text top="489" left="89" width="22" height="9" font="font7" id="p2_t39" reading_order_no="38" segment_no="8" tag_type="title">II. B</text>
<text top="491" left="111" width="75" height="7" font="font8" id="p2_t40" reading_order_no="39" segment_no="8" tag_type="title">ACKGROUND AND</text>
<text top="489" left="190" width="7" height="9" font="font7" id="p2_t41" reading_order_no="40" segment_no="8" tag_type="title">R</text>
<text top="491" left="197" width="33" height="7" font="font8" id="p2_t42" reading_order_no="41" segment_no="8" tag_type="title">ELATED</text>
<text top="489" left="232" width="9" height="9" font="font7" id="p2_t43" reading_order_no="42" segment_no="8" tag_type="title">W</text>
<text top="491" left="242" width="18" height="7" font="font8" id="p2_t44" reading_order_no="43" segment_no="8" tag_type="title">ORK</text>
<text top="504" left="59" width="241" height="9" font="font7" id="p2_t45" reading_order_no="44" segment_no="9" tag_type="text">In this section, first, we review the universal adversarial</text>
<text top="516" left="49" width="251" height="9" font="font7" id="p2_t46" reading_order_no="45" segment_no="9" tag_type="text">perturbation <a href="deeplearning_paper26.html#9">[13], </a>which is utilized by the proposed method.</text>
<text top="528" left="49" width="251" height="9" font="font7" id="p2_t47" reading_order_no="46" segment_no="9" tag_type="text">Second, we review the related works on backdoor attacks and</text>
<text top="540" left="49" width="37" height="9" font="font7" id="p2_t48" reading_order_no="47" segment_no="9" tag_type="text">defenses.</text>
<text top="565" left="49" width="194" height="9" font="font12" id="p2_t49" reading_order_no="48" segment_no="10" tag_type="title">A. Universarial Adversarial Perturbations <a href="deeplearning_paper26.html#9">[13]</a></text>
<text top="580" left="59" width="241" height="9" font="font7" id="p2_t50" reading_order_no="49" segment_no="11" tag_type="text">It is known that deep neural networks (DNNs) are vul-</text>
<text top="592" left="49" width="251" height="9" font="font7" id="p2_t51" reading_order_no="50" segment_no="11" tag_type="text">nerable to well-crafted small adversarial perturbations. When</text>
<text top="604" left="49" width="251" height="9" font="font7" id="p2_t52" reading_order_no="51" segment_no="11" tag_type="text">added with adversarial perturbation, input image will be mis-</text>
<text top="616" left="49" width="251" height="9" font="font7" id="p2_t53" reading_order_no="52" segment_no="11" tag_type="text">classified by the model <a href="deeplearning_paper26.html#9">[14]. </a>Universal adversarial perturbation</text>
<text top="628" left="49" width="251" height="9" font="font7" id="p2_t54" reading_order_no="53" segment_no="11" tag_type="text">(UAP) <a href="deeplearning_paper26.html#9">[13] </a>is a kind of image-agnostic adversarial perturba-</text>
<text top="640" left="49" width="251" height="9" font="font7" id="p2_t55" reading_order_no="54" segment_no="11" tag_type="text">tion. Different from image-specific adversarial perturbation,</text>
<text top="652" left="49" width="251" height="9" font="font7" id="p2_t56" reading_order_no="55" segment_no="11" tag_type="text">which is specifically crafted for each image <a href="deeplearning_paper26.html#9">[14], </a>UAP is</text>
<text top="664" left="49" width="251" height="9" font="font7" id="p2_t57" reading_order_no="56" segment_no="11" tag_type="text">generated based on a model with a small set of clean images</text>
<text top="676" left="49" width="251" height="9" font="font7" id="p2_t58" reading_order_no="57" segment_no="11" tag_type="text"><a href="deeplearning_paper26.html#9">[13]. </a>As a result, the model will also misclassify other images</text>
<text top="688" left="49" width="175" height="9" font="font7" id="p2_t59" reading_order_no="58" segment_no="11" tag_type="text">with the universal adversarial perturbation.</text>
<text top="713" left="49" width="85" height="9" font="font12" id="p2_t60" reading_order_no="59" segment_no="12" tag_type="title">B. Backdoor Attacks</text>
<text top="728" left="59" width="241" height="9" font="font7" id="p2_t61" reading_order_no="60" segment_no="13" tag_type="text">Recently, a number of researches <a href="deeplearning_paper26.html#9">[1]–[3] </a>indicate that the</text>
<text top="740" left="49" width="251" height="9" font="font7" id="p2_t62" reading_order_no="61" segment_no="13" tag_type="text">backdoor can be embeded into DNN models through injecting</text>
<text top="58" left="312" width="251" height="9" font="font7" id="p2_t63" reading_order_no="62" segment_no="2" tag_type="text">well-crafted backdoor instances into the training set. After the</text>
<text top="70" left="312" width="251" height="9" font="font7" id="p2_t64" reading_order_no="63" segment_no="2" tag_type="text">training process, the model will behave normally on clean</text>
<text top="82" left="312" width="251" height="9" font="font7" id="p2_t65" reading_order_no="64" segment_no="2" tag_type="text">inputs. However, the malicious functionality hidden in the</text>
<text top="94" left="312" width="251" height="9" font="font7" id="p2_t66" reading_order_no="65" segment_no="2" tag_type="text">backdoored model will be triggered by the input images</text>
<text top="106" left="312" width="251" height="9" font="font7" id="p2_t67" reading_order_no="66" segment_no="2" tag_type="text">containing the trigger, and these backdoor instances will be</text>
<text top="118" left="312" width="251" height="9" font="font7" id="p2_t68" reading_order_no="67" segment_no="2" tag_type="text">classified as the target class <a href="deeplearning_paper26.html#9">[2], [15]. </a>Since the performance</text>
<text top="130" left="312" width="251" height="9" font="font7" id="p2_t69" reading_order_no="68" segment_no="2" tag_type="text">of backdoored model is similar to the performance of clean</text>
<text top="142" left="312" width="251" height="9" font="font7" id="p2_t70" reading_order_no="69" segment_no="2" tag_type="text">model on clean inputs, it is difficult for users to perceive the</text>
<text top="154" left="312" width="251" height="9" font="font7" id="p2_t71" reading_order_no="70" segment_no="2" tag_type="text">existence of the backdoor. However, the attacker can trigger</text>
<text top="166" left="312" width="229" height="9" font="font7" id="p2_t72" reading_order_no="71" segment_no="2" tag_type="text">the malicious behavior by inputting backdoor instances.</text>
<text top="197" left="312" width="128" height="9" font="font12" id="p2_t73" reading_order_no="72" segment_no="4" tag_type="title">C. Existing Backdoor Defenses</text>
<text top="214" left="322" width="241" height="9" font="font7" id="p2_t74" reading_order_no="73" segment_no="6" tag_type="text">To date, some defense methods have been proposed to detect</text>
<text top="226" left="312" width="154" height="9" font="font7" id="p2_t75" reading_order_no="74" segment_no="6" tag_type="text">and mitigate the backdoor attacks. Liu</text>
<text top="226" left="468" width="20" height="9" font="font12" id="p2_t76" reading_order_no="75" segment_no="6" tag_type="text">et al.</text>
<text top="226" left="491" width="72" height="9" font="font7" id="p2_t77" reading_order_no="76" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[7] </a>adopted a pre-</text>
<text top="238" left="312" width="251" height="9" font="font7" id="p2_t78" reading_order_no="77" segment_no="6" tag_type="text">trained auto-encoder to preprocess the input image in order to</text>
<text top="250" left="312" width="251" height="9" font="font7" id="p2_t79" reading_order_no="78" segment_no="6" tag_type="text">disable the trigger. They also retrain the backdoored model</text>
<text top="262" left="312" width="251" height="9" font="font7" id="p2_t80" reading_order_no="79" segment_no="6" tag_type="text">with clean images so as to remove the hidden backdoor. Xu</text>
<text top="274" left="312" width="21" height="9" font="font12" id="p2_t81" reading_order_no="80" segment_no="6" tag_type="text">et al.</text>
<text top="274" left="337" width="226" height="9" font="font7" id="p2_t82" reading_order_no="81" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[5] </a>generates a set of backdoor instances as the query</text>
<text top="285" left="312" width="251" height="9" font="font7" id="p2_t83" reading_order_no="82" segment_no="6" tag_type="text">set. Then, they inputs the query set into backdoored models</text>
<text top="297" left="312" width="251" height="9" font="font7" id="p2_t84" reading_order_no="83" segment_no="6" tag_type="text">and clean models to extract representation vectors from those</text>
<text top="309" left="312" width="251" height="9" font="font7" id="p2_t85" reading_order_no="84" segment_no="6" tag_type="text">models. They use the resulting vectors as input to train a meta-</text>
<text top="321" left="312" width="251" height="9" font="font7" id="p2_t86" reading_order_no="85" segment_no="6" tag_type="text">classifier which can predict whether a model is backdoored <a href="deeplearning_paper26.html#9">[5].</a></text>
<text top="333" left="312" width="251" height="9" font="font7" id="p2_t87" reading_order_no="86" segment_no="6" tag_type="text">However, the method needs the knowledge of the trigger size</text>
<text top="345" left="312" width="152" height="9" font="font7" id="p2_t88" reading_order_no="87" segment_no="6" tag_type="text">to craft those backdoor instances. Liu</text>
<text top="345" left="467" width="20" height="9" font="font12" id="p2_t89" reading_order_no="88" segment_no="6" tag_type="text">et al.</text>
<text top="345" left="490" width="73" height="9" font="font7" id="p2_t90" reading_order_no="89" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[16] </a>demonstrated</text>
<text top="357" left="312" width="251" height="9" font="font7" id="p2_t91" reading_order_no="90" segment_no="6" tag_type="text">that the functionality of the backdoor depends on some specific</text>
<text top="369" left="312" width="251" height="9" font="font7" id="p2_t92" reading_order_no="91" segment_no="6" tag_type="text">neurons in the model. These specific neurons are usually</text>
<text top="381" left="312" width="251" height="9" font="font7" id="p2_t93" reading_order_no="92" segment_no="6" tag_type="text">dormant when the model is queried with clean images <a href="deeplearning_paper26.html#9">[16].</a></text>
<text top="393" left="312" width="251" height="9" font="font7" id="p2_t94" reading_order_no="93" segment_no="6" tag_type="text">Defenders can find these neurons by inputting clean images</text>
<text top="405" left="312" width="251" height="9" font="font7" id="p2_t95" reading_order_no="94" segment_no="6" tag_type="text">into the model. Then these malicious neurons can be pruned</text>
<text top="417" left="312" width="251" height="9" font="font7" id="p2_t96" reading_order_no="95" segment_no="6" tag_type="text">so as to remove the backdoor. However, the pruned model</text>
<text top="429" left="312" width="251" height="9" font="font7" id="p2_t97" reading_order_no="96" segment_no="6" tag_type="text">suffers from the degradation in classification accuracy on clean</text>
<text top="441" left="312" width="151" height="9" font="font7" id="p2_t98" reading_order_no="97" segment_no="6" tag_type="text">inputs due to the pruning <a href="deeplearning_paper26.html#9">[16]. </a>Zhang</text>
<text top="441" left="465" width="20" height="9" font="font12" id="p2_t99" reading_order_no="98" segment_no="6" tag_type="text">et al.</text>
<text top="441" left="488" width="75" height="9" font="font7" id="p2_t100" reading_order_no="99" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[4] </a>training a large</text>
<text top="453" left="312" width="251" height="9" font="font7" id="p2_t101" reading_order_no="100" segment_no="6" tag_type="text">number of backdoored models and clean models to generate</text>
<text top="465" left="312" width="251" height="9" font="font7" id="p2_t102" reading_order_no="101" segment_no="6" tag_type="text">corresponding universal perturbations <a href="deeplearning_paper26.html#9">[13]. </a>Then they use the</text>
<text top="477" left="312" width="251" height="9" font="font7" id="p2_t103" reading_order_no="102" segment_no="6" tag_type="text">UAPs <a href="deeplearning_paper26.html#9">[13] </a>as the input to train a two-class classifier as the</text>
<text top="489" left="312" width="251" height="9" font="font7" id="p2_t104" reading_order_no="103" segment_no="6" tag_type="text">Trojan detector. However, the computational cost to generate</text>
<text top="501" left="312" width="251" height="9" font="font7" id="p2_t105" reading_order_no="104" segment_no="6" tag_type="text">those large number of backdoored models and clean models</text>
<text top="513" left="312" width="207" height="9" font="font7" id="p2_t106" reading_order_no="105" segment_no="6" tag_type="text">is high, which is unaffordable to most users. Chen</text>
<text top="513" left="522" width="21" height="9" font="font12" id="p2_t107" reading_order_no="106" segment_no="6" tag_type="text">et al.</text>
<text top="513" left="546" width="17" height="9" font="font7" id="p2_t108" reading_order_no="107" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[17]</a></text>
<text top="525" left="312" width="251" height="9" font="font7" id="p2_t109" reading_order_no="108" segment_no="6" tag_type="text">analyze the neuron activations to the training data to determine</text>
<text top="537" left="312" width="251" height="9" font="font7" id="p2_t110" reading_order_no="109" segment_no="6" tag_type="text">whether it has backdoor instances. It separates the activations</text>
<text top="549" left="312" width="251" height="9" font="font7" id="p2_t111" reading_order_no="110" segment_no="6" tag_type="text">of all training data into two clusters by applying 2-means</text>
<text top="560" left="312" width="251" height="9" font="font7" id="p2_t112" reading_order_no="111" segment_no="6" tag_type="text">clustering. The high silhouette score means that this cluster</text>
<text top="572" left="312" width="204" height="9" font="font7" id="p2_t113" reading_order_no="112" segment_no="6" tag_type="text">corresponds to the backdoor instances <a href="deeplearning_paper26.html#9">[17]. </a>Gao</text>
<text top="573" left="520" width="22" height="9" font="font12" id="p2_t114" reading_order_no="113" segment_no="6" tag_type="text">et al.</text>
<text top="572" left="546" width="17" height="9" font="font7" id="p2_t115" reading_order_no="114" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="584" left="312" width="251" height="9" font="font7" id="p2_t116" reading_order_no="115" segment_no="6" tag_type="text">add a set of other images from different classes to the input</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p2_t117" reading_order_no="116" segment_no="6" tag_type="text">image separately so as to generate a set of blended images.</text>
<text top="608" left="312" width="251" height="9" font="font7" id="p2_t118" reading_order_no="117" segment_no="6" tag_type="text">Then, the entropy of the predicted results on these blended</text>
<text top="620" left="312" width="251" height="9" font="font7" id="p2_t119" reading_order_no="118" segment_no="6" tag_type="text">images is calculated. The lower the entropy, the input image</text>
<text top="632" left="312" width="251" height="9" font="font7" id="p2_t120" reading_order_no="119" segment_no="6" tag_type="text">is more likely to carry a trigger <a href="deeplearning_paper26.html#9">[12]. </a>However, the trigger in</text>
<text top="644" left="312" width="251" height="9" font="font7" id="p2_t121" reading_order_no="120" segment_no="6" tag_type="text">the blended image may be destroyed. As a result, the backdoor</text>
<text top="656" left="312" width="251" height="9" font="font7" id="p2_t122" reading_order_no="121" segment_no="6" tag_type="text">instance will be incorrectly considered to be a clean one by</text>
<text top="668" left="312" width="56" height="9" font="font7" id="p2_t123" reading_order_no="122" segment_no="6" tag_type="text">STRIP. Wang</text>
<text top="668" left="372" width="22" height="9" font="font12" id="p2_t124" reading_order_no="123" segment_no="6" tag_type="text">et al.</text>
<text top="668" left="398" width="165" height="9" font="font7" id="p2_t125" reading_order_no="124" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[18] </a>proposed a defense method named</text>
<text top="680" left="312" width="251" height="9" font="font7" id="p2_t126" reading_order_no="125" segment_no="6" tag_type="text">Neural Cleanse (NC) to reverse engineer the trigger from the</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p2_t127" reading_order_no="126" segment_no="6" tag_type="text">backoored model. For each class, NC computes the minimized</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p2_t128" reading_order_no="127" segment_no="6" tag_type="text">amount of modification to make the model predict images from</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p2_t129" reading_order_no="128" segment_no="6" tag_type="text">different classes as this class. Among these modifications, if</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p2_t130" reading_order_no="129" segment_no="6" tag_type="text">a modification is substantially smaller than the others, NC</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p2_t131" reading_order_no="130" segment_no="6" tag_type="text">will consider it as a trigger <a href="deeplearning_paper26.html#9">[18]. </a>However, this method is</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font13" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font14" size="6" family="TIMES NEW ROMAN" color="#000000"/>
	<fontspec id="font15" size="6" family="TIMES NEW ROMAN" color="#c00000"/>
	<fontspec id="font16" size="6" family="TIMES NEW ROMAN" color="#000000"/>
	<fontspec id="font17" size="6" family="TIMES NEW ROMAN,BoldItalic" color="#000000"/>
	<fontspec id="font18" size="6" family="TIMES NEW ROMAN,Bold" color="#0000ff"/>
	<fontspec id="font19" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font20" size="7" family="CMMI7" color="#000000"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="text">3</text>
<text top="58" left="49" width="251" height="9" font="font7" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="text">computationally expensive considering the reverse-engineering</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p3_t3" reading_order_no="2" segment_no="1" tag_type="text">process, especially when the model has a large number of</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p3_t4" reading_order_no="3" segment_no="1" tag_type="text">output classes. Moreover, the reversed trigger is just similar</text>
<text top="94" left="49" width="251" height="9" font="font7" id="p3_t5" reading_order_no="4" segment_no="1" tag_type="text">to the true trigger. In addition, when the true trigger is big or</text>
<text top="106" left="49" width="251" height="9" font="font7" id="p3_t6" reading_order_no="5" segment_no="1" tag_type="text">discrete, the reversed trigger even will not be similar to the</text>
<text top="118" left="49" width="71" height="9" font="font7" id="p3_t7" reading_order_no="6" segment_no="1" tag_type="text">true trigger. Qiao</text>
<text top="118" left="123" width="21" height="9" font="font12" id="p3_t8" reading_order_no="7" segment_no="1" tag_type="text">et al.</text>
<text top="118" left="148" width="152" height="9" font="font7" id="p3_t9" reading_order_no="8" segment_no="1" tag_type="text"><a href="deeplearning_paper26.html#9">[8] </a>proposed a max-entropy staircase</text>
<text top="130" left="49" width="251" height="9" font="font7" id="p3_t10" reading_order_no="9" segment_no="1" tag_type="text">approximator (MESA) algorithm to reverse a set of candidate</text>
<text top="142" left="49" width="251" height="9" font="font7" id="p3_t11" reading_order_no="10" segment_no="1" tag_type="text">triggers. Then, backdoor instances are generated by separately</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p3_t12" reading_order_no="11" segment_no="1" tag_type="text">adding these candidate triggers to clean images. The model</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p3_t13" reading_order_no="12" segment_no="1" tag_type="text">is fine-tuned on these backdoor instances with correct labels</text>
<text top="178" left="49" width="251" height="9" font="font7" id="p3_t14" reading_order_no="13" segment_no="1" tag_type="text">to remove the backdoor <a href="deeplearning_paper26.html#9">[8]. </a>However, the MESA algorithm</text>
<text top="190" left="49" width="251" height="9" font="font7" id="p3_t15" reading_order_no="14" segment_no="1" tag_type="text">requires the information of the trigger size, which is difficult to</text>
<text top="202" left="49" width="206" height="9" font="font7" id="p3_t16" reading_order_no="15" segment_no="1" tag_type="text">obtain by the defender in realistic scenarios. Chen</text>
<text top="202" left="259" width="21" height="9" font="font12" id="p3_t17" reading_order_no="16" segment_no="1" tag_type="text">et al.</text>
<text top="202" left="283" width="17" height="9" font="font7" id="p3_t18" reading_order_no="17" segment_no="1" tag_type="text"><a href="deeplearning_paper26.html#9">[19]</a></text>
<text top="214" left="49" width="251" height="9" font="font7" id="p3_t19" reading_order_no="18" segment_no="1" tag_type="text">proposed a GAN-based defense method called DeepInspect.</text>
<text top="226" left="49" width="251" height="9" font="font7" id="p3_t20" reading_order_no="19" segment_no="1" tag_type="text">DeepInspect reconstructs the potential trigger and generates</text>
<text top="238" left="49" width="251" height="9" font="font7" id="p3_t21" reading_order_no="20" segment_no="1" tag_type="text">the backdoor instances by patching these reconstructed trigger</text>
<text top="250" left="49" width="251" height="9" font="font7" id="p3_t22" reading_order_no="21" segment_no="1" tag_type="text">to the clean images with ground truth labels. Then, the</text>
<text top="262" left="49" width="251" height="9" font="font7" id="p3_t23" reading_order_no="22" segment_no="1" tag_type="text">backdoored model is fine-tuned on these generated backdoor</text>
<text top="274" left="49" width="160" height="9" font="font7" id="p3_t24" reading_order_no="23" segment_no="1" tag_type="text">instances to remove the backdoor <a href="deeplearning_paper26.html#9">[19].</a></text>
<text top="289" left="59" width="241" height="9" font="font7" id="p3_t25" reading_order_no="24" segment_no="7" tag_type="text">The main advantages of the proposed approach over the</text>
<text top="301" left="49" width="184" height="9" font="font7" id="p3_t26" reading_order_no="25" segment_no="7" tag_type="text">existing defenses are summarized as follows.</text>
<text top="323" left="59" width="4" height="7" font="font10" id="p3_t27" reading_order_no="26" segment_no="8" tag_type="list">•</text>
<text top="321" left="69" width="231" height="9" font="font7" id="p3_t28" reading_order_no="27" segment_no="8" tag_type="list">Compared with <a href="deeplearning_paper26.html#9">[5], [8], </a>which both need to know the</text>
<text top="333" left="69" width="231" height="9" font="font7" id="p3_t29" reading_order_no="28" segment_no="8" tag_type="list">trigger size, the proposed method does not require any</text>
<text top="345" left="69" width="186" height="9" font="font7" id="p3_t30" reading_order_no="29" segment_no="8" tag_type="list">backdoor attack information. Moreover, Liu</text>
<text top="345" left="260" width="23" height="9" font="font12" id="p3_t31" reading_order_no="30" segment_no="8" tag_type="list">et al.</text>
<text top="345" left="288" width="12" height="9" font="font7" id="p3_t32" reading_order_no="31" segment_no="8" tag_type="list"><a href="deeplearning_paper26.html#9">[7]</a></text>
<text top="357" left="69" width="231" height="9" font="font7" id="p3_t33" reading_order_no="32" segment_no="8" tag_type="list">requires a large number of trusted images to remove</text>
<text top="369" left="69" width="88" height="9" font="font7" id="p3_t34" reading_order_no="33" segment_no="8" tag_type="list">the backdoor (10,000</text>
<text top="368" left="161" width="8" height="9" font="font13" id="p3_t35" reading_order_no="34" segment_no="8" tag_type="list">∼</text>
<text top="369" left="172" width="128" height="9" font="font7" id="p3_t36" reading_order_no="35" segment_no="8" tag_type="list">60,000 images for MNIST). In</text>
<text top="381" left="69" width="231" height="9" font="font7" id="p3_t37" reading_order_no="36" segment_no="8" tag_type="list">comparison, the proposed method only requires a small</text>
<text top="393" left="69" width="231" height="9" font="font7" id="p3_t38" reading_order_no="37" segment_no="8" tag_type="list">set of clean images (300 clean images) to generate the</text>
<text top="405" left="69" width="138" height="9" font="font7" id="p3_t39" reading_order_no="38" segment_no="8" tag_type="list">universal adversarial perturbation.</text>
<text top="419" left="59" width="4" height="7" font="font10" id="p3_t40" reading_order_no="39" segment_no="9" tag_type="list">•</text>
<text top="417" left="69" width="231" height="9" font="font7" id="p3_t41" reading_order_no="40" segment_no="9" tag_type="list">The detection process of the work <a href="deeplearning_paper26.html#9">[4] </a>requires training</text>
<text top="429" left="69" width="231" height="9" font="font7" id="p3_t42" reading_order_no="41" segment_no="9" tag_type="list">a large number of shadow models (backdoored mod-</text>
<text top="441" left="69" width="231" height="9" font="font7" id="p3_t43" reading_order_no="42" segment_no="9" tag_type="list">els and clean models). Nevertheless, the computational</text>
<text top="453" left="69" width="231" height="9" font="font7" id="p3_t44" reading_order_no="43" segment_no="9" tag_type="list">resources for training such a large number of shadow</text>
<text top="465" left="69" width="231" height="9" font="font7" id="p3_t45" reading_order_no="44" segment_no="9" tag_type="list">models are unaffordable for most of the users. In contrast,</text>
<text top="477" left="69" width="231" height="9" font="font7" id="p3_t46" reading_order_no="45" segment_no="9" tag_type="list">the proposed method only needs to generate one single</text>
<text top="489" left="69" width="231" height="9" font="font7" id="p3_t47" reading_order_no="46" segment_no="9" tag_type="list">universal perturbation and only needs the model to make</text>
<text top="501" left="69" width="231" height="9" font="font7" id="p3_t48" reading_order_no="47" segment_no="9" tag_type="list">predictions on the unperturbed image and the perturbed</text>
<text top="513" left="69" width="211" height="9" font="font7" id="p3_t49" reading_order_no="48" segment_no="9" tag_type="list">image, which requires low computational overhead.</text>
<text top="526" left="59" width="4" height="7" font="font10" id="p3_t50" reading_order_no="49" segment_no="11" tag_type="list">•</text>
<text top="525" left="69" width="231" height="9" font="font7" id="p3_t51" reading_order_no="50" segment_no="11" tag_type="list">STRIP <a href="deeplearning_paper26.html#9">[12] </a>directly superimposes a number of images</text>
<text top="537" left="69" width="231" height="9" font="font7" id="p3_t52" reading_order_no="51" segment_no="11" tag_type="list">from different classes to the input image. This will not</text>
<text top="549" left="69" width="231" height="9" font="font7" id="p3_t53" reading_order_no="52" segment_no="11" tag_type="list">only destroy the main content of the input image, but</text>
<text top="560" left="69" width="231" height="9" font="font7" id="p3_t54" reading_order_no="53" segment_no="11" tag_type="list">may also accidentally break the trigger. Once the trigger</text>
<text top="572" left="69" width="231" height="9" font="font7" id="p3_t55" reading_order_no="54" segment_no="11" tag_type="list">is destroyed, the entropy of this backdoor instance will</text>
<text top="584" left="69" width="231" height="9" font="font7" id="p3_t56" reading_order_no="55" segment_no="11" tag_type="list">be similar to the entropy of a clean image. Hence STRIP</text>
<text top="596" left="69" width="231" height="9" font="font7" id="p3_t57" reading_order_no="56" segment_no="11" tag_type="list"><a href="deeplearning_paper26.html#9">[12] </a>will fail to detect this backdoor instance. In contrast,</text>
<text top="608" left="69" width="231" height="9" font="font7" id="p3_t58" reading_order_no="57" segment_no="11" tag_type="list">the proposed method perturbs the untrusted image with</text>
<text top="620" left="69" width="231" height="9" font="font7" id="p3_t59" reading_order_no="58" segment_no="11" tag_type="list">universal adversarial perturbation (UAP) <a href="deeplearning_paper26.html#9">[13]. </a>This will</text>
<text top="632" left="69" width="231" height="9" font="font7" id="p3_t60" reading_order_no="59" segment_no="11" tag_type="list">not destroy the trigger and ensures that the predicted</text>
<text top="644" left="69" width="231" height="9" font="font7" id="p3_t61" reading_order_no="60" segment_no="11" tag_type="list">label of the backdoor instance keeps unchanged even</text>
<text top="656" left="69" width="231" height="9" font="font7" id="p3_t62" reading_order_no="61" segment_no="11" tag_type="list">after perturbation. Moreover, for each input image, STRIP</text>
<text top="668" left="69" width="231" height="9" font="font7" id="p3_t63" reading_order_no="62" segment_no="11" tag_type="list"><a href="deeplearning_paper26.html#9">[12] </a>needs to predict a set of blended images in order</text>
<text top="680" left="69" width="231" height="9" font="font7" id="p3_t64" reading_order_no="63" segment_no="11" tag_type="list">to estimate the entropy of the predicted labels of those</text>
<text top="692" left="69" width="231" height="9" font="font7" id="p3_t65" reading_order_no="64" segment_no="11" tag_type="list">blended images. In comparison, for each image, the</text>
<text top="704" left="69" width="231" height="9" font="font7" id="p3_t66" reading_order_no="65" segment_no="11" tag_type="list">proposed method only needs to predict two images (the</text>
<text top="716" left="69" width="231" height="9" font="font7" id="p3_t67" reading_order_no="66" segment_no="11" tag_type="list">perturbed image and the unperturbed image). Therefore,</text>
<text top="728" left="69" width="231" height="9" font="font7" id="p3_t68" reading_order_no="67" segment_no="11" tag_type="list">the backdoor detection efficiency of the proposed method</text>
<text top="740" left="69" width="118" height="9" font="font7" id="p3_t69" reading_order_no="68" segment_no="11" tag_type="list">is higher than that of STRIP.</text>
<text top="58" left="374" width="26" height="9" font="font7" id="p3_t70" reading_order_no="69" segment_no="2" tag_type="title">III. T</text>
<text top="60" left="400" width="11" height="7" font="font8" id="p3_t71" reading_order_no="70" segment_no="2" tag_type="title">HE</text>
<text top="58" left="414" width="6" height="9" font="font7" id="p3_t72" reading_order_no="71" segment_no="2" tag_type="title">P</text>
<text top="60" left="420" width="39" height="7" font="font8" id="p3_t73" reading_order_no="72" segment_no="2" tag_type="title">ROPOSED</text>
<text top="58" left="462" width="9" height="9" font="font7" id="p3_t74" reading_order_no="73" segment_no="2" tag_type="title">M</text>
<text top="60" left="472" width="29" height="7" font="font8" id="p3_t75" reading_order_no="74" segment_no="2" tag_type="title">ETHOD</text>
<text top="75" left="322" width="241" height="9" font="font7" id="p3_t76" reading_order_no="75" segment_no="3" tag_type="text">In this section, first, the overall procedure of the proposed</text>
<text top="87" left="312" width="251" height="9" font="font7" id="p3_t77" reading_order_no="76" segment_no="3" tag_type="text">backdoor detection method is presented in Section <a href="deeplearning_paper26.html#3">III-A. </a>The</text>
<text top="99" left="312" width="251" height="9" font="font7" id="p3_t78" reading_order_no="77" segment_no="3" tag_type="text">proposed method can be divided into two steps, which are</text>
<text top="111" left="312" width="251" height="9" font="font7" id="p3_t79" reading_order_no="78" segment_no="3" tag_type="text">elaborated in Section <a href="deeplearning_paper26.html#4">III-B </a>and Section <a href="deeplearning_paper26.html#4">III-C, </a>respectively.</text>
<text top="123" left="312" width="251" height="9" font="font7" id="p3_t80" reading_order_no="79" segment_no="3" tag_type="text">Finally, the reason why choosing universal adversarial pertur-</text>
<text top="135" left="312" width="251" height="9" font="font7" id="p3_t81" reading_order_no="80" segment_no="3" tag_type="text">bation <a href="deeplearning_paper26.html#9">[13] </a>for adversarial perturbation generation is discussed</text>
<text top="147" left="312" width="68" height="9" font="font7" id="p3_t82" reading_order_no="81" segment_no="3" tag_type="text">in Section <a href="deeplearning_paper26.html#5">III-D.</a></text>
<text top="177" left="312" width="64" height="9" font="font12" id="p3_t83" reading_order_no="82" segment_no="4" tag_type="title">A. Overall flow</text>
<text top="193" left="322" width="241" height="9" font="font7" id="p3_t84" reading_order_no="83" segment_no="5" tag_type="text">As shown in Fig. <a href="deeplearning_paper26.html#3">1, </a>the proposed defense method consists of</text>
<text top="205" left="312" width="251" height="9" font="font7" id="p3_t85" reading_order_no="84" segment_no="5" tag_type="text">two steps. The first step is to generate the universal adversarial</text>
<text top="217" left="312" width="251" height="9" font="font7" id="p3_t86" reading_order_no="85" segment_no="5" tag_type="text">perturbation <a href="deeplearning_paper26.html#9">[13] </a>from the backdoored model with a small set</text>
<text top="229" left="312" width="67" height="9" font="font7" id="p3_t87" reading_order_no="86" segment_no="5" tag_type="text">of clean images.</text>
<text top="242" left="322" width="241" height="9" font="font7" id="p3_t88" reading_order_no="87" segment_no="6" tag_type="text">The second step is backdoor detection, which is summarized</text>
<text top="254" left="312" width="251" height="9" font="font7" id="p3_t89" reading_order_no="88" segment_no="6" tag_type="text">as follows. As shown in Fig. <a href="deeplearning_paper26.html#3">1, </a>given an untrusted image, the</text>
<text top="266" left="312" width="251" height="9" font="font7" id="p3_t90" reading_order_no="89" segment_no="6" tag_type="text">universal perturbation generated in previous step is added to</text>
<text top="277" left="312" width="251" height="9" font="font7" id="p3_t91" reading_order_no="90" segment_no="6" tag_type="text">this image. Then, both the perturbed image and corresponding</text>
<text top="289" left="312" width="251" height="9" font="font7" id="p3_t92" reading_order_no="91" segment_no="6" tag_type="text">unperturbed image are input into the untrusted model. If</text>
<text top="301" left="312" width="251" height="9" font="font7" id="p3_t93" reading_order_no="92" segment_no="6" tag_type="text">the untrusted model is backdoored, the backdoor instance</text>
<text top="313" left="312" width="251" height="9" font="font7" id="p3_t94" reading_order_no="93" segment_no="6" tag_type="text">without perturbation will be misclassified as the target label.</text>
<text top="325" left="312" width="251" height="9" font="font7" id="p3_t95" reading_order_no="94" segment_no="6" tag_type="text">When added with universal adversarial perturbation <a href="deeplearning_paper26.html#9">[13], </a>the</text>
<text top="337" left="312" width="251" height="9" font="font7" id="p3_t96" reading_order_no="95" segment_no="6" tag_type="text">backdoor instance which carries a trigger will still be classified</text>
<text top="349" left="312" width="251" height="9" font="font7" id="p3_t97" reading_order_no="96" segment_no="6" tag_type="text">as the target label. However, given a clean image, its predicted</text>
<text top="361" left="312" width="251" height="9" font="font7" id="p3_t98" reading_order_no="97" segment_no="6" tag_type="text">label will change to another label when added with perturba-</text>
<text top="373" left="312" width="251" height="9" font="font7" id="p3_t99" reading_order_no="98" segment_no="6" tag_type="text">tion. Hence, if the backdoored model always predicts an image</text>
<text top="385" left="312" width="251" height="9" font="font7" id="p3_t100" reading_order_no="99" segment_no="6" tag_type="text">as the same label with or without universal perturbation, the</text>
<text top="397" left="312" width="251" height="9" font="font7" id="p3_t101" reading_order_no="100" segment_no="6" tag_type="text">image is considered to be a backdoor instance. Meanwhile,</text>
<text top="409" left="312" width="251" height="9" font="font7" id="p3_t102" reading_order_no="101" segment_no="6" tag_type="text">the predicted label is considered to be the target label. For</text>
<text top="421" left="312" width="75" height="9" font="font7" id="p3_t103" reading_order_no="102" segment_no="6" tag_type="text">instance, the label</text>
<text top="421" left="391" width="18" height="9" font="font12" id="p3_t104" reading_order_no="103" segment_no="6" tag_type="text">Stop</text>
<text top="421" left="413" width="151" height="9" font="font7" id="p3_t105" reading_order_no="104" segment_no="6" tag_type="text">in Fig. <a href="deeplearning_paper26.html#3">1 </a>is the target label, and the</text>
<text top="433" left="312" width="155" height="9" font="font7" id="p3_t106" reading_order_no="105" segment_no="6" tag_type="text">corresponding image carries a trigger.</text>
<text top="568" left="491" width="0" height="7" font="font14" id="p3_t107" reading_order_no="122" segment_no="10" tag_type="figure">B</text>
<text top="573" left="491" width="0" height="10" font="font14" id="p3_t108" reading_order_no="123" segment_no="10" tag_type="figure">ac</text>
<text top="578" left="491" width="0" height="10" font="font14" id="p3_t109" reading_order_no="124" segment_no="10" tag_type="figure">kd</text>
<text top="584" left="491" width="0" height="10" font="font14" id="p3_t110" reading_order_no="125" segment_no="10" tag_type="figure">oo</text>
<text top="591" left="491" width="0" height="9" font="font14" id="p3_t111" reading_order_no="126" segment_no="10" tag_type="figure">re</text>
<text top="596" left="491" width="0" height="12" font="font14" id="p3_t112" reading_order_no="127" segment_no="10" tag_type="figure">d M</text>
<text top="606" left="491" width="0" height="10" font="font14" id="p3_t113" reading_order_no="128" segment_no="10" tag_type="figure">od</text>
<text top="612" left="491" width="0" height="10" font="font14" id="p3_t114" reading_order_no="129" segment_no="10" tag_type="figure">el</text>
<text top="568" left="514" width="40" height="7" font="font15" id="p3_t115" reading_order_no="130" segment_no="10" tag_type="figure">Stop (incorrect)</text>
<text top="586" left="514" width="40" height="7" font="font15" id="p3_t116" reading_order_no="131" segment_no="10" tag_type="figure">Stop (incorrect)</text>
<text top="621" left="514" width="55" height="7" font="font15" id="p3_t117" reading_order_no="133" segment_no="10" tag_type="figure">Pedestrian (incorrect)</text>
<text top="603" left="514" width="43" height="7" font="font16" id="p3_t118" reading_order_no="132" segment_no="10" tag_type="figure">Caution (correct)</text>
<text top="612" left="338" width="26" height="7" font="font17" id="p3_t119" reading_order_no="112" segment_no="10" tag_type="figure"><i><b>Untrusted</b></i></text>
<text top="619" left="342" width="19" height="7" font="font17" id="p3_t120" reading_order_no="113" segment_no="10" tag_type="figure"><i><b>Images</b></i></text>
<text top="639" left="438" width="44" height="7" font="font17" id="p3_t121" reading_order_no="121" segment_no="10" tag_type="figure"><i><b>Perturbed Image</b></i></text>
<text top="617" left="423" width="11" height="7" font="font16" id="p3_t122" reading_order_no="119" segment_no="10" tag_type="figure">Add</text>
<text top="624" left="412" width="31" height="7" font="font16" id="p3_t123" reading_order_no="120" segment_no="10" tag_type="figure">Perturbation</text>
<text top="551" left="372" width="49" height="7" font="font17" id="p3_t124" reading_order_no="114" segment_no="10" tag_type="figure"><i><b>Backdoor Instance</b></i></text>
<text top="639" left="515" width="17" height="7" font="font17" id="p3_t125" reading_order_no="134" segment_no="10" tag_type="figure"><i><b>Labels</b></i></text>
<text top="565" left="422" width="11" height="7" font="font16" id="p3_t126" reading_order_no="115" segment_no="10" tag_type="figure">Add</text>
<text top="572" left="412" width="31" height="7" font="font16" id="p3_t127" reading_order_no="116" segment_no="10" tag_type="figure">Perturbation</text>
<text top="639" left="379" width="34" height="7" font="font17" id="p3_t128" reading_order_no="118" segment_no="10" tag_type="figure"><i><b>Clean Image</b></i></text>
<text top="551" left="438" width="44" height="7" font="font17" id="p3_t129" reading_order_no="117" segment_no="10" tag_type="figure"><i><b>Perturbed Image</b></i></text>
<text top="491" left="342" width="36" height="7" font="font17" id="p3_t130" reading_order_no="107" segment_no="10" tag_type="figure"><i><b>Clean Images</b></i></text>
<text top="537" left="351" width="16" height="7" font="font17" id="p3_t131" reading_order_no="108" segment_no="10" tag_type="figure"><i><b>Model</b></i></text>
<text top="496" left="467" width="92" height="7" font="font17" id="p3_t132" reading_order_no="110" segment_no="10" tag_type="figure"><i><b>Universal Adversarial Perturbation</b></i></text>
<text top="491" left="405" width="21" height="7" font="font16" id="p3_t133" reading_order_no="109" segment_no="10" tag_type="figure">generate</text>
<text top="496" left="313" width="17" height="7" font="font18" id="p3_t134" reading_order_no="106" segment_no="10" tag_type="figure"><b>Step 1</b></text>
<text top="595" left="313" width="18" height="7" font="font18" id="p3_t135" reading_order_no="111" segment_no="10" tag_type="figure"><b>Step 2 </b></text>
<text top="660" left="312" width="22" height="7" font="font8" id="p3_t136" reading_order_no="135" segment_no="12" tag_type="text">Fig. 1.</text>
<text top="660" left="343" width="220" height="7" font="font8" id="p3_t137" reading_order_no="136" segment_no="12" tag_type="text">The overall flow of the proposed method: adversarial perturbation</text>
<text top="669" left="312" width="160" height="7" font="font8" id="p3_t138" reading_order_no="137" segment_no="12" tag_type="text">generation (Step 1); backdoor detection (Step 2).</text>
<text top="692" left="322" width="241" height="9" font="font7" id="p3_t139" reading_order_no="138" segment_no="13" tag_type="text">The overall flow of the proposed method is outlined in</text>
<text top="704" left="312" width="167" height="9" font="font7" id="p3_t140" reading_order_no="139" segment_no="13" tag_type="text">Algorithm <a href="deeplearning_paper26.html#4">1 </a>and is described as follows:</text>
<text top="716" left="322" width="116" height="9" font="font7" id="p3_t141" reading_order_no="140" segment_no="14" tag_type="text">1) Given an untrusted model</text>
<text top="716" left="441" width="5" height="9" font="font19" id="p3_t142" reading_order_no="141" segment_no="14" tag_type="text">f</text>
<text top="719" left="446" width="13" height="6" font="font20" id="p3_t143" reading_order_no="142" segment_no="14" tag_type="text">unt</text>
<text top="716" left="459" width="104" height="9" font="font7" id="p3_t144" reading_order_no="143" segment_no="14" tag_type="text">, the universal adversarial</text>
<text top="728" left="312" width="68" height="9" font="font7" id="p3_t145" reading_order_no="144" segment_no="14" tag_type="text">perturbation <a href="deeplearning_paper26.html#9">[13]</a></text>
<text top="728" left="384" width="5" height="9" font="font19" id="p3_t146" reading_order_no="145" segment_no="14" tag_type="text">η</text>
<text top="728" left="392" width="171" height="9" font="font7" id="p3_t147" reading_order_no="146" segment_no="14" tag_type="text">is generated based on the untrusted model</text>
<text top="740" left="312" width="5" height="9" font="font19" id="p3_t148" reading_order_no="147" segment_no="14" tag_type="text">f</text>
<text top="743" left="317" width="13" height="6" font="font20" id="p3_t149" reading_order_no="148" segment_no="14" tag_type="text">unt</text>
<text top="740" left="333" width="133" height="9" font="font7" id="p3_t150" reading_order_no="149" segment_no="14" tag_type="text">with a small set of clean images</text>
<text top="740" left="470" width="8" height="9" font="font19" id="p3_t151" reading_order_no="150" segment_no="14" tag_type="text">X</text>
<text top="740" left="483" width="77" height="9" font="font7" id="p3_t152" reading_order_no="151" segment_no="14" tag_type="text">(only 300 images).</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font21" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font22" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font23" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font24" size="10" family="MSBM10" color="#000000"/>
	<fontspec id="font25" size="10" family="TIMES NEW ROMAN" color="#000000"/>
	<fontspec id="font26" size="6" family="Times New Roman,Italic" color="#000000"/>
	<fontspec id="font27" size="10" family="Times New Roman,Italic" color="#000000"/>
	<fontspec id="font28" size="10" family="Symbol" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">4</text>
<text top="58" left="59" width="8" height="9" font="font7" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="text">2)</text>
<text top="58" left="73" width="8" height="9" font="font19" id="p4_t3" reading_order_no="2" segment_no="1" tag_type="text">D</text>
<text top="62" left="81" width="13" height="6" font="font20" id="p4_t4" reading_order_no="3" segment_no="1" tag_type="text">unt</text>
<text top="58" left="102" width="8" height="9" font="font21" id="p4_t5" reading_order_no="4" segment_no="1" tag_type="text">=</text>
<text top="58" left="116" width="5" height="9" font="font13" id="p4_t6" reading_order_no="5" segment_no="1" tag_type="text">{</text>
<text top="58" left="121" width="5" height="9" font="font19" id="p4_t7" reading_order_no="6" segment_no="1" tag_type="text">d</text>
<text top="62" left="127" width="4" height="6" font="font22" id="p4_t8" reading_order_no="7" segment_no="1" tag_type="text">1</text>
<text top="58" left="131" width="27" height="9" font="font19" id="p4_t9" reading_order_no="8" segment_no="1" tag_type="text">, . . . , d</text>
<text top="62" left="158" width="5" height="6" font="font20" id="p4_t10" reading_order_no="9" segment_no="1" tag_type="text">n</text>
<text top="58" left="164" width="5" height="9" font="font13" id="p4_t11" reading_order_no="10" segment_no="1" tag_type="text">}</text>
<text top="58" left="175" width="125" height="9" font="font7" id="p4_t12" reading_order_no="11" segment_no="1" tag_type="text">denotes the untrusted images</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p4_t13" reading_order_no="12" segment_no="1" tag_type="text">(in the training stage, it represents the training data; in the</text>
<text top="82" left="49" width="218" height="9" font="font7" id="p4_t14" reading_order_no="13" segment_no="1" tag_type="text">inference stage, it represents a single input image with</text>
<text top="82" left="270" width="6" height="9" font="font19" id="p4_t15" reading_order_no="14" segment_no="1" tag_type="text">n</text>
<text top="82" left="279" width="15" height="9" font="font21" id="p4_t16" reading_order_no="15" segment_no="1" tag_type="text">= 1</text>
<text top="82" left="294" width="6" height="9" font="font7" id="p4_t17" reading_order_no="16" segment_no="1" tag_type="text">).</text>
<text top="94" left="49" width="68" height="9" font="font7" id="p4_t18" reading_order_no="17" segment_no="1" tag_type="text">The perturbation</text>
<text top="94" left="121" width="5" height="9" font="font19" id="p4_t19" reading_order_no="18" segment_no="1" tag_type="text">η</text>
<text top="94" left="130" width="111" height="9" font="font7" id="p4_t20" reading_order_no="19" segment_no="1" tag_type="text">is then added to the image</text>
<text top="94" left="245" width="5" height="9" font="font19" id="p4_t21" reading_order_no="20" segment_no="1" tag_type="text">d</text>
<text top="98" left="250" width="3" height="6" font="font20" id="p4_t22" reading_order_no="21" segment_no="1" tag_type="text">i</text>
<text top="93" left="257" width="7" height="9" font="font13" id="p4_t23" reading_order_no="22" segment_no="1" tag_type="text">∈</text>
<text top="94" left="267" width="8" height="9" font="font19" id="p4_t24" reading_order_no="23" segment_no="1" tag_type="text">D</text>
<text top="98" left="275" width="13" height="6" font="font20" id="p4_t25" reading_order_no="24" segment_no="1" tag_type="text">unt</text>
<text top="94" left="292" width="8" height="9" font="font7" id="p4_t26" reading_order_no="25" segment_no="1" tag_type="text">to</text>
<text top="106" left="49" width="119" height="9" font="font7" id="p4_t27" reading_order_no="26" segment_no="1" tag_type="text">generate the perturbed image</text>
<text top="103" left="173" width="5" height="9" font="font21" id="p4_t28" reading_order_no="27" segment_no="1" tag_type="text">ˆ</text>
<text top="106" left="171" width="5" height="9" font="font19" id="p4_t29" reading_order_no="28" segment_no="1" tag_type="text">d</text>
<text top="110" left="177" width="3" height="6" font="font20" id="p4_t30" reading_order_no="29" segment_no="1" tag_type="text">i</text>
<text top="106" left="183" width="8" height="9" font="font21" id="p4_t31" reading_order_no="30" segment_no="1" tag_type="text">=</text>
<text top="106" left="193" width="5" height="9" font="font19" id="p4_t32" reading_order_no="31" segment_no="1" tag_type="text">d</text>
<text top="110" left="198" width="3" height="6" font="font20" id="p4_t33" reading_order_no="32" segment_no="1" tag_type="text">i</text>
<text top="106" left="204" width="8" height="9" font="font21" id="p4_t34" reading_order_no="33" segment_no="1" tag_type="text">+</text>
<text top="106" left="214" width="5" height="9" font="font19" id="p4_t35" reading_order_no="34" segment_no="1" tag_type="text">η</text>
<text top="106" left="219" width="2" height="9" font="font7" id="p4_t36" reading_order_no="35" segment_no="1" tag_type="text">.</text>
<text top="119" left="59" width="112" height="9" font="font7" id="p4_t37" reading_order_no="36" segment_no="4" tag_type="text">3) Both unperturbed image</text>
<text top="119" left="175" width="5" height="9" font="font19" id="p4_t38" reading_order_no="37" segment_no="4" tag_type="text">d</text>
<text top="122" left="180" width="3" height="6" font="font20" id="p4_t39" reading_order_no="38" segment_no="4" tag_type="text">i</text>
<text top="119" left="187" width="85" height="9" font="font7" id="p4_t40" reading_order_no="39" segment_no="4" tag_type="text">and perturbed image</text>
<text top="116" left="277" width="5" height="9" font="font21" id="p4_t41" reading_order_no="41" segment_no="4" tag_type="text">ˆ</text>
<text top="119" left="275" width="5" height="9" font="font19" id="p4_t42" reading_order_no="40" segment_no="4" tag_type="text">d</text>
<text top="122" left="281" width="3" height="6" font="font20" id="p4_t43" reading_order_no="42" segment_no="4" tag_type="text">i</text>
<text top="119" left="288" width="12" height="9" font="font7" id="p4_t44" reading_order_no="43" segment_no="4" tag_type="text">are</text>
<text top="131" left="49" width="251" height="9" font="font7" id="p4_t45" reading_order_no="44" segment_no="4" tag_type="text">input into the untrusted model. The predictions of the model</text>
<text top="143" left="49" width="228" height="9" font="font7" id="p4_t46" reading_order_no="45" segment_no="4" tag_type="text">on the unperturbed image and the perturbed image are</text>
<text top="142" left="280" width="5" height="9" font="font19" id="p4_t47" reading_order_no="46" segment_no="4" tag_type="text">y</text>
<text top="146" left="285" width="3" height="6" font="font20" id="p4_t48" reading_order_no="47" segment_no="4" tag_type="text">i</text>
<text top="142" left="292" width="8" height="9" font="font21" id="p4_t49" reading_order_no="48" segment_no="4" tag_type="text">=</text>
<text top="154" left="49" width="5" height="9" font="font19" id="p4_t50" reading_order_no="49" segment_no="4" tag_type="text">f</text>
<text top="158" left="54" width="13" height="6" font="font20" id="p4_t51" reading_order_no="50" segment_no="4" tag_type="text">unt</text>
<text top="154" left="67" width="4" height="9" font="font21" id="p4_t52" reading_order_no="51" segment_no="4" tag_type="text">(</text>
<text top="154" left="71" width="5" height="9" font="font19" id="p4_t53" reading_order_no="52" segment_no="4" tag_type="text">d</text>
<text top="158" left="76" width="3" height="6" font="font20" id="p4_t54" reading_order_no="53" segment_no="4" tag_type="text">i</text>
<text top="154" left="79" width="4" height="9" font="font21" id="p4_t55" reading_order_no="54" segment_no="4" tag_type="text">)</text>
<text top="155" left="87" width="14" height="9" font="font7" id="p4_t56" reading_order_no="55" segment_no="4" tag_type="text">and</text>
<text top="154" left="106" width="5" height="9" font="font21" id="p4_t57" reading_order_no="57" segment_no="4" tag_type="text">ˆ</text>
<text top="154" left="104" width="5" height="9" font="font19" id="p4_t58" reading_order_no="56" segment_no="4" tag_type="text">y</text>
<text top="158" left="109" width="3" height="6" font="font20" id="p4_t59" reading_order_no="58" segment_no="4" tag_type="text">i</text>
<text top="154" left="115" width="8" height="9" font="font21" id="p4_t60" reading_order_no="59" segment_no="4" tag_type="text">=</text>
<text top="154" left="126" width="5" height="9" font="font19" id="p4_t61" reading_order_no="60" segment_no="4" tag_type="text">f</text>
<text top="158" left="131" width="13" height="6" font="font20" id="p4_t62" reading_order_no="61" segment_no="4" tag_type="text">unt</text>
<text top="154" left="144" width="11" height="9" font="font21" id="p4_t63" reading_order_no="62" segment_no="4" tag_type="text">( ˆ</text>
<text top="154" left="148" width="5" height="9" font="font19" id="p4_t64" reading_order_no="63" segment_no="4" tag_type="text">d</text>
<text top="158" left="153" width="3" height="6" font="font20" id="p4_t65" reading_order_no="64" segment_no="4" tag_type="text">i</text>
<text top="154" left="156" width="4" height="9" font="font21" id="p4_t66" reading_order_no="65" segment_no="4" tag_type="text">)</text>
<text top="155" left="160" width="65" height="9" font="font7" id="p4_t67" reading_order_no="66" segment_no="4" tag_type="text">, respectively. If</text>
<text top="154" left="229" width="5" height="9" font="font19" id="p4_t68" reading_order_no="67" segment_no="4" tag_type="text">y</text>
<text top="158" left="233" width="3" height="6" font="font20" id="p4_t69" reading_order_no="68" segment_no="4" tag_type="text">i</text>
<text top="154" left="240" width="16" height="9" font="font21" id="p4_t70" reading_order_no="69" segment_no="4" tag_type="text">= ˆ</text>
<text top="154" left="250" width="5" height="9" font="font19" id="p4_t71" reading_order_no="70" segment_no="4" tag_type="text">y</text>
<text top="158" left="255" width="3" height="6" font="font20" id="p4_t72" reading_order_no="71" segment_no="4" tag_type="text">i</text>
<text top="155" left="258" width="42" height="9" font="font7" id="p4_t73" reading_order_no="72" segment_no="4" tag_type="text">, the input</text>
<text top="167" left="49" width="24" height="9" font="font7" id="p4_t74" reading_order_no="73" segment_no="4" tag_type="text">image</text>
<text top="166" left="77" width="5" height="9" font="font19" id="p4_t75" reading_order_no="74" segment_no="4" tag_type="text">d</text>
<text top="170" left="82" width="3" height="6" font="font20" id="p4_t76" reading_order_no="75" segment_no="4" tag_type="text">i</text>
<text top="167" left="89" width="211" height="9" font="font7" id="p4_t77" reading_order_no="76" segment_no="4" tag_type="text">will be regarded as a backdoor instance. Otherwise,</text>
<text top="179" left="49" width="199" height="9" font="font7" id="p4_t78" reading_order_no="77" segment_no="4" tag_type="text">the input image will be regarded as a clean one.</text>
<text top="204" left="49" width="53" height="9" font="font23" id="p4_t79" reading_order_no="78" segment_no="6" tag_type="title">Algorithm 1</text>
<text top="204" left="105" width="175" height="9" font="font7" id="p4_t80" reading_order_no="79" segment_no="6" tag_type="title">The Proposed Backdoor Detection Method</text>
<text top="217" left="49" width="27" height="9" font="font23" id="p4_t81" reading_order_no="80" segment_no="7" tag_type="code">Input:</text>
<text top="217" left="79" width="68" height="9" font="font7" id="p4_t82" reading_order_no="81" segment_no="7" tag_type="code">a clean image set</text>
<text top="217" left="149" width="8" height="9" font="font19" id="p4_t83" reading_order_no="82" segment_no="7" tag_type="code">X</text>
<text top="217" left="159" width="79" height="9" font="font7" id="p4_t84" reading_order_no="83" segment_no="7" tag_type="code">, backdoored model</text>
<text top="217" left="240" width="5" height="9" font="font19" id="p4_t85" reading_order_no="84" segment_no="7" tag_type="code">f</text>
<text top="221" left="245" width="13" height="6" font="font20" id="p4_t86" reading_order_no="85" segment_no="7" tag_type="code">unt</text>
<text top="217" left="258" width="42" height="9" font="font7" id="p4_t87" reading_order_no="86" segment_no="7" tag_type="code">, untrusted</text>
<text top="229" left="49" width="39" height="9" font="font7" id="p4_t88" reading_order_no="87" segment_no="7" tag_type="code">image set</text>
<text top="229" left="91" width="8" height="9" font="font19" id="p4_t89" reading_order_no="88" segment_no="7" tag_type="code">D</text>
<text top="233" left="100" width="13" height="6" font="font20" id="p4_t90" reading_order_no="89" segment_no="7" tag_type="code">unt</text>
<text top="229" left="116" width="8" height="9" font="font21" id="p4_t91" reading_order_no="90" segment_no="7" tag_type="code">=</text>
<text top="229" left="126" width="5" height="9" font="font13" id="p4_t92" reading_order_no="91" segment_no="7" tag_type="code">{</text>
<text top="229" left="131" width="5" height="9" font="font19" id="p4_t93" reading_order_no="92" segment_no="7" tag_type="code">d</text>
<text top="233" left="136" width="4" height="6" font="font22" id="p4_t94" reading_order_no="93" segment_no="7" tag_type="code">1</text>
<text top="229" left="141" width="27" height="9" font="font19" id="p4_t95" reading_order_no="94" segment_no="7" tag_type="code">, . . . , d</text>
<text top="233" left="168" width="5" height="6" font="font20" id="p4_t96" reading_order_no="95" segment_no="7" tag_type="code">n</text>
<text top="229" left="173" width="5" height="9" font="font13" id="p4_t97" reading_order_no="96" segment_no="7" tag_type="code">}</text>
<text top="241" left="49" width="34" height="9" font="font23" id="p4_t98" reading_order_no="97" segment_no="7" tag_type="code">Output:</text>
<text top="241" left="87" width="93" height="9" font="font7" id="p4_t99" reading_order_no="98" segment_no="7" tag_type="code">the backdoor instances</text>
<text top="241" left="183" width="8" height="9" font="font19" id="p4_t100" reading_order_no="99" segment_no="7" tag_type="code">D</text>
<text top="245" left="191" width="8" height="6" font="font20" id="p4_t101" reading_order_no="100" segment_no="7" tag_type="code">bd</text>
<text top="257" left="55" width="6" height="7" font="font8" id="p4_t102" reading_order_no="101" segment_no="7" tag_type="code">1:</text>
<text top="255" left="66" width="8" height="9" font="font19" id="p4_t103" reading_order_no="102" segment_no="7" tag_type="code">D</text>
<text top="259" left="74" width="8" height="6" font="font20" id="p4_t104" reading_order_no="103" segment_no="7" tag_type="code">bd</text>
<text top="255" left="85" width="10" height="9" font="font13" id="p4_t105" reading_order_no="104" segment_no="7" tag_type="code">←</text>
<text top="257" left="98" width="8" height="8" font="font24" id="p4_t106" reading_order_no="105" segment_no="7" tag_type="code">∅</text>
<text top="255" left="106" width="3" height="9" font="font7" id="p4_t107" reading_order_no="106" segment_no="7" tag_type="code">;</text>
<text top="269" left="55" width="6" height="7" font="font8" id="p4_t108" reading_order_no="107" segment_no="7" tag_type="code">2:</text>
<text top="267" left="66" width="5" height="9" font="font19" id="p4_t109" reading_order_no="108" segment_no="7" tag_type="code">η</text>
<text top="266" left="74" width="10" height="9" font="font13" id="p4_t110" reading_order_no="109" segment_no="7" tag_type="code">←</text>
<text top="267" left="87" width="6" height="9" font="font19" id="p4_t111" reading_order_no="110" segment_no="7" tag_type="code">F</text>
<text top="271" left="93" width="17" height="6" font="font20" id="p4_t112" reading_order_no="111" segment_no="7" tag_type="code">U AP</text>
<text top="267" left="112" width="4" height="9" font="font21" id="p4_t113" reading_order_no="112" segment_no="7" tag_type="code">(</text>
<text top="267" left="116" width="5" height="9" font="font19" id="p4_t114" reading_order_no="113" segment_no="7" tag_type="code">f</text>
<text top="271" left="121" width="13" height="6" font="font20" id="p4_t115" reading_order_no="114" segment_no="7" tag_type="code">unt</text>
<text top="267" left="134" width="13" height="9" font="font19" id="p4_t116" reading_order_no="115" segment_no="7" tag_type="code">, X</text>
<text top="267" left="147" width="4" height="9" font="font21" id="p4_t117" reading_order_no="116" segment_no="7" tag_type="code">)</text>
<text top="267" left="151" width="3" height="9" font="font7" id="p4_t118" reading_order_no="117" segment_no="7" tag_type="code">;</text>
<text top="280" left="55" width="6" height="7" font="font8" id="p4_t119" reading_order_no="118" segment_no="7" tag_type="code">3:</text>
<text top="279" left="66" width="12" height="9" font="font23" id="p4_t120" reading_order_no="119" segment_no="7" tag_type="code">for</text>
<text top="279" left="82" width="3" height="9" font="font19" id="p4_t121" reading_order_no="120" segment_no="7" tag_type="code">i</text>
<text top="279" left="88" width="15" height="9" font="font21" id="p4_t122" reading_order_no="121" segment_no="7" tag_type="code">= 1</text>
<text top="279" left="104" width="28" height="9" font="font19" id="p4_t123" reading_order_no="122" segment_no="7" tag_type="code">, . . . , n</text>
<text top="279" left="135" width="11" height="9" font="font23" id="p4_t124" reading_order_no="123" segment_no="7" tag_type="code">do</text>
<text top="292" left="55" width="6" height="7" font="font8" id="p4_t125" reading_order_no="124" segment_no="7" tag_type="code">4:</text>
<text top="291" left="76" width="5" height="9" font="font19" id="p4_t126" reading_order_no="125" segment_no="7" tag_type="code">y</text>
<text top="294" left="81" width="3" height="6" font="font20" id="p4_t127" reading_order_no="126" segment_no="7" tag_type="code">i</text>
<text top="290" left="87" width="10" height="9" font="font13" id="p4_t128" reading_order_no="127" segment_no="7" tag_type="code">←</text>
<text top="291" left="100" width="5" height="9" font="font19" id="p4_t129" reading_order_no="128" segment_no="7" tag_type="code">f</text>
<text top="294" left="104" width="13" height="6" font="font20" id="p4_t130" reading_order_no="129" segment_no="7" tag_type="code">unt</text>
<text top="291" left="118" width="4" height="9" font="font21" id="p4_t131" reading_order_no="130" segment_no="7" tag_type="code">(</text>
<text top="291" left="121" width="5" height="9" font="font19" id="p4_t132" reading_order_no="131" segment_no="7" tag_type="code">d</text>
<text top="294" left="127" width="3" height="6" font="font20" id="p4_t133" reading_order_no="132" segment_no="7" tag_type="code">i</text>
<text top="291" left="130" width="4" height="9" font="font21" id="p4_t134" reading_order_no="133" segment_no="7" tag_type="code">)</text>
<text top="291" left="134" width="3" height="9" font="font7" id="p4_t135" reading_order_no="134" segment_no="7" tag_type="code">;</text>
<text top="304" left="55" width="6" height="7" font="font8" id="p4_t136" reading_order_no="135" segment_no="7" tag_type="code">5:</text>
<text top="300" left="78" width="5" height="9" font="font21" id="p4_t137" reading_order_no="137" segment_no="7" tag_type="code">ˆ</text>
<text top="303" left="76" width="5" height="9" font="font19" id="p4_t138" reading_order_no="136" segment_no="7" tag_type="code">d</text>
<text top="307" left="81" width="3" height="6" font="font20" id="p4_t139" reading_order_no="138" segment_no="7" tag_type="code">i</text>
<text top="302" left="87" width="10" height="9" font="font13" id="p4_t140" reading_order_no="139" segment_no="7" tag_type="code">←</text>
<text top="303" left="100" width="5" height="9" font="font19" id="p4_t141" reading_order_no="140" segment_no="7" tag_type="code">d</text>
<text top="307" left="105" width="3" height="6" font="font20" id="p4_t142" reading_order_no="141" segment_no="7" tag_type="code">i</text>
<text top="303" left="111" width="8" height="9" font="font21" id="p4_t143" reading_order_no="142" segment_no="7" tag_type="code">+</text>
<text top="303" left="121" width="5" height="9" font="font19" id="p4_t144" reading_order_no="143" segment_no="7" tag_type="code">η</text>
<text top="303" left="126" width="3" height="9" font="font7" id="p4_t145" reading_order_no="144" segment_no="7" tag_type="code">;</text>
<text top="316" left="55" width="6" height="7" font="font8" id="p4_t146" reading_order_no="145" segment_no="7" tag_type="code">6:</text>
<text top="315" left="77" width="5" height="9" font="font21" id="p4_t147" reading_order_no="146" segment_no="7" tag_type="code">ˆ</text>
<text top="315" left="76" width="5" height="9" font="font19" id="p4_t148" reading_order_no="147" segment_no="7" tag_type="code">y</text>
<text top="318" left="81" width="3" height="6" font="font20" id="p4_t149" reading_order_no="148" segment_no="7" tag_type="code">i</text>
<text top="314" left="87" width="10" height="9" font="font13" id="p4_t150" reading_order_no="149" segment_no="7" tag_type="code">←</text>
<text top="315" left="100" width="5" height="9" font="font19" id="p4_t151" reading_order_no="150" segment_no="7" tag_type="code">f</text>
<text top="318" left="104" width="13" height="6" font="font20" id="p4_t152" reading_order_no="151" segment_no="7" tag_type="code">unt</text>
<text top="315" left="118" width="11" height="9" font="font21" id="p4_t153" reading_order_no="152" segment_no="7" tag_type="code">( ˆ</text>
<text top="315" left="121" width="5" height="9" font="font19" id="p4_t154" reading_order_no="153" segment_no="7" tag_type="code">d</text>
<text top="318" left="127" width="3" height="6" font="font20" id="p4_t155" reading_order_no="154" segment_no="7" tag_type="code">i</text>
<text top="315" left="130" width="4" height="9" font="font21" id="p4_t156" reading_order_no="155" segment_no="7" tag_type="code">)</text>
<text top="315" left="134" width="3" height="9" font="font7" id="p4_t157" reading_order_no="156" segment_no="7" tag_type="code">;</text>
<text top="328" left="55" width="6" height="7" font="font8" id="p4_t158" reading_order_no="157" segment_no="7" tag_type="code">7:</text>
<text top="327" left="76" width="6" height="9" font="font23" id="p4_t159" reading_order_no="158" segment_no="7" tag_type="code">if</text>
<text top="327" left="85" width="5" height="9" font="font19" id="p4_t160" reading_order_no="159" segment_no="7" tag_type="code">y</text>
<text top="330" left="90" width="3" height="6" font="font20" id="p4_t161" reading_order_no="160" segment_no="7" tag_type="code">i</text>
<text top="327" left="96" width="16" height="9" font="font21" id="p4_t162" reading_order_no="161" segment_no="7" tag_type="code">= ˆ</text>
<text top="327" left="107" width="5" height="9" font="font19" id="p4_t163" reading_order_no="162" segment_no="7" tag_type="code">y</text>
<text top="330" left="112" width="3" height="6" font="font20" id="p4_t164" reading_order_no="163" segment_no="7" tag_type="code">i</text>
<text top="327" left="119" width="19" height="9" font="font23" id="p4_t165" reading_order_no="164" segment_no="7" tag_type="code">then</text>
<text top="340" left="55" width="6" height="7" font="font8" id="p4_t166" reading_order_no="165" segment_no="7" tag_type="code">8:</text>
<text top="339" left="86" width="16" height="9" font="font19" id="p4_t167" reading_order_no="166" segment_no="7" tag_type="code">add</text>
<text top="339" left="101" width="4" height="9" font="font21" id="p4_t168" reading_order_no="167" segment_no="7" tag_type="code">(</text>
<text top="339" left="105" width="5" height="9" font="font19" id="p4_t169" reading_order_no="168" segment_no="7" tag_type="code">d</text>
<text top="342" left="111" width="3" height="6" font="font20" id="p4_t170" reading_order_no="169" segment_no="7" tag_type="code">i</text>
<text top="339" left="114" width="13" height="9" font="font19" id="p4_t171" reading_order_no="170" segment_no="7" tag_type="code">, D</text>
<text top="342" left="127" width="8" height="6" font="font20" id="p4_t172" reading_order_no="171" segment_no="7" tag_type="code">bd</text>
<text top="339" left="135" width="4" height="9" font="font21" id="p4_t173" reading_order_no="172" segment_no="7" tag_type="code">)</text>
<text top="339" left="139" width="3" height="9" font="font7" id="p4_t174" reading_order_no="173" segment_no="7" tag_type="code">;</text>
<text top="352" left="55" width="6" height="7" font="font8" id="p4_t175" reading_order_no="174" segment_no="7" tag_type="code">9:</text>
<text top="351" left="76" width="25" height="9" font="font23" id="p4_t176" reading_order_no="175" segment_no="7" tag_type="code">end if</text>
<text top="364" left="51" width="10" height="7" font="font8" id="p4_t177" reading_order_no="176" segment_no="7" tag_type="code">10:</text>
<text top="363" left="66" width="31" height="9" font="font23" id="p4_t178" reading_order_no="177" segment_no="7" tag_type="code">end for</text>
<text top="376" left="51" width="10" height="7" font="font8" id="p4_t179" reading_order_no="178" segment_no="7" tag_type="code">11:</text>
<text top="375" left="66" width="27" height="9" font="font23" id="p4_t180" reading_order_no="179" segment_no="7" tag_type="code">return</text>
<text top="375" left="100" width="8" height="9" font="font19" id="p4_t181" reading_order_no="180" segment_no="7" tag_type="code">D</text>
<text top="378" left="108" width="8" height="6" font="font20" id="p4_t182" reading_order_no="181" segment_no="7" tag_type="code">bd</text>
<text top="407" left="59" width="241" height="9" font="font7" id="p4_t183" reading_order_no="182" segment_no="9" tag_type="text">In the following sections, the perturbation generation pro-</text>
<text top="419" left="49" width="251" height="9" font="font7" id="p4_t184" reading_order_no="183" segment_no="9" tag_type="text">cess and the backdoor detection process of the proposed</text>
<text top="431" left="49" width="146" height="9" font="font7" id="p4_t185" reading_order_no="184" segment_no="9" tag_type="text">method, are elaborated respectively.</text>
<text top="464" left="49" width="113" height="9" font="font12" id="p4_t186" reading_order_no="185" segment_no="11" tag_type="title">B. Perturbation Generation</text>
<text top="481" left="59" width="241" height="9" font="font7" id="p4_t187" reading_order_no="186" segment_no="12" tag_type="text">The adversarial perturbation generation method used in</text>
<text top="493" left="49" width="251" height="9" font="font7" id="p4_t188" reading_order_no="187" segment_no="12" tag_type="text">this paper is universal adversarial perturbation (UAP) <a href="deeplearning_paper26.html#9">[13].</a></text>
<text top="505" left="49" width="38" height="9" font="font7" id="p4_t189" reading_order_no="188" segment_no="12" tag_type="text">Formally,</text>
<text top="505" left="92" width="8" height="9" font="font19" id="p4_t190" reading_order_no="189" segment_no="12" tag_type="text">X</text>
<text top="505" left="105" width="8" height="9" font="font21" id="p4_t191" reading_order_no="190" segment_no="12" tag_type="text">=</text>
<text top="504" left="118" width="5" height="9" font="font13" id="p4_t192" reading_order_no="191" segment_no="12" tag_type="text">{</text>
<text top="505" left="123" width="6" height="9" font="font19" id="p4_t193" reading_order_no="192" segment_no="12" tag_type="text">x</text>
<text top="508" left="129" width="4" height="6" font="font22" id="p4_t194" reading_order_no="193" segment_no="12" tag_type="text">1</text>
<text top="505" left="133" width="28" height="9" font="font19" id="p4_t195" reading_order_no="194" segment_no="12" tag_type="text">, . . . , x</text>
<text top="508" left="161" width="12" height="6" font="font22" id="p4_t196" reading_order_no="195" segment_no="12" tag_type="text">300</text>
<text top="504" left="173" width="5" height="9" font="font13" id="p4_t197" reading_order_no="196" segment_no="12" tag_type="text">}</text>
<text top="505" left="183" width="117" height="9" font="font7" id="p4_t198" reading_order_no="197" segment_no="12" tag_type="text">denotes the clean image set</text>
<text top="517" left="49" width="14" height="9" font="font7" id="p4_t199" reading_order_no="198" segment_no="12" tag_type="text">and</text>
<text top="517" left="67" width="5" height="9" font="font19" id="p4_t200" reading_order_no="199" segment_no="12" tag_type="text">f</text>
<text top="520" left="72" width="13" height="6" font="font20" id="p4_t201" reading_order_no="200" segment_no="12" tag_type="text">unt</text>
<text top="517" left="88" width="212" height="9" font="font7" id="p4_t202" reading_order_no="201" segment_no="12" tag_type="text">represents the backdoored model, which outputs the</text>
<text top="529" left="49" width="79" height="9" font="font7" id="p4_t203" reading_order_no="202" segment_no="12" tag_type="text">corresponding label</text>
<text top="529" left="131" width="5" height="9" font="font19" id="p4_t204" reading_order_no="203" segment_no="12" tag_type="text">f</text>
<text top="532" left="136" width="13" height="6" font="font20" id="p4_t205" reading_order_no="204" segment_no="12" tag_type="text">unt</text>
<text top="529" left="149" width="4" height="9" font="font21" id="p4_t206" reading_order_no="205" segment_no="12" tag_type="text">(</text>
<text top="529" left="153" width="6" height="9" font="font19" id="p4_t207" reading_order_no="206" segment_no="12" tag_type="text">x</text>
<text top="529" left="159" width="4" height="9" font="font21" id="p4_t208" reading_order_no="207" segment_no="12" tag_type="text">)</text>
<text top="529" left="165" width="60" height="9" font="font7" id="p4_t209" reading_order_no="208" segment_no="12" tag_type="text">for each image</text>
<text top="529" left="228" width="6" height="9" font="font19" id="p4_t210" reading_order_no="209" segment_no="12" tag_type="text">x</text>
<text top="532" left="234" width="3" height="6" font="font20" id="p4_t211" reading_order_no="210" segment_no="12" tag_type="text">i</text>
<text top="528" left="240" width="7" height="9" font="font13" id="p4_t212" reading_order_no="211" segment_no="12" tag_type="text">∈</text>
<text top="529" left="249" width="8" height="9" font="font19" id="p4_t213" reading_order_no="212" segment_no="12" tag_type="text">X</text>
<text top="529" left="258" width="42" height="9" font="font7" id="p4_t214" reading_order_no="213" segment_no="12" tag_type="text">. Different</text>
<text top="541" left="49" width="214" height="9" font="font7" id="p4_t215" reading_order_no="214" segment_no="12" tag_type="text">from the UAP generation method in <a href="deeplearning_paper26.html#9">[13] </a>where the</text>
<text top="541" left="267" width="4" height="9" font="font19" id="p4_t216" reading_order_no="215" segment_no="12" tag_type="text">`</text>
<text top="544" left="271" width="4" height="6" font="font22" id="p4_t217" reading_order_no="216" segment_no="12" tag_type="text">2</text>
<text top="541" left="279" width="21" height="9" font="font7" id="p4_t218" reading_order_no="217" segment_no="12" tag_type="text">norm</text>
<text top="553" left="49" width="251" height="9" font="font7" id="p4_t219" reading_order_no="218" segment_no="12" tag_type="text">is used to constrain the intensity of UAP, in this paper, we use</text>
<text top="564" left="49" width="4" height="9" font="font19" id="p4_t220" reading_order_no="219" segment_no="12" tag_type="text">`</text>
<text top="568" left="53" width="8" height="7" font="font10" id="p4_t221" reading_order_no="220" segment_no="12" tag_type="text">∞</text>
<text top="565" left="64" width="220" height="9" font="font7" id="p4_t222" reading_order_no="221" segment_no="12" tag_type="text">norm to constrain the intensity of the perturbation. The</text>
<text top="564" left="287" width="4" height="9" font="font19" id="p4_t223" reading_order_no="222" segment_no="12" tag_type="text">`</text>
<text top="568" left="292" width="8" height="7" font="font10" id="p4_t224" reading_order_no="223" segment_no="12" tag_type="text">∞</text>
<text top="577" left="49" width="251" height="9" font="font7" id="p4_t225" reading_order_no="224" segment_no="12" tag_type="text">norm represents the maximum value of the perturbation. The</text>
<text top="588" left="49" width="186" height="9" font="font7" id="p4_t226" reading_order_no="225" segment_no="12" tag_type="text">perturbation generated under the constraint of</text>
<text top="588" left="238" width="4" height="9" font="font19" id="p4_t227" reading_order_no="226" segment_no="12" tag_type="text">`</text>
<text top="592" left="242" width="8" height="7" font="font10" id="p4_t228" reading_order_no="227" segment_no="12" tag_type="text">∞</text>
<text top="588" left="254" width="46" height="9" font="font7" id="p4_t229" reading_order_no="228" segment_no="12" tag_type="text">norm is the</text>
<text top="600" left="49" width="251" height="9" font="font7" id="p4_t230" reading_order_no="229" segment_no="12" tag_type="text">minimal necessary perturbation, which is smaller than the one</text>
<text top="612" left="49" width="137" height="9" font="font7" id="p4_t231" reading_order_no="230" segment_no="12" tag_type="text">generated under the constraint of</text>
<text top="612" left="190" width="4" height="9" font="font19" id="p4_t232" reading_order_no="231" segment_no="12" tag_type="text">`</text>
<text top="616" left="194" width="4" height="6" font="font22" id="p4_t233" reading_order_no="232" segment_no="12" tag_type="text">2</text>
<text top="612" left="202" width="98" height="9" font="font7" id="p4_t234" reading_order_no="233" segment_no="12" tag_type="text">norm. In the process of</text>
<text top="624" left="49" width="251" height="9" font="font7" id="p4_t235" reading_order_no="234" segment_no="12" tag_type="text">generating adversarial perturbation, the universal perturbation</text>
<text top="636" left="49" width="5" height="9" font="font19" id="p4_t236" reading_order_no="235" segment_no="12" tag_type="text">η</text>
<text top="636" left="58" width="242" height="9" font="font7" id="p4_t237" reading_order_no="236" segment_no="12" tag_type="text">is generated by solving the following optimization problem</text>
<text top="648" left="49" width="19" height="9" font="font7" id="p4_t238" reading_order_no="237" segment_no="12" tag_type="text"><a href="deeplearning_paper26.html#9">[13]:</a></text>
<text top="679" left="60" width="32" height="9" font="font21" id="p4_t239" reading_order_no="238" segment_no="14" tag_type="formula">arg min</text>
<text top="689" left="74" width="4" height="6" font="font20" id="p4_t240" reading_order_no="239" segment_no="14" tag_type="formula">η</text>
<text top="679" left="92" width="5" height="9" font="font13" id="p4_t241" reading_order_no="240" segment_no="14" tag_type="formula">k</text>
<text top="679" left="97" width="5" height="9" font="font19" id="p4_t242" reading_order_no="241" segment_no="14" tag_type="formula">η</text>
<text top="679" left="103" width="5" height="9" font="font13" id="p4_t243" reading_order_no="242" segment_no="14" tag_type="formula">k</text>
<text top="683" left="108" width="8" height="7" font="font10" id="p4_t244" reading_order_no="243" segment_no="14" tag_type="formula">∞</text>
<text top="680" left="119" width="12" height="9" font="font7" id="p4_t245" reading_order_no="244" segment_no="14" tag_type="formula">s.t.</text>
<text top="679" left="135" width="5" height="9" font="font19" id="p4_t246" reading_order_no="245" segment_no="14" tag_type="formula">f</text>
<text top="683" left="139" width="13" height="6" font="font20" id="p4_t247" reading_order_no="246" segment_no="14" tag_type="formula">unt</text>
<text top="679" left="154" width="4" height="9" font="font21" id="p4_t248" reading_order_no="247" segment_no="14" tag_type="formula">(</text>
<text top="679" left="158" width="6" height="9" font="font19" id="p4_t249" reading_order_no="248" segment_no="14" tag_type="formula">x</text>
<text top="683" left="164" width="3" height="6" font="font20" id="p4_t250" reading_order_no="249" segment_no="14" tag_type="formula">i</text>
<text top="679" left="169" width="8" height="9" font="font21" id="p4_t251" reading_order_no="250" segment_no="14" tag_type="formula">+</text>
<text top="679" left="179" width="5" height="9" font="font19" id="p4_t252" reading_order_no="251" segment_no="14" tag_type="formula">η</text>
<text top="679" left="185" width="4" height="9" font="font21" id="p4_t253" reading_order_no="252" segment_no="14" tag_type="formula">)</text>
<text top="679" left="191" width="0" height="9" font="font13" id="p4_t254" reading_order_no="253" segment_no="14" tag_type="formula">6</text>
<text top="679" left="191" width="8" height="9" font="font21" id="p4_t255" reading_order_no="254" segment_no="14" tag_type="formula">=</text>
<text top="679" left="202" width="5" height="9" font="font19" id="p4_t256" reading_order_no="255" segment_no="14" tag_type="formula">f</text>
<text top="683" left="207" width="13" height="6" font="font20" id="p4_t257" reading_order_no="256" segment_no="14" tag_type="formula">unt</text>
<text top="679" left="221" width="4" height="9" font="font21" id="p4_t258" reading_order_no="257" segment_no="14" tag_type="formula">(</text>
<text top="679" left="225" width="6" height="9" font="font19" id="p4_t259" reading_order_no="258" segment_no="14" tag_type="formula">x</text>
<text top="683" left="231" width="3" height="6" font="font20" id="p4_t260" reading_order_no="259" segment_no="14" tag_type="formula">i</text>
<text top="679" left="234" width="4" height="9" font="font21" id="p4_t261" reading_order_no="260" segment_no="14" tag_type="formula">)</text>
<text top="679" left="240" width="13" height="9" font="font19" id="p4_t262" reading_order_no="261" segment_no="14" tag_type="formula">, x</text>
<text top="683" left="253" width="3" height="6" font="font20" id="p4_t263" reading_order_no="262" segment_no="14" tag_type="formula">i</text>
<text top="679" left="259" width="7" height="9" font="font13" id="p4_t264" reading_order_no="263" segment_no="14" tag_type="formula">∈</text>
<text top="679" left="268" width="8" height="9" font="font19" id="p4_t265" reading_order_no="264" segment_no="14" tag_type="formula">X</text>
<text top="680" left="288" width="12" height="9" font="font7" id="p4_t266" reading_order_no="265" segment_no="15" tag_type="text">(1)</text>
<text top="704" left="59" width="241" height="9" font="font7" id="p4_t267" reading_order_no="266" segment_no="16" tag_type="text">As shown in Eq. <a href="deeplearning_paper26.html#4">(1), </a>in each iteration, for the clean image</text>
<text top="716" left="49" width="6" height="9" font="font19" id="p4_t268" reading_order_no="267" segment_no="16" tag_type="text">x</text>
<text top="719" left="55" width="3" height="6" font="font20" id="p4_t269" reading_order_no="268" segment_no="16" tag_type="text">i</text>
<text top="716" left="61" width="19" height="9" font="font7" id="p4_t270" reading_order_no="269" segment_no="16" tag_type="text">from</text>
<text top="716" left="83" width="8" height="9" font="font19" id="p4_t271" reading_order_no="270" segment_no="16" tag_type="text">X</text>
<text top="716" left="92" width="18" height="9" font="font7" id="p4_t272" reading_order_no="271" segment_no="16" tag_type="text">, the</text>
<text top="716" left="113" width="4" height="9" font="font19" id="p4_t273" reading_order_no="272" segment_no="16" tag_type="text">`</text>
<text top="719" left="117" width="8" height="7" font="font10" id="p4_t274" reading_order_no="273" segment_no="16" tag_type="text">∞</text>
<text top="716" left="129" width="99" height="9" font="font7" id="p4_t275" reading_order_no="274" segment_no="16" tag_type="text">norm of the perturbation</text>
<text top="716" left="231" width="5" height="9" font="font19" id="p4_t276" reading_order_no="275" segment_no="16" tag_type="text">η</text>
<text top="716" left="239" width="61" height="9" font="font7" id="p4_t277" reading_order_no="276" segment_no="16" tag_type="text">is calculated in</text>
<text top="728" left="49" width="210" height="9" font="font7" id="p4_t278" reading_order_no="277" segment_no="16" tag_type="text">order to find the desired perturbation with minimal</text>
<text top="728" left="263" width="4" height="9" font="font19" id="p4_t279" reading_order_no="278" segment_no="16" tag_type="text">`</text>
<text top="731" left="267" width="8" height="7" font="font10" id="p4_t280" reading_order_no="279" segment_no="16" tag_type="text">∞</text>
<text top="728" left="279" width="21" height="9" font="font7" id="p4_t281" reading_order_no="280" segment_no="16" tag_type="text">norm</text>
<text top="740" left="49" width="19" height="9" font="font7" id="p4_t282" reading_order_no="281" segment_no="16" tag_type="text"><a href="deeplearning_paper26.html#9">[13].</a></text>
<text top="58" left="312" width="95" height="9" font="font12" id="p4_t283" reading_order_no="282" segment_no="2" tag_type="title">C. Backdoor Detection</text>
<text top="74" left="322" width="241" height="9" font="font7" id="p4_t284" reading_order_no="283" segment_no="3" tag_type="text">The proposed method can be applied in two scenarios,</text>
<text top="86" left="312" width="251" height="9" font="font7" id="p4_t285" reading_order_no="284" segment_no="3" tag_type="text">working in the training stage, and working in the inference</text>
<text top="98" left="312" width="251" height="9" font="font7" id="p4_t286" reading_order_no="285" segment_no="3" tag_type="text">stage. In the training stage, the proposed method aims to detect</text>
<text top="110" left="312" width="251" height="9" font="font7" id="p4_t287" reading_order_no="286" segment_no="3" tag_type="text">whether the training dataset contains backdoor instances and</text>
<text top="122" left="312" width="251" height="9" font="font7" id="p4_t288" reading_order_no="287" segment_no="3" tag_type="text">then remove the backdoor instances. In the inference stage,</text>
<text top="134" left="312" width="251" height="9" font="font7" id="p4_t289" reading_order_no="288" segment_no="3" tag_type="text">the goal of the proposed method is to detect whether an input</text>
<text top="146" left="312" width="251" height="9" font="font7" id="p4_t290" reading_order_no="289" segment_no="3" tag_type="text">image contains a trigger. The backdoor detection procedure is</text>
<text top="158" left="312" width="80" height="9" font="font7" id="p4_t291" reading_order_no="290" segment_no="3" tag_type="text">presented in Fig. <a href="deeplearning_paper26.html#4">2.</a></text>
<text top="338" left="469" width="12" height="11" font="font25" id="p4_t292" reading_order_no="322" segment_no="5" tag_type="figure">No</text>
<text top="352" left="406" width="15" height="11" font="font25" id="p4_t293" reading_order_no="323" segment_no="5" tag_type="figure">Yes</text>
<text top="370" left="390" width="72" height="11" font="font25" id="p4_t294" reading_order_no="324" segment_no="5" tag_type="figure">Backdoor Instance</text>
<text top="371" left="481" width="49" height="11" font="font25" id="p4_t295" reading_order_no="325" segment_no="5" tag_type="figure">Clean Image</text>
<text top="252" left="359" width="90" height="11" font="font25" id="p4_t296" reading_order_no="301" segment_no="5" tag_type="figure">          Untrusted Model</text>
<text top="257" left="458" width="7" height="6" font="font26" id="p4_t297" reading_order_no="303" segment_no="5" tag_type="figure"><i>unt</i></text>
<text top="251" left="455" width="3" height="11" font="font27" id="p4_t298" reading_order_no="302" segment_no="5" tag_type="figure"><i>f</i></text>
<text top="211" left="441" width="64" height="11" font="font25" id="p4_t299" reading_order_no="295" segment_no="5" tag_type="figure">Perturbed Image</text>
<text top="220" left="457" width="3" height="11" font="font25" id="p4_t300" reading_order_no="297" segment_no="5" tag_type="figure">ˆ</text>
<text top="223" left="455" width="5" height="11" font="font27" id="p4_t301" reading_order_no="296" segment_no="5" tag_type="figure"><i>d</i></text>
<text top="223" left="471" width="5" height="11" font="font27" id="p4_t302" reading_order_no="299" segment_no="5" tag_type="figure"><i>d</i></text>
<text top="221" left="484" width="6" height="12" font="font28" id="p4_t303" reading_order_no="300" segment_no="5" tag_type="figure"></text>
<text top="222" left="463" width="20" height="12" font="font28" id="p4_t304" reading_order_no="298" segment_no="5" tag_type="figure"> </text>
<text top="184" left="435" width="71" height="11" font="font25" id="p4_t305" reading_order_no="293" segment_no="5" tag_type="figure"> Add Perturbation </text>
<text top="182" left="504" width="6" height="12" font="font28" id="p4_t306" reading_order_no="294" segment_no="5" tag_type="figure"></text>
<text top="184" left="344" width="65" height="11" font="font25" id="p4_t307" reading_order_no="291" segment_no="5" tag_type="figure">Untrusted Image</text>
<text top="184" left="411" width="5" height="11" font="font27" id="p4_t308" reading_order_no="292" segment_no="5" tag_type="figure"><i>d</i></text>
<text top="326" left="430" width="9" height="11" font="font25" id="p4_t309" reading_order_no="321" segment_no="5" tag_type="figure">ˆ ?</text>
<text top="326" left="414" width="4" height="11" font="font27" id="p4_t310" reading_order_no="318" segment_no="5" tag_type="figure"><i>y</i></text>
<text top="326" left="429" width="4" height="11" font="font27" id="p4_t311" reading_order_no="320" segment_no="5" tag_type="figure"><i>y</i></text>
<text top="325" left="421" width="5" height="12" font="font28" id="p4_t312" reading_order_no="319" segment_no="5" tag_type="figure"></text>
<text top="282" left="388" width="13" height="11" font="font25" id="p4_t313" reading_order_no="308" segment_no="5" tag_type="figure">( )</text>
<text top="288" left="379" width="7" height="6" font="font26" id="p4_t314" reading_order_no="307" segment_no="5" tag_type="figure"><i>unt</i></text>
<text top="282" left="360" width="4" height="11" font="font27" id="p4_t315" reading_order_no="304" segment_no="5" tag_type="figure"><i>y</i></text>
<text top="282" left="376" width="3" height="11" font="font27" id="p4_t316" reading_order_no="306" segment_no="5" tag_type="figure"><i>f</i></text>
<text top="282" left="391" width="5" height="11" font="font27" id="p4_t317" reading_order_no="309" segment_no="5" tag_type="figure"><i>d</i></text>
<text top="281" left="367" width="5" height="12" font="font28" id="p4_t318" reading_order_no="305" segment_no="5" tag_type="figure"></text>
<text top="280" left="488" width="3" height="11" font="font25" id="p4_t319" reading_order_no="317" segment_no="5" tag_type="figure">ˆ</text>
<text top="282" left="455" width="3" height="11" font="font25" id="p4_t320" reading_order_no="311" segment_no="5" tag_type="figure">ˆ</text>
<text top="283" left="482" width="13" height="11" font="font25" id="p4_t321" reading_order_no="315" segment_no="5" tag_type="figure">( )</text>
<text top="289" left="474" width="7" height="6" font="font26" id="p4_t322" reading_order_no="314" segment_no="5" tag_type="figure"><i>unt</i></text>
<text top="283" left="454" width="4" height="11" font="font27" id="p4_t323" reading_order_no="310" segment_no="5" tag_type="figure"><i>y</i></text>
<text top="283" left="471" width="3" height="11" font="font27" id="p4_t324" reading_order_no="313" segment_no="5" tag_type="figure"><i>f</i></text>
<text top="283" left="486" width="5" height="11" font="font27" id="p4_t325" reading_order_no="316" segment_no="5" tag_type="figure"><i>d</i></text>
<text top="282" left="461" width="5" height="12" font="font28" id="p4_t326" reading_order_no="312" segment_no="5" tag_type="figure"></text>
<text top="399" left="312" width="22" height="7" font="font8" id="p4_t327" reading_order_no="326" segment_no="8" tag_type="text">Fig. 2.</text>
<text top="399" left="344" width="219" height="7" font="font8" id="p4_t328" reading_order_no="327" segment_no="8" tag_type="text">The workflow of the backdoor detection process of the proposed</text>
<text top="408" left="312" width="24" height="7" font="font8" id="p4_t329" reading_order_no="328" segment_no="8" tag_type="text">method</text>
<text top="429" left="322" width="190" height="9" font="font23" id="p4_t330" reading_order_no="329" segment_no="10" tag_type="text">Backdoor Detection in the Training Stage:</text>
<text top="429" left="516" width="47" height="9" font="font7" id="p4_t331" reading_order_no="330" segment_no="10" tag_type="text">In this sce-</text>
<text top="441" left="312" width="251" height="9" font="font7" id="p4_t332" reading_order_no="331" segment_no="10" tag_type="text">nario, the training data is obtained from untrusted sources.</text>
<text top="453" left="312" width="251" height="9" font="font7" id="p4_t333" reading_order_no="332" segment_no="10" tag_type="text">The defender attempts to figure out whether the training</text>
<text top="465" left="312" width="251" height="9" font="font7" id="p4_t334" reading_order_no="333" segment_no="10" tag_type="text">dataset contains backdoor instances. If the training dataset</text>
<text top="477" left="312" width="251" height="9" font="font7" id="p4_t335" reading_order_no="334" segment_no="10" tag_type="text">contains backdoor instances, the defender aims to remove the</text>
<text top="489" left="312" width="251" height="9" font="font7" id="p4_t336" reading_order_no="335" segment_no="10" tag_type="text">backdoor instances injected in the training dataset. For each</text>
<text top="501" left="312" width="251" height="9" font="font7" id="p4_t337" reading_order_no="336" segment_no="10" tag_type="text">image in the training set, it will be added with the universal</text>
<text top="512" left="312" width="251" height="9" font="font7" id="p4_t338" reading_order_no="337" segment_no="10" tag_type="text">perturbation <a href="deeplearning_paper26.html#9">[13], </a>and then input into the untrusted model.</text>
<text top="524" left="312" width="251" height="9" font="font7" id="p4_t339" reading_order_no="338" segment_no="10" tag_type="text">The unperturbed image will also be input into the untrusted</text>
<text top="536" left="312" width="251" height="9" font="font7" id="p4_t340" reading_order_no="339" segment_no="10" tag_type="text">model. If the predictions of the model on the perturbed image</text>
<text top="548" left="312" width="251" height="9" font="font7" id="p4_t341" reading_order_no="340" segment_no="10" tag_type="text">and unperturbed image are consistent, this image will be</text>
<text top="560" left="312" width="251" height="9" font="font7" id="p4_t342" reading_order_no="341" segment_no="10" tag_type="text">considered as a backdoor instance. Meanwhile, the untrusted</text>
<text top="572" left="312" width="251" height="9" font="font7" id="p4_t343" reading_order_no="342" segment_no="10" tag_type="text">model is considered to be backdoored. This backdoor detection</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p4_t344" reading_order_no="343" segment_no="10" tag_type="text">procedure will be applied for each image in the training set.</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p4_t345" reading_order_no="344" segment_no="10" tag_type="text">Once the backdoor instances are removed, a clean model can</text>
<text top="608" left="312" width="177" height="9" font="font7" id="p4_t346" reading_order_no="345" segment_no="10" tag_type="text">be trained on the sanitized training dataset.</text>
<text top="620" left="322" width="196" height="9" font="font23" id="p4_t347" reading_order_no="346" segment_no="13" tag_type="text">Backdoor Detection in the Inference Stage:</text>
<text top="620" left="522" width="41" height="9" font="font7" id="p4_t348" reading_order_no="347" segment_no="13" tag_type="text">In the in-</text>
<text top="632" left="312" width="251" height="9" font="font7" id="p4_t349" reading_order_no="348" segment_no="13" tag_type="text">ference stage, the well-trained model is deployed to provide</text>
<text top="644" left="312" width="251" height="9" font="font7" id="p4_t350" reading_order_no="349" segment_no="13" tag_type="text">prediction services. The goal of the defender in this scenario</text>
<text top="656" left="312" width="251" height="9" font="font7" id="p4_t351" reading_order_no="350" segment_no="13" tag_type="text">is to detect whether an input image carries a trigger. Given</text>
<text top="668" left="312" width="62" height="9" font="font7" id="p4_t352" reading_order_no="351" segment_no="13" tag_type="text">an input image</text>
<text top="668" left="378" width="5" height="9" font="font19" id="p4_t353" reading_order_no="352" segment_no="13" tag_type="text">d</text>
<text top="668" left="383" width="152" height="9" font="font7" id="p4_t354" reading_order_no="353" segment_no="13" tag_type="text">, after being added with perturbation</text>
<text top="668" left="539" width="5" height="9" font="font19" id="p4_t355" reading_order_no="354" segment_no="13" tag_type="text">η</text>
<text top="668" left="545" width="19" height="9" font="font7" id="p4_t356" reading_order_no="355" segment_no="13" tag_type="text">, the</text>
<text top="680" left="312" width="66" height="9" font="font7" id="p4_t357" reading_order_no="356" segment_no="13" tag_type="text">perturbed image</text>
<text top="677" left="383" width="5" height="9" font="font21" id="p4_t358" reading_order_no="358" segment_no="13" tag_type="text">ˆ</text>
<text top="680" left="381" width="5" height="9" font="font19" id="p4_t359" reading_order_no="357" segment_no="13" tag_type="text">d</text>
<text top="680" left="390" width="109" height="9" font="font7" id="p4_t360" reading_order_no="359" segment_no="13" tag_type="text">and the unperturbed image</text>
<text top="680" left="502" width="5" height="9" font="font19" id="p4_t361" reading_order_no="360" segment_no="13" tag_type="text">d</text>
<text top="680" left="511" width="52" height="9" font="font7" id="p4_t362" reading_order_no="361" segment_no="13" tag_type="text">will be input</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p4_t363" reading_order_no="362" segment_no="13" tag_type="text">into the model. If the predicted labels of the perturbed image</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p4_t364" reading_order_no="363" segment_no="13" tag_type="text">is consistent with that of the unperturbed image, the input</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p4_t365" reading_order_no="364" segment_no="13" tag_type="text">image is considered to be a backdoor instance. Meanwhile,</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p4_t366" reading_order_no="365" segment_no="13" tag_type="text">the model is considered to be a backdoored model, and the</text>
<text top="740" left="312" width="208" height="9" font="font7" id="p4_t367" reading_order_no="366" segment_no="13" tag_type="text">predicted label is considered to be the target label.</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font29" size="10" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font30" size="9" family="TimesNewRomanPSMT" color="#000000"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">5</text>
<text top="58" left="49" width="251" height="9" font="font12" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="text">D. Why choose UAP <a href="deeplearning_paper26.html#9">[13] </a>for adversarial perturbation gener-</text>
<text top="70" left="49" width="25" height="9" font="font12" id="p5_t3" reading_order_no="2" segment_no="1" tag_type="text">ation?</text>
<text top="88" left="59" width="241" height="9" font="font7" id="p5_t4" reading_order_no="3" segment_no="4" tag_type="text">In this paper, we exploit adversarial perturbation to perturb</text>
<text top="100" left="49" width="251" height="9" font="font7" id="p5_t5" reading_order_no="4" segment_no="4" tag_type="text">the main content of the backdoor instance other than the</text>
<text top="112" left="49" width="251" height="9" font="font7" id="p5_t6" reading_order_no="5" segment_no="4" tag_type="text">trigger. However, not all kinds of adversarial perturbation</text>
<text top="124" left="49" width="251" height="9" font="font7" id="p5_t7" reading_order_no="6" segment_no="4" tag_type="text">generation methods are suitable to use in the proposed method.</text>
<text top="136" left="49" width="251" height="9" font="font7" id="p5_t8" reading_order_no="7" segment_no="4" tag_type="text">We evaluate four different adversarial perturbation generation</text>
<text top="148" left="49" width="251" height="9" font="font7" id="p5_t9" reading_order_no="8" segment_no="4" tag_type="text">methods <a href="deeplearning_paper26.html#9">[13], [20]–[22] </a>in Section <a href="deeplearning_paper26.html#7">IV-D. </a>The experimental</text>
<text top="160" left="49" width="251" height="9" font="font7" id="p5_t10" reading_order_no="9" segment_no="4" tag_type="text">results show that when the image is perturbed by universal</text>
<text top="172" left="49" width="251" height="9" font="font7" id="p5_t11" reading_order_no="10" segment_no="4" tag_type="text">adversarial perturbation <a href="deeplearning_paper26.html#9">[13], </a>the detection performance of the</text>
<text top="184" left="49" width="251" height="9" font="font7" id="p5_t12" reading_order_no="11" segment_no="4" tag_type="text">proposed method is the highest among the four adversarial</text>
<text top="196" left="49" width="134" height="9" font="font7" id="p5_t13" reading_order_no="12" segment_no="4" tag_type="text">perturbation generation methods.</text>
<text top="209" left="59" width="241" height="9" font="font7" id="p5_t14" reading_order_no="13" segment_no="7" tag_type="text">The reason is as follows. The existing adversarial example</text>
<text top="221" left="49" width="251" height="9" font="font7" id="p5_t15" reading_order_no="14" segment_no="7" tag_type="text">attacks can be divided into two categories, image-specific</text>
<text top="232" left="49" width="251" height="9" font="font7" id="p5_t16" reading_order_no="15" segment_no="7" tag_type="text">adversarial attack and image-agnostic adversarial attack <a href="deeplearning_paper26.html#9">[23].</a></text>
<text top="244" left="49" width="251" height="9" font="font7" id="p5_t17" reading_order_no="16" segment_no="7" tag_type="text">For image-specific adversarial attack, one perturbation can</text>
<text top="256" left="49" width="251" height="9" font="font7" id="p5_t18" reading_order_no="17" segment_no="7" tag_type="text">only fool the model for one specific image <a href="deeplearning_paper26.html#9">[24]. </a>The ground-</text>
<text top="268" left="49" width="251" height="9" font="font7" id="p5_t19" reading_order_no="18" segment_no="7" tag_type="text">truth label of the specific image is required in order to generate</text>
<text top="280" left="49" width="251" height="9" font="font7" id="p5_t20" reading_order_no="19" segment_no="7" tag_type="text">the image-specific perturbation which can cause the perturbed</text>
<text top="292" left="49" width="251" height="9" font="font7" id="p5_t21" reading_order_no="20" segment_no="7" tag_type="text">image to be misclassified from its ground-truth label to other</text>
<text top="304" left="49" width="251" height="9" font="font7" id="p5_t22" reading_order_no="21" segment_no="7" tag_type="text">label <a href="deeplearning_paper26.html#9">[24]. </a>However, for backdoor instance, the label used</text>
<text top="316" left="49" width="251" height="9" font="font7" id="p5_t23" reading_order_no="22" segment_no="7" tag_type="text">to generate the image-specific perturbation is the target label</text>
<text top="328" left="49" width="251" height="9" font="font7" id="p5_t24" reading_order_no="23" segment_no="7" tag_type="text">rather than the ground-truth label. In other words, for backdoor</text>
<text top="340" left="49" width="251" height="9" font="font7" id="p5_t25" reading_order_no="24" segment_no="7" tag_type="text">instances, the image-specific perturbation is generated in order</text>
<text top="352" left="49" width="251" height="9" font="font7" id="p5_t26" reading_order_no="25" segment_no="7" tag_type="text">to change the predicted result of perturbed backdoor instance</text>
<text top="364" left="49" width="251" height="9" font="font7" id="p5_t27" reading_order_no="26" segment_no="7" tag_type="text">from the target label to other one. Under this circumstance,</text>
<text top="376" left="49" width="251" height="9" font="font7" id="p5_t28" reading_order_no="27" segment_no="7" tag_type="text">the generated image-specific perturbation will strongly affect</text>
<text top="388" left="49" width="251" height="9" font="font7" id="p5_t29" reading_order_no="28" segment_no="7" tag_type="text">the trigger, as the trigger contributes heavily to the predicted</text>
<text top="400" left="49" width="251" height="9" font="font7" id="p5_t30" reading_order_no="29" segment_no="7" tag_type="text">result and the predicted result is the target label. Once the</text>
<text top="412" left="49" width="251" height="9" font="font7" id="p5_t31" reading_order_no="30" segment_no="7" tag_type="text">trigger is strongly affected by the image-specific perturbation,</text>
<text top="424" left="49" width="251" height="9" font="font7" id="p5_t32" reading_order_no="31" segment_no="7" tag_type="text">the predicted label of the backdoor instance after perturba-</text>
<text top="436" left="49" width="251" height="9" font="font7" id="p5_t33" reading_order_no="32" segment_no="7" tag_type="text">tion will change. Then the detection method will incorrectly</text>
<text top="448" left="49" width="251" height="9" font="font7" id="p5_t34" reading_order_no="33" segment_no="7" tag_type="text">consider this backdoor instance as a clean one. For image-</text>
<text top="460" left="49" width="251" height="9" font="font7" id="p5_t35" reading_order_no="34" segment_no="7" tag_type="text">agnostic adversarial attack, it only needs to generate one</text>
<text top="472" left="49" width="251" height="9" font="font7" id="p5_t36" reading_order_no="35" segment_no="7" tag_type="text">single perturbation, which can cause misclassification for all</text>
<text top="484" left="49" width="251" height="9" font="font7" id="p5_t37" reading_order_no="36" segment_no="7" tag_type="text">images when the perturbation is added to those images <a href="deeplearning_paper26.html#9">[13].</a></text>
<text top="495" left="49" width="251" height="9" font="font7" id="p5_t38" reading_order_no="37" segment_no="7" tag_type="text">This single perturbation is generated based on a small set of</text>
<text top="507" left="49" width="251" height="9" font="font7" id="p5_t39" reading_order_no="38" segment_no="7" tag_type="text">clean images <a href="deeplearning_paper26.html#9">[13]. </a>Therefore, the trigger stamped in backdoor</text>
<text top="519" left="49" width="251" height="9" font="font7" id="p5_t40" reading_order_no="39" segment_no="7" tag_type="text">instance will only be slightly affected by the generated image-</text>
<text top="531" left="49" width="88" height="9" font="font7" id="p5_t41" reading_order_no="40" segment_no="7" tag_type="text">agnostic perturbation.</text>
<text top="544" left="59" width="241" height="9" font="font7" id="p5_t42" reading_order_no="41" segment_no="13" tag_type="text">In summary, UAP <a href="deeplearning_paper26.html#9">[13], </a>as a kind of image-agnostic per-</text>
<text top="556" left="49" width="251" height="9" font="font7" id="p5_t43" reading_order_no="42" segment_no="13" tag_type="text">turbation, has much less influence on the trigger than the</text>
<text top="568" left="49" width="251" height="9" font="font7" id="p5_t44" reading_order_no="43" segment_no="13" tag_type="text">image-specific perturbation, so the label of backdoor instance</text>
<text top="580" left="49" width="251" height="9" font="font7" id="p5_t45" reading_order_no="44" segment_no="13" tag_type="text">will keep unchanged even after being perturbed by UAP <a href="deeplearning_paper26.html#9">[13].</a></text>
<text top="592" left="49" width="251" height="9" font="font7" id="p5_t46" reading_order_no="45" segment_no="13" tag_type="text">Therefore, we choose UAP <a href="deeplearning_paper26.html#9">[13] </a>as the perturbation generation</text>
<text top="604" left="49" width="154" height="9" font="font7" id="p5_t47" reading_order_no="46" segment_no="13" tag_type="text">method used in the proposed method.</text>
<text top="636" left="111" width="24" height="9" font="font7" id="p5_t48" reading_order_no="47" segment_no="16" tag_type="title">IV. E</text>
<text top="638" left="136" width="60" height="7" font="font8" id="p5_t49" reading_order_no="48" segment_no="16" tag_type="title">XPERIMENTAL</text>
<text top="636" left="200" width="7" height="9" font="font7" id="p5_t50" reading_order_no="49" segment_no="16" tag_type="title">R</text>
<text top="638" left="207" width="31" height="7" font="font8" id="p5_t51" reading_order_no="50" segment_no="16" tag_type="title">ESULTS</text>
<text top="656" left="59" width="241" height="9" font="font7" id="p5_t52" reading_order_no="51" segment_no="17" tag_type="text">In this section, first, we introduce the datasets, the cor-</text>
<text top="668" left="49" width="251" height="9" font="font7" id="p5_t53" reading_order_no="52" segment_no="17" tag_type="text">responding DNN models, and the metrics used to evaluate</text>
<text top="680" left="49" width="251" height="9" font="font7" id="p5_t54" reading_order_no="53" segment_no="17" tag_type="text">the proposed approach. Second, the experimental results are</text>
<text top="692" left="49" width="251" height="9" font="font7" id="p5_t55" reading_order_no="54" segment_no="17" tag_type="text">analyzed. Third, we evaluate the defense performance of</text>
<text top="704" left="49" width="251" height="9" font="font7" id="p5_t56" reading_order_no="55" segment_no="17" tag_type="text">the proposed method against backdoor attacks with different</text>
<text top="716" left="49" width="251" height="9" font="font7" id="p5_t57" reading_order_no="56" segment_no="17" tag_type="text">settings (trigger transparency, trigger size and trigger pattern).</text>
<text top="728" left="49" width="251" height="9" font="font7" id="p5_t58" reading_order_no="57" segment_no="17" tag_type="text">Last, performance comparisons between the proposed method</text>
<text top="740" left="49" width="242" height="9" font="font7" id="p5_t59" reading_order_no="58" segment_no="17" tag_type="text">and the existing backdoor detection technique is presented.</text>
<text top="58" left="312" width="93" height="9" font="font12" id="p5_t60" reading_order_no="59" segment_no="2" tag_type="title">A. Experimental Setup</text>
<text top="73" left="322" width="8" height="9" font="font12" id="p5_t61" reading_order_no="60" segment_no="3" tag_type="text">1)</text>
<text top="73" left="335" width="35" height="9" font="font29" id="p5_t62" reading_order_no="61" segment_no="3" tag_type="text">Datasets</text>
<text top="73" left="370" width="3" height="9" font="font12" id="p5_t63" reading_order_no="62" segment_no="3" tag_type="text">:</text>
<text top="73" left="378" width="185" height="9" font="font7" id="p5_t64" reading_order_no="63" segment_no="3" tag_type="text">We evaluate the proposed method on three</text>
<text top="85" left="312" width="251" height="9" font="font7" id="p5_t65" reading_order_no="64" segment_no="3" tag_type="text">benchmark datasets: Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and</text>
<text top="97" left="312" width="90" height="9" font="font7" id="p5_t66" reading_order_no="65" segment_no="3" tag_type="text">GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets.</text>
<text top="113" left="322" width="4" height="7" font="font10" id="p5_t67" reading_order_no="66" segment_no="5" tag_type="list">•</text>
<text top="111" left="332" width="69" height="9" font="font23" id="p5_t68" reading_order_no="67" segment_no="5" tag_type="list">Fashion-MNIST</text>
<text top="111" left="404" width="159" height="9" font="font7" id="p5_t69" reading_order_no="68" segment_no="5" tag_type="list"><a href="deeplearning_paper26.html#9">[9] </a>is a dataset consists of a training set</text>
<text top="123" left="332" width="231" height="9" font="font7" id="p5_t70" reading_order_no="69" segment_no="5" tag_type="list">with 60,000 images and a test set with 10,000 images.</text>
<text top="135" left="332" width="66" height="9" font="font7" id="p5_t71" reading_order_no="70" segment_no="5" tag_type="list">Each image is a</text>
<text top="135" left="401" width="10" height="9" font="font21" id="p5_t72" reading_order_no="71" segment_no="5" tag_type="list">28</text>
<text top="134" left="413" width="8" height="9" font="font13" id="p5_t73" reading_order_no="72" segment_no="5" tag_type="list">×</text>
<text top="135" left="423" width="10" height="9" font="font21" id="p5_t74" reading_order_no="73" segment_no="5" tag_type="list">28</text>
<text top="135" left="436" width="127" height="9" font="font7" id="p5_t75" reading_order_no="74" segment_no="5" tag_type="list">grayscale image, assigned with</text>
<text top="147" left="332" width="231" height="9" font="font7" id="p5_t76" reading_order_no="75" segment_no="5" tag_type="list">a label from 10 classes <a href="deeplearning_paper26.html#9">[9]. </a>In the experiment, the model</text>
<text top="159" left="332" width="166" height="9" font="font7" id="p5_t77" reading_order_no="76" segment_no="5" tag_type="list">trained on this dataset is DenseNet <a href="deeplearning_paper26.html#9">[25].</a></text>
<text top="172" left="322" width="4" height="7" font="font10" id="p5_t78" reading_order_no="77" segment_no="6" tag_type="list">•</text>
<text top="171" left="332" width="44" height="9" font="font23" id="p5_t79" reading_order_no="78" segment_no="6" tag_type="list">CIFAR-10</text>
<text top="171" left="381" width="182" height="9" font="font7" id="p5_t80" reading_order_no="79" segment_no="6" tag_type="list"><a href="deeplearning_paper26.html#9">[10] </a>consists of a training set with 50,000</text>
<text top="183" left="332" width="231" height="9" font="font7" id="p5_t81" reading_order_no="80" segment_no="6" tag_type="list">images and a test set with 10,000 images. Each image is</text>
<text top="195" left="332" width="4" height="9" font="font7" id="p5_t82" reading_order_no="81" segment_no="6" tag_type="list">a</text>
<text top="195" left="340" width="10" height="9" font="font21" id="p5_t83" reading_order_no="82" segment_no="6" tag_type="list">32</text>
<text top="194" left="352" width="8" height="9" font="font13" id="p5_t84" reading_order_no="83" segment_no="6" tag_type="list">×</text>
<text top="195" left="361" width="10" height="9" font="font21" id="p5_t85" reading_order_no="84" segment_no="6" tag_type="list">32</text>
<text top="195" left="375" width="188" height="9" font="font7" id="p5_t86" reading_order_no="85" segment_no="6" tag_type="list">colored image, belonging to one of 10 classes</text>
<text top="207" left="332" width="231" height="9" font="font7" id="p5_t87" reading_order_no="86" segment_no="6" tag_type="list"><a href="deeplearning_paper26.html#9">[10]. </a>In the experiment, the model trained on this dataset</text>
<text top="219" left="332" width="62" height="9" font="font7" id="p5_t88" reading_order_no="87" segment_no="6" tag_type="list">is ResNet <a href="deeplearning_paper26.html#9">[26].</a></text>
<text top="232" left="322" width="4" height="7" font="font10" id="p5_t89" reading_order_no="88" segment_no="8" tag_type="list">•</text>
<text top="231" left="332" width="34" height="9" font="font23" id="p5_t90" reading_order_no="89" segment_no="8" tag_type="list">GTSRB</text>
<text top="231" left="372" width="191" height="9" font="font7" id="p5_t91" reading_order_no="90" segment_no="8" tag_type="list"><a href="deeplearning_paper26.html#9">[11] </a>is a dataset containing 39,209 labeled</text>
<text top="243" left="332" width="231" height="9" font="font7" id="p5_t92" reading_order_no="91" segment_no="8" tag_type="list">images, which are categorized into 43 classes. GTSRB</text>
<text top="255" left="332" width="231" height="9" font="font7" id="p5_t93" reading_order_no="92" segment_no="8" tag_type="list">has 35,209 training images, 4,000 validation images and</text>
<text top="267" left="332" width="231" height="9" font="font7" id="p5_t94" reading_order_no="93" segment_no="8" tag_type="list">12,630 test images <a href="deeplearning_paper26.html#9">[11]. </a>In the experiment, the model</text>
<text top="279" left="332" width="160" height="9" font="font7" id="p5_t95" reading_order_no="94" segment_no="8" tag_type="list">trained on this dataset is AlexNet <a href="deeplearning_paper26.html#9">[27].</a></text>
<text top="292" left="322" width="8" height="9" font="font12" id="p5_t96" reading_order_no="95" segment_no="9" tag_type="text">2)</text>
<text top="292" left="335" width="176" height="9" font="font29" id="p5_t97" reading_order_no="96" segment_no="9" tag_type="text">Experimental Settings of Backdoor Attack</text>
<text top="292" left="511" width="3" height="9" font="font12" id="p5_t98" reading_order_no="97" segment_no="9" tag_type="text">:</text>
<text top="292" left="518" width="45" height="9" font="font7" id="p5_t99" reading_order_no="98" segment_no="9" tag_type="text">The trigger</text>
<text top="304" left="312" width="176" height="9" font="font7" id="p5_t100" reading_order_no="99" segment_no="9" tag_type="text">used in Fashion-MNIST <a href="deeplearning_paper26.html#9">[9] </a>images is four</text>
<text top="304" left="492" width="5" height="9" font="font21" id="p5_t101" reading_order_no="100" segment_no="9" tag_type="text">1</text>
<text top="304" left="499" width="8" height="9" font="font13" id="p5_t102" reading_order_no="101" segment_no="9" tag_type="text">×</text>
<text top="304" left="509" width="10" height="9" font="font21" id="p5_t103" reading_order_no="102" segment_no="9" tag_type="text">10</text>
<text top="304" left="523" width="40" height="9" font="font7" id="p5_t104" reading_order_no="103" segment_no="9" tag_type="text">rectangles</text>
<text top="316" left="312" width="251" height="9" font="font7" id="p5_t105" reading_order_no="104" segment_no="9" tag_type="text">placed at the corners (four corners in total) of the image,</text>
<text top="328" left="312" width="251" height="9" font="font7" id="p5_t106" reading_order_no="105" segment_no="9" tag_type="text">and the intensity of the trigger is 0.15. The trigger used in</text>
<text top="340" left="312" width="191" height="9" font="font7" id="p5_t107" reading_order_no="106" segment_no="9" tag_type="text">CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>images is a</text>
<text top="340" left="507" width="5" height="9" font="font21" id="p5_t108" reading_order_no="107" segment_no="9" tag_type="text">4</text>
<text top="339" left="515" width="8" height="9" font="font13" id="p5_t109" reading_order_no="108" segment_no="9" tag_type="text">×</text>
<text top="340" left="525" width="5" height="9" font="font21" id="p5_t110" reading_order_no="109" segment_no="9" tag_type="text">4</text>
<text top="340" left="535" width="28" height="9" font="font7" id="p5_t111" reading_order_no="110" segment_no="9" tag_type="text">square.</text>
<text top="352" left="312" width="251" height="9" font="font7" id="p5_t112" reading_order_no="111" segment_no="9" tag_type="text">The intensities of triggers in CIFAR-10 and GTSRB are set</text>
<text top="364" left="312" width="251" height="9" font="font7" id="p5_t113" reading_order_no="112" segment_no="9" tag_type="text">to be 0.5 and 0.2 respectively. Some backdoor instances used</text>
<text top="376" left="312" width="176" height="9" font="font7" id="p5_t114" reading_order_no="113" segment_no="9" tag_type="text">in the experiments are illustrated in Fig. <a href="deeplearning_paper26.html#5">3.</a></text>
<text top="466" left="340" width="10" height="8" font="font30" id="p5_t115" reading_order_no="114" segment_no="10" tag_type="figure">(a)</text>
<text top="466" left="523" width="10" height="8" font="font30" id="p5_t116" reading_order_no="116" segment_no="10" tag_type="figure">(c)</text>
<text top="466" left="430" width="11" height="8" font="font30" id="p5_t117" reading_order_no="115" segment_no="10" tag_type="figure">(b)</text>
<text top="488" left="312" width="22" height="7" font="font8" id="p5_t118" reading_order_no="117" segment_no="11" tag_type="text">Fig. 3.</text>
<text top="488" left="344" width="219" height="7" font="font8" id="p5_t119" reading_order_no="118" segment_no="11" tag_type="text">Examples of backdoor instances: (a) Fashion-MNIST images; (b)</text>
<text top="497" left="312" width="129" height="7" font="font8" id="p5_t120" reading_order_no="119" segment_no="11" tag_type="text">CIFAR-10 images; (c) GTSRB images.</text>
<text top="519" left="322" width="8" height="9" font="font12" id="p5_t121" reading_order_no="120" segment_no="12" tag_type="text">3)</text>
<text top="518" left="335" width="31" height="9" font="font29" id="p5_t122" reading_order_no="121" segment_no="12" tag_type="text">Metrics</text>
<text top="519" left="366" width="3" height="9" font="font12" id="p5_t123" reading_order_no="122" segment_no="12" tag_type="text">:</text>
<text top="518" left="376" width="187" height="9" font="font23" id="p5_t124" reading_order_no="123" segment_no="12" tag_type="text">Backdoor Attack Success Rate (BASR).</text>
<text top="530" left="312" width="251" height="9" font="font7" id="p5_t125" reading_order_no="124" segment_no="12" tag_type="text">Backdoor Attack Success Rate is defined as the percentage</text>
<text top="542" left="312" width="251" height="9" font="font7" id="p5_t126" reading_order_no="125" segment_no="12" tag_type="text">of backdoor instances that are successfully classified as the</text>
<text top="554" left="312" width="188" height="9" font="font7" id="p5_t127" reading_order_no="126" segment_no="12" tag_type="text">target class among all backdoor instances <a href="deeplearning_paper26.html#9">[1].</a></text>
<text top="566" left="322" width="152" height="9" font="font23" id="p5_t128" reading_order_no="127" segment_no="14" tag_type="text">Backdoor Detection Rate (BDR).</text>
<text top="566" left="480" width="83" height="9" font="font7" id="p5_t129" reading_order_no="128" segment_no="14" tag_type="text">Backdoor Detection</text>
<text top="578" left="312" width="251" height="9" font="font7" id="p5_t130" reading_order_no="129" segment_no="14" tag_type="text">Rate is defined as the percentage of backdoor instances that</text>
<text top="590" left="312" width="251" height="9" font="font7" id="p5_t131" reading_order_no="130" segment_no="14" tag_type="text">are successfully detected by the proposed method among all</text>
<text top="602" left="312" width="80" height="9" font="font7" id="p5_t132" reading_order_no="131" segment_no="14" tag_type="text">backdoor instances.</text>
<text top="614" left="322" width="183" height="9" font="font23" id="p5_t133" reading_order_no="132" segment_no="15" tag_type="text">Clean Image Identification Rate (CIIR).</text>
<text top="614" left="510" width="53" height="9" font="font7" id="p5_t134" reading_order_no="133" segment_no="15" tag_type="text">Clean Image</text>
<text top="626" left="312" width="251" height="9" font="font7" id="p5_t135" reading_order_no="134" segment_no="15" tag_type="text">Identification Rate is defined as the percentage of clean images</text>
<text top="638" left="312" width="251" height="9" font="font7" id="p5_t136" reading_order_no="135" segment_no="15" tag_type="text">that are correctly classified as clean ones among all clean</text>
<text top="650" left="312" width="31" height="9" font="font7" id="p5_t137" reading_order_no="136" segment_no="15" tag_type="text">images.</text>
<text top="677" left="312" width="167" height="9" font="font12" id="p5_t138" reading_order_no="137" segment_no="18" tag_type="title">B. Effectiveness of the Proposed Method</text>
<text top="692" left="322" width="241" height="9" font="font7" id="p5_t139" reading_order_no="138" segment_no="19" tag_type="text">In this section, the defense performances of the proposed</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p5_t140" reading_order_no="139" segment_no="19" tag_type="text">method on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB</text>
<text top="716" left="312" width="111" height="9" font="font7" id="p5_t141" reading_order_no="140" segment_no="19" tag_type="text"><a href="deeplearning_paper26.html#9">[11] </a>datasets are presented.</text>
<text top="728" left="322" width="241" height="9" font="font7" id="p5_t142" reading_order_no="141" segment_no="20" tag_type="text">Table <a href="deeplearning_paper26.html#6">I </a>shows the backdoor attack success rate of the back-</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p5_t143" reading_order_no="142" segment_no="20" tag_type="text">door attack on the Fashion-MNIST, CIFAR-10 and GTSRB</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font31" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="text">6</text>
<text top="58" left="49" width="251" height="9" font="font7" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">datasets and the corresponding classification accuracy of the</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p6_t3" reading_order_no="2" segment_no="1" tag_type="text">backdoored model. For each dataset, all test images are in-</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p6_t4" reading_order_no="3" segment_no="1" tag_type="text">jected with trigger to evaluate the backdoor attack success rate.</text>
<text top="94" left="49" width="251" height="9" font="font7" id="p6_t5" reading_order_no="4" segment_no="1" tag_type="text">The classification accuracy is evaluated on all the clean test</text>
<text top="106" left="49" width="251" height="9" font="font7" id="p6_t6" reading_order_no="5" segment_no="1" tag_type="text">images for each dataset. As shown in Table <a href="deeplearning_paper26.html#6">I, </a>the classification</text>
<text top="118" left="49" width="251" height="9" font="font7" id="p6_t7" reading_order_no="6" segment_no="1" tag_type="text">accuracy on clean images of the backdoored model is 92.19%,</text>
<text top="130" left="49" width="251" height="9" font="font7" id="p6_t8" reading_order_no="7" segment_no="1" tag_type="text">92.77% and 95.16% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="142" left="49" width="251" height="9" font="font7" id="p6_t9" reading_order_no="8" segment_no="1" tag_type="text">and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively. Without the proposed defense</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p6_t10" reading_order_no="9" segment_no="1" tag_type="text">method, the backdoor attack success rate (BASR) is 99.47%,</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p6_t11" reading_order_no="10" segment_no="1" tag_type="text">99.77% and 97.89% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="178" left="49" width="123" height="9" font="font7" id="p6_t12" reading_order_no="11" segment_no="1" tag_type="text">and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively.</text>
<text top="197" left="159" width="30" height="7" font="font8" id="p6_t13" reading_order_no="12" segment_no="4" tag_type="title">TABLE I</text>
<text top="206" left="51" width="5" height="7" font="font8" id="p6_t14" reading_order_no="13" segment_no="5" tag_type="text">C</text>
<text top="207" left="57" width="241" height="6" font="font31" id="p6_t15" reading_order_no="14" segment_no="5" tag_type="text">LASSIFICATION ACCURACY AND BACKDOOR ATTACK SUCCESS RATE ON</text>
<text top="216" left="59" width="231" height="6" font="font31" id="p6_t16" reading_order_no="15" segment_no="5" tag_type="text">THREE DIFFERENT CLASSIFICATION TASKS WITHOUT THE PROPOSED</text>
<text top="225" left="156" width="36" height="6" font="font31" id="p6_t17" reading_order_no="16" segment_no="5" tag_type="text">APPROACH</text>
<text top="247" left="90" width="77" height="9" font="font7" id="p6_t18" reading_order_no="17" segment_no="6" tag_type="table">Benchmark dataset</text>
<text top="247" left="198" width="38" height="9" font="font7" id="p6_t19" reading_order_no="18" segment_no="6" tag_type="table">Accuracy</text>
<text top="247" left="250" width="26" height="9" font="font7" id="p6_t20" reading_order_no="19" segment_no="6" tag_type="table">BASR</text>
<text top="264" left="71" width="115" height="9" font="font7" id="p6_t21" reading_order_no="20" segment_no="6" tag_type="table">Fashion-MNIST (DenseNet)</text>
<text top="264" left="201" width="31" height="9" font="font7" id="p6_t22" reading_order_no="21" segment_no="6" tag_type="table">92.19%</text>
<text top="264" left="248" width="31" height="9" font="font7" id="p6_t23" reading_order_no="22" segment_no="6" tag_type="table">99.47%</text>
<text top="276" left="88" width="81" height="9" font="font7" id="p6_t24" reading_order_no="23" segment_no="6" tag_type="table">CIFAR-10 (ResNet)</text>
<text top="276" left="201" width="31" height="9" font="font7" id="p6_t25" reading_order_no="24" segment_no="6" tag_type="table">92.77%</text>
<text top="276" left="248" width="31" height="9" font="font7" id="p6_t26" reading_order_no="25" segment_no="6" tag_type="table">99.77%</text>
<text top="288" left="90" width="76" height="9" font="font7" id="p6_t27" reading_order_no="26" segment_no="6" tag_type="table">GTSRB (AlexNet)</text>
<text top="288" left="201" width="31" height="9" font="font7" id="p6_t28" reading_order_no="27" segment_no="6" tag_type="table">95.16%</text>
<text top="288" left="248" width="31" height="9" font="font7" id="p6_t29" reading_order_no="28" segment_no="6" tag_type="table">97.89%</text>
<text top="314" left="59" width="241" height="9" font="font7" id="p6_t30" reading_order_no="29" segment_no="7" tag_type="text">Table <a href="deeplearning_paper26.html#6">II </a>shows the clean image identification rate, the</text>
<text top="326" left="49" width="251" height="9" font="font7" id="p6_t31" reading_order_no="30" segment_no="7" tag_type="text">backdoor detection rate and the intensity of universal pertur-</text>
<text top="338" left="49" width="251" height="9" font="font7" id="p6_t32" reading_order_no="31" segment_no="7" tag_type="text">bation on three datasets after the proposed method is applied.</text>
<text top="350" left="49" width="251" height="9" font="font7" id="p6_t33" reading_order_no="32" segment_no="7" tag_type="text">In this paper, after the proposed method is deployed, the</text>
<text top="362" left="49" width="251" height="9" font="font7" id="p6_t34" reading_order_no="33" segment_no="7" tag_type="text">clean image identification rate is calculated on a set of 2,000</text>
<text top="374" left="49" width="251" height="9" font="font7" id="p6_t35" reading_order_no="34" segment_no="7" tag_type="text">clean images randomly selected from the test images for each</text>
<text top="386" left="49" width="251" height="9" font="font7" id="p6_t36" reading_order_no="35" segment_no="7" tag_type="text">dataset. Similarly, in this paper, the backdoor detection rate</text>
<text top="398" left="49" width="251" height="9" font="font7" id="p6_t37" reading_order_no="36" segment_no="7" tag_type="text">is calculated on a set of 2,000 backdoor instances generated</text>
<text top="410" left="49" width="251" height="9" font="font7" id="p6_t38" reading_order_no="37" segment_no="7" tag_type="text">by adding trigger to 2,000 images randomly selected from the</text>
<text top="422" left="49" width="251" height="9" font="font7" id="p6_t39" reading_order_no="38" segment_no="7" tag_type="text">test images for each dataset. As shown in Table <a href="deeplearning_paper26.html#6">II, </a>after the</text>
<text top="434" left="49" width="251" height="9" font="font7" id="p6_t40" reading_order_no="39" segment_no="7" tag_type="text">proposed defense method is deployed, the backdoor detection</text>
<text top="446" left="49" width="251" height="9" font="font7" id="p6_t41" reading_order_no="40" segment_no="7" tag_type="text">rate is 99.63%, 99.76% and 99.91% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9],</a></text>
<text top="458" left="49" width="251" height="9" font="font7" id="p6_t42" reading_order_no="41" segment_no="7" tag_type="text">CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively. Meanwhile,</text>
<text top="470" left="49" width="251" height="9" font="font7" id="p6_t43" reading_order_no="42" segment_no="7" tag_type="text">the clean image identification rate (CIIR) of the proposed</text>
<text top="482" left="49" width="251" height="9" font="font7" id="p6_t44" reading_order_no="43" segment_no="7" tag_type="text">method is 90.66%, 89.82% and 98.85% on Fashion-MNIST</text>
<text top="494" left="49" width="206" height="9" font="font7" id="p6_t45" reading_order_no="44" segment_no="7" tag_type="text"><a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively.</text>
<text top="513" left="158" width="33" height="7" font="font8" id="p6_t46" reading_order_no="45" segment_no="11" tag_type="title">TABLE II</text>
<text top="522" left="54" width="5" height="7" font="font8" id="p6_t47" reading_order_no="46" segment_no="12" tag_type="text">T</text>
<text top="523" left="59" width="110" height="6" font="font31" id="p6_t48" reading_order_no="47" segment_no="12" tag_type="text">HE BACKDOOR DETECTION RATE</text>
<text top="522" left="170" width="2" height="7" font="font8" id="p6_t49" reading_order_no="48" segment_no="12" tag_type="text">,</text>
<text top="523" left="174" width="120" height="6" font="font31" id="p6_t50" reading_order_no="49" segment_no="12" tag_type="text">THE CLEAN IMAGE IDENTIFICATION</text>
<text top="532" left="55" width="239" height="6" font="font31" id="p6_t51" reading_order_no="50" segment_no="12" tag_type="text">RATE AND THE PERTURBATION INTENSITY ON THREE DATASETS AFTER</text>
<text top="541" left="97" width="154" height="6" font="font31" id="p6_t52" reading_order_no="51" segment_no="12" tag_type="text">THE PROPOSED DEFENSE METHOD IS APPLIED</text>
<text top="563" left="93" width="30" height="9" font="font7" id="p6_t53" reading_order_no="52" segment_no="14" tag_type="table">Dataset</text>
<text top="563" left="166" width="20" height="9" font="font7" id="p6_t54" reading_order_no="53" segment_no="14" tag_type="table">CIIR</text>
<text top="563" left="209" width="20" height="9" font="font7" id="p6_t55" reading_order_no="54" segment_no="14" tag_type="table">BDR</text>
<text top="563" left="246" width="35" height="9" font="font7" id="p6_t56" reading_order_no="55" segment_no="14" tag_type="table">Intensity</text>
<text top="580" left="68" width="81" height="9" font="font7" id="p6_t57" reading_order_no="56" segment_no="14" tag_type="table">Fashion-MNIST <a href="deeplearning_paper26.html#9">[9]</a></text>
<text top="580" left="161" width="31" height="9" font="font7" id="p6_t58" reading_order_no="57" segment_no="14" tag_type="table">90.66%</text>
<text top="580" left="203" width="31" height="9" font="font7" id="p6_t59" reading_order_no="58" segment_no="14" tag_type="table">99.63%</text>
<text top="580" left="250" width="27" height="9" font="font7" id="p6_t60" reading_order_no="59" segment_no="14" tag_type="table">2.8715</text>
<text top="592" left="77" width="62" height="9" font="font7" id="p6_t61" reading_order_no="60" segment_no="14" tag_type="table">CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="592" left="161" width="31" height="9" font="font7" id="p6_t62" reading_order_no="61" segment_no="14" tag_type="table">89.82%</text>
<text top="592" left="203" width="31" height="9" font="font7" id="p6_t63" reading_order_no="62" segment_no="14" tag_type="table">99.76%</text>
<text top="592" left="250" width="27" height="9" font="font7" id="p6_t64" reading_order_no="63" segment_no="14" tag_type="table">3.0513</text>
<text top="604" left="82" width="52" height="9" font="font7" id="p6_t65" reading_order_no="64" segment_no="14" tag_type="table">GTSRB <a href="deeplearning_paper26.html#9">[11]</a></text>
<text top="604" left="161" width="31" height="9" font="font7" id="p6_t66" reading_order_no="65" segment_no="14" tag_type="table">98.85%</text>
<text top="604" left="203" width="31" height="9" font="font7" id="p6_t67" reading_order_no="66" segment_no="14" tag_type="table">99.91%</text>
<text top="604" left="250" width="27" height="9" font="font7" id="p6_t68" reading_order_no="67" segment_no="14" tag_type="table">2.4362</text>
<text top="630" left="59" width="241" height="9" font="font7" id="p6_t69" reading_order_no="68" segment_no="15" tag_type="text">Overall, experimental results show that the proposed defense</text>
<text top="642" left="49" width="251" height="9" font="font7" id="p6_t70" reading_order_no="69" segment_no="15" tag_type="text">method can effectively detect backdoor attacks on different</text>
<text top="654" left="49" width="251" height="9" font="font7" id="p6_t71" reading_order_no="70" segment_no="15" tag_type="text">datasets and DNN architectures. In the three datasets, the</text>
<text top="666" left="49" width="251" height="9" font="font7" id="p6_t72" reading_order_no="71" segment_no="15" tag_type="text">proposed method can achieve high backdoor detection rate</text>
<text top="678" left="49" width="165" height="9" font="font7" id="p6_t73" reading_order_no="72" segment_no="15" tag_type="text">and high clean image identification rate.</text>
<text top="701" left="49" width="251" height="9" font="font12" id="p6_t74" reading_order_no="73" segment_no="17" tag_type="title">C. Defense Performance of the Proposed Method under Dif-</text>
<text top="713" left="49" width="87" height="9" font="font12" id="p6_t75" reading_order_no="74" segment_no="17" tag_type="title">ferent Attack Settings</text>
<text top="728" left="59" width="241" height="9" font="font7" id="p6_t76" reading_order_no="75" segment_no="18" tag_type="text">In this section, we evaluate the performance of the proposed</text>
<text top="740" left="49" width="251" height="9" font="font7" id="p6_t77" reading_order_no="76" segment_no="18" tag_type="text">method under different trigger settings (trigger transparency</text>
<text top="58" left="312" width="153" height="9" font="font7" id="p6_t78" reading_order_no="77" segment_no="2" tag_type="text"><a href="deeplearning_paper26.html#9">[12], </a>trigger size and trigger pattern).</text>
<text top="71" left="322" width="8" height="9" font="font12" id="p6_t79" reading_order_no="78" segment_no="3" tag_type="text">1)</text>
<text top="71" left="335" width="92" height="9" font="font29" id="p6_t80" reading_order_no="79" segment_no="3" tag_type="text">Trigger Transparency</text>
<text top="71" left="427" width="3" height="9" font="font12" id="p6_t81" reading_order_no="80" segment_no="3" tag_type="text">:</text>
<text top="71" left="434" width="129" height="9" font="font7" id="p6_t82" reading_order_no="81" segment_no="3" tag_type="text">In the experiment, we evaluate</text>
<text top="83" left="312" width="251" height="9" font="font7" id="p6_t83" reading_order_no="82" segment_no="3" tag_type="text">the performance of the proposed method against backdoor</text>
<text top="95" left="312" width="251" height="9" font="font7" id="p6_t84" reading_order_no="83" segment_no="3" tag_type="text">attacks with different trigger transparency settings <a href="deeplearning_paper26.html#9">[12]. </a>The</text>
<text top="107" left="312" width="251" height="9" font="font7" id="p6_t85" reading_order_no="84" segment_no="3" tag_type="text">values of the trigger transparency in the experiment are set</text>
<text top="119" left="312" width="251" height="9" font="font7" id="p6_t86" reading_order_no="85" segment_no="3" tag_type="text">to be 50%, 60%, 70% and 80%, respectively. As shown</text>
<text top="131" left="312" width="251" height="9" font="font7" id="p6_t87" reading_order_no="86" segment_no="3" tag_type="text">in Table <a href="deeplearning_paper26.html#6">III, </a>for the backdoor attacks with different trigger</text>
<text top="143" left="312" width="251" height="9" font="font7" id="p6_t88" reading_order_no="87" segment_no="3" tag_type="text">transparency settings, the backdoor detection rates are all at</text>
<text top="155" left="312" width="251" height="9" font="font7" id="p6_t89" reading_order_no="88" segment_no="3" tag_type="text">a high level on the three datasets. Specifically, when the</text>
<text top="167" left="312" width="251" height="9" font="font7" id="p6_t90" reading_order_no="89" segment_no="3" tag_type="text">trigger transparency is 50%, the backdoor detection rate is</text>
<text top="179" left="312" width="251" height="9" font="font7" id="p6_t91" reading_order_no="90" segment_no="3" tag_type="text">98.80%, 99.70% and 99.96% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-</text>
<text top="191" left="312" width="251" height="9" font="font7" id="p6_t92" reading_order_no="91" segment_no="3" tag_type="text">10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets respectively. When the</text>
<text top="202" left="312" width="251" height="9" font="font7" id="p6_t93" reading_order_no="92" segment_no="3" tag_type="text">trigger transparency increases to 80%, after the proposed</text>
<text top="214" left="312" width="251" height="9" font="font7" id="p6_t94" reading_order_no="93" segment_no="3" tag_type="text">method is applied, the backdoor detection rate is still at a</text>
<text top="226" left="312" width="251" height="9" font="font7" id="p6_t95" reading_order_no="94" segment_no="3" tag_type="text">high level (99.37%, 96.30% and 99.07% on Fashion-MNIST,</text>
<text top="238" left="312" width="251" height="9" font="font7" id="p6_t96" reading_order_no="95" segment_no="3" tag_type="text">CIFAR-10 and GTSRB datasets respectively). The experimen-</text>
<text top="250" left="312" width="251" height="9" font="font7" id="p6_t97" reading_order_no="96" segment_no="3" tag_type="text">tal results indicate that, the proposed defense method can</text>
<text top="262" left="312" width="251" height="9" font="font7" id="p6_t98" reading_order_no="97" segment_no="3" tag_type="text">effectively detect the backdoor instances with different trigger</text>
<text top="274" left="312" width="251" height="9" font="font7" id="p6_t99" reading_order_no="98" segment_no="3" tag_type="text">transparency settings. The reason is as follows. When the</text>
<text top="286" left="312" width="251" height="9" font="font7" id="p6_t100" reading_order_no="99" segment_no="3" tag_type="text">trigger transparency is set to be 0%, the trigger is opaque.</text>
<text top="298" left="312" width="251" height="9" font="font7" id="p6_t101" reading_order_no="100" segment_no="3" tag_type="text">When the trigger transparency is set to be 90%, the trigger</text>
<text top="310" left="312" width="251" height="9" font="font7" id="p6_t102" reading_order_no="101" segment_no="3" tag_type="text">is almost invisible. Generally, the higher the transparency of</text>
<text top="322" left="312" width="251" height="9" font="font7" id="p6_t103" reading_order_no="102" segment_no="3" tag_type="text">the trigger, the trigger is more susceptible to the perturbation.</text>
<text top="334" left="312" width="251" height="9" font="font7" id="p6_t104" reading_order_no="103" segment_no="3" tag_type="text">However, UAP <a href="deeplearning_paper26.html#9">[13] </a>is a kind of image-agnostic perturbation,</text>
<text top="346" left="312" width="251" height="9" font="font7" id="p6_t105" reading_order_no="104" segment_no="3" tag_type="text">which is generated based on clean images. Therefore, when</text>
<text top="358" left="312" width="251" height="9" font="font7" id="p6_t106" reading_order_no="105" segment_no="3" tag_type="text">UAP is added to a backdoor instance, the trigger in the</text>
<text top="370" left="312" width="251" height="9" font="font7" id="p6_t107" reading_order_no="106" segment_no="3" tag_type="text">backdoor instance will only be slightly affected. As a result,</text>
<text top="382" left="312" width="194" height="9" font="font7" id="p6_t108" reading_order_no="107" segment_no="3" tag_type="text">even the transparency of trigger is high (50%</text>
<text top="381" left="510" width="8" height="9" font="font13" id="p6_t109" reading_order_no="108" segment_no="3" tag_type="text">∼</text>
<text top="382" left="522" width="41" height="9" font="font7" id="p6_t110" reading_order_no="109" segment_no="3" tag_type="text">80%), the</text>
<text top="394" left="312" width="251" height="9" font="font7" id="p6_t111" reading_order_no="110" segment_no="3" tag_type="text">proposed method can still achieve high backdoor detection</text>
<text top="406" left="312" width="17" height="9" font="font7" id="p6_t112" reading_order_no="111" segment_no="3" tag_type="text">rate.</text>
<text top="426" left="420" width="36" height="7" font="font8" id="p6_t113" reading_order_no="112" segment_no="8" tag_type="title">TABLE III</text>
<text top="435" left="316" width="5" height="7" font="font8" id="p6_t114" reading_order_no="113" segment_no="9" tag_type="text">T</text>
<text top="436" left="321" width="238" height="6" font="font31" id="p6_t115" reading_order_no="114" segment_no="9" tag_type="text">HE BACKDOOR DETECTION RATE OF THE PROPOSED METHOD AGAINST</text>
<text top="445" left="328" width="220" height="6" font="font31" id="p6_t116" reading_order_no="115" segment_no="9" tag_type="text">BACKDOOR ATTACKS WITH DIFFERENT TRIGGER TRANSPARENCY</text>
<text top="454" left="377" width="121" height="6" font="font31" id="p6_t117" reading_order_no="116" segment_no="9" tag_type="text">SETTINGS ON THE THREE DATASETS</text>
<text top="475" left="323" width="54" height="9" font="font7" id="p6_t118" reading_order_no="117" segment_no="10" tag_type="table">Transparency</text>
<text top="475" left="389" width="66" height="9" font="font7" id="p6_t119" reading_order_no="118" segment_no="10" tag_type="table">Fashion-MNIST</text>
<text top="475" left="466" width="42" height="9" font="font7" id="p6_t120" reading_order_no="119" segment_no="10" tag_type="table">CIFAR-10</text>
<text top="475" left="520" width="32" height="9" font="font7" id="p6_t121" reading_order_no="120" segment_no="10" tag_type="table">GTSRB</text>
<text top="493" left="341" width="18" height="9" font="font7" id="p6_t122" reading_order_no="121" segment_no="10" tag_type="table">80%</text>
<text top="493" left="406" width="31" height="9" font="font7" id="p6_t123" reading_order_no="122" segment_no="10" tag_type="table">99.37%</text>
<text top="493" left="472" width="31" height="9" font="font7" id="p6_t124" reading_order_no="123" segment_no="10" tag_type="table">96.30%</text>
<text top="493" left="521" width="31" height="9" font="font7" id="p6_t125" reading_order_no="124" segment_no="10" tag_type="table">99.07%</text>
<text top="504" left="341" width="18" height="9" font="font7" id="p6_t126" reading_order_no="125" segment_no="10" tag_type="table">70%</text>
<text top="504" left="406" width="31" height="9" font="font7" id="p6_t127" reading_order_no="126" segment_no="10" tag_type="table">99.72%</text>
<text top="504" left="472" width="31" height="9" font="font7" id="p6_t128" reading_order_no="127" segment_no="10" tag_type="table">98.75%</text>
<text top="504" left="521" width="31" height="9" font="font7" id="p6_t129" reading_order_no="128" segment_no="10" tag_type="table">98.62%</text>
<text top="516" left="341" width="18" height="9" font="font7" id="p6_t130" reading_order_no="129" segment_no="10" tag_type="table">60%</text>
<text top="516" left="406" width="31" height="9" font="font7" id="p6_t131" reading_order_no="130" segment_no="10" tag_type="table">95.40%</text>
<text top="516" left="472" width="31" height="9" font="font7" id="p6_t132" reading_order_no="131" segment_no="10" tag_type="table">98.62%</text>
<text top="516" left="521" width="31" height="9" font="font7" id="p6_t133" reading_order_no="132" segment_no="10" tag_type="table">99.90%</text>
<text top="528" left="341" width="18" height="9" font="font7" id="p6_t134" reading_order_no="133" segment_no="10" tag_type="table">50%</text>
<text top="528" left="406" width="31" height="9" font="font7" id="p6_t135" reading_order_no="134" segment_no="10" tag_type="table">98.80%</text>
<text top="528" left="472" width="31" height="9" font="font7" id="p6_t136" reading_order_no="135" segment_no="10" tag_type="table">99.70%</text>
<text top="528" left="521" width="31" height="9" font="font7" id="p6_t137" reading_order_no="136" segment_no="10" tag_type="table">99.96%</text>
<text top="560" left="322" width="8" height="9" font="font12" id="p6_t138" reading_order_no="137" segment_no="13" tag_type="text">2)</text>
<text top="560" left="335" width="53" height="9" font="font29" id="p6_t139" reading_order_no="138" segment_no="13" tag_type="text">Trigger Size</text>
<text top="560" left="388" width="3" height="9" font="font12" id="p6_t140" reading_order_no="139" segment_no="13" tag_type="text">:</text>
<text top="560" left="396" width="167" height="9" font="font7" id="p6_t141" reading_order_no="140" segment_no="13" tag_type="text">In this section, we evaluate the perfor-</text>
<text top="572" left="312" width="251" height="9" font="font7" id="p6_t142" reading_order_no="141" segment_no="13" tag_type="text">mance of the proposed method against backdoor attacks with</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p6_t143" reading_order_no="142" segment_no="13" tag_type="text">three different trigger sizes. The experiment results are shown</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p6_t144" reading_order_no="143" segment_no="13" tag_type="text">in Table <a href="deeplearning_paper26.html#7">IV. </a>As shown in Table <a href="deeplearning_paper26.html#7">IV, </a>for different trigger sizes,</text>
<text top="608" left="312" width="251" height="9" font="font7" id="p6_t145" reading_order_no="144" segment_no="13" tag_type="text">the proposed method can achieve very high backdoor detection</text>
<text top="620" left="312" width="251" height="9" font="font7" id="p6_t146" reading_order_no="145" segment_no="13" tag_type="text">rates (over 99% mostly). Even if the trigger is small, such as</text>
<text top="631" left="312" width="5" height="9" font="font21" id="p6_t147" reading_order_no="146" segment_no="13" tag_type="text">1</text>
<text top="631" left="318" width="8" height="9" font="font13" id="p6_t148" reading_order_no="147" segment_no="13" tag_type="text">×</text>
<text top="631" left="327" width="5" height="9" font="font21" id="p6_t149" reading_order_no="148" segment_no="13" tag_type="text">4</text>
<text top="632" left="332" width="2" height="9" font="font7" id="p6_t150" reading_order_no="149" segment_no="13" tag_type="text">,</text>
<text top="631" left="338" width="5" height="9" font="font21" id="p6_t151" reading_order_no="150" segment_no="13" tag_type="text">2</text>
<text top="631" left="344" width="8" height="9" font="font13" id="p6_t152" reading_order_no="151" segment_no="13" tag_type="text">×</text>
<text top="631" left="353" width="5" height="9" font="font21" id="p6_t153" reading_order_no="152" segment_no="13" tag_type="text">2</text>
<text top="632" left="358" width="2" height="9" font="font7" id="p6_t154" reading_order_no="153" segment_no="13" tag_type="text">,</text>
<text top="631" left="364" width="5" height="9" font="font21" id="p6_t155" reading_order_no="154" segment_no="13" tag_type="text">2</text>
<text top="631" left="370" width="8" height="9" font="font13" id="p6_t156" reading_order_no="155" segment_no="13" tag_type="text">×</text>
<text top="631" left="379" width="5" height="9" font="font21" id="p6_t157" reading_order_no="156" segment_no="13" tag_type="text">2</text>
<text top="632" left="387" width="176" height="9" font="font7" id="p6_t158" reading_order_no="157" segment_no="13" tag_type="text">in Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and</text>
<text top="643" left="312" width="251" height="9" font="font7" id="p6_t159" reading_order_no="158" segment_no="13" tag_type="text">GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively, the proposed method can</text>
<text top="655" left="312" width="251" height="9" font="font7" id="p6_t160" reading_order_no="159" segment_no="13" tag_type="text">still achieve high backdoor detection rates (99.65%, 99.07%,</text>
<text top="667" left="312" width="88" height="9" font="font7" id="p6_t161" reading_order_no="160" segment_no="13" tag_type="text">98.25% respectively).</text>
<text top="680" left="322" width="8" height="9" font="font12" id="p6_t162" reading_order_no="161" segment_no="16" tag_type="text">3)</text>
<text top="680" left="335" width="68" height="9" font="font29" id="p6_t163" reading_order_no="162" segment_no="16" tag_type="text">Trigger Pattern</text>
<text top="680" left="403" width="3" height="9" font="font12" id="p6_t164" reading_order_no="163" segment_no="16" tag_type="text">:</text>
<text top="680" left="413" width="150" height="9" font="font7" id="p6_t165" reading_order_no="164" segment_no="16" tag_type="text">The performance of the proposed</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p6_t166" reading_order_no="165" segment_no="16" tag_type="text">method against backdoor attacks with different trigger patterns</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p6_t167" reading_order_no="166" segment_no="16" tag_type="text">is also evaluated. In the experiment, the square trigger and</text>
<text top="716" left="312" width="153" height="9" font="font7" id="p6_t168" reading_order_no="167" segment_no="16" tag_type="text">cross pattern trigger (referred to as</text>
<text top="716" left="471" width="39" height="9" font="font12" id="p6_t169" reading_order_no="168" segment_no="16" tag_type="text">trigger A</text>
<text top="716" left="516" width="14" height="9" font="font7" id="p6_t170" reading_order_no="169" segment_no="16" tag_type="text">and</text>
<text top="716" left="536" width="27" height="9" font="font12" id="p6_t171" reading_order_no="170" segment_no="16" tag_type="text">trigger</text>
<text top="728" left="312" width="6" height="9" font="font12" id="p6_t172" reading_order_no="171" segment_no="16" tag_type="text">B</text>
<text top="728" left="323" width="240" height="9" font="font7" id="p6_t173" reading_order_no="172" segment_no="16" tag_type="text">respectively) are used to evaluate the proposed method.</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p6_t174" reading_order_no="173" segment_no="16" tag_type="text">The square trigger and the cross pattern trigger for the three</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font32" size="10" family="TimesNewRomanPSMT" color="#000000"/>
	<fontspec id="font33" size="10" family="TimesNewRomanPS,Italic" color="#000000"/>
	<fontspec id="font34" size="8" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font35" size="9" family="TimesNewRomanPS,Italic" color="#000000"/>
<text top="30" left="563" width="3" height="6" font="font0" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="text">7</text>
<text top="53" left="288" width="36" height="7" font="font8" id="p7_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE IV</text>
<text top="62" left="132" width="5" height="7" font="font8" id="p7_t3" reading_order_no="2" segment_no="2" tag_type="text">B</text>
<text top="63" left="138" width="342" height="6" font="font31" id="p7_t4" reading_order_no="3" segment_no="2" tag_type="text">ACKDOOR DETECTION RATES OF THE BACKDOOR ATTACKS WITH DIFFERENT TRIGGER SIZE SETTINGS</text>
<text top="81" left="96" width="30" height="9" font="font7" id="p7_t5" reading_order_no="4" segment_no="3" tag_type="table">Dataset</text>
<text top="81" left="156" width="81" height="9" font="font7" id="p7_t6" reading_order_no="5" segment_no="3" tag_type="table">Fashion-MNIST <a href="deeplearning_paper26.html#9">[9]</a></text>
<text top="81" left="294" width="62" height="9" font="font7" id="p7_t7" reading_order_no="6" segment_no="3" tag_type="table">CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="81" left="428" width="52" height="9" font="font7" id="p7_t8" reading_order_no="7" segment_no="3" tag_type="table">GTSRB <a href="deeplearning_paper26.html#9">[11]</a></text>
<text top="94" left="102" width="17" height="9" font="font7" id="p7_t9" reading_order_no="8" segment_no="3" tag_type="table">Size</text>
<text top="94" left="142" width="5" height="9" font="font21" id="p7_t10" reading_order_no="9" segment_no="3" tag_type="table">1</text>
<text top="93" left="149" width="8" height="9" font="font13" id="p7_t11" reading_order_no="10" segment_no="3" tag_type="table">×</text>
<text top="94" left="159" width="5" height="9" font="font21" id="p7_t12" reading_order_no="11" segment_no="3" tag_type="table">4</text>
<text top="94" left="185" width="5" height="9" font="font21" id="p7_t13" reading_order_no="12" segment_no="3" tag_type="table">1</text>
<text top="93" left="192" width="8" height="9" font="font13" id="p7_t14" reading_order_no="13" segment_no="3" tag_type="table">×</text>
<text top="94" left="202" width="5" height="9" font="font21" id="p7_t15" reading_order_no="14" segment_no="3" tag_type="table">6</text>
<text top="94" left="228" width="5" height="9" font="font21" id="p7_t16" reading_order_no="15" segment_no="3" tag_type="table">1</text>
<text top="93" left="235" width="8" height="9" font="font13" id="p7_t17" reading_order_no="16" segment_no="3" tag_type="table">×</text>
<text top="94" left="245" width="5" height="9" font="font21" id="p7_t18" reading_order_no="17" segment_no="3" tag_type="table">8</text>
<text top="94" left="271" width="5" height="9" font="font21" id="p7_t19" reading_order_no="18" segment_no="3" tag_type="table">2</text>
<text top="93" left="278" width="8" height="9" font="font13" id="p7_t20" reading_order_no="19" segment_no="3" tag_type="table">×</text>
<text top="94" left="288" width="5" height="9" font="font21" id="p7_t21" reading_order_no="20" segment_no="3" tag_type="table">2</text>
<text top="94" left="314" width="5" height="9" font="font21" id="p7_t22" reading_order_no="21" segment_no="3" tag_type="table">4</text>
<text top="93" left="321" width="8" height="9" font="font13" id="p7_t23" reading_order_no="22" segment_no="3" tag_type="table">×</text>
<text top="94" left="331" width="5" height="9" font="font21" id="p7_t24" reading_order_no="23" segment_no="3" tag_type="table">4</text>
<text top="94" left="357" width="5" height="9" font="font21" id="p7_t25" reading_order_no="24" segment_no="3" tag_type="table">6</text>
<text top="93" left="365" width="8" height="9" font="font13" id="p7_t26" reading_order_no="25" segment_no="3" tag_type="table">×</text>
<text top="94" left="375" width="5" height="9" font="font21" id="p7_t27" reading_order_no="26" segment_no="3" tag_type="table">6</text>
<text top="94" left="400" width="5" height="9" font="font21" id="p7_t28" reading_order_no="27" segment_no="3" tag_type="table">2</text>
<text top="93" left="408" width="8" height="9" font="font13" id="p7_t29" reading_order_no="28" segment_no="3" tag_type="table">×</text>
<text top="94" left="418" width="5" height="9" font="font21" id="p7_t30" reading_order_no="29" segment_no="3" tag_type="table">2</text>
<text top="94" left="444" width="5" height="9" font="font21" id="p7_t31" reading_order_no="30" segment_no="3" tag_type="table">4</text>
<text top="93" left="451" width="8" height="9" font="font13" id="p7_t32" reading_order_no="31" segment_no="3" tag_type="table">×</text>
<text top="94" left="461" width="5" height="9" font="font21" id="p7_t33" reading_order_no="32" segment_no="3" tag_type="table">4</text>
<text top="94" left="487" width="5" height="9" font="font21" id="p7_t34" reading_order_no="33" segment_no="3" tag_type="table">6</text>
<text top="93" left="494" width="8" height="9" font="font13" id="p7_t35" reading_order_no="34" segment_no="3" tag_type="table">×</text>
<text top="94" left="504" width="5" height="9" font="font21" id="p7_t36" reading_order_no="35" segment_no="3" tag_type="table">6</text>
<text top="106" left="100" width="20" height="9" font="font7" id="p7_t37" reading_order_no="36" segment_no="3" tag_type="table">BDR</text>
<text top="106" left="138" width="31" height="9" font="font7" id="p7_t38" reading_order_no="37" segment_no="3" tag_type="table">99.65%</text>
<text top="106" left="181" width="31" height="9" font="font7" id="p7_t39" reading_order_no="38" segment_no="3" tag_type="table">99.67%</text>
<text top="106" left="224" width="31" height="9" font="font7" id="p7_t40" reading_order_no="39" segment_no="3" tag_type="table">99.75%</text>
<text top="106" left="267" width="31" height="9" font="font7" id="p7_t41" reading_order_no="40" segment_no="3" tag_type="table">99.07%</text>
<text top="106" left="310" width="31" height="9" font="font7" id="p7_t42" reading_order_no="41" segment_no="3" tag_type="table">99.70%</text>
<text top="106" left="353" width="31" height="9" font="font7" id="p7_t43" reading_order_no="42" segment_no="3" tag_type="table">99.75%</text>
<text top="106" left="396" width="31" height="9" font="font7" id="p7_t44" reading_order_no="43" segment_no="3" tag_type="table">98.25%</text>
<text top="106" left="439" width="31" height="9" font="font7" id="p7_t45" reading_order_no="44" segment_no="3" tag_type="table">99.07%</text>
<text top="106" left="482" width="31" height="9" font="font7" id="p7_t46" reading_order_no="45" segment_no="3" tag_type="table">99.60%</text>
<text top="142" left="49" width="251" height="9" font="font7" id="p7_t47" reading_order_no="46" segment_no="4" tag_type="text">datasets are shown in Fig. <a href="deeplearning_paper26.html#7">4. </a>There are 16 pixels and 7 pixels</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p7_t48" reading_order_no="47" segment_no="4" tag_type="text">contained in the square trigger and the cross pattern trigger,</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p7_t49" reading_order_no="48" segment_no="4" tag_type="text">respectively. As shown in Fig. <a href="deeplearning_paper26.html#7">5, </a>for all the three datasets, the</text>
<text top="178" left="49" width="251" height="9" font="font7" id="p7_t50" reading_order_no="49" segment_no="4" tag_type="text">proposed method can achieve high backdoor detection rates</text>
<text top="190" left="49" width="142" height="9" font="font7" id="p7_t51" reading_order_no="50" segment_no="4" tag_type="text">against the backdoor attacks with</text>
<text top="190" left="196" width="38" height="9" font="font12" id="p7_t52" reading_order_no="51" segment_no="4" tag_type="text">trigger A</text>
<text top="190" left="240" width="14" height="9" font="font7" id="p7_t53" reading_order_no="52" segment_no="4" tag_type="text">and</text>
<text top="190" left="259" width="38" height="9" font="font12" id="p7_t54" reading_order_no="53" segment_no="4" tag_type="text">trigger B</text>
<text top="190" left="298" width="2" height="9" font="font7" id="p7_t55" reading_order_no="54" segment_no="4" tag_type="text">,</text>
<text top="202" left="49" width="83" height="9" font="font7" id="p7_t56" reading_order_no="55" segment_no="4" tag_type="text">respectively. For the</text>
<text top="202" left="136" width="38" height="9" font="font12" id="p7_t57" reading_order_no="56" segment_no="4" tag_type="text">trigger A</text>
<text top="202" left="174" width="126" height="9" font="font7" id="p7_t58" reading_order_no="57" segment_no="4" tag_type="text">, after the proposed method is</text>
<text top="214" left="49" width="251" height="9" font="font7" id="p7_t59" reading_order_no="58" segment_no="4" tag_type="text">applied, the backdoor detection rates are 99.30%, 98.57%, and</text>
<text top="226" left="49" width="251" height="9" font="font7" id="p7_t60" reading_order_no="59" segment_no="4" tag_type="text">99.47% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB</text>
<text top="238" left="49" width="102" height="9" font="font7" id="p7_t61" reading_order_no="60" segment_no="4" tag_type="text"><a href="deeplearning_paper26.html#9">[11], </a>respectively. For the</text>
<text top="238" left="154" width="36" height="9" font="font12" id="p7_t62" reading_order_no="61" segment_no="4" tag_type="text">trigger B</text>
<text top="238" left="190" width="110" height="9" font="font7" id="p7_t63" reading_order_no="62" segment_no="4" tag_type="text">, after the proposed method</text>
<text top="250" left="49" width="251" height="9" font="font7" id="p7_t64" reading_order_no="63" segment_no="4" tag_type="text">is applied, the backdoor detection rates are 98.20%, 98.45%,</text>
<text top="262" left="49" width="251" height="9" font="font7" id="p7_t65" reading_order_no="64" segment_no="4" tag_type="text">99.22% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB</text>
<text top="274" left="49" width="107" height="9" font="font7" id="p7_t66" reading_order_no="65" segment_no="4" tag_type="text"><a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively.</text>
<text top="427" left="266" width="11" height="9" font="font32" id="p7_t67" reading_order_no="66" segment_no="7" tag_type="figure">(c)</text>
<text top="427" left="190" width="11" height="9" font="font32" id="p7_t68" reading_order_no="67" segment_no="7" tag_type="figure">(b)</text>
<text top="427" left="112" width="11" height="9" font="font32" id="p7_t69" reading_order_no="68" segment_no="7" tag_type="figure">(a)</text>
<text top="322" left="52" width="35" height="9" font="font33" id="p7_t70" reading_order_no="69" segment_no="7" tag_type="figure"><i>trigger A</i></text>
<text top="392" left="52" width="35" height="9" font="font33" id="p7_t71" reading_order_no="70" segment_no="7" tag_type="figure"><i>trigger B</i></text>
<text top="449" left="49" width="22" height="7" font="font8" id="p7_t72" reading_order_no="71" segment_no="9" tag_type="text">Fig. 4.</text>
<text top="449" left="82" width="218" height="7" font="font8" id="p7_t73" reading_order_no="72" segment_no="9" tag_type="text">Examples of backdoor instances with different triggers. The first</text>
<text top="458" left="49" width="149" height="7" font="font8" id="p7_t74" reading_order_no="73" segment_no="9" tag_type="text">row is examples of backdoor instances with</text>
<text top="458" left="201" width="30" height="7" font="font34" id="p7_t75" reading_order_no="74" segment_no="9" tag_type="text">trigger A</text>
<text top="458" left="232" width="68" height="7" font="font8" id="p7_t76" reading_order_no="75" segment_no="9" tag_type="text">. The second row is</text>
<text top="467" left="49" width="121" height="7" font="font8" id="p7_t77" reading_order_no="76" segment_no="9" tag_type="text">examples of backdoor instances with</text>
<text top="467" left="173" width="30" height="7" font="font34" id="p7_t78" reading_order_no="77" segment_no="9" tag_type="text">trigger B</text>
<text top="467" left="203" width="97" height="7" font="font8" id="p7_t79" reading_order_no="78" segment_no="9" tag_type="text">. (a) Fashion-MNIST images.</text>
<text top="476" left="49" width="140" height="7" font="font8" id="p7_t80" reading_order_no="79" segment_no="9" tag_type="text">(b) CIFAR-10 images. (c) GTSRB images.</text>
<text top="681" left="129" width="25" height="8" font="font35" id="p7_t81" reading_order_no="104" segment_no="10" tag_type="figure"><i>t r i g g e r   A</i></text>
<text top="681" left="216" width="25" height="8" font="font35" id="p7_t82" reading_order_no="105" segment_no="10" tag_type="figure"><i>t r i g g e r   B</i></text>
<text top="672" left="94" width="0" height="8" font="font30" id="p7_t83" reading_order_no="103" segment_no="10" tag_type="figure">0</text>
<text top="647" left="89" width="4" height="8" font="font30" id="p7_t84" reading_order_no="102" segment_no="10" tag_type="figure">2 0</text>
<text top="621" left="89" width="4" height="8" font="font30" id="p7_t85" reading_order_no="101" segment_no="10" tag_type="figure">4 0</text>
<text top="596" left="89" width="4" height="8" font="font30" id="p7_t86" reading_order_no="100" segment_no="10" tag_type="figure">6 0</text>
<text top="570" left="89" width="4" height="8" font="font30" id="p7_t87" reading_order_no="99" segment_no="10" tag_type="figure">8 0</text>
<text top="545" left="84" width="8" height="8" font="font30" id="p7_t88" reading_order_no="98" segment_no="10" tag_type="figure">1 0 0</text>
<text top="655" left="81" width="0" height="8" font="font30" id="p7_t89" reading_order_no="97" segment_no="10" tag_type="figure">B</text>
<text top="649" left="81" width="0" height="8" font="font30" id="p7_t90" reading_order_no="96" segment_no="10" tag_type="figure">ac</text>
<text top="642" left="81" width="0" height="8" font="font30" id="p7_t91" reading_order_no="95" segment_no="10" tag_type="figure">k</text>
<text top="638" left="81" width="0" height="8" font="font30" id="p7_t92" reading_order_no="94" segment_no="10" tag_type="figure">d</text>
<text top="633" left="81" width="0" height="8" font="font30" id="p7_t93" reading_order_no="93" segment_no="10" tag_type="figure">o</text>
<text top="629" left="81" width="0" height="8" font="font30" id="p7_t94" reading_order_no="92" segment_no="10" tag_type="figure">o</text>
<text top="625" left="81" width="0" height="8" font="font30" id="p7_t95" reading_order_no="91" segment_no="10" tag_type="figure">r </text>
<text top="620" left="81" width="0" height="8" font="font30" id="p7_t96" reading_order_no="90" segment_no="10" tag_type="figure">D</text>
<text top="614" left="81" width="0" height="8" font="font30" id="p7_t97" reading_order_no="89" segment_no="10" tag_type="figure">et</text>
<text top="608" left="81" width="0" height="8" font="font30" id="p7_t98" reading_order_no="88" segment_no="10" tag_type="figure">ec</text>
<text top="601" left="81" width="0" height="8" font="font30" id="p7_t99" reading_order_no="87" segment_no="10" tag_type="figure">ti</text>
<text top="596" left="81" width="0" height="8" font="font30" id="p7_t100" reading_order_no="86" segment_no="10" tag_type="figure">o</text>
<text top="592" left="81" width="0" height="8" font="font30" id="p7_t101" reading_order_no="85" segment_no="10" tag_type="figure">n</text>
<text top="587" left="81" width="0" height="8" font="font30" id="p7_t102" reading_order_no="84" segment_no="10" tag_type="figure"> R</text>
<text top="580" left="81" width="0" height="8" font="font30" id="p7_t103" reading_order_no="83" segment_no="10" tag_type="figure">at</text>
<text top="574" left="81" width="0" height="8" font="font30" id="p7_t104" reading_order_no="82" segment_no="10" tag_type="figure">e </text>
<text top="568" left="81" width="0" height="8" font="font30" id="p7_t105" reading_order_no="81" segment_no="10" tag_type="figure">(%</text>
<text top="558" left="81" width="0" height="8" font="font30" id="p7_t106" reading_order_no="80" segment_no="10" tag_type="figure">)</text>
<text top="694" left="157" width="57" height="8" font="font30" id="p7_t107" reading_order_no="106" segment_no="10" tag_type="figure">P a t t e r n   o f   T r i g g e r</text>
<text top="518" left="215" width="48" height="8" font="font30" id="p7_t108" reading_order_no="107" segment_no="10" tag_type="figure">  F a s h i o n - M N I S T</text>
<text top="528" left="215" width="31" height="8" font="font30" id="p7_t109" reading_order_no="108" segment_no="10" tag_type="figure">  C I F A R - 1 0</text>
<text top="539" left="215" width="22" height="8" font="font30" id="p7_t110" reading_order_no="109" segment_no="10" tag_type="figure">  G T S R B</text>
<text top="716" left="49" width="22" height="7" font="font8" id="p7_t111" reading_order_no="110" segment_no="13" tag_type="text">Fig. 5.</text>
<text top="716" left="80" width="220" height="7" font="font8" id="p7_t112" reading_order_no="111" segment_no="13" tag_type="text">Backdoor detection rates with different settings of trigger patterns</text>
<text top="725" left="49" width="149" height="7" font="font8" id="p7_t113" reading_order_no="112" segment_no="13" tag_type="text">after the proposed defense method is applied.</text>
<text top="143" left="312" width="251" height="9" font="font12" id="p7_t114" reading_order_no="113" segment_no="5" tag_type="text">D. Experiment Results of the Proposed Method with Four</text>
<text top="155" left="312" width="229" height="9" font="font12" id="p7_t115" reading_order_no="114" segment_no="5" tag_type="text">Different Adversarial Perturbation Generation Methods.</text>
<text top="170" left="322" width="241" height="9" font="font7" id="p7_t116" reading_order_no="115" segment_no="6" tag_type="text">In this section, the effectiveness of different adversarial</text>
<text top="182" left="312" width="251" height="9" font="font7" id="p7_t117" reading_order_no="116" segment_no="6" tag_type="text">perturbation generation methods is evaluated. The four dif-</text>
<text top="194" left="312" width="251" height="9" font="font7" id="p7_t118" reading_order_no="117" segment_no="6" tag_type="text">ferent adversarial perturbation generation methods evaluated</text>
<text top="206" left="312" width="251" height="9" font="font7" id="p7_t119" reading_order_no="118" segment_no="6" tag_type="text">in the experiment are C&amp;W <a href="deeplearning_paper26.html#9">[20], </a>DeepFool <a href="deeplearning_paper26.html#9">[21], </a>PGD <a href="deeplearning_paper26.html#9">[22]</a></text>
<text top="218" left="312" width="251" height="9" font="font7" id="p7_t120" reading_order_no="119" segment_no="6" tag_type="text">and UAP <a href="deeplearning_paper26.html#9">[13]. </a>These four adversarial perturbation generation</text>
<text top="230" left="312" width="251" height="9" font="font7" id="p7_t121" reading_order_no="120" segment_no="6" tag_type="text">methods are separately used to generate the perturbation</text>
<text top="242" left="312" width="251" height="9" font="font7" id="p7_t122" reading_order_no="121" segment_no="6" tag_type="text">which is later utilized in the proposed method. As shown</text>
<text top="254" left="312" width="251" height="9" font="font7" id="p7_t123" reading_order_no="122" segment_no="6" tag_type="text">in Fig. <a href="deeplearning_paper26.html#7">6 </a>and Fig. <a href="deeplearning_paper26.html#8">7, </a>for the three datsets, using UAP <a href="deeplearning_paper26.html#9">[13]</a></text>
<text top="266" left="312" width="251" height="9" font="font7" id="p7_t124" reading_order_no="123" segment_no="6" tag_type="text">in the proposed method obtains the highest BDR (99.63%,</text>
<text top="278" left="312" width="251" height="9" font="font7" id="p7_t125" reading_order_no="124" segment_no="6" tag_type="text">99.76% and 99.91% in Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="290" left="312" width="251" height="9" font="font7" id="p7_t126" reading_order_no="125" segment_no="6" tag_type="text">and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively), and the highest CIIR</text>
<text top="302" left="312" width="251" height="9" font="font7" id="p7_t127" reading_order_no="126" segment_no="6" tag_type="text">(90.66%, 89.82% and 98.85% in Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-</text>
<text top="314" left="312" width="251" height="9" font="font7" id="p7_t128" reading_order_no="127" segment_no="6" tag_type="text">10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively) among all</text>
<text top="326" left="312" width="251" height="9" font="font7" id="p7_t129" reading_order_no="128" segment_no="6" tag_type="text">the four adversarial perturbation generation methods. With the</text>
<text top="337" left="312" width="251" height="9" font="font7" id="p7_t130" reading_order_no="129" segment_no="6" tag_type="text">adversarial perturbation generated by C&amp;W <a href="deeplearning_paper26.html#9">[20], </a>DeepFool</text>
<text top="349" left="312" width="251" height="9" font="font7" id="p7_t131" reading_order_no="130" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[21] </a>and PGD <a href="deeplearning_paper26.html#9">[22], </a>the performance of the proposed method</text>
<text top="361" left="312" width="251" height="9" font="font7" id="p7_t132" reading_order_no="131" segment_no="6" tag_type="text">is less effective, as the backdoor detection rates are as low as</text>
<text top="373" left="312" width="251" height="9" font="font7" id="p7_t133" reading_order_no="132" segment_no="6" tag_type="text">74.42%, 65.22% and 91.52% by using C&amp;W <a href="deeplearning_paper26.html#9">[20], </a>DeepFool</text>
<text top="385" left="312" width="251" height="9" font="font7" id="p7_t134" reading_order_no="133" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[21] </a>and PGD <a href="deeplearning_paper26.html#9">[22], </a>respectively (as shown in Fig. <a href="deeplearning_paper26.html#7">6), </a>and the</text>
<text top="397" left="312" width="251" height="9" font="font7" id="p7_t135" reading_order_no="134" segment_no="6" tag_type="text">clean image identification rates are as low as 41.92%, 47.85%</text>
<text top="409" left="312" width="251" height="9" font="font7" id="p7_t136" reading_order_no="135" segment_no="6" tag_type="text">and 67.77% by using C&amp;W <a href="deeplearning_paper26.html#9">[20], </a>DeepFool <a href="deeplearning_paper26.html#9">[21] </a>and PGD</text>
<text top="421" left="312" width="159" height="9" font="font7" id="p7_t137" reading_order_no="136" segment_no="6" tag_type="text"><a href="deeplearning_paper26.html#9">[22], </a>respectively (as shown in Fig. <a href="deeplearning_paper26.html#8">7).</a></text>
<text top="602" left="371" width="27" height="8" font="font30" id="p7_t138" reading_order_no="161" segment_no="8" tag_type="figure">C W   [ 2 0 ]</text>
<text top="602" left="416" width="24" height="8" font="font30" id="p7_t139" reading_order_no="162" segment_no="8" tag_type="figure">D F   [ 2 1 ]</text>
<text top="602" left="456" width="30" height="8" font="font30" id="p7_t140" reading_order_no="163" segment_no="8" tag_type="figure">P G D   [ 2 2 ]</text>
<text top="602" left="499" width="32" height="8" font="font30" id="p7_t141" reading_order_no="164" segment_no="8" tag_type="figure">U A P     [ 1 3 ]</text>
<text top="593" left="356" width="0" height="8" font="font30" id="p7_t142" reading_order_no="160" segment_no="8" tag_type="figure">0</text>
<text top="567" left="351" width="4" height="8" font="font30" id="p7_t143" reading_order_no="159" segment_no="8" tag_type="figure">2 0</text>
<text top="542" left="351" width="4" height="8" font="font30" id="p7_t144" reading_order_no="158" segment_no="8" tag_type="figure">4 0</text>
<text top="516" left="351" width="4" height="8" font="font30" id="p7_t145" reading_order_no="157" segment_no="8" tag_type="figure">6 0</text>
<text top="491" left="351" width="4" height="8" font="font30" id="p7_t146" reading_order_no="156" segment_no="8" tag_type="figure">8 0</text>
<text top="465" left="347" width="8" height="8" font="font30" id="p7_t147" reading_order_no="155" segment_no="8" tag_type="figure">1 0 0</text>
<text top="575" left="344" width="0" height="8" font="font30" id="p7_t148" reading_order_no="154" segment_no="8" tag_type="figure">B</text>
<text top="570" left="344" width="0" height="8" font="font30" id="p7_t149" reading_order_no="153" segment_no="8" tag_type="figure">ac</text>
<text top="562" left="344" width="0" height="8" font="font30" id="p7_t150" reading_order_no="152" segment_no="8" tag_type="figure">k</text>
<text top="558" left="344" width="0" height="8" font="font30" id="p7_t151" reading_order_no="151" segment_no="8" tag_type="figure">d</text>
<text top="554" left="344" width="0" height="8" font="font30" id="p7_t152" reading_order_no="150" segment_no="8" tag_type="figure">o</text>
<text top="550" left="344" width="0" height="8" font="font30" id="p7_t153" reading_order_no="149" segment_no="8" tag_type="figure">o</text>
<text top="545" left="344" width="0" height="8" font="font30" id="p7_t154" reading_order_no="148" segment_no="8" tag_type="figure">r </text>
<text top="541" left="344" width="0" height="8" font="font30" id="p7_t155" reading_order_no="147" segment_no="8" tag_type="figure">D</text>
<text top="534" left="344" width="0" height="8" font="font30" id="p7_t156" reading_order_no="146" segment_no="8" tag_type="figure">et</text>
<text top="528" left="344" width="0" height="8" font="font30" id="p7_t157" reading_order_no="145" segment_no="8" tag_type="figure">ec</text>
<text top="521" left="344" width="0" height="8" font="font30" id="p7_t158" reading_order_no="144" segment_no="8" tag_type="figure">ti</text>
<text top="516" left="344" width="0" height="8" font="font30" id="p7_t159" reading_order_no="143" segment_no="8" tag_type="figure">o</text>
<text top="512" left="344" width="0" height="8" font="font30" id="p7_t160" reading_order_no="142" segment_no="8" tag_type="figure">n</text>
<text top="508" left="344" width="0" height="8" font="font30" id="p7_t161" reading_order_no="141" segment_no="8" tag_type="figure"> R</text>
<text top="500" left="344" width="0" height="8" font="font30" id="p7_t162" reading_order_no="140" segment_no="8" tag_type="figure">at</text>
<text top="494" left="344" width="0" height="8" font="font30" id="p7_t163" reading_order_no="139" segment_no="8" tag_type="figure">e </text>
<text top="488" left="344" width="0" height="8" font="font30" id="p7_t164" reading_order_no="138" segment_no="8" tag_type="figure">(%</text>
<text top="478" left="344" width="0" height="8" font="font30" id="p7_t165" reading_order_no="137" segment_no="8" tag_type="figure">)</text>
<text top="615" left="369" width="147" height="8" font="font30" id="p7_t166" reading_order_no="165" segment_no="8" tag_type="figure">A d v e r s a r i a l   P e r t u r b a t i o n   G e n e r a t i o n   M e t h o d</text>
<text top="446" left="388" width="48" height="8" font="font30" id="p7_t167" reading_order_no="166" segment_no="8" tag_type="figure">  F a s h i o n - M N I S T</text>
<text top="457" left="388" width="31" height="8" font="font30" id="p7_t168" reading_order_no="167" segment_no="8" tag_type="figure">  C I F A R - 1 0</text>
<text top="468" left="388" width="22" height="8" font="font30" id="p7_t169" reading_order_no="168" segment_no="8" tag_type="figure">  G T S R B</text>
<text top="636" left="312" width="251" height="7" font="font8" id="p7_t170" reading_order_no="169" segment_no="11" tag_type="text">Fig. 6. The backdoor detection rate of the proposed method by using different</text>
<text top="645" left="312" width="145" height="7" font="font8" id="p7_t171" reading_order_no="170" segment_no="11" tag_type="text">adversarial perturbation generation methods.</text>
<text top="668" left="322" width="241" height="9" font="font7" id="p7_t172" reading_order_no="171" segment_no="12" tag_type="text">As mentioned in Section <a href="deeplearning_paper26.html#5">III-D, </a>in the perturbation gener-</text>
<text top="680" left="312" width="251" height="9" font="font7" id="p7_t173" reading_order_no="172" segment_no="12" tag_type="text">ation process, image-specific adversarial attack requires the</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p7_t174" reading_order_no="173" segment_no="12" tag_type="text">label of each specific image to generate corresponding image-</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p7_t175" reading_order_no="174" segment_no="12" tag_type="text">specific perturbation. If the image is a backdoor instance,</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p7_t176" reading_order_no="175" segment_no="12" tag_type="text">its label will be the target label, then the image-specific</text>
<text top="728" left="312" width="251" height="9" font="font7" id="p7_t177" reading_order_no="176" segment_no="12" tag_type="text">perturbation will be generated based on the target label instead</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p7_t178" reading_order_no="177" segment_no="12" tag_type="text">of its ground-truth label. Hence, the generated image-specific</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font36" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
<text top="30" left="45" width="3" height="6" font="font0" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="text">8</text>
<text top="208" left="108" width="27" height="8" font="font30" id="p8_t2" reading_order_no="27" segment_no="1" tag_type="figure">C W   [ 2 0 ]</text>
<text top="208" left="153" width="24" height="8" font="font30" id="p8_t3" reading_order_no="28" segment_no="1" tag_type="figure">D F   [ 2 1 ]</text>
<text top="208" left="193" width="30" height="8" font="font30" id="p8_t4" reading_order_no="29" segment_no="1" tag_type="figure">P G D   [ 2 2 ]</text>
<text top="208" left="237" width="30" height="8" font="font30" id="p8_t5" reading_order_no="30" segment_no="1" tag_type="figure">U A P   [ 1 3 ]</text>
<text top="199" left="94" width="0" height="8" font="font30" id="p8_t6" reading_order_no="26" segment_no="1" tag_type="figure">0</text>
<text top="174" left="89" width="4" height="8" font="font30" id="p8_t7" reading_order_no="25" segment_no="1" tag_type="figure">2 0</text>
<text top="148" left="89" width="4" height="8" font="font30" id="p8_t8" reading_order_no="24" segment_no="1" tag_type="figure">4 0</text>
<text top="123" left="89" width="4" height="8" font="font30" id="p8_t9" reading_order_no="23" segment_no="1" tag_type="figure">6 0</text>
<text top="97" left="89" width="4" height="8" font="font30" id="p8_t10" reading_order_no="22" segment_no="1" tag_type="figure">8 0</text>
<text top="72" left="84" width="8" height="8" font="font30" id="p8_t11" reading_order_no="21" segment_no="1" tag_type="figure">1 0 0</text>
<text top="194" left="81" width="0" height="8" font="font30" id="p8_t12" reading_order_no="20" segment_no="1" tag_type="figure">C</text>
<text top="188" left="81" width="0" height="8" font="font30" id="p8_t13" reading_order_no="19" segment_no="1" tag_type="figure">le</text>
<text top="182" left="81" width="0" height="8" font="font30" id="p8_t14" reading_order_no="18" segment_no="1" tag_type="figure">an</text>
<text top="174" left="81" width="0" height="8" font="font30" id="p8_t15" reading_order_no="17" segment_no="1" tag_type="figure"> I</text>
<text top="170" left="81" width="0" height="8" font="font30" id="p8_t16" reading_order_no="16" segment_no="1" tag_type="figure">m</text>
<text top="163" left="81" width="0" height="8" font="font30" id="p8_t17" reading_order_no="15" segment_no="1" tag_type="figure">ag</text>
<text top="155" left="81" width="0" height="8" font="font30" id="p8_t18" reading_order_no="14" segment_no="1" tag_type="figure">e </text>
<text top="149" left="81" width="0" height="8" font="font30" id="p8_t19" reading_order_no="13" segment_no="1" tag_type="figure">Id</text>
<text top="142" left="81" width="0" height="8" font="font30" id="p8_t20" reading_order_no="12" segment_no="1" tag_type="figure">en</text>
<text top="134" left="81" width="0" height="8" font="font30" id="p8_t21" reading_order_no="11" segment_no="1" tag_type="figure">ti</text>
<text top="130" left="81" width="0" height="8" font="font30" id="p8_t22" reading_order_no="10" segment_no="1" tag_type="figure">fi</text>
<text top="125" left="81" width="0" height="8" font="font30" id="p8_t23" reading_order_no="9" segment_no="1" tag_type="figure">ca</text>
<text top="117" left="81" width="0" height="8" font="font30" id="p8_t24" reading_order_no="8" segment_no="1" tag_type="figure">ti</text>
<text top="112" left="81" width="0" height="8" font="font30" id="p8_t25" reading_order_no="7" segment_no="1" tag_type="figure">o</text>
<text top="108" left="81" width="0" height="8" font="font30" id="p8_t26" reading_order_no="6" segment_no="1" tag_type="figure">n</text>
<text top="104" left="81" width="0" height="8" font="font30" id="p8_t27" reading_order_no="5" segment_no="1" tag_type="figure"> R</text>
<text top="96" left="81" width="0" height="8" font="font30" id="p8_t28" reading_order_no="4" segment_no="1" tag_type="figure">at</text>
<text top="90" left="81" width="0" height="8" font="font30" id="p8_t29" reading_order_no="3" segment_no="1" tag_type="figure">e </text>
<text top="85" left="81" width="0" height="8" font="font30" id="p8_t30" reading_order_no="2" segment_no="1" tag_type="figure">(%</text>
<text top="75" left="81" width="0" height="8" font="font30" id="p8_t31" reading_order_no="1" segment_no="1" tag_type="figure">)</text>
<text top="218" left="106" width="146" height="8" font="font30" id="p8_t32" reading_order_no="31" segment_no="1" tag_type="figure">A d v e r s a r i a l   P e r t u r b a t i o n   G e n e r a t i o n   M e t h o d</text>
<text top="57" left="126" width="48" height="8" font="font30" id="p8_t33" reading_order_no="32" segment_no="1" tag_type="figure">  F a s h i o n - M N I S T</text>
<text top="68" left="126" width="31" height="8" font="font30" id="p8_t34" reading_order_no="33" segment_no="1" tag_type="figure">  C I F A R - 1 0</text>
<text top="78" left="126" width="22" height="8" font="font30" id="p8_t35" reading_order_no="34" segment_no="1" tag_type="figure">  G T S R B</text>
<text top="240" left="49" width="251" height="7" font="font8" id="p8_t36" reading_order_no="35" segment_no="4" tag_type="text">Fig. 7. The clean image identification rates of the proposed method by using</text>
<text top="249" left="49" width="175" height="7" font="font8" id="p8_t37" reading_order_no="36" segment_no="4" tag_type="text">different adversarial perturbation generation methods.</text>
<text top="280" left="49" width="251" height="9" font="font7" id="p8_t38" reading_order_no="37" segment_no="5" tag_type="text">perturbation will strongly affect the backdoor trigger. Once</text>
<text top="292" left="49" width="251" height="9" font="font7" id="p8_t39" reading_order_no="38" segment_no="5" tag_type="text">the backdoor trigger is strongly affected, it cannot trigger the</text>
<text top="304" left="49" width="251" height="9" font="font7" id="p8_t40" reading_order_no="39" segment_no="5" tag_type="text">backdoor, leading to the inconsistence of predicted labels of</text>
<text top="316" left="49" width="251" height="9" font="font7" id="p8_t41" reading_order_no="40" segment_no="5" tag_type="text">backdoor instance before and after perturbation. As a result,</text>
<text top="328" left="49" width="251" height="9" font="font7" id="p8_t42" reading_order_no="41" segment_no="5" tag_type="text">this backdoor instance will be incorrectly considered as a</text>
<text top="340" left="49" width="251" height="9" font="font7" id="p8_t43" reading_order_no="42" segment_no="5" tag_type="text">benign image. Therefore, the performance of the proposed</text>
<text top="352" left="49" width="251" height="9" font="font7" id="p8_t44" reading_order_no="43" segment_no="5" tag_type="text">method with image-specific perturbation is low. UAP <a href="deeplearning_paper26.html#9">[13]</a></text>
<text top="364" left="49" width="251" height="9" font="font7" id="p8_t45" reading_order_no="44" segment_no="5" tag_type="text">is a kind of image-agnostic perturbation, which is generated</text>
<text top="376" left="49" width="251" height="9" font="font7" id="p8_t46" reading_order_no="45" segment_no="5" tag_type="text">based on a small set of clean images. The influence of UAP is</text>
<text top="388" left="49" width="251" height="9" font="font7" id="p8_t47" reading_order_no="46" segment_no="5" tag_type="text">mostly on the salient regions of the backdoor instance instead</text>
<text top="400" left="49" width="251" height="9" font="font7" id="p8_t48" reading_order_no="47" segment_no="5" tag_type="text">of the trigger. Therefore, the predicted labels of the backdoor</text>
<text top="412" left="49" width="251" height="9" font="font7" id="p8_t49" reading_order_no="48" segment_no="5" tag_type="text">instance before and after perturbation are consistent, which</text>
<text top="424" left="49" width="251" height="9" font="font7" id="p8_t50" reading_order_no="49" segment_no="5" tag_type="text">makes the image be correctly detected as a backdoor instance.</text>
<text top="436" left="49" width="251" height="9" font="font7" id="p8_t51" reading_order_no="50" segment_no="5" tag_type="text">In summary, among these four adversarial perturbation gen-</text>
<text top="448" left="49" width="251" height="9" font="font7" id="p8_t52" reading_order_no="51" segment_no="5" tag_type="text">eration methods, UAP <a href="deeplearning_paper26.html#9">[13] </a>is most suitable for use in the</text>
<text top="460" left="49" width="72" height="9" font="font7" id="p8_t53" reading_order_no="52" segment_no="5" tag_type="text">proposed method.</text>
<text top="486" left="49" width="142" height="9" font="font12" id="p8_t54" reading_order_no="53" segment_no="9" tag_type="title">E. Comparison with Related Work</text>
<text top="501" left="59" width="241" height="9" font="font7" id="p8_t55" reading_order_no="54" segment_no="10" tag_type="text">In this section, the proposed method is compared with</text>
<text top="513" left="49" width="251" height="9" font="font7" id="p8_t56" reading_order_no="55" segment_no="10" tag_type="text">STRIP <a href="deeplearning_paper26.html#9">[12]. </a>In the detection process of STRIP <a href="deeplearning_paper26.html#9">[12], </a>a set</text>
<text top="525" left="49" width="251" height="9" font="font7" id="p8_t57" reading_order_no="56" segment_no="10" tag_type="text">of other images from different classes are added to the input</text>
<text top="537" left="49" width="251" height="9" font="font7" id="p8_t58" reading_order_no="57" segment_no="10" tag_type="text">image separately in order to generate a set of blended images</text>
<text top="549" left="49" width="251" height="9" font="font7" id="p8_t59" reading_order_no="58" segment_no="10" tag_type="text"><a href="deeplearning_paper26.html#9">[12]. </a>Then, STRIP utilizes entropy to measure the randomness</text>
<text top="561" left="49" width="251" height="9" font="font7" id="p8_t60" reading_order_no="59" segment_no="10" tag_type="text">of the predicted labels of all the blended images <a href="deeplearning_paper26.html#9">[12]. </a>The</text>
<text top="573" left="49" width="251" height="9" font="font7" id="p8_t61" reading_order_no="60" segment_no="10" tag_type="text">entropy of clean images is significantly lower than the entropy</text>
<text top="584" left="49" width="251" height="9" font="font7" id="p8_t62" reading_order_no="61" segment_no="10" tag_type="text">of backdoor instances. As a result, the smaller the entropy, the</text>
<text top="596" left="49" width="213" height="9" font="font7" id="p8_t63" reading_order_no="62" segment_no="10" tag_type="text">input image is more likely to contain a trigger <a href="deeplearning_paper26.html#9">[12].</a></text>
<text top="608" left="59" width="241" height="9" font="font7" id="p8_t64" reading_order_no="63" segment_no="12" tag_type="text">The advantages of the proposed method over STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="620" left="49" width="251" height="9" font="font7" id="p8_t65" reading_order_no="64" segment_no="12" tag_type="text">are as follows. (i) The proposed method perturbs the image</text>
<text top="632" left="49" width="251" height="9" font="font7" id="p8_t66" reading_order_no="65" segment_no="12" tag_type="text">with universal perturbation <a href="deeplearning_paper26.html#9">[13] </a>rather than other images from</text>
<text top="644" left="49" width="114" height="9" font="font7" id="p8_t67" reading_order_no="66" segment_no="12" tag_type="text">different classes. Since the</text>
<text top="644" left="168" width="4" height="9" font="font19" id="p8_t68" reading_order_no="67" segment_no="12" tag_type="text">`</text>
<text top="647" left="172" width="8" height="7" font="font10" id="p8_t69" reading_order_no="68" segment_no="12" tag_type="text">∞</text>
<text top="644" left="185" width="115" height="9" font="font7" id="p8_t70" reading_order_no="69" segment_no="12" tag_type="text">norm of UAP <a href="deeplearning_paper26.html#9">[13] </a>is very</text>
<text top="656" left="49" width="251" height="9" font="font7" id="p8_t71" reading_order_no="70" segment_no="12" tag_type="text">low, the perturbation is very small. In addition, because UAP</text>
<text top="668" left="49" width="251" height="9" font="font7" id="p8_t72" reading_order_no="71" segment_no="12" tag_type="text">is generated from clean images, the generated UAP mainly</text>
<text top="680" left="49" width="251" height="9" font="font7" id="p8_t73" reading_order_no="72" segment_no="12" tag_type="text">focuses on perturbing the salient regions of an image rather</text>
<text top="692" left="49" width="251" height="9" font="font7" id="p8_t74" reading_order_no="73" segment_no="12" tag_type="text">than the trigger. Therefore, the trigger is almost unaffected.</text>
<text top="704" left="49" width="251" height="9" font="font7" id="p8_t75" reading_order_no="74" segment_no="12" tag_type="text">However, in STRIP, other images are directly added to the</text>
<text top="716" left="49" width="251" height="9" font="font7" id="p8_t76" reading_order_no="75" segment_no="12" tag_type="text">input image, so the input image is globally affected <a href="deeplearning_paper26.html#9">[12]. </a>It</text>
<text top="728" left="49" width="251" height="9" font="font7" id="p8_t77" reading_order_no="76" segment_no="12" tag_type="text">will not only destroy the main content of the input image, but</text>
<text top="740" left="49" width="251" height="9" font="font7" id="p8_t78" reading_order_no="77" segment_no="12" tag_type="text">also may break the trigger. (ii) For each image, the proposed</text>
<text top="58" left="312" width="251" height="9" font="font7" id="p8_t79" reading_order_no="78" segment_no="2" tag_type="text">method only needs to generate one extra perturbed image</text>
<text top="70" left="312" width="251" height="9" font="font7" id="p8_t80" reading_order_no="79" segment_no="2" tag_type="text">and predict two images (perturbed and unperturbed image).</text>
<text top="82" left="312" width="251" height="9" font="font7" id="p8_t81" reading_order_no="80" segment_no="2" tag_type="text">However, for each input image, STRIP needs to generate a set</text>
<text top="94" left="312" width="251" height="9" font="font7" id="p8_t82" reading_order_no="81" segment_no="2" tag_type="text">of blended images and input them to the model in order to</text>
<text top="106" left="312" width="251" height="9" font="font7" id="p8_t83" reading_order_no="82" segment_no="2" tag_type="text">estimate the entropy of the predicted labels of these blended</text>
<text top="118" left="312" width="251" height="9" font="font7" id="p8_t84" reading_order_no="83" segment_no="2" tag_type="text">images <a href="deeplearning_paper26.html#9">[12]. </a>Therefore, the detection process of STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="130" left="312" width="251" height="9" font="font7" id="p8_t85" reading_order_no="84" segment_no="2" tag_type="text">is more complex than the proposed method, thus the proposed</text>
<text top="142" left="312" width="153" height="9" font="font7" id="p8_t86" reading_order_no="85" segment_no="2" tag_type="text">method is more efficient than STRIP.</text>
<text top="155" left="322" width="241" height="9" font="font7" id="p8_t87" reading_order_no="86" segment_no="3" tag_type="text">The experiment is also conducted to compare the proposed</text>
<text top="167" left="312" width="251" height="9" font="font7" id="p8_t88" reading_order_no="87" segment_no="3" tag_type="text">method with STRIP <a href="deeplearning_paper26.html#9">[12], </a>and the experimental results are</text>
<text top="179" left="312" width="251" height="9" font="font7" id="p8_t89" reading_order_no="88" segment_no="3" tag_type="text">shown in Table <a href="deeplearning_paper26.html#8">V. </a>We reproduce STRIP by following the</text>
<text top="191" left="312" width="251" height="9" font="font7" id="p8_t90" reading_order_no="89" segment_no="3" tag_type="text">method proposed in <a href="deeplearning_paper26.html#9">[12] </a>for comparision. As shown in Table</text>
<text top="203" left="312" width="251" height="9" font="font7" id="p8_t91" reading_order_no="90" segment_no="3" tag_type="text"><a href="deeplearning_paper26.html#8">V, </a>the performance of the proposed method is significantly</text>
<text top="215" left="312" width="251" height="9" font="font7" id="p8_t92" reading_order_no="91" segment_no="3" tag_type="text">better than that of STRIP <a href="deeplearning_paper26.html#9">[12] </a>on all the three datasets.</text>
<text top="227" left="312" width="251" height="9" font="font7" id="p8_t93" reading_order_no="92" segment_no="3" tag_type="text">Specifically, for STRIP, the backdoor detection rate is 63.40%,</text>
<text top="238" left="312" width="251" height="9" font="font7" id="p8_t94" reading_order_no="93" segment_no="3" tag_type="text">96.32% and 73.95% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10 <a href="deeplearning_paper26.html#9">[10]</a></text>
<text top="250" left="312" width="251" height="9" font="font7" id="p8_t95" reading_order_no="94" segment_no="3" tag_type="text">and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>respectively. For the proposed approach,</text>
<text top="262" left="312" width="251" height="9" font="font7" id="p8_t96" reading_order_no="95" segment_no="3" tag_type="text">the backdoor detection rate is 99.63%, 99.76% and 99.91%</text>
<text top="274" left="312" width="251" height="9" font="font7" id="p8_t97" reading_order_no="96" segment_no="3" tag_type="text">on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9]</a>, CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11]</a></text>
<text top="286" left="312" width="251" height="9" font="font7" id="p8_t98" reading_order_no="97" segment_no="3" tag_type="text">datasets, respectively. For the clean image identification rate,</text>
<text top="298" left="312" width="251" height="9" font="font7" id="p8_t99" reading_order_no="98" segment_no="3" tag_type="text">the proposed method also has better performance than STRIP</text>
<text top="310" left="312" width="251" height="9" font="font7" id="p8_t100" reading_order_no="99" segment_no="3" tag_type="text"><a href="deeplearning_paper26.html#9">[12]. </a>Specifically, for STRIP, the clean image identification</text>
<text top="322" left="312" width="251" height="9" font="font7" id="p8_t101" reading_order_no="100" segment_no="3" tag_type="text">rate is 67.40%, 89.57% and 88.80% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9],</a></text>
<text top="334" left="312" width="251" height="9" font="font7" id="p8_t102" reading_order_no="101" segment_no="3" tag_type="text">CIFAR-10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively. For</text>
<text top="346" left="312" width="251" height="9" font="font7" id="p8_t103" reading_order_no="102" segment_no="3" tag_type="text">the proposed method, the clean image identification rate is</text>
<text top="358" left="312" width="251" height="9" font="font7" id="p8_t104" reading_order_no="103" segment_no="3" tag_type="text">90.66%, 89.82% and 98.85% on Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-</text>
<text top="370" left="312" width="194" height="9" font="font7" id="p8_t105" reading_order_no="104" segment_no="3" tag_type="text">10 <a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively.</text>
<text top="393" left="421" width="33" height="7" font="font8" id="p8_t106" reading_order_no="105" segment_no="6" tag_type="title">TABLE V</text>
<text top="402" left="320" width="4" height="7" font="font8" id="p8_t107" reading_order_no="106" segment_no="7" tag_type="text">P</text>
<text top="403" left="324" width="231" height="6" font="font31" id="p8_t108" reading_order_no="107" segment_no="7" tag_type="text">ERFORMANCE COMPARISON BETWEEN THE PROPOSED METHOD AND</text>
<text top="411" left="417" width="40" height="7" font="font8" id="p8_t109" reading_order_no="108" segment_no="7" tag_type="text">STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="450" left="336" width="26" height="8" font="font36" id="p8_t110" reading_order_no="109" segment_no="8" tag_type="table">Dataset</text>
<text top="434" left="414" width="29" height="8" font="font36" id="p8_t111" reading_order_no="110" segment_no="8" tag_type="table">CIIR on</text>
<text top="445" left="405" width="46" height="8" font="font36" id="p8_t112" reading_order_no="111" segment_no="8" tag_type="table">clean images</text>
<text top="434" left="503" width="30" height="8" font="font36" id="p8_t113" reading_order_no="112" segment_no="8" tag_type="table">BDR on</text>
<text top="445" left="484" width="68" height="8" font="font36" id="p8_t114" reading_order_no="113" segment_no="8" tag_type="table">backdoor instances</text>
<text top="460" left="389" width="42" height="8" font="font36" id="p8_t115" reading_order_no="114" segment_no="8" tag_type="table">STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="460" left="446" width="17" height="8" font="font36" id="p8_t116" reading_order_no="115" segment_no="8" tag_type="table">Ours</text>
<text top="460" left="479" width="42" height="8" font="font36" id="p8_t117" reading_order_no="116" segment_no="8" tag_type="table">STRIP <a href="deeplearning_paper26.html#9">[12]</a></text>
<text top="460" left="536" width="17" height="8" font="font36" id="p8_t118" reading_order_no="117" segment_no="8" tag_type="table">Ours</text>
<text top="475" left="320" width="58" height="8" font="font36" id="p8_t119" reading_order_no="118" segment_no="8" tag_type="table">Fashion-MNIST</text>
<text top="475" left="396" width="27" height="8" font="font36" id="p8_t120" reading_order_no="119" segment_no="8" tag_type="table">67.40%</text>
<text top="475" left="441" width="27" height="8" font="font36" id="p8_t121" reading_order_no="120" segment_no="8" tag_type="table">90.66%</text>
<text top="475" left="486" width="27" height="8" font="font36" id="p8_t122" reading_order_no="121" segment_no="8" tag_type="table">63.40%</text>
<text top="475" left="531" width="27" height="8" font="font36" id="p8_t123" reading_order_no="122" segment_no="8" tag_type="table">99.63%</text>
<text top="485" left="331" width="37" height="8" font="font36" id="p8_t124" reading_order_no="123" segment_no="8" tag_type="table">CIFAR-10</text>
<text top="485" left="396" width="27" height="8" font="font36" id="p8_t125" reading_order_no="124" segment_no="8" tag_type="table">89.57%</text>
<text top="485" left="441" width="27" height="8" font="font36" id="p8_t126" reading_order_no="125" segment_no="8" tag_type="table">89.82%</text>
<text top="485" left="486" width="27" height="8" font="font36" id="p8_t127" reading_order_no="126" segment_no="8" tag_type="table">96.32%</text>
<text top="485" left="531" width="27" height="8" font="font36" id="p8_t128" reading_order_no="127" segment_no="8" tag_type="table">99.76%</text>
<text top="496" left="335" width="28" height="8" font="font36" id="p8_t129" reading_order_no="128" segment_no="8" tag_type="table">GTSRB</text>
<text top="496" left="396" width="27" height="8" font="font36" id="p8_t130" reading_order_no="129" segment_no="8" tag_type="table">88.80%</text>
<text top="496" left="441" width="27" height="8" font="font36" id="p8_t131" reading_order_no="130" segment_no="8" tag_type="table">98.85%</text>
<text top="496" left="486" width="27" height="8" font="font36" id="p8_t132" reading_order_no="131" segment_no="8" tag_type="table">73.95%</text>
<text top="496" left="531" width="27" height="8" font="font36" id="p8_t133" reading_order_no="132" segment_no="8" tag_type="table">99.91%</text>
<text top="525" left="322" width="241" height="9" font="font7" id="p8_t134" reading_order_no="133" segment_no="11" tag_type="text">Note that, the intensity of trigger in this experiment is at a</text>
<text top="537" left="312" width="251" height="9" font="font7" id="p8_t135" reading_order_no="134" segment_no="11" tag_type="text">low level (0.15, 0.5, 0.2 for Fashion-MNIST <a href="deeplearning_paper26.html#9">[9], </a>CIFAR-10</text>
<text top="549" left="312" width="251" height="9" font="font7" id="p8_t136" reading_order_no="135" segment_no="11" tag_type="text"><a href="deeplearning_paper26.html#9">[10] </a>and GTSRB <a href="deeplearning_paper26.html#9">[11] </a>datasets, respectively). For STRIP <a href="deeplearning_paper26.html#9">[12],</a></text>
<text top="560" left="312" width="251" height="9" font="font7" id="p8_t137" reading_order_no="136" segment_no="11" tag_type="text">the backdoor instance is totally superimposed with other image</text>
<text top="572" left="312" width="251" height="9" font="font7" id="p8_t138" reading_order_no="137" segment_no="11" tag_type="text">from different classes, so the trigger is inevitably blended</text>
<text top="584" left="312" width="251" height="9" font="font7" id="p8_t139" reading_order_no="138" segment_no="11" tag_type="text">with other pixels. Generally, when the intensity of trigger is</text>
<text top="596" left="312" width="251" height="9" font="font7" id="p8_t140" reading_order_no="139" segment_no="11" tag_type="text">normal, the trigger in the blended image may still activate the</text>
<text top="608" left="312" width="251" height="9" font="font7" id="p8_t141" reading_order_no="140" segment_no="11" tag_type="text">backdoor. However, the intensity of trigger in this experiment</text>
<text top="620" left="312" width="251" height="9" font="font7" id="p8_t142" reading_order_no="141" segment_no="11" tag_type="text">is low, so the trigger in the blended backdoor instance is</text>
<text top="632" left="312" width="251" height="9" font="font7" id="p8_t143" reading_order_no="142" segment_no="11" tag_type="text">destroyed and will be ignored by the model. Therefore, the</text>
<text top="644" left="312" width="251" height="9" font="font7" id="p8_t144" reading_order_no="143" segment_no="11" tag_type="text">entropy of this backdoor instance is similar to the entropy of</text>
<text top="656" left="312" width="251" height="9" font="font7" id="p8_t145" reading_order_no="144" segment_no="11" tag_type="text">clean images. As a result, STRIP <a href="deeplearning_paper26.html#9">[12] </a>will incorrectly consider</text>
<text top="668" left="312" width="251" height="9" font="font7" id="p8_t146" reading_order_no="145" segment_no="11" tag_type="text">the backdoor instance as a clean one. In comparison, the</text>
<text top="680" left="312" width="251" height="9" font="font7" id="p8_t147" reading_order_no="146" segment_no="11" tag_type="text">proposed method uses the universal adversarial perturbation</text>
<text top="692" left="312" width="251" height="9" font="font7" id="p8_t148" reading_order_no="147" segment_no="11" tag_type="text">(UAP <a href="deeplearning_paper26.html#9">[13]) </a>to perturb the input image. First, unlike STRIP</text>
<text top="704" left="312" width="251" height="9" font="font7" id="p8_t149" reading_order_no="148" segment_no="11" tag_type="text"><a href="deeplearning_paper26.html#9">[12], </a>adversarial perturbation will not globally perturb the</text>
<text top="716" left="312" width="251" height="9" font="font7" id="p8_t150" reading_order_no="149" segment_no="11" tag_type="text">input image. The perturbation will only modify limited number</text>
<text top="728" left="312" width="64" height="9" font="font7" id="p8_t151" reading_order_no="150" segment_no="11" tag_type="text">of pixels as the</text>
<text top="728" left="379" width="4" height="9" font="font19" id="p8_t152" reading_order_no="151" segment_no="11" tag_type="text">`</text>
<text top="731" left="384" width="8" height="7" font="font10" id="p8_t153" reading_order_no="152" segment_no="11" tag_type="text">∞</text>
<text top="728" left="396" width="167" height="9" font="font7" id="p8_t154" reading_order_no="153" segment_no="11" tag_type="text">norm of UAP is very low. Second, UAP</text>
<text top="740" left="312" width="251" height="9" font="font7" id="p8_t155" reading_order_no="154" segment_no="11" tag_type="text"><a href="deeplearning_paper26.html#9">[13] </a>is generated from a small set of clean images. Therefore,</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
<text top="30" left="563" width="3" height="6" font="font0" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="text">9</text>
<text top="58" left="49" width="251" height="9" font="font7" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="text">even if the backdoor instance is perturbed, the trigger in</text>
<text top="70" left="49" width="251" height="9" font="font7" id="p9_t3" reading_order_no="2" segment_no="1" tag_type="text">the backdoor instance will only be slightly affected. As a</text>
<text top="82" left="49" width="251" height="9" font="font7" id="p9_t4" reading_order_no="3" segment_no="1" tag_type="text">result, the proposed method can successfully detect backdoor</text>
<text top="94" left="49" width="251" height="9" font="font7" id="p9_t5" reading_order_no="4" segment_no="1" tag_type="text">instances carrying the trigger with low intensity. However,</text>
<text top="106" left="49" width="251" height="9" font="font7" id="p9_t6" reading_order_no="5" segment_no="1" tag_type="text">STRIP <a href="deeplearning_paper26.html#9">[12] </a>fails to detect some backdoor instances in this</text>
<text top="118" left="49" width="242" height="9" font="font7" id="p9_t7" reading_order_no="6" segment_no="1" tag_type="text">experiment, where the intensity of trigger is at a low level.</text>
<text top="130" left="59" width="241" height="9" font="font7" id="p9_t8" reading_order_no="7" segment_no="5" tag_type="text">In summary, there are two advantages of the proposed</text>
<text top="142" left="49" width="251" height="9" font="font7" id="p9_t9" reading_order_no="8" segment_no="5" tag_type="text">method over STRIP <a href="deeplearning_paper26.html#9">[12]. </a>First, the the proposed method</text>
<text top="154" left="49" width="251" height="9" font="font7" id="p9_t10" reading_order_no="9" segment_no="5" tag_type="text">is more effective than STRIP, as the proposed method will</text>
<text top="166" left="49" width="251" height="9" font="font7" id="p9_t11" reading_order_no="10" segment_no="5" tag_type="text">not destroy the trigger while STRIP may destroy the trigger.</text>
<text top="178" left="49" width="251" height="9" font="font7" id="p9_t12" reading_order_no="11" segment_no="5" tag_type="text">Second, the proposed method is more efficient than STRIP,</text>
<text top="190" left="49" width="251" height="9" font="font7" id="p9_t13" reading_order_no="12" segment_no="5" tag_type="text">as the proposed method only needs to predict two images</text>
<text top="202" left="49" width="251" height="9" font="font7" id="p9_t14" reading_order_no="13" segment_no="5" tag_type="text">(perturbed and unperturbed image) for each untrusted image</text>
<text top="214" left="49" width="221" height="9" font="font7" id="p9_t15" reading_order_no="14" segment_no="5" tag_type="text">and STRIP needs to predicts a set of blended images.</text>
<text top="242" left="139" width="21" height="9" font="font7" id="p9_t16" reading_order_no="15" segment_no="9" tag_type="title">V. C</text>
<text top="244" left="160" width="50" height="7" font="font8" id="p9_t17" reading_order_no="16" segment_no="9" tag_type="title">ONCLUSION</text>
<text top="259" left="59" width="241" height="9" font="font7" id="p9_t18" reading_order_no="17" segment_no="11" tag_type="text">In this paper, we propose a novel backdoor detection method</text>
<text top="271" left="49" width="251" height="9" font="font7" id="p9_t19" reading_order_no="18" segment_no="11" tag_type="text">based on adversarial perturbations. Specifically, the universal</text>
<text top="283" left="49" width="251" height="9" font="font7" id="p9_t20" reading_order_no="19" segment_no="11" tag_type="text">adversarial perturbation <a href="deeplearning_paper26.html#9">[13] </a>is first generated from the model,</text>
<text top="295" left="49" width="251" height="9" font="font7" id="p9_t21" reading_order_no="20" segment_no="11" tag_type="text">then the generated perturbation is added to the image. If</text>
<text top="307" left="49" width="251" height="9" font="font7" id="p9_t22" reading_order_no="21" segment_no="11" tag_type="text">the prediction of model on the perturbed image is consistent</text>
<text top="319" left="49" width="251" height="9" font="font7" id="p9_t23" reading_order_no="22" segment_no="11" tag_type="text">with the one on the unperturbed image, the input image is</text>
<text top="331" left="49" width="251" height="9" font="font7" id="p9_t24" reading_order_no="23" segment_no="11" tag_type="text">considered as a backdoor instance. Experimental results show</text>
<text top="343" left="49" width="251" height="9" font="font7" id="p9_t25" reading_order_no="24" segment_no="11" tag_type="text">that, the proposed defense method can achieve high backdoor</text>
<text top="355" left="49" width="251" height="9" font="font7" id="p9_t26" reading_order_no="25" segment_no="11" tag_type="text">detection rate and high clean image identification rate, while</text>
<text top="367" left="49" width="251" height="9" font="font7" id="p9_t27" reading_order_no="26" segment_no="11" tag_type="text">maintaining the visual quality of the image. Besides, the</text>
<text top="379" left="49" width="251" height="9" font="font7" id="p9_t28" reading_order_no="27" segment_no="11" tag_type="text">defense performance of the proposed method against backdoor</text>
<text top="391" left="49" width="251" height="9" font="font7" id="p9_t29" reading_order_no="28" segment_no="11" tag_type="text">attacks under different settings is also demonstrated to be</text>
<text top="403" left="49" width="251" height="9" font="font7" id="p9_t30" reading_order_no="29" segment_no="11" tag_type="text">effective. Our future work will explore the defenses against</text>
<text top="415" left="49" width="200" height="9" font="font7" id="p9_t31" reading_order_no="30" segment_no="11" tag_type="text">physical backdoor attacks in real physical world.</text>
<text top="443" left="147" width="7" height="9" font="font7" id="p9_t32" reading_order_no="31" segment_no="18" tag_type="title">R</text>
<text top="444" left="154" width="49" height="7" font="font8" id="p9_t33" reading_order_no="32" segment_no="18" tag_type="title">EFERENCES</text>
<text top="461" left="53" width="247" height="7" font="font8" id="p9_t34" reading_order_no="33" segment_no="20" tag_type="text">[1] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “BadNets: Evaluating</text>
<text top="470" left="67" width="151" height="7" font="font8" id="p9_t35" reading_order_no="34" segment_no="20" tag_type="text">backdooring attacks on deep neural networks,”</text>
<text top="470" left="221" width="41" height="7" font="font34" id="p9_t36" reading_order_no="35" segment_no="20" tag_type="text">IEEE Access</text>
<text top="470" left="263" width="37" height="7" font="font8" id="p9_t37" reading_order_no="36" segment_no="20" tag_type="text">, vol. 7, pp.</text>
<text top="479" left="67" width="69" height="7" font="font8" id="p9_t38" reading_order_no="37" segment_no="20" tag_type="text">47 230–47 244, 2019.</text>
<text top="488" left="53" width="247" height="7" font="font8" id="p9_t39" reading_order_no="38" segment_no="22" tag_type="text">[2] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks</text>
<text top="497" left="67" width="168" height="7" font="font8" id="p9_t40" reading_order_no="39" segment_no="22" tag_type="text">on deep learning systems using data poisoning,”</text>
<text top="497" left="240" width="58" height="7" font="font34" id="p9_t41" reading_order_no="40" segment_no="22" tag_type="text">arXiv:1712.05526</text>
<text top="497" left="298" width="2" height="7" font="font8" id="p9_t42" reading_order_no="41" segment_no="22" tag_type="text">,</text>
<text top="506" left="67" width="18" height="7" font="font8" id="p9_t43" reading_order_no="42" segment_no="22" tag_type="text">2017.</text>
<text top="515" left="53" width="247" height="7" font="font8" id="p9_t44" reading_order_no="43" segment_no="24" tag_type="text">[3] M. Barni, K. Kallas, and B. Tondi, “A new backdoor attack in CNNS by</text>
<text top="524" left="67" width="167" height="7" font="font8" id="p9_t45" reading_order_no="44" segment_no="24" tag_type="text">training set corruption without label poisoning,” in</text>
<text top="525" left="238" width="62" height="7" font="font34" id="p9_t46" reading_order_no="45" segment_no="24" tag_type="text">IEEE International</text>
<text top="533" left="67" width="108" height="7" font="font34" id="p9_t47" reading_order_no="46" segment_no="24" tag_type="text">Conference on Image Processing</text>
<text top="533" left="175" width="68" height="7" font="font8" id="p9_t48" reading_order_no="47" segment_no="24" tag_type="text">, 2019, pp. 101–105.</text>
<text top="543" left="53" width="247" height="7" font="font8" id="p9_t49" reading_order_no="48" segment_no="25" tag_type="text">[4] X. Zhang, A. Mian, R. Gupta, N. Rahnavard, and M. Shah, “Cas-</text>
<text top="552" left="67" width="233" height="7" font="font8" id="p9_t50" reading_order_no="49" segment_no="25" tag_type="text">sandra: Detecting trojaned networks from adversarial perturbations,”</text>
<text top="561" left="67" width="58" height="7" font="font34" id="p9_t51" reading_order_no="50" segment_no="25" tag_type="text">arXiv:2007.14433</text>
<text top="560" left="125" width="23" height="7" font="font8" id="p9_t52" reading_order_no="51" segment_no="25" tag_type="text">, 2020.</text>
<text top="570" left="53" width="247" height="7" font="font8" id="p9_t53" reading_order_no="52" segment_no="26" tag_type="text">[5] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “Detecting</text>
<text top="579" left="67" width="127" height="7" font="font8" id="p9_t54" reading_order_no="53" segment_no="26" tag_type="text">AI trojans using meta neural analysis,”</text>
<text top="579" left="197" width="58" height="7" font="font34" id="p9_t55" reading_order_no="54" segment_no="26" tag_type="text">arXiv:1910.03137</text>
<text top="579" left="256" width="23" height="7" font="font8" id="p9_t56" reading_order_no="55" segment_no="26" tag_type="text">, 2019.</text>
<text top="588" left="53" width="247" height="7" font="font8" id="p9_t57" reading_order_no="56" segment_no="27" tag_type="text">[6] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “Universal</text>
<text top="597" left="67" width="195" height="7" font="font8" id="p9_t58" reading_order_no="57" segment_no="27" tag_type="text">Litmus Patterns: Revealing backdoor attacks in CNNs,” in</text>
<text top="597" left="265" width="35" height="7" font="font34" id="p9_t59" reading_order_no="58" segment_no="27" tag_type="text">IEEE/CVF</text>
<text top="606" left="67" width="181" height="7" font="font34" id="p9_t60" reading_order_no="59" segment_no="27" tag_type="text">Conference on Computer Vision and Pattern Recognition</text>
<text top="606" left="248" width="52" height="7" font="font8" id="p9_t61" reading_order_no="60" segment_no="27" tag_type="text">, 2020, pp. 298–</text>
<text top="615" left="67" width="14" height="7" font="font8" id="p9_t62" reading_order_no="61" segment_no="27" tag_type="text">307.</text>
<text top="624" left="53" width="184" height="7" font="font8" id="p9_t63" reading_order_no="62" segment_no="28" tag_type="text">[7] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in</text>
<text top="624" left="239" width="61" height="7" font="font34" id="p9_t64" reading_order_no="63" segment_no="28" tag_type="text">IEEE International</text>
<text top="633" left="67" width="107" height="7" font="font34" id="p9_t65" reading_order_no="64" segment_no="28" tag_type="text">Conference on Computer Design</text>
<text top="633" left="174" width="60" height="7" font="font8" id="p9_t66" reading_order_no="65" segment_no="28" tag_type="text">, 2017, pp. 45–48.</text>
<text top="642" left="53" width="247" height="7" font="font8" id="p9_t67" reading_order_no="66" segment_no="29" tag_type="text">[8] X. Qiao, Y. Yang, and H. Li, “Defending neural backdoors via generative</text>
<text top="651" left="67" width="86" height="7" font="font8" id="p9_t68" reading_order_no="67" segment_no="29" tag_type="text">distribution modeling,” in</text>
<text top="651" left="157" width="143" height="7" font="font34" id="p9_t69" reading_order_no="68" segment_no="29" tag_type="text">Annual Conference on Neural Information</text>
<text top="660" left="67" width="63" height="7" font="font34" id="p9_t70" reading_order_no="69" segment_no="29" tag_type="text">Processing Systems</text>
<text top="660" left="130" width="87" height="7" font="font8" id="p9_t71" reading_order_no="70" segment_no="29" tag_type="text">, 2019, pp. 14 004–14 013.</text>
<text top="669" left="53" width="247" height="7" font="font8" id="p9_t72" reading_order_no="71" segment_no="30" tag_type="text">[9] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-MNIST: a novel</text>
<text top="678" left="67" width="19" height="7" font="font8" id="p9_t73" reading_order_no="72" segment_no="30" tag_type="text">image</text>
<text top="678" left="94" width="22" height="7" font="font8" id="p9_t74" reading_order_no="73" segment_no="30" tag_type="text">dataset</text>
<text top="678" left="124" width="9" height="7" font="font8" id="p9_t75" reading_order_no="74" segment_no="30" tag_type="text">for</text>
<text top="678" left="140" width="46" height="7" font="font8" id="p9_t76" reading_order_no="75" segment_no="30" tag_type="text">benchmarking</text>
<text top="678" left="193" width="27" height="7" font="font8" id="p9_t77" reading_order_no="76" segment_no="30" tag_type="text">machine</text>
<text top="678" left="227" width="26" height="7" font="font8" id="p9_t78" reading_order_no="77" segment_no="30" tag_type="text">learning</text>
<text top="678" left="261" width="39" height="7" font="font8" id="p9_t79" reading_order_no="78" segment_no="30" tag_type="text">algorithms,”</text>
<text top="687" left="67" width="58" height="7" font="font34" id="p9_t80" reading_order_no="79" segment_no="30" tag_type="text">arXiv:1708.07747</text>
<text top="687" left="125" width="52" height="7" font="font8" id="p9_t81" reading_order_no="80" segment_no="30" tag_type="text">, pp. 1–6, 2017.</text>
<text top="696" left="49" width="103" height="7" font="font8" id="p9_t82" reading_order_no="81" segment_no="31" tag_type="text">[10] A. Krizhevsky, G. Hinton</text>
<text top="696" left="155" width="17" height="7" font="font34" id="p9_t83" reading_order_no="82" segment_no="31" tag_type="text">et al.</text>
<text top="696" left="172" width="128" height="7" font="font8" id="p9_t84" reading_order_no="83" segment_no="31" tag_type="text">, “Learning multiple layers of features</text>
<text top="705" left="67" width="115" height="7" font="font8" id="p9_t85" reading_order_no="84" segment_no="31" tag_type="text">from tiny images,” pp. 1–60, 2009.</text>
<text top="714" left="49" width="251" height="7" font="font8" id="p9_t86" reading_order_no="85" segment_no="32" tag_type="text">[11] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “The german traffic</text>
<text top="723" left="67" width="233" height="7" font="font8" id="p9_t87" reading_order_no="86" segment_no="32" tag_type="text">sign recognition benchmark: A multi-class classification competition,”</text>
<text top="732" left="67" width="6" height="7" font="font8" id="p9_t88" reading_order_no="87" segment_no="32" tag_type="text">in</text>
<text top="732" left="76" width="167" height="7" font="font34" id="p9_t89" reading_order_no="88" segment_no="32" tag_type="text">International Joint Conference on Neural Networks</text>
<text top="732" left="243" width="57" height="7" font="font8" id="p9_t90" reading_order_no="89" segment_no="32" tag_type="text">, 2011, pp. 1453–</text>
<text top="741" left="67" width="18" height="7" font="font8" id="p9_t91" reading_order_no="90" segment_no="32" tag_type="text">1460.</text>
<text top="60" left="312" width="251" height="7" font="font8" id="p9_t92" reading_order_no="91" segment_no="2" tag_type="text">[12] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal,</text>
<text top="69" left="330" width="233" height="7" font="font8" id="p9_t93" reading_order_no="92" segment_no="2" tag_type="text">“STRIP: A defence against trojan attacks on deep neural networks,”</text>
<text top="78" left="330" width="6" height="7" font="font8" id="p9_t94" reading_order_no="93" segment_no="2" tag_type="text">in</text>
<text top="78" left="341" width="222" height="7" font="font34" id="p9_t95" reading_order_no="94" segment_no="2" tag_type="text">Proceedings of the 35th Annual Computer Security Applications</text>
<text top="87" left="330" width="36" height="7" font="font34" id="p9_t96" reading_order_no="95" segment_no="2" tag_type="text">Conference</text>
<text top="87" left="367" width="68" height="7" font="font8" id="p9_t97" reading_order_no="96" segment_no="2" tag_type="text">, 2019, pp. 113–125.</text>
<text top="96" left="312" width="251" height="7" font="font8" id="p9_t98" reading_order_no="97" segment_no="3" tag_type="text">[13] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal</text>
<text top="105" left="330" width="93" height="7" font="font8" id="p9_t99" reading_order_no="98" segment_no="3" tag_type="text">adversarial perturbations,” in</text>
<text top="105" left="426" width="137" height="7" font="font34" id="p9_t100" reading_order_no="99" segment_no="3" tag_type="text">IEEE Conference on Computer Vision and</text>
<text top="114" left="330" width="65" height="7" font="font34" id="p9_t101" reading_order_no="100" segment_no="3" tag_type="text">Pattern Recognition</text>
<text top="113" left="395" width="60" height="7" font="font8" id="p9_t102" reading_order_no="101" segment_no="3" tag_type="text">, 2017, pp. 86–94.</text>
<text top="122" left="312" width="251" height="7" font="font8" id="p9_t103" reading_order_no="102" segment_no="4" tag_type="text">[14] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing</text>
<text top="131" left="330" width="85" height="7" font="font8" id="p9_t104" reading_order_no="103" segment_no="4" tag_type="text">adversarial examples,” in</text>
<text top="131" left="419" width="144" height="7" font="font34" id="p9_t105" reading_order_no="104" segment_no="4" tag_type="text">3rd International Conference on Learning</text>
<text top="140" left="330" width="51" height="7" font="font34" id="p9_t106" reading_order_no="105" segment_no="4" tag_type="text">Representations</text>
<text top="140" left="381" width="56" height="7" font="font8" id="p9_t107" reading_order_no="106" segment_no="4" tag_type="text">, 2015, pp. 1–11.</text>
<text top="149" left="312" width="251" height="7" font="font8" id="p9_t108" reading_order_no="107" segment_no="6" tag_type="text">[15] M. Xue, C. He, J. Wang, and W. Liu, “One-to-N &amp; N-to-One: Two</text>
<text top="158" left="330" width="189" height="7" font="font8" id="p9_t109" reading_order_no="108" segment_no="6" tag_type="text">advanced backdoor attacks against deep learning models,”</text>
<text top="158" left="522" width="41" height="7" font="font34" id="p9_t110" reading_order_no="109" segment_no="6" tag_type="text">IEEE Trans-</text>
<text top="167" left="330" width="152" height="7" font="font34" id="p9_t111" reading_order_no="110" segment_no="6" tag_type="text">actions on Dependable and Secure Computing</text>
<text top="167" left="482" width="70" height="7" font="font8" id="p9_t112" reading_order_no="111" segment_no="6" tag_type="text">, 2020, Early Access.</text>
<text top="176" left="312" width="251" height="7" font="font8" id="p9_t113" reading_order_no="112" segment_no="7" tag_type="text">[16] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-Pruning: Defending against</text>
<text top="185" left="330" width="164" height="7" font="font8" id="p9_t114" reading_order_no="113" segment_no="7" tag_type="text">backdooring attacks on deep neural networks,” in</text>
<text top="185" left="498" width="65" height="7" font="font34" id="p9_t115" reading_order_no="114" segment_no="7" tag_type="text">Proceedings of 21st</text>
<text top="194" left="330" width="199" height="7" font="font34" id="p9_t116" reading_order_no="115" segment_no="7" tag_type="text">International Symposium on Attacks, Intrusions, and Defenses</text>
<text top="194" left="529" width="34" height="7" font="font8" id="p9_t117" reading_order_no="116" segment_no="7" tag_type="text">, 2018, pp.</text>
<text top="203" left="330" width="30" height="7" font="font8" id="p9_t118" reading_order_no="117" segment_no="7" tag_type="text">273–294.</text>
<text top="212" left="312" width="251" height="7" font="font8" id="p9_t119" reading_order_no="118" segment_no="8" tag_type="text">[17] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee,</text>
<text top="221" left="330" width="233" height="7" font="font8" id="p9_t120" reading_order_no="119" segment_no="8" tag_type="text">I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural</text>
<text top="230" left="330" width="127" height="7" font="font8" id="p9_t121" reading_order_no="120" segment_no="8" tag_type="text">networks by activation clustering,” in</text>
<text top="230" left="461" width="102" height="7" font="font34" id="p9_t122" reading_order_no="121" segment_no="8" tag_type="text">the 33th AAAI Conference on</text>
<text top="239" left="330" width="69" height="7" font="font34" id="p9_t123" reading_order_no="122" segment_no="8" tag_type="text">Artificial Intelligence</text>
<text top="239" left="399" width="92" height="7" font="font8" id="p9_t124" reading_order_no="123" segment_no="8" tag_type="text">, vol. 2301, 2019, pp. 1–10.</text>
<text top="248" left="312" width="251" height="7" font="font8" id="p9_t125" reading_order_no="124" segment_no="10" tag_type="text">[18] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.</text>
<text top="257" left="330" width="233" height="7" font="font8" id="p9_t126" reading_order_no="125" segment_no="10" tag_type="text">Zhao, “Neural Cleanse: Identifying and mitigating backdoor attacks in</text>
<text top="266" left="330" width="67" height="7" font="font8" id="p9_t127" reading_order_no="126" segment_no="10" tag_type="text">neural networks,” in</text>
<text top="266" left="400" width="140" height="7" font="font34" id="p9_t128" reading_order_no="127" segment_no="10" tag_type="text">IEEE Symposium on Security and Privacy</text>
<text top="266" left="540" width="23" height="7" font="font8" id="p9_t129" reading_order_no="128" segment_no="10" tag_type="text">, 2019,</text>
<text top="275" left="330" width="43" height="7" font="font8" id="p9_t130" reading_order_no="129" segment_no="10" tag_type="text">pp. 707–723.</text>
<text top="284" left="312" width="251" height="7" font="font8" id="p9_t131" reading_order_no="130" segment_no="12" tag_type="text">[19] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “DeepInspect: A black-box</text>
<text top="293" left="330" width="233" height="7" font="font8" id="p9_t132" reading_order_no="131" segment_no="12" tag_type="text">trojan detection and mitigation framework for deep neural networks,”</text>
<text top="302" left="330" width="6" height="7" font="font8" id="p9_t133" reading_order_no="132" segment_no="12" tag_type="text">in</text>
<text top="302" left="339" width="224" height="7" font="font34" id="p9_t134" reading_order_no="133" segment_no="12" tag_type="text">Proceedings of the 28th International Joint Conference on Artificial</text>
<text top="311" left="330" width="38" height="7" font="font34" id="p9_t135" reading_order_no="134" segment_no="12" tag_type="text">Intelligence</text>
<text top="311" left="368" width="76" height="7" font="font8" id="p9_t136" reading_order_no="135" segment_no="12" tag_type="text">, 2019, pp. 4658–4664.</text>
<text top="320" left="312" width="251" height="7" font="font8" id="p9_t137" reading_order_no="136" segment_no="13" tag_type="text">[20] N. Carlini and D. A. Wagner, “Towards evaluating the robustness of</text>
<text top="329" left="330" width="67" height="7" font="font8" id="p9_t138" reading_order_no="137" segment_no="13" tag_type="text">neural networks,” in</text>
<text top="329" left="400" width="140" height="7" font="font34" id="p9_t139" reading_order_no="138" segment_no="13" tag_type="text">IEEE Symposium on Security and Privacy</text>
<text top="329" left="540" width="23" height="7" font="font8" id="p9_t140" reading_order_no="139" segment_no="13" tag_type="text">, 2017,</text>
<text top="338" left="330" width="35" height="7" font="font8" id="p9_t141" reading_order_no="140" segment_no="13" tag_type="text">pp. 39–57.</text>
<text top="347" left="312" width="251" height="7" font="font8" id="p9_t142" reading_order_no="141" segment_no="14" tag_type="text">[21] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “DeepFool: A simple</text>
<text top="356" left="330" width="175" height="7" font="font8" id="p9_t143" reading_order_no="142" segment_no="14" tag_type="text">and accurate method to fool deep neural networks,” in</text>
<text top="356" left="507" width="56" height="7" font="font34" id="p9_t144" reading_order_no="143" segment_no="14" tag_type="text">IEEE Conference</text>
<text top="365" left="330" width="147" height="7" font="font34" id="p9_t145" reading_order_no="144" segment_no="14" tag_type="text">on Computer Vision and Pattern Recognition</text>
<text top="365" left="477" width="76" height="7" font="font8" id="p9_t146" reading_order_no="145" segment_no="14" tag_type="text">, 2016, pp. 2574–2582.</text>
<text top="374" left="312" width="251" height="7" font="font8" id="p9_t147" reading_order_no="146" segment_no="15" tag_type="text">[22] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards</text>
<text top="382" left="330" width="190" height="7" font="font8" id="p9_t148" reading_order_no="147" segment_no="15" tag_type="text">deep learning models resistant to adversarial attacks,” in</text>
<text top="383" left="523" width="40" height="7" font="font34" id="p9_t149" reading_order_no="148" segment_no="15" tag_type="text">6th Interna-</text>
<text top="392" left="330" width="154" height="7" font="font34" id="p9_t150" reading_order_no="149" segment_no="15" tag_type="text">tional Conference on Learning Representations</text>
<text top="391" left="485" width="56" height="7" font="font8" id="p9_t151" reading_order_no="150" segment_no="15" tag_type="text">, 2018, pp. 1–28.</text>
<text top="400" left="312" width="251" height="7" font="font8" id="p9_t152" reading_order_no="151" segment_no="16" tag_type="text">[23] N. Akhtar and A. S. Mian, “Threat of adversarial attacks on deep</text>
<text top="409" left="330" width="126" height="7" font="font8" id="p9_t153" reading_order_no="152" segment_no="16" tag_type="text">learning in computer vision: A survey,”</text>
<text top="409" left="458" width="41" height="7" font="font34" id="p9_t154" reading_order_no="153" segment_no="16" tag_type="text">IEEE Access</text>
<text top="409" left="499" width="64" height="7" font="font8" id="p9_t155" reading_order_no="154" segment_no="16" tag_type="text">, vol. 6, pp. 14 410–</text>
<text top="418" left="330" width="44" height="7" font="font8" id="p9_t156" reading_order_no="155" segment_no="16" tag_type="text">14 430, 2018.</text>
<text top="427" left="312" width="251" height="7" font="font8" id="p9_t157" reading_order_no="156" segment_no="17" tag_type="text">[24] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.</text>
<text top="436" left="330" width="233" height="7" font="font8" id="p9_t158" reading_order_no="157" segment_no="17" tag_type="text">Goodfellow, and R. Fergus, “Intriguing properties of neural networks,”</text>
<text top="445" left="330" width="54" height="7" font="font34" id="p9_t159" reading_order_no="158" segment_no="17" tag_type="text">arXiv:1312.6199</text>
<text top="445" left="384" width="23" height="7" font="font8" id="p9_t160" reading_order_no="159" segment_no="17" tag_type="text">, 2013.</text>
<text top="454" left="312" width="251" height="7" font="font8" id="p9_t161" reading_order_no="160" segment_no="19" tag_type="text">[25] G. Huang, Z. Liu, and K. Q. Weinberger, “Densely connected convolu-</text>
<text top="463" left="330" width="55" height="7" font="font8" id="p9_t162" reading_order_no="161" segment_no="19" tag_type="text">tional networks,”</text>
<text top="463" left="388" width="58" height="7" font="font34" id="p9_t163" reading_order_no="162" segment_no="19" tag_type="text">arXiv:1608.06993</text>
<text top="463" left="446" width="23" height="7" font="font8" id="p9_t164" reading_order_no="163" segment_no="19" tag_type="text">, 2016.</text>
<text top="472" left="312" width="251" height="7" font="font8" id="p9_t165" reading_order_no="164" segment_no="21" tag_type="text">[26] S. Targ, D. Almeida, and K. Lyman, “ResNet in ResNet: Generalizing</text>
<text top="481" left="330" width="74" height="7" font="font8" id="p9_t166" reading_order_no="165" segment_no="21" tag_type="text">residual architectures,”</text>
<text top="481" left="407" width="58" height="7" font="font34" id="p9_t167" reading_order_no="166" segment_no="21" tag_type="text">arXiv:1603.08029</text>
<text top="481" left="465" width="23" height="7" font="font8" id="p9_t168" reading_order_no="167" segment_no="21" tag_type="text">, 2016.</text>
<text top="490" left="312" width="251" height="7" font="font8" id="p9_t169" reading_order_no="168" segment_no="23" tag_type="text">[27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification</text>
<text top="499" left="330" width="137" height="7" font="font8" id="p9_t170" reading_order_no="169" segment_no="23" tag_type="text">with deep convolutional neural networks,”</text>
<text top="499" left="470" width="93" height="7" font="font34" id="p9_t171" reading_order_no="170" segment_no="23" tag_type="text">Advances in neural informa-</text>
<text top="508" left="330" width="77" height="7" font="font34" id="p9_t172" reading_order_no="171" segment_no="23" tag_type="text">tion processing systems</text>
<text top="508" left="407" width="104" height="7" font="font8" id="p9_t173" reading_order_no="172" segment_no="23" tag_type="text">, vol. 25, pp. 1097–1105, 2012.</text>
</page>
<outline>
<item page="1">I Introduction</item>
<item page="2">II Background and Related Work</item>
<outline>
<item page="2">II-A Universarial Adversarial Perturbations Moosavi-Dezfooli17</item>
<item page="2">II-B Backdoor Attacks</item>
<item page="2">II-C Existing Backdoor Defenses</item>
</outline>
<item page="3">III The Proposed Method</item>
<outline>
<item page="3">III-A Overall flow</item>
<item page="4">III-B Perturbation Generation</item>
<item page="4">III-C Backdoor Detection</item>
<item page="5">III-D Why choose UAP Moosavi-Dezfooli17 for adversarial perturbation generation?</item>
</outline>
<item page="5">IV Experimental Results</item>
<outline>
<item page="5">IV-A Experimental Setup</item>
<outline>
<item page="5">IV-A1 Datasets</item>
<item page="5">IV-A2 Experimental Settings of Backdoor Attack</item>
<item page="5">IV-A3 Metrics</item>
</outline>
<item page="5">IV-B Effectiveness of the Proposed Method</item>
<item page="6">IV-C Defense Performance of the Proposed Method under Different Attack Settings</item>
<outline>
<item page="6">IV-C1 Trigger Transparency</item>
<item page="6">IV-C2 Trigger Size</item>
<item page="6">IV-C3 Trigger Pattern</item>
</outline>
<item page="7">IV-D Experiment Results of the Proposed Method with Four Different Adversarial Perturbation Generation Methods.</item>
<item page="8">IV-E Comparison with Related Work</item>
</outline>
<item page="9">V Conclusion</item>
<item page="9">References</item>
</outline>
</pdf2xml>
