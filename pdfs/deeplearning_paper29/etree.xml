<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="24" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="11" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font2" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="9" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font4" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font5" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font6" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font7" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font8" size="20" family="Times" color="#7f7f7f"/>
<text top="55" left="79" width="454" height="21" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">PALMAR: Towards Adaptive Multi-inhabitant</text>
<text top="83" left="68" width="476" height="21" font="font0" id="p1_t2" reading_order_no="2" segment_no="0" tag_type="title">Activity Recognition in Point-Cloud Technology</text>
<text top="123" left="148" width="316" height="10" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="text">Mohammad Arif Ul Alam, Md Mahmudur Rahman, Jared Q Widberg</text>
<text top="136" left="136" width="336" height="9" font="font2" id="p1_t4" reading_order_no="4" segment_no="1" tag_type="text">Department of Computer Science, University of Massachusetts Lowell, MA, USA</text>
<text top="172" left="59" width="241" height="8" font="font3" id="p1_t5" reading_order_no="5" segment_no="2" tag_type="text">Abstract —With the advancement of deep neural networks and</text>
<text top="182" left="49" width="251" height="8" font="font4" id="p1_t6" reading_order_no="6" segment_no="2" tag_type="text">computer vision-based Human Activity Recognition, employment</text>
<text top="192" left="49" width="251" height="8" font="font4" id="p1_t7" reading_order_no="7" segment_no="2" tag_type="text">of Point-Cloud Data technologies (LiDAR, mmWave) has seen a</text>
<text top="202" left="49" width="251" height="8" font="font4" id="p1_t8" reading_order_no="8" segment_no="2" tag_type="text">lot interests due to its privacy preserving nature. Given the high</text>
<text top="212" left="49" width="252" height="8" font="font4" id="p1_t9" reading_order_no="9" segment_no="2" tag_type="text">promise of accurate PCD technologies, we develop, PALMAR,</text>
<text top="222" left="49" width="251" height="8" font="font4" id="p1_t10" reading_order_no="10" segment_no="2" tag_type="text">a multiple-inhabitant activity recognition system by employing</text>
<text top="232" left="49" width="251" height="8" font="font4" id="p1_t11" reading_order_no="11" segment_no="2" tag_type="text">efficient signal processing and novel machine learning techniques</text>
<text top="242" left="49" width="253" height="8" font="font4" id="p1_t12" reading_order_no="12" segment_no="2" tag_type="text">to track individual person towards developing an adaptive multi-</text>
<text top="252" left="49" width="251" height="8" font="font4" id="p1_t13" reading_order_no="13" segment_no="2" tag_type="text">inhabitant tracking and HAR system. More specifically, we</text>
<text top="262" left="49" width="251" height="8" font="font4" id="p1_t14" reading_order_no="14" segment_no="2" tag_type="text">propose (i) a voxelized feature representation-based real-time</text>
<text top="272" left="49" width="251" height="8" font="font4" id="p1_t15" reading_order_no="15" segment_no="2" tag_type="text">PCD fine-tuning method, (ii) efficient clustering (DBSCAN and</text>
<text top="282" left="49" width="253" height="8" font="font4" id="p1_t16" reading_order_no="16" segment_no="2" tag_type="text">BIRCH), Adaptive Order Hidden Markov Model based multi-</text>
<text top="292" left="49" width="251" height="8" font="font4" id="p1_t17" reading_order_no="17" segment_no="2" tag_type="text">person tracking and crossover ambiguity reduction techniques</text>
<text top="302" left="49" width="251" height="8" font="font4" id="p1_t18" reading_order_no="18" segment_no="2" tag_type="text">and (iii) novel adaptive deep learning-based domain adaptation</text>
<text top="312" left="49" width="251" height="8" font="font4" id="p1_t19" reading_order_no="19" segment_no="2" tag_type="text">technique to improve the accuracy of HAR in presence of data</text>
<text top="322" left="49" width="253" height="8" font="font4" id="p1_t20" reading_order_no="20" segment_no="2" tag_type="text">scarcity and diversity (device, location and population diversity).</text>
<text top="332" left="49" width="252" height="8" font="font4" id="p1_t21" reading_order_no="21" segment_no="2" tag_type="text">We experimentally evaluate our framework and systems using</text>
<text top="342" left="49" width="251" height="8" font="font4" id="p1_t22" reading_order_no="22" segment_no="2" tag_type="text">(i) a real-time PCD collected by three devices (3D LiDAR</text>
<text top="352" left="49" width="251" height="8" font="font4" id="p1_t23" reading_order_no="23" segment_no="2" tag_type="text">and 79 GHz mmWave) from 6 participants, (ii) one publicly</text>
<text top="362" left="49" width="251" height="8" font="font4" id="p1_t24" reading_order_no="24" segment_no="2" tag_type="text">available 3D LiDAR activity data (28 participants) and (iii) an</text>
<text top="372" left="49" width="251" height="8" font="font4" id="p1_t25" reading_order_no="25" segment_no="2" tag_type="text">embedded hardware prototype system which provided promising</text>
<text top="381" left="49" width="251" height="8" font="font4" id="p1_t26" reading_order_no="26" segment_no="2" tag_type="text">HAR performances in multi-inhabitants (96%) scenario with</text>
<text top="391" left="49" width="251" height="8" font="font4" id="p1_t27" reading_order_no="27" segment_no="2" tag_type="text">a 63% improvement of multi-person tracking than state-of-art</text>
<text top="401" left="49" width="251" height="8" font="font4" id="p1_t28" reading_order_no="28" segment_no="2" tag_type="text">framework without losing significant system performances in the</text>
<text top="411" left="49" width="91" height="8" font="font4" id="p1_t29" reading_order_no="29" segment_no="2" tag_type="text">edge computing device.</text>
<text top="422" left="59" width="242" height="8" font="font3" id="p1_t30" reading_order_no="30" segment_no="6" tag_type="text">Index Terms —Human Activity Recognition, Domain Adapta-</text>
<text top="432" left="49" width="186" height="8" font="font4" id="p1_t31" reading_order_no="31" segment_no="6" tag_type="text">tion, PCD Sensor Technology, Edge Computing</text>
<text top="454" left="136" width="77" height="9" font="font2" id="p1_t32" reading_order_no="32" segment_no="7" tag_type="title">I. I NTRODUCTION</text>
<text top="470" left="59" width="242" height="9" font="font2" id="p1_t33" reading_order_no="33" segment_no="8" tag_type="text">Though, recent advancement of internet-of-things (IoT)</text>
<text top="482" left="49" width="253" height="9" font="font2" id="p1_t34" reading_order_no="34" segment_no="8" tag_type="text">sensors and deep learning techniques result significant improve-</text>
<text top="494" left="49" width="251" height="9" font="font2" id="p1_t35" reading_order_no="35" segment_no="8" tag_type="text">ment in HAR, the most accurate Multiple Person Tracking and</text>
<text top="506" left="49" width="251" height="9" font="font2" id="p1_t36" reading_order_no="36" segment_no="8" tag_type="text">Human Activity Recognition (MPT-HAR) state-of-arts are still</text>
<text top="518" left="49" width="252" height="9" font="font2" id="p1_t37" reading_order_no="37" segment_no="8" tag_type="text">computer vision based techniques [30], [31], [32], [36], [37],</text>
<text top="530" left="49" width="251" height="9" font="font2" id="p1_t38" reading_order_no="38" segment_no="8" tag_type="text">[38], [39], [40], [41], [42], [43], [44], [45], [46], [47]. Due</text>
<text top="542" left="49" width="251" height="9" font="font2" id="p1_t39" reading_order_no="39" segment_no="8" tag_type="text">to the lower acceptance of privacy-concerned camera-based</text>
<text top="554" left="49" width="251" height="9" font="font2" id="p1_t40" reading_order_no="40" segment_no="8" tag_type="text">frameworks, many researchers employed privacy-preserving</text>
<text top="566" left="49" width="251" height="9" font="font2" id="p1_t41" reading_order_no="41" segment_no="8" tag_type="text">PCD technologies such as Light Detection and Ranging</text>
<text top="578" left="49" width="251" height="9" font="font2" id="p1_t42" reading_order_no="42" segment_no="8" tag_type="text">(LiDAR), millimeter Wave (mmWave) and Ultrasound (US) to</text>
<text top="590" left="49" width="251" height="9" font="font2" id="p1_t43" reading_order_no="43" segment_no="8" tag_type="text">investigate MPT-HAR problem to reach the closest accuracy</text>
<text top="602" left="49" width="251" height="9" font="font2" id="p1_t44" reading_order_no="44" segment_no="8" tag_type="text">to camera-based techniques by introducing extensive signal</text>
<text top="614" left="49" width="252" height="9" font="font2" id="p1_t45" reading_order_no="45" segment_no="8" tag_type="text">processing and complex deep learning models [30], [31],</text>
<text top="626" left="49" width="251" height="9" font="font2" id="p1_t46" reading_order_no="46" segment_no="8" tag_type="text">[32], [36]. In this paper, we propose a novel MPT-HAR</text>
<text top="638" left="49" width="251" height="9" font="font2" id="p1_t47" reading_order_no="47" segment_no="8" tag_type="text">system that can transfer the domain knowledge from accurate</text>
<text top="650" left="49" width="253" height="9" font="font2" id="p1_t48" reading_order_no="48" segment_no="8" tag_type="text">privacy concerned technology (say Computer Vision) to privacy-</text>
<text top="662" left="49" width="251" height="9" font="font2" id="p1_t49" reading_order_no="49" segment_no="8" tag_type="text">preserving PCD technologies by utilizing signal processing</text>
<text top="674" left="49" width="173" height="9" font="font2" id="p1_t50" reading_order_no="50" segment_no="8" tag_type="text">and adaptive machine learning techniques.</text>
<text top="686" left="59" width="241" height="9" font="font2" id="p1_t51" reading_order_no="51" segment_no="10" tag_type="text">Many PCD technologies, such as Ultrasound, mmWave and</text>
<text top="698" left="49" width="251" height="9" font="font2" id="p1_t52" reading_order_no="52" segment_no="10" tag_type="text">LiDAR, have been used by researchers before for gait pattern</text>
<text top="710" left="49" width="253" height="9" font="font2" id="p1_t53" reading_order_no="53" segment_no="10" tag_type="text">identification [5], activity recognition [5], person tracking [1].</text>
<text top="172" left="312" width="253" height="9" font="font2" id="p1_t54" reading_order_no="54" segment_no="3" tag_type="text">All of the above sensors provide PCD that are, in general,</text>
<text top="184" left="312" width="251" height="9" font="font2" id="p1_t55" reading_order_no="55" segment_no="3" tag_type="text">large data sets composed of 3D point data. PCD are derived</text>
<text top="196" left="312" width="251" height="9" font="font2" id="p1_t56" reading_order_no="56" segment_no="3" tag_type="text">from raw data scanned from physical objects such as building</text>
<text top="208" left="312" width="252" height="9" font="font2" id="p1_t57" reading_order_no="57" segment_no="3" tag_type="text">exteriors and interiors, humans, process plants, topographies,</text>
<text top="220" left="312" width="251" height="9" font="font2" id="p1_t58" reading_order_no="58" segment_no="3" tag_type="text">and manufactured items. More specifically for PCD generating</text>
<text top="231" left="312" width="251" height="9" font="font2" id="p1_t59" reading_order_no="59" segment_no="3" tag_type="text">systems that emit light pulses outside visible light spectrum</text>
<text top="243" left="312" width="251" height="9" font="font2" id="p1_t60" reading_order_no="60" segment_no="3" tag_type="text">and capture the duration of its return. The distance vector of</text>
<text top="255" left="312" width="251" height="9" font="font2" id="p1_t61" reading_order_no="61" segment_no="3" tag_type="text">returned pulse is saved as point of cloud which is represented</text>
<text top="267" left="312" width="139" height="9" font="font2" id="p1_t62" reading_order_no="62" segment_no="3" tag_type="text">as x , y and z values in 3D space.</text>
<text top="279" left="322" width="241" height="9" font="font2" id="p1_t63" reading_order_no="63" segment_no="4" tag_type="text">Previously, expensive PCD systems have been used to solve</text>
<text top="291" left="312" width="251" height="9" font="font2" id="p1_t64" reading_order_no="64" segment_no="4" tag_type="text">different problems such as object detection [14], distance</text>
<text top="303" left="312" width="251" height="9" font="font2" id="p1_t65" reading_order_no="65" segment_no="4" tag_type="text">identification [29], 3D imaging [29], multiple person tracking</text>
<text top="315" left="312" width="251" height="9" font="font2" id="p1_t66" reading_order_no="66" segment_no="4" tag_type="text">[1] and gait recognition [1]. The above solutions have been</text>
<text top="327" left="312" width="251" height="9" font="font2" id="p1_t67" reading_order_no="67" segment_no="4" tag_type="text">applied on many applications such as automated driving</text>
<text top="339" left="312" width="251" height="9" font="font2" id="p1_t68" reading_order_no="68" segment_no="4" tag_type="text">[29], remote health monitoring [1] and elderly care [1]. In</text>
<text top="351" left="312" width="251" height="9" font="font2" id="p1_t69" reading_order_no="69" segment_no="4" tag_type="text">current state-of-art methods, employment of intense signal</text>
<text top="363" left="312" width="251" height="9" font="font2" id="p1_t70" reading_order_no="70" segment_no="4" tag_type="text">processing and deep learning techniques on PCD Data resulted</text>
<text top="375" left="312" width="251" height="9" font="font2" id="p1_t71" reading_order_no="71" segment_no="4" tag_type="text">maximum accuracy of multiple person tracking with 89% [1]</text>
<text top="387" left="312" width="251" height="9" font="font2" id="p1_t72" reading_order_no="72" segment_no="4" tag_type="text">and multiple person HAR with 75% [5] which needs significant</text>
<text top="399" left="312" width="55" height="9" font="font2" id="p1_t73" reading_order_no="73" segment_no="4" tag_type="text">improvement.</text>
<text top="411" left="322" width="241" height="9" font="font2" id="p1_t74" reading_order_no="74" segment_no="5" tag_type="text">In this paper, we argue that privacy preserving PCD data</text>
<text top="423" left="312" width="251" height="9" font="font2" id="p1_t75" reading_order_no="75" segment_no="5" tag_type="text">based HAR can be significantly improved by existing computer</text>
<text top="435" left="312" width="251" height="9" font="font2" id="p1_t76" reading_order_no="76" segment_no="5" tag_type="text">vision data which needs careful utilization of signal processing</text>
<text top="447" left="312" width="252" height="9" font="font2" id="p1_t77" reading_order_no="77" segment_no="5" tag_type="text">as well as novel domain adaptation techniques. In this regard,</text>
<text top="459" left="312" width="251" height="9" font="font2" id="p1_t78" reading_order_no="78" segment_no="5" tag_type="text">we propose novel trios: Adaptive Order Hidden Markov Model</text>
<text top="471" left="312" width="253" height="9" font="font2" id="p1_t79" reading_order_no="79" segment_no="5" tag_type="text">to track multiple persons, Crossover Path Disambiguation Algo-</text>
<text top="483" left="312" width="253" height="9" font="font2" id="p1_t80" reading_order_no="80" segment_no="5" tag_type="text">rithm (CPDA) to improve tracking and variational autoencoder-</text>
<text top="494" left="312" width="253" height="9" font="font2" id="p1_t81" reading_order_no="81" segment_no="5" tag_type="text">based domain adaptation algorithm to improve activity recog-</text>
<text top="506" left="312" width="251" height="9" font="font2" id="p1_t82" reading_order_no="82" segment_no="5" tag_type="text">nition. The core of our proposed architecture is an adaptive</text>
<text top="518" left="312" width="252" height="9" font="font2" id="p1_t83" reading_order_no="83" segment_no="5" tag_type="text">deep learning framework that consists of the following modules:</text>
<text top="530" left="312" width="251" height="9" font="font7" id="p1_t84" reading_order_no="84" segment_no="5" tag_type="text">Person tracker module combines voxel representator, DBSCAN</text>
<text top="542" left="312" width="251" height="9" font="font2" id="p1_t85" reading_order_no="85" segment_no="5" tag_type="text">and BIRCH clustering method to represent LiDAR PCD to</text>
<text top="554" left="312" width="253" height="9" font="font2" id="p1_t86" reading_order_no="86" segment_no="5" tag_type="text">reduce data dimension i.e. improve computational efficiency.</text>
<text top="566" left="312" width="251" height="9" font="font2" id="p1_t87" reading_order_no="87" segment_no="5" tag_type="text">The feature extractor module, which is a Convolutional Neural</text>
<text top="578" left="312" width="251" height="9" font="font2" id="p1_t88" reading_order_no="88" segment_no="5" tag_type="text">Network (CNN), cooperates with the activity recognizer to</text>
<text top="590" left="312" width="252" height="9" font="font2" id="p1_t89" reading_order_no="89" segment_no="5" tag_type="text">recognize person tracker provided multiple-humans’ activities,</text>
<text top="602" left="312" width="251" height="9" font="font2" id="p1_t90" reading_order_no="90" segment_no="5" tag_type="text">and simultaneously, minimize the KL divergence of variational</text>
<text top="614" left="312" width="251" height="9" font="font2" id="p1_t91" reading_order_no="91" segment_no="5" tag_type="text">autoencoder-based domain adaptation module to diminish</text>
<text top="626" left="312" width="251" height="9" font="font2" id="p1_t92" reading_order_no="92" segment_no="5" tag_type="text">environment/subject-independent divergence. Our framework</text>
<text top="638" left="312" width="251" height="9" font="font2" id="p1_t93" reading_order_no="93" segment_no="5" tag_type="text">not only improves the performance of MPT-HAR state-of-arts</text>
<text top="650" left="312" width="251" height="9" font="font2" id="p1_t94" reading_order_no="94" segment_no="5" tag_type="text">in supervised learning scenario, but also the domain adaptation</text>
<text top="662" left="312" width="251" height="9" font="font2" id="p1_t95" reading_order_no="95" segment_no="5" tag_type="text">techniques significantly improve HAR performance to facilitate</text>
<text top="674" left="312" width="251" height="9" font="font2" id="p1_t96" reading_order_no="96" segment_no="5" tag_type="text">low or unlabelled target environments/domains without losing</text>
<text top="686" left="312" width="251" height="9" font="font2" id="p1_t97" reading_order_no="97" segment_no="5" tag_type="text">any significant performance reduction in the edge computing</text>
<text top="698" left="312" width="251" height="9" font="font2" id="p1_t98" reading_order_no="98" segment_no="5" tag_type="text">system. The experimental results demonstrate the superiority</text>
<text top="710" left="312" width="251" height="9" font="font2" id="p1_t99" reading_order_no="99" segment_no="5" tag_type="text">of PALMAR frameworks and systems in terms of effectiveness</text>
<text top="546" left="32" width="0" height="18" font="font8" id="p1_t100" reading_order_no="0" segment_no="9" tag_type="title">arXiv:2106.11902v1  [cs.CV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font9" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font10" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font11" size="7" family="CMMI7" color="#000000"/>
<text top="52" left="49" width="82" height="9" font="font2" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="text">and generalizability.</text>
<text top="220" left="132" width="84" height="7" font="font5" id="p2_t2" reading_order_no="1" segment_no="6" tag_type="text">Fig. 1. System Overview</text>
<text top="261" left="125" width="100" height="9" font="font2" id="p2_t3" reading_order_no="2" segment_no="7" tag_type="title">II. S YSTEM O VERVIEW</text>
<text top="277" left="59" width="239" height="9" font="font2" id="p2_t4" reading_order_no="3" segment_no="8" tag_type="text">As shown in Fig. 1, PALMAR consists of five components</text>
<text top="291" left="59" width="241" height="9" font="font10" id="p2_t5" reading_order_no="4" segment_no="9" tag_type="list">• Edge Computing Hardware Setup : We integrated a</text>
<text top="302" left="69" width="231" height="10" font="font6" id="p2_t6" reading_order_no="5" segment_no="9" tag_type="list">N V IDIA R J etson N ano T M Developer Kit with the</text>
<text top="315" left="69" width="231" height="9" font="font2" id="p2_t7" reading_order_no="6" segment_no="9" tag_type="list">three PCD generating sensors (3D LiDAR and 79 GHz</text>
<text top="327" left="69" width="231" height="9" font="font2" id="p2_t8" reading_order_no="7" segment_no="9" tag_type="list">mmWave) and a camera using microUSB cable which</text>
<text top="339" left="69" width="231" height="9" font="font2" id="p2_t9" reading_order_no="8" segment_no="9" tag_type="list">has been used as our edge computing device in collecting</text>
<text top="351" left="69" width="113" height="9" font="font2" id="p2_t10" reading_order_no="9" segment_no="9" tag_type="list">data and system evaluation.</text>
<text top="363" left="59" width="241" height="9" font="font10" id="p2_t11" reading_order_no="10" segment_no="12" tag_type="list">• Data Collection : In this module, we consider a scenario</text>
<text top="375" left="69" width="233" height="9" font="font2" id="p2_t12" reading_order_no="11" segment_no="12" tag_type="list">where the human activities are monitored in different en-</text>
<text top="387" left="69" width="233" height="9" font="font2" id="p2_t13" reading_order_no="12" segment_no="12" tag_type="list">vironments such as different sized rooms, indoor, outdoor,</text>
<text top="399" left="69" width="231" height="9" font="font2" id="p2_t14" reading_order_no="13" segment_no="12" tag_type="list">single inhabitant, multiple inhabitants. Our system first</text>
<text top="411" left="69" width="231" height="9" font="font2" id="p2_t15" reading_order_no="14" segment_no="12" tag_type="list">collects the activity data in each environment during the</text>
<text top="423" left="69" width="231" height="9" font="font2" id="p2_t16" reading_order_no="15" segment_no="12" tag_type="list">monitoring process. In this regard, we placed all of the</text>
<text top="435" left="69" width="233" height="9" font="font2" id="p2_t17" reading_order_no="16" segment_no="12" tag_type="list">testbed sensors in a cardboard along with an IP camera.</text>
<text top="447" left="69" width="231" height="9" font="font2" id="p2_t18" reading_order_no="17" segment_no="12" tag_type="list">The camera records the videos of the performed activities</text>
<text top="459" left="69" width="231" height="9" font="font2" id="p2_t19" reading_order_no="18" segment_no="12" tag_type="list">which have been used to label activity ground truth as</text>
<text top="471" left="69" width="200" height="9" font="font2" id="p2_t20" reading_order_no="19" segment_no="12" tag_type="list">well as computer vision-based HAR recognition.</text>
<text top="482" left="59" width="241" height="10" font="font10" id="p2_t21" reading_order_no="20" segment_no="13" tag_type="list">• Data Preprocessing : In this module, we first normalize</text>
<text top="494" left="69" width="231" height="9" font="font2" id="p2_t22" reading_order_no="21" segment_no="13" tag_type="list">the acquired signal and then transform the signal to a</text>
<text top="506" left="69" width="231" height="9" font="font2" id="p2_t23" reading_order_no="22" segment_no="13" tag_type="list">form suitable for analysis. Finally we split the transformed</text>
<text top="518" left="69" width="231" height="9" font="font2" id="p2_t24" reading_order_no="23" segment_no="13" tag_type="list">signal into short segments to train the activity recognition</text>
<text top="530" left="69" width="231" height="9" font="font2" id="p2_t25" reading_order_no="24" segment_no="13" tag_type="list">model. Then we represent the PCD to voxel format and</text>
<text top="542" left="69" width="217" height="9" font="font2" id="p2_t26" reading_order_no="25" segment_no="13" tag_type="list">apply signal processing techniques to fine tune PCD.</text>
<text top="554" left="59" width="241" height="9" font="font10" id="p2_t27" reading_order_no="26" segment_no="15" tag_type="list">• Multiple person tracker : In this module, we apply</text>
<text top="566" left="69" width="231" height="9" font="font2" id="p2_t28" reading_order_no="27" segment_no="15" tag_type="list">BIRCH and DBSCAN clustering method to identify</text>
<text top="578" left="69" width="232" height="9" font="font2" id="p2_t29" reading_order_no="28" segment_no="15" tag_type="list">number of persons present in the field of view (FOV)</text>
<text top="590" left="69" width="231" height="9" font="font2" id="p2_t30" reading_order_no="29" segment_no="15" tag_type="list">and track multiple persons related PCD clusters centroid</text>
<text top="602" left="69" width="232" height="9" font="font2" id="p2_t31" reading_order_no="30" segment_no="15" tag_type="list">using an Adaptive Order Hidden Markov (AO-HMM)</text>
<text top="614" left="69" width="231" height="9" font="font2" id="p2_t32" reading_order_no="31" segment_no="15" tag_type="list">Model and a Crossover Path Disambiguation Algorithm</text>
<text top="626" left="69" width="81" height="9" font="font2" id="p2_t33" reading_order_no="32" segment_no="15" tag_type="list">(CPDA) algorithms.</text>
<text top="638" left="59" width="241" height="9" font="font10" id="p2_t34" reading_order_no="33" segment_no="17" tag_type="list">• Deep Learning Model : We develop a baseline CNN</text>
<text top="650" left="69" width="231" height="9" font="font2" id="p2_t35" reading_order_no="34" segment_no="17" tag_type="list">model that takes the pre-processed and multiple-person</text>
<text top="662" left="69" width="233" height="9" font="font2" id="p2_t36" reading_order_no="35" segment_no="17" tag_type="list">related voxelized PCD representation as input and rec-</text>
<text top="674" left="69" width="231" height="9" font="font2" id="p2_t37" reading_order_no="36" segment_no="17" tag_type="list">ognizes activities of in real-time. We also proposed a</text>
<text top="686" left="69" width="231" height="9" font="font2" id="p2_t38" reading_order_no="37" segment_no="17" tag_type="list">deep variational autoencoder based domain adaptation</text>
<text top="698" left="69" width="231" height="9" font="font2" id="p2_t39" reading_order_no="38" segment_no="17" tag_type="list">model which incorporates a customized KL-divergence</text>
<text top="710" left="69" width="231" height="9" font="font2" id="p2_t40" reading_order_no="39" segment_no="17" tag_type="list">optimization technique to diminish the divergence between</text>
<text top="52" left="332" width="231" height="9" font="font2" id="p2_t41" reading_order_no="40" segment_no="1" tag_type="text">source and target models’ bottlenecks and apparently</text>
<text top="64" left="332" width="231" height="9" font="font2" id="p2_t42" reading_order_no="41" segment_no="1" tag_type="text">improves the domain adaptation. This model can take</text>
<text top="76" left="332" width="231" height="9" font="font2" id="p2_t43" reading_order_no="42" segment_no="1" tag_type="text">advantages of computer vision HAR dataset to scale the</text>
<text top="88" left="332" width="231" height="9" font="font2" id="p2_t44" reading_order_no="43" segment_no="1" tag_type="text">activity recognition accuracy significantly via domain</text>
<text top="100" left="332" width="180" height="9" font="font2" id="p2_t45" reading_order_no="44" segment_no="1" tag_type="text">adaptation in presence of scarce target data.</text>
<text top="120" left="365" width="144" height="9" font="font2" id="p2_t46" reading_order_no="45" segment_no="2" tag_type="title">III. M ULTIPLE P ERSON T RACKER</text>
<text top="136" left="322" width="241" height="9" font="font2" id="p2_t47" reading_order_no="46" segment_no="3" tag_type="text">The person tracker framework consists of four modules that</text>
<text top="148" left="312" width="197" height="9" font="font2" id="p2_t48" reading_order_no="47" segment_no="3" tag_type="text">operate in a pipeline fashion as shown in Fig 2.</text>
<text top="168" left="312" width="99" height="9" font="font7" id="p2_t49" reading_order_no="48" segment_no="4" tag_type="title">A. Data Transformation</text>
<text top="184" left="322" width="241" height="9" font="font2" id="p2_t50" reading_order_no="49" segment_no="5" tag_type="text">At first, we convert the PCD sensors provided distance</text>
<text top="196" left="312" width="251" height="9" font="font2" id="p2_t51" reading_order_no="50" segment_no="5" tag_type="text">measure based spherical coordinates to Cartesian coordinates</text>
<text top="208" left="312" width="251" height="9" font="font2" id="p2_t52" reading_order_no="51" segment_no="5" tag_type="text">in our transformation phase of preprocessing. Then, we record</text>
<text top="220" left="312" width="251" height="9" font="font2" id="p2_t53" reading_order_no="52" segment_no="5" tag_type="text">the background with static room structure and subtract the</text>
<text top="232" left="312" width="251" height="9" font="font2" id="p2_t54" reading_order_no="53" segment_no="5" tag_type="text">background. Due to measurement variations there are quite a</text>
<text top="244" left="312" width="251" height="9" font="font2" id="p2_t55" reading_order_no="54" segment_no="5" tag_type="text">few points with near zero but, not exactly zero values. As part</text>
<text top="256" left="312" width="251" height="9" font="font2" id="p2_t56" reading_order_no="55" segment_no="5" tag_type="text">of transformation, we subtract minimal distance and maximum</text>
<text top="267" left="312" width="251" height="9" font="font2" id="p2_t57" reading_order_no="56" segment_no="5" tag_type="text">distance factors which has been determined using distance</text>
<text top="279" left="312" width="141" height="9" font="font2" id="p2_t58" reading_order_no="57" segment_no="5" tag_type="text">factor transformation method [34].</text>
<text top="300" left="312" width="65" height="9" font="font7" id="p2_t59" reading_order_no="58" segment_no="10" tag_type="title">B. Voxel Fitting</text>
<text top="315" left="322" width="242" height="9" font="font2" id="p2_t60" reading_order_no="59" segment_no="11" tag_type="text">Voxel fitting algorithm is popular in brain MRI processing,</text>
<text top="327" left="312" width="251" height="9" font="font2" id="p2_t61" reading_order_no="60" segment_no="11" tag_type="text">3D scene analysis and depth camera analysis. We apply</text>
<text top="339" left="312" width="251" height="9" font="font2" id="p2_t62" reading_order_no="61" segment_no="11" tag_type="text">voxel fitting algorithm on PCD [2], [4]. We run a series of</text>
<text top="351" left="312" width="252" height="9" font="font2" id="p2_t63" reading_order_no="62" segment_no="11" tag_type="text">experiments to determine right voxel fitting parameters i.e.,</text>
<text top="363" left="312" width="252" height="9" font="font2" id="p2_t64" reading_order_no="63" segment_no="11" tag_type="text">length, breadth, and thickness of the voxel. In this regard, at first,</text>
<text top="375" left="312" width="251" height="9" font="font2" id="p2_t65" reading_order_no="64" segment_no="11" tag_type="text">we capture 3D grid of point density and variation profile of 4</text>
<text top="387" left="312" width="251" height="9" font="font2" id="p2_t66" reading_order_no="65" segment_no="11" tag_type="text">different objects (laptop box, smart phone box, flower vast and</text>
<text top="399" left="312" width="251" height="9" font="font2" id="p2_t67" reading_order_no="66" segment_no="11" tag_type="text">printer) were created along a different axis. We measured the</text>
<text top="411" left="312" width="251" height="9" font="font2" id="p2_t68" reading_order_no="67" segment_no="11" tag_type="text">volume of each of the object and calibrated the corresponding</text>
<text top="423" left="312" width="251" height="9" font="font2" id="p2_t69" reading_order_no="68" segment_no="11" tag_type="text">3D PCD distance ratio towards measuring accurate volume</text>
<text top="435" left="312" width="251" height="9" font="font2" id="p2_t70" reading_order_no="69" segment_no="11" tag_type="text">(length and height) of each object from three different distance</text>
<text top="447" left="312" width="251" height="9" font="font2" id="p2_t71" reading_order_no="70" segment_no="11" tag_type="text">measure, 1m, 2.5m and 5m. Apart from that, we also consider</text>
<text top="459" left="312" width="251" height="9" font="font2" id="p2_t72" reading_order_no="71" segment_no="11" tag_type="text">three different statistical voxel mapping methods: average</text>
<text top="471" left="312" width="251" height="9" font="font2" id="p2_t73" reading_order_no="72" segment_no="11" tag_type="text">mapping, average depth mapping (ADM) and average exclusive</text>
<text top="483" left="312" width="251" height="9" font="font2" id="p2_t74" reading_order_no="73" segment_no="11" tag_type="text">or mapping (AXOR). Then we run experiments on obtaining</text>
<text top="494" left="312" width="251" height="9" font="font2" id="p2_t75" reading_order_no="74" segment_no="11" tag_type="text">highest voxel resolution size with lowest error in detecting</text>
<text top="506" left="312" width="251" height="9" font="font2" id="p2_t76" reading_order_no="75" segment_no="11" tag_type="text">object volumes using allometric equation for object volume of</text>
<text top="518" left="312" width="92" height="9" font="font2" id="p2_t77" reading_order_no="76" segment_no="11" tag_type="text">estimation method [2].</text>
<text top="539" left="312" width="56" height="9" font="font7" id="p2_t78" reading_order_no="77" segment_no="14" tag_type="title">C. Clustering</text>
<text top="554" left="322" width="241" height="9" font="font2" id="p2_t79" reading_order_no="78" segment_no="16" tag_type="text">The generated voxel PCD are dispersed and not informative</text>
<text top="566" left="312" width="251" height="9" font="font2" id="p2_t80" reading_order_no="79" segment_no="16" tag_type="text">enough to detect distinct objects. Moreover, although static</text>
<text top="578" left="312" width="251" height="9" font="font2" id="p2_t81" reading_order_no="80" segment_no="16" tag_type="text">objects are discarded through our transformation phase (can</text>
<text top="590" left="312" width="251" height="9" font="font2" id="p2_t82" reading_order_no="81" segment_no="16" tag_type="text">be considered as background), the remaining points are not</text>
<text top="602" left="312" width="253" height="9" font="font2" id="p2_t83" reading_order_no="82" segment_no="16" tag_type="text">necessarily all reflected by moving people. To identify point-</text>
<text top="614" left="312" width="251" height="9" font="font2" id="p2_t84" reading_order_no="83" segment_no="16" tag_type="text">clouds generated from humans only, PALMAR uses a sequence</text>
<text top="626" left="312" width="251" height="9" font="font2" id="p2_t85" reading_order_no="84" segment_no="16" tag_type="text">of clustering algorithms that gives more distinction between</text>
<text top="638" left="312" width="122" height="9" font="font2" id="p2_t86" reading_order_no="85" segment_no="16" tag_type="text">different objects in the frame.</text>
<text top="650" left="312" width="253" height="9" font="font10" id="p2_t87" reading_order_no="86" segment_no="18" tag_type="text">DBSCAN : We use DBSCAN (density-aware clustering tech-</text>
<text top="662" left="312" width="251" height="9" font="font2" id="p2_t88" reading_order_no="87" segment_no="18" tag_type="text">nique) to separate point-clouds of same cluster (i.e. same</text>
<text top="674" left="312" width="251" height="9" font="font2" id="p2_t89" reading_order_no="88" segment_no="18" tag_type="text">person) in 3D voxel space. A major advantage is that it does</text>
<text top="686" left="312" width="251" height="9" font="font2" id="p2_t90" reading_order_no="89" segment_no="18" tag_type="text">not require the number of clusters to be specified a priori, as in</text>
<text top="698" left="312" width="251" height="9" font="font2" id="p2_t91" reading_order_no="90" segment_no="18" tag_type="text">our case people walk in and fade out of the monitored scene at</text>
<text top="710" left="312" width="251" height="9" font="font2" id="p2_t92" reading_order_no="91" segment_no="18" tag_type="text">random. Additionally, DBScan can automatically mark outliers</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font12" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font13" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font14" size="7" family="CMR7" color="#000000"/>
<text top="221" left="162" width="289" height="7" font="font5" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="text">Fig. 2. LiDAR (top) and mmWave (bottom) Point-cloud Processing Steps Visualization</text>
<text top="252" left="49" width="251" height="9" font="font2" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="text">to cope with noise. However, in a real-world measurement</text>
<text top="264" left="49" width="251" height="9" font="font2" id="p3_t3" reading_order_no="2" segment_no="1" tag_type="text">study, we observed that points of the same person are coherent</text>
<text top="276" left="49" width="251" height="9" font="font2" id="p3_t4" reading_order_no="3" segment_no="1" tag_type="text">in the horizontal (x-y) plane, but more scattered and difficult</text>
<text top="288" left="49" width="251" height="9" font="font2" id="p3_t5" reading_order_no="4" segment_no="1" tag_type="text">to merge along the vertical (z) axis. We hence modify the</text>
<text top="300" left="49" width="251" height="9" font="font2" id="p3_t6" reading_order_no="5" segment_no="1" tag_type="text">Euclidean distance to place less weight on the contribution</text>
<text top="312" left="49" width="152" height="9" font="font2" id="p3_t7" reading_order_no="6" segment_no="1" tag_type="text">from the vertical z-axis in clustering:</text>
<text top="328" left="62" width="213" height="14" font="font6" id="p3_t8" reading_order_no="7" segment_no="3" tag_type="formula">D ( p i , p j ) = ( p i x − p j x ) 2 + ( p i y − p j y ) 2 + α ( p i z − p j z ) 2</text>
<text top="332" left="288" width="12" height="9" font="font2" id="p3_t9" reading_order_no="8" segment_no="3" tag_type="text">(1)</text>
<text top="351" left="49" width="252" height="9" font="font2" id="p3_t10" reading_order_no="9" segment_no="5" tag_type="text">where p i and p j are two different points and the parameter</text>
<text top="363" left="49" width="252" height="9" font="font6" id="p3_t11" reading_order_no="10" segment_no="5" tag_type="text">α regulates the contribution of vertical distance. Additionally,</text>
<text top="375" left="49" width="251" height="9" font="font2" id="p3_t12" reading_order_no="11" segment_no="5" tag_type="text">we perform fine tuning on all of the distance weights based</text>
<text top="387" left="49" width="251" height="9" font="font2" id="p3_t13" reading_order_no="12" segment_no="5" tag_type="text">on sample multi-person data sequences. Doing so, we place</text>
<text top="399" left="49" width="253" height="9" font="font2" id="p3_t14" reading_order_no="13" segment_no="5" tag_type="text">weights along the horizontal ( x ) and depth ( z ) distances.</text>
<text top="411" left="49" width="251" height="9" font="font2" id="p3_t15" reading_order_no="14" segment_no="5" tag_type="text">DBscan clustering thus will give back all relevant PCD and</text>
<text top="423" left="49" width="251" height="9" font="font2" id="p3_t16" reading_order_no="15" segment_no="5" tag_type="text">can be easily applied for multi-person tracking. Finally, we</text>
<text top="435" left="49" width="251" height="9" font="font2" id="p3_t17" reading_order_no="16" segment_no="5" tag_type="text">perform frame trimming to extract the final sequence for activity</text>
<text top="447" left="49" width="251" height="9" font="font2" id="p3_t18" reading_order_no="17" segment_no="5" tag_type="text">recognition training. During the DBscan clustering step, we</text>
<text top="459" left="49" width="251" height="9" font="font2" id="p3_t19" reading_order_no="18" segment_no="5" tag_type="text">elect to drop frames where there was no cluster identified nor</text>
<text top="471" left="49" width="184" height="9" font="font2" id="p3_t20" reading_order_no="19" segment_no="5" tag_type="text">a large enough cluster to constitute a person.</text>
<text top="482" left="49" width="251" height="10" font="font10" id="p3_t21" reading_order_no="20" segment_no="8" tag_type="text">BIRCH : Although, DBSCAN is a highly accurate in detecting</text>
<text top="494" left="49" width="251" height="9" font="font2" id="p3_t22" reading_order_no="21" segment_no="8" tag_type="text">number of clusters i.e. number of people in a given LiDAR</text>
<text top="506" left="49" width="253" height="9" font="font2" id="p3_t23" reading_order_no="22" segment_no="8" tag_type="text">PCD, it often fails when the number of people is large (say &gt;2).</text>
<text top="518" left="49" width="251" height="9" font="font2" id="p3_t24" reading_order_no="23" segment_no="8" tag_type="text">DBScan also fails to categorize each data point in case of high</text>
<text top="530" left="49" width="252" height="9" font="font2" id="p3_t25" reading_order_no="24" segment_no="8" tag_type="text">dimensional sequence of data. Considering these limitations,</text>
<text top="542" left="49" width="251" height="9" font="font2" id="p3_t26" reading_order_no="25" segment_no="8" tag_type="text">we use BIRCH clustering algorithm after running DBSCAN</text>
<text top="554" left="49" width="251" height="9" font="font2" id="p3_t27" reading_order_no="26" segment_no="8" tag_type="text">clustering on voxelized PCD. The BIRCH (Balanced Iterative</text>
<text top="566" left="49" width="251" height="9" font="font2" id="p3_t28" reading_order_no="27" segment_no="8" tag_type="text">Reducing and Clustering using Hierarchies) algorithm is more</text>
<text top="578" left="49" width="251" height="9" font="font2" id="p3_t29" reading_order_no="28" segment_no="8" tag_type="text">suitable for the case where the amount of data is large and the</text>
<text top="590" left="49" width="251" height="9" font="font2" id="p3_t30" reading_order_no="29" segment_no="8" tag_type="text">number of categories K is relatively large [10]. It runs very</text>
<text top="602" left="49" width="251" height="9" font="font2" id="p3_t31" reading_order_no="30" segment_no="8" tag_type="text">fast, and it only needs a single pass to scan the data set for</text>
<text top="614" left="49" width="251" height="9" font="font2" id="p3_t32" reading_order_no="31" segment_no="8" tag_type="text">clustering that empower BIRCH algorithm to more accurately</text>
<text top="626" left="49" width="251" height="9" font="font2" id="p3_t33" reading_order_no="32" segment_no="8" tag_type="text">track multiple clusters in a sequence of PCD. Optionally, the</text>
<text top="638" left="49" width="251" height="9" font="font2" id="p3_t34" reading_order_no="33" segment_no="8" tag_type="text">algorithm can make further scans through the data to improve</text>
<text top="650" left="49" width="251" height="9" font="font2" id="p3_t35" reading_order_no="34" segment_no="8" tag_type="text">the clustering quality. We use the power of BIRCH algorithm on</text>
<text top="662" left="49" width="251" height="9" font="font2" id="p3_t36" reading_order_no="35" segment_no="8" tag_type="text">DBSCAN provided clustered PCD and number of categories</text>
<text top="674" left="49" width="251" height="9" font="font6" id="p3_t37" reading_order_no="36" segment_no="8" tag_type="text">K . The BIRCH clustering algorithm consists of two main</text>
<text top="686" left="49" width="251" height="9" font="font2" id="p3_t38" reading_order_no="37" segment_no="8" tag_type="text">phases or steps (i) Build the CF Tree : In this phase, it loads the</text>
<text top="698" left="49" width="252" height="9" font="font2" id="p3_t39" reading_order_no="38" segment_no="8" tag_type="text">data into memory by building a cluster-feature tree (CF tree)</text>
<text top="710" left="49" width="251" height="9" font="font2" id="p3_t40" reading_order_no="39" segment_no="8" tag_type="text">and optionally, condenses this initial CF tree into a smaller</text>
<text top="252" left="312" width="251" height="9" font="font2" id="p3_t41" reading_order_no="40" segment_no="2" tag_type="text">CF. (ii) Global Clustering : In this phase, it applies an existing</text>
<text top="264" left="312" width="251" height="9" font="font2" id="p3_t42" reading_order_no="41" segment_no="2" tag_type="text">clustering algorithm (say DBSCAN) on the leaves of the CF</text>
<text top="276" left="312" width="251" height="9" font="font2" id="p3_t43" reading_order_no="42" segment_no="2" tag_type="text">tree and optionally, refines these clusters. Although, we are</text>
<text top="288" left="312" width="251" height="9" font="font2" id="p3_t44" reading_order_no="43" segment_no="2" tag_type="text">using a sequence of two clustering algorithms to fine-tune and</text>
<text top="300" left="312" width="251" height="9" font="font2" id="p3_t45" reading_order_no="44" segment_no="2" tag_type="text">cluster multiple person related PCD, due to the light weight</text>
<text top="312" left="312" width="251" height="9" font="font2" id="p3_t46" reading_order_no="45" segment_no="2" tag_type="text">nature of DBSCAN and BIRCH methods, it does not cost</text>
<text top="324" left="312" width="215" height="9" font="font2" id="p3_t47" reading_order_no="46" segment_no="2" tag_type="text">significant time while detecting clusters in real-time.</text>
<text top="343" left="312" width="81" height="9" font="font7" id="p3_t48" reading_order_no="47" segment_no="4" tag_type="title">D. Person Tracking</text>
<text top="359" left="322" width="241" height="9" font="font2" id="p3_t49" reading_order_no="48" segment_no="6" tag_type="text">To capture continuous individual PCD to track and identify</text>
<text top="371" left="312" width="251" height="9" font="font2" id="p3_t50" reading_order_no="49" segment_no="6" tag_type="text">a person, we require an effective temporal association of</text>
<text top="383" left="312" width="251" height="9" font="font2" id="p3_t51" reading_order_no="50" segment_no="6" tag_type="text">detections as well as correction and prediction of sensor</text>
<text top="395" left="312" width="251" height="9" font="font2" id="p3_t52" reading_order_no="51" segment_no="6" tag_type="text">noise. In this regard, we consider each cluster centroid (after</text>
<text top="406" left="312" width="251" height="9" font="font2" id="p3_t53" reading_order_no="52" segment_no="6" tag_type="text">DBSCAN and BIRCH ) as nodes in the 3D voxel grid</text>
<text top="418" left="312" width="251" height="9" font="font2" id="p3_t54" reading_order_no="53" segment_no="6" tag_type="text">that formulate a problem of multiple-particle tracking in 3D</text>
<text top="430" left="312" width="251" height="9" font="font2" id="p3_t55" reading_order_no="54" segment_no="6" tag_type="text">space. However, tracking the centroid using sequential multiple</text>
<text top="442" left="312" width="227" height="9" font="font2" id="p3_t56" reading_order_no="55" segment_no="6" tag_type="text">particle filter models will face the following challenges</text>
<text top="456" left="322" width="243" height="9" font="font2" id="p3_t57" reading_order_no="56" segment_no="7" tag_type="list">1) Requirement of fast tracking of individual targets i.e.</text>
<text top="468" left="336" width="227" height="9" font="font2" id="p3_t58" reading_order_no="57" segment_no="7" tag_type="list">cluster centroid from a static voxelized PCD network in</text>
<text top="480" left="336" width="227" height="9" font="font2" id="p3_t59" reading_order_no="58" segment_no="7" tag_type="list">the infrastructure. This needs to resolve unreliable node</text>
<text top="492" left="336" width="183" height="9" font="font2" id="p3_t60" reading_order_no="59" segment_no="7" tag_type="list">sequences, system noise and path ambiguity.</text>
<text top="504" left="322" width="241" height="9" font="font2" id="p3_t61" reading_order_no="60" segment_no="9" tag_type="list">2) Requirement of scaling for multi-user tracking where</text>
<text top="516" left="336" width="227" height="9" font="font2" id="p3_t62" reading_order_no="61" segment_no="9" tag_type="list">user motion trajectories may crossover with each other</text>
<text top="528" left="336" width="84" height="9" font="font2" id="p3_t63" reading_order_no="62" segment_no="9" tag_type="list">in all possible ways.</text>
<text top="540" left="322" width="241" height="9" font="font2" id="p3_t64" reading_order_no="63" segment_no="10" tag_type="list">3) After multi-user tracking of cluster centroids, re-cluster</text>
<text top="552" left="336" width="224" height="9" font="font2" id="p3_t65" reading_order_no="64" segment_no="10" tag_type="list">voxelized PCD for activity recognition of each person.</text>
<text top="566" left="322" width="241" height="9" font="font2" id="p3_t66" reading_order_no="65" segment_no="11" tag_type="text">We propose to use Adaptive Order Hidden Markov Model</text>
<text top="578" left="312" width="251" height="9" font="font2" id="p3_t67" reading_order_no="66" segment_no="11" tag_type="text">(AO-HMM) and Crossover Path Disambiguation Algorithm</text>
<text top="590" left="312" width="183" height="9" font="font2" id="p3_t68" reading_order_no="67" segment_no="11" tag_type="text">(CPDA) to address the above challenges [6].</text>
<text top="602" left="322" width="243" height="9" font="font10" id="p3_t69" reading_order_no="68" segment_no="12" tag_type="text">Adaptive Order Hidden Markov Model (AO-HMM) : AO-</text>
<text top="614" left="312" width="251" height="9" font="font2" id="p3_t70" reading_order_no="69" segment_no="12" tag_type="text">HMM is a modified Hidden Markov Model (HMM) with a</text>
<text top="626" left="312" width="253" height="9" font="font2" id="p3_t71" reading_order_no="70" segment_no="12" tag_type="text">discrete time stochastic process. During state selection, AO-</text>
<text top="638" left="312" width="251" height="9" font="font2" id="p3_t72" reading_order_no="71" segment_no="12" tag_type="text">HMM chooses only the subset of states that are active and</text>
<text top="650" left="312" width="251" height="9" font="font2" id="p3_t73" reading_order_no="72" segment_no="12" tag_type="text">the neighbor (1-hop or 2-hop in Extended Activity Transition</text>
<text top="662" left="312" width="251" height="9" font="font2" id="p3_t74" reading_order_no="73" segment_no="12" tag_type="text">Graph) states i.e. order of HMM will be changed based on the</text>
<text top="674" left="312" width="251" height="9" font="font2" id="p3_t75" reading_order_no="74" segment_no="12" tag_type="text">number of active states and their neighbors. This reduces the</text>
<text top="686" left="312" width="251" height="9" font="font2" id="p3_t76" reading_order_no="75" segment_no="12" tag_type="text">computational complexity without compromising the accuracy</text>
<text top="698" left="312" width="251" height="9" font="font2" id="p3_t77" reading_order_no="76" segment_no="12" tag_type="text">of particle/cluster centroid tracking. However, The sub-state</text>
<text top="710" left="312" width="251" height="9" font="font2" id="p3_t78" reading_order_no="77" segment_no="12" tag_type="text">selection in AO-HMM also does not affect the optimality</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font15" size="10" family="CMBX10" color="#000000"/>
	<fontspec id="font16" size="5" family="CMMI5" color="#000000"/>
	<fontspec id="font17" size="10" family="MSBM10" color="#000000"/>
<text top="52" left="49" width="251" height="9" font="font2" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">of HMM model and Viterbi computation in our application</text>
<text top="64" left="49" width="253" height="9" font="font2" id="p4_t2" reading_order_no="1" segment_no="0" tag_type="text">scenario as we apply AO-HMM on pre-constructed voxel space.</text>
<text top="76" left="49" width="251" height="9" font="font2" id="p4_t3" reading_order_no="2" segment_no="0" tag_type="text">Standard Viterbi decoding algorithm is modified for i) multiple</text>
<text top="88" left="49" width="251" height="9" font="font2" id="p4_t4" reading_order_no="3" segment_no="0" tag_type="text">observation, ii) multiple sequence decoding, and iii) fitting</text>
<text top="100" left="49" width="251" height="9" font="font2" id="p4_t5" reading_order_no="4" segment_no="0" tag_type="text">for activity awareness. For nonoverlapping motion, viterbi</text>
<text top="112" left="49" width="251" height="9" font="font2" id="p4_t6" reading_order_no="5" segment_no="0" tag_type="text">algorithm is computed on first order HMM [7] where transitions</text>
<text top="123" left="49" width="252" height="10" font="font2" id="p4_t7" reading_order_no="6" segment_no="0" tag_type="text">from time ( t − 1) to t are considered. For overlapping motion,</text>
<text top="136" left="49" width="251" height="9" font="font2" id="p4_t8" reading_order_no="7" segment_no="0" tag_type="text">viterbi algorithm is computed on second order HMM [8] where</text>
<text top="147" left="49" width="250" height="10" font="font2" id="p4_t9" reading_order_no="8" segment_no="0" tag_type="text">transitions from time ( t − 2) and ( t − 1) to t are considered.</text>
<text top="160" left="59" width="243" height="10" font="font10" id="p4_t10" reading_order_no="9" segment_no="2" tag_type="text">Crossover Path Disambiguation Algorithm (CPDA) :</text>
<text top="173" left="49" width="251" height="9" font="font2" id="p4_t11" reading_order_no="10" segment_no="2" tag_type="text">There were some constraints to directly using standard HMM</text>
<text top="184" left="49" width="251" height="9" font="font2" id="p4_t12" reading_order_no="11" segment_no="2" tag_type="text">model and Viterbi algorithm to our real-time application</text>
<text top="196" left="49" width="251" height="9" font="font2" id="p4_t13" reading_order_no="12" segment_no="2" tag_type="text">scenario. Regarding length of time window (say W ) the</text>
<text top="208" left="49" width="251" height="9" font="font2" id="p4_t14" reading_order_no="13" segment_no="2" tag_type="text">standard Viterbi algorithm requires O ( W ) operations. But the</text>
<text top="220" left="49" width="251" height="9" font="font2" id="p4_t15" reading_order_no="14" segment_no="2" tag_type="text">standard algorithm is not applicable in the case of a streamed</text>
<text top="232" left="49" width="251" height="9" font="font2" id="p4_t16" reading_order_no="15" segment_no="2" tag_type="text">input (with potentially no ending in sequence) and requirement</text>
<text top="244" left="49" width="251" height="9" font="font2" id="p4_t17" reading_order_no="16" segment_no="2" tag_type="text">of output within bounded delay. Regarding size of state</text>
<text top="254" left="49" width="251" height="11" font="font2" id="p4_t18" reading_order_no="17" segment_no="2" tag_type="text">space (say S ), the standard Viterbi algorithm requires O ( S ) 2</text>
<text top="268" left="49" width="167" height="9" font="font2" id="p4_t19" reading_order_no="18" segment_no="2" tag_type="text">operations, and still even on average O ( S</text>
<text top="259" left="217" width="8" height="9" font="font13" id="p4_t20" reading_order_no="19" segment_no="2" tag_type="text">√</text>
<text top="268" left="225" width="75" height="9" font="font6" id="p4_t21" reading_order_no="20" segment_no="2" tag_type="text">S ) operations by a</text>
<text top="280" left="49" width="251" height="9" font="font2" id="p4_t22" reading_order_no="21" segment_no="2" tag_type="text">modified version of Viterbi [9]. On the other hand, the output</text>
<text top="292" left="49" width="251" height="9" font="font2" id="p4_t23" reading_order_no="22" segment_no="2" tag_type="text">state sequences from AO-HMM in each time window is partially</text>
<text top="304" left="49" width="253" height="9" font="font2" id="p4_t24" reading_order_no="23" segment_no="2" tag_type="text">disambiguated from the effect of path overlap or crossover.</text>
<text top="316" left="49" width="251" height="9" font="font2" id="p4_t25" reading_order_no="24" segment_no="2" tag_type="text">But it cannot always remove longer term path ambiguity</text>
<text top="328" left="49" width="251" height="9" font="font2" id="p4_t26" reading_order_no="25" segment_no="2" tag_type="text">that spreads beyond the Adaptive-HMM time window. To</text>
<text top="340" left="49" width="251" height="9" font="font2" id="p4_t27" reading_order_no="26" segment_no="2" tag_type="text">alleviate this, PALMAR applies Crossover Path Disambiguation</text>
<text top="352" left="49" width="251" height="9" font="font2" id="p4_t28" reading_order_no="27" segment_no="2" tag_type="text">Algorithm or CPDA to the joint Adaptive-HMM output of last</text>
<text top="364" left="49" width="251" height="9" font="font6" id="p4_t29" reading_order_no="28" segment_no="2" tag_type="text">C number of time windows. Unlike FindingHumo [6] where</text>
<text top="376" left="49" width="251" height="9" font="font2" id="p4_t30" reading_order_no="29" segment_no="2" tag_type="text">authors addressed binary motion sensor-assisted smart home</text>
<text top="388" left="49" width="251" height="9" font="font2" id="p4_t31" reading_order_no="30" segment_no="2" tag_type="text">motion tracking of multiple persons in 2D space, our problem</text>
<text top="400" left="49" width="252" height="9" font="font2" id="p4_t32" reading_order_no="31" segment_no="2" tag_type="text">domain lies on 3D space i.e. multi-person’s path ambiguity,</text>
<text top="412" left="49" width="251" height="9" font="font2" id="p4_t33" reading_order_no="32" segment_no="2" tag_type="text">node sequences and trajectory crossover spread over all x, y</text>
<text top="424" left="49" width="251" height="9" font="font2" id="p4_t34" reading_order_no="33" segment_no="2" tag_type="text">and z axis. In this regard, we reduce the dimension to 2D</text>
<text top="436" left="49" width="251" height="9" font="font2" id="p4_t35" reading_order_no="34" segment_no="2" tag_type="text">by removing z-axis of our 3D voxelized PCD system after</text>
<text top="447" left="49" width="251" height="9" font="font2" id="p4_t36" reading_order_no="35" segment_no="2" tag_type="text">clustering phase and apply AO-HMM and CPDA algorithms on</text>
<text top="459" left="49" width="251" height="9" font="font2" id="p4_t37" reading_order_no="36" segment_no="2" tag_type="text">it. While AO-HMM addresses the multiple cluster (i.e. multiple</text>
<text top="471" left="49" width="251" height="9" font="font2" id="p4_t38" reading_order_no="37" segment_no="2" tag_type="text">person) sequence tracking and CPDA handles multi-person’s</text>
<text top="483" left="49" width="164" height="9" font="font2" id="p4_t39" reading_order_no="38" segment_no="2" tag_type="text">path ambiguity and trajectory crossover.</text>
<text top="701" left="49" width="251" height="7" font="font5" id="p4_t40" reading_order_no="39" segment_no="7" tag_type="text">Fig. 3. Working Example of 3 Inhabitants Person Tracking on Cluster Centroids</text>
<text top="710" left="49" width="26" height="7" font="font5" id="p4_t41" reading_order_no="40" segment_no="7" tag_type="text">(Nodes)</text>
<text top="52" left="322" width="241" height="9" font="font10" id="p4_t42" reading_order_no="41" segment_no="1" tag_type="text">Working Example on Our Target Scenario : Figure 3</text>
<text top="64" left="312" width="251" height="9" font="font2" id="p4_t43" reading_order_no="42" segment_no="1" tag_type="text">illustrates how AO-HMM and CPDA algorithms together solve</text>
<text top="76" left="312" width="251" height="9" font="font2" id="p4_t44" reading_order_no="43" segment_no="1" tag_type="text">multiple-person tracking problem on LiDAR PCD in 3D voxel</text>
<text top="88" left="312" width="251" height="9" font="font2" id="p4_t45" reading_order_no="44" segment_no="1" tag_type="text">space. After applying several signal processing and clustering</text>
<text top="100" left="312" width="251" height="9" font="font2" id="p4_t46" reading_order_no="45" segment_no="1" tag_type="text">techniques, we have cluster centroid for each person in a</text>
<text top="112" left="312" width="253" height="9" font="font2" id="p4_t47" reading_order_no="46" segment_no="1" tag_type="text">multi-inhabitant smart home. We pass the cluster nodes to AO-</text>
<text top="124" left="312" width="251" height="9" font="font2" id="p4_t48" reading_order_no="47" segment_no="1" tag_type="text">HMM and later to CPDA algorithms. The unreliable cluster</text>
<text top="136" left="312" width="251" height="9" font="font2" id="p4_t49" reading_order_no="48" segment_no="1" tag_type="text">node sequence with system noise spread over 3D voxel space</text>
<text top="148" left="312" width="251" height="9" font="font2" id="p4_t50" reading_order_no="49" segment_no="1" tag_type="text">has been reduced first by applying dimension reduction to</text>
<text top="160" left="312" width="251" height="9" font="font2" id="p4_t51" reading_order_no="50" segment_no="1" tag_type="text">2D space which is the input of AO-HMM algorithm. This is</text>
<text top="172" left="312" width="251" height="9" font="font2" id="p4_t52" reading_order_no="51" segment_no="1" tag_type="text">refined by applying AO-HMM in the next step. The decoded</text>
<text top="184" left="312" width="251" height="9" font="font2" id="p4_t53" reading_order_no="52" segment_no="1" tag_type="text">state sequence may still contain error due to path crossover</text>
<text top="196" left="312" width="251" height="9" font="font2" id="p4_t54" reading_order_no="53" segment_no="1" tag_type="text">(e.g. crossover of decoded path for user 2 and user 3 at node</text>
<text top="208" left="312" width="251" height="9" font="font2" id="p4_t55" reading_order_no="54" segment_no="1" tag_type="text">99 in the Fig 3 ). This is further corrected by stitching the</text>
<text top="220" left="312" width="251" height="9" font="font2" id="p4_t56" reading_order_no="55" segment_no="1" tag_type="text">decoded paths and forming an Interaction Graph, which is</text>
<text top="231" left="312" width="253" height="9" font="font2" id="p4_t57" reading_order_no="56" segment_no="1" tag_type="text">then disambiguated by applying proposed CPDA algorithm.</text>
<text top="243" left="312" width="251" height="9" font="font2" id="p4_t58" reading_order_no="57" segment_no="1" tag_type="text">This results in final decoded motion trajectories. It is worth</text>
<text top="255" left="312" width="251" height="9" font="font2" id="p4_t59" reading_order_no="58" segment_no="1" tag_type="text">mentioning that the position of user is presented in form of</text>
<text top="267" left="312" width="251" height="9" font="font2" id="p4_t60" reading_order_no="59" segment_no="1" tag_type="text">sensor nodes’ position. Thus the tracking accuracy will be</text>
<text top="279" left="312" width="251" height="9" font="font2" id="p4_t61" reading_order_no="60" segment_no="1" tag_type="text">more (w.r.t the actual physical location of user) if the number</text>
<text top="291" left="312" width="107" height="9" font="font2" id="p4_t62" reading_order_no="61" segment_no="1" tag_type="text">of cluster is less (say &lt;3).</text>
<text top="316" left="333" width="210" height="9" font="font2" id="p4_t63" reading_order_no="62" segment_no="3" tag_type="title">IV. H ETEROGENEOUS D OMAIN A DAPTATION VIA</text>
<text top="328" left="374" width="127" height="9" font="font2" id="p4_t64" reading_order_no="63" segment_no="3" tag_type="title">V ARIATIONAL A UTOENCODER</text>
<text top="347" left="312" width="100" height="9" font="font7" id="p4_t65" reading_order_no="64" segment_no="4" tag_type="title">A. Problem Formulation</text>
<text top="363" left="322" width="241" height="12" font="font2" id="p4_t66" reading_order_no="65" segment_no="5" tag_type="text">We can have our source domain D s = { ( x s i , y i s ) } n s , where</text>
<text top="375" left="312" width="10" height="12" font="font15" id="p4_t67" reading_order_no="66" segment_no="5" tag_type="text">x s i</text>
<text top="375" left="328" width="235" height="12" font="font12" id="p4_t68" reading_order_no="67" segment_no="5" tag_type="text">= { x 1 , ..., x n s } ∈ R d s is the i th source data with d s</text>
<text top="388" left="312" width="251" height="10" font="font2" id="p4_t69" reading_order_no="68" segment_no="5" tag_type="text">feature dimension and n s sample size. Here, source labels</text>
<text top="399" left="312" width="251" height="12" font="font15" id="p4_t70" reading_order_no="69" segment_no="5" tag_type="text">y s i ∈ Y s , where Y s = { 1 , 2 , ..., n c } is the associated class</text>
<text top="411" left="312" width="253" height="12" font="font2" id="p4_t71" reading_order_no="70" segment_no="5" tag_type="text">label of corresponding i th sample of the source data x s i .</text>
<text top="424" left="312" width="251" height="10" font="font2" id="p4_t72" reading_order_no="71" segment_no="5" tag_type="text">The source classification task T s is to correctly predict the</text>
<text top="435" left="312" width="251" height="11" font="font2" id="p4_t73" reading_order_no="72" segment_no="5" tag_type="text">labels y s from the data x s . The task T s can also be viewed</text>
<text top="447" left="312" width="251" height="11" font="font2" id="p4_t74" reading_order_no="73" segment_no="5" tag_type="text">as a conditional probability distribution P ( y s | x s ) from the</text>
<text top="460" left="312" width="251" height="9" font="font2" id="p4_t75" reading_order_no="74" segment_no="5" tag_type="text">probabilistic perspective. Similar to the source domain, now we</text>
<text top="470" left="312" width="175" height="11" font="font2" id="p4_t76" reading_order_no="75" segment_no="5" tag_type="text">let our target domain D t = D l ∪ D u = { ( x l</text>
<text top="470" left="484" width="17" height="13" font="font6" id="p4_t77" reading_order_no="76" segment_no="5" tag_type="text">i , y l</text>
<text top="470" left="498" width="50" height="13" font="font12" id="p4_t78" reading_order_no="77" segment_no="5" tag_type="text">i ) } n l ∪ { ( x u</text>
<text top="470" left="543" width="19" height="13" font="font13" id="p4_t79" reading_order_no="78" segment_no="5" tag_type="text">i } n u</text>
<text top="483" left="312" width="251" height="10" font="font2" id="p4_t80" reading_order_no="79" segment_no="5" tag_type="text">where D l and D u is the labeled and un-labeled target data</text>
<text top="494" left="312" width="251" height="12" font="font2" id="p4_t81" reading_order_no="80" segment_no="5" tag_type="text">respectively. x l i ∈ R d t is the i th labeled and x u i ∈ R d t is the</text>
<text top="506" left="312" width="253" height="11" font="font6" id="p4_t82" reading_order_no="81" segment_no="5" tag_type="text">i th un-labeled target data with d t dimensional feature space.</text>
<text top="518" left="312" width="252" height="12" font="font15" id="p4_t83" reading_order_no="82" segment_no="5" tag_type="text">y l i ∈ Y t is the corresponding class label of i th labeled target</text>
<text top="532" left="312" width="169" height="9" font="font2" id="p4_t84" reading_order_no="83" segment_no="5" tag_type="text">data. In our domain adaptation setting, n s</text>
<text top="532" left="497" width="41" height="9" font="font6" id="p4_t85" reading_order_no="84" segment_no="5" tag_type="text">n l and n u</text>
<text top="532" left="553" width="11" height="9" font="font6" id="p4_t86" reading_order_no="85" segment_no="5" tag_type="text">n l .</text>
<text top="543" left="312" width="251" height="10" font="font2" id="p4_t87" reading_order_no="86" segment_no="5" tag_type="text">The target classification task, T t is to predict the target class</text>
<text top="554" left="312" width="252" height="12" font="font2" id="p4_t88" reading_order_no="87" segment_no="5" tag_type="text">labels y u i with the unlabeled target data x u i , more specifically,</text>
<text top="566" left="312" width="191" height="12" font="font2" id="p4_t89" reading_order_no="88" segment_no="5" tag_type="text">learning the probability distribution P ( y u i | x u i ) .</text>
<text top="579" left="322" width="241" height="10" font="font2" id="p4_t90" reading_order_no="89" segment_no="6" tag_type="text">Assume that the source domain data x s consists of source</text>
<text top="591" left="312" width="253" height="10" font="font2" id="p4_t91" reading_order_no="90" segment_no="6" tag_type="text">domain specific information U s and domain invariant informa-</text>
<text top="603" left="312" width="251" height="10" font="font2" id="p4_t92" reading_order_no="91" segment_no="6" tag_type="text">tion V . On the other hand, the target domain data x t consists</text>
<text top="615" left="312" width="251" height="10" font="font2" id="p4_t93" reading_order_no="92" segment_no="6" tag_type="text">of target domain specific information U t and domain invariant</text>
<text top="628" left="312" width="251" height="9" font="font2" id="p4_t94" reading_order_no="93" segment_no="6" tag_type="text">information V . The goal of our work is to map the domain</text>
<text top="640" left="312" width="251" height="9" font="font2" id="p4_t95" reading_order_no="94" segment_no="6" tag_type="text">invariant information of both source and target domain into a</text>
<text top="652" left="312" width="251" height="9" font="font2" id="p4_t96" reading_order_no="95" segment_no="6" tag_type="text">common feature representation space which holds the domain</text>
<text top="664" left="312" width="251" height="9" font="font2" id="p4_t97" reading_order_no="96" segment_no="6" tag_type="text">invariant information V of both domains. If we denote the</text>
<text top="674" left="312" width="251" height="11" font="font15" id="p4_t98" reading_order_no="97" segment_no="6" tag_type="text">Z symbol as the representation space, e.g. Z s is the feature</text>
<text top="686" left="312" width="110" height="11" font="font2" id="p4_t99" reading_order_no="98" segment_no="6" tag_type="text">representation space of x s ,</text>
<text top="707" left="381" width="111" height="12" font="font6" id="p4_t100" reading_order_no="99" segment_no="8" tag_type="formula">P ( Z s | x s ) = P ( Z t | x t ) = V</text>
<text top="710" left="551" width="12" height="9" font="font2" id="p4_t101" reading_order_no="100" segment_no="8" tag_type="text">(2)</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font18" size="10" family="CMEX10" color="#000000"/>
<text top="216" left="55" width="142" height="7" font="font5" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">Fig. 4. Baseline Deep Activity Recognizer</text>
<text top="216" left="277" width="227" height="7" font="font5" id="p5_t2" reading_order_no="1" segment_no="1" tag_type="text">Fig. 5. Our Variational Autoencoder Domain Adaptation Framework</text>
<text top="377" left="125" width="100" height="7" font="font5" id="p5_t3" reading_order_no="2" segment_no="6" tag_type="text">Fig. 6. Our Hardware System</text>
<text top="415" left="49" width="117" height="9" font="font7" id="p5_t4" reading_order_no="3" segment_no="7" tag_type="title">B. Deep Activity Recognizer</text>
<text top="431" left="59" width="242" height="9" font="font2" id="p5_t5" reading_order_no="4" segment_no="8" tag_type="text">We develop a basic deep convolutional neural network (CNN)</text>
<text top="443" left="49" width="253" height="9" font="font2" id="p5_t6" reading_order_no="5" segment_no="8" tag_type="text">model as a baseline activity recognizer as shown in Fig 4.</text>
<text top="455" left="49" width="253" height="9" font="font2" id="p5_t7" reading_order_no="6" segment_no="8" tag_type="text">The baseline activity recognizer consists of two components:</text>
<text top="467" left="49" width="251" height="9" font="font2" id="p5_t8" reading_order_no="7" segment_no="8" tag_type="text">(i) feature extractor component consists of a CNN layer, a</text>
<text top="479" left="49" width="252" height="9" font="font2" id="p5_t9" reading_order_no="8" segment_no="8" tag_type="text">maxpooling layer, a CNN, a maxpooling, a fully connected;</text>
<text top="491" left="49" width="251" height="9" font="font2" id="p5_t10" reading_order_no="9" segment_no="8" tag_type="text">(ii) classification component consists of two fully connected</text>
<text top="503" left="49" width="251" height="9" font="font2" id="p5_t11" reading_order_no="10" segment_no="8" tag_type="text">layer with an output dimension of number of classes. We</text>
<text top="515" left="49" width="251" height="9" font="font2" id="p5_t12" reading_order_no="11" segment_no="8" tag_type="text">consider this baseline activity recognition structure as our base</text>
<text top="527" left="49" width="251" height="9" font="font2" id="p5_t13" reading_order_no="12" segment_no="8" tag_type="text">structure for variational autoencoder based domain adaptation</text>
<text top="539" left="49" width="27" height="9" font="font2" id="p5_t14" reading_order_no="13" segment_no="8" tag_type="text">model.</text>
<text top="561" left="49" width="222" height="9" font="font7" id="p5_t15" reading_order_no="14" segment_no="11" tag_type="title">C. Domain Adaptation with Variational Auto-Encoder</text>
<text top="578" left="59" width="241" height="9" font="font2" id="p5_t16" reading_order_no="15" segment_no="13" tag_type="text">As depicted in figure 5, our proposed framework have three</text>
<text top="590" left="49" width="251" height="9" font="font2" id="p5_t17" reading_order_no="16" segment_no="13" tag_type="text">main components, the Source variational auto-encoder, the</text>
<text top="601" left="49" width="251" height="9" font="font2" id="p5_t18" reading_order_no="17" segment_no="13" tag_type="text">target variational auto-encoder and the classifier. Each of the</text>
<text top="613" left="49" width="251" height="9" font="font2" id="p5_t19" reading_order_no="18" segment_no="13" tag_type="text">autoencoder has the structure of the feature extractor component</text>
<text top="625" left="49" width="251" height="9" font="font2" id="p5_t20" reading_order_no="19" segment_no="13" tag_type="text">of baseline deep activity recognizer. Our complete training</text>
<text top="637" left="49" width="253" height="9" font="font2" id="p5_t21" reading_order_no="20" segment_no="13" tag_type="text">process consists of two stages, training the variational auto-</text>
<text top="649" left="49" width="144" height="9" font="font2" id="p5_t22" reading_order_no="21" segment_no="13" tag_type="text">encoders and training the classifier.</text>
<text top="662" left="59" width="243" height="9" font="font7" id="p5_t23" reading_order_no="22" segment_no="16" tag_type="text">1) Training Variational Auto-encoders: In this training stage,</text>
<text top="674" left="49" width="251" height="9" font="font2" id="p5_t24" reading_order_no="23" segment_no="16" tag_type="text">source encoder E s and the target encoder E t map the input</text>
<text top="684" left="49" width="252" height="11" font="font2" id="p5_t25" reading_order_no="24" segment_no="16" tag_type="text">data x s and x t to the feature spaces Z s and Z t respectively</text>
<text top="698" left="49" width="251" height="9" font="font2" id="p5_t26" reading_order_no="25" segment_no="16" tag_type="text">in a probabilistic fashion. In our implementation of variational</text>
<text top="710" left="49" width="251" height="9" font="font2" id="p5_t27" reading_order_no="26" segment_no="16" tag_type="text">auto-encoder, we used gaussian function to sample the latent</text>
<text top="247" left="312" width="251" height="9" font="font2" id="p5_t28" reading_order_no="27" segment_no="2" tag_type="text">space Z from learned mean and varience matrix. The goal of</text>
<text top="259" left="312" width="251" height="9" font="font2" id="p5_t29" reading_order_no="28" segment_no="2" tag_type="text">the training is to minimize the distribution discrepancy between</text>
<text top="269" left="312" width="195" height="11" font="font2" id="p5_t30" reading_order_no="29" segment_no="2" tag_type="text">the source and target feature spaces Z s and Z t .</text>
<text top="290" left="366" width="143" height="12" font="font17" id="p5_t31" reading_order_no="30" segment_no="3" tag_type="formula">Z s = E s ( x s ) ∼ N ( µ s ( x s ) , σ s ( x s ))</text>
<text top="292" left="551" width="12" height="9" font="font2" id="p5_t32" reading_order_no="31" segment_no="3" tag_type="text">(3)</text>
<text top="325" left="369" width="138" height="12" font="font17" id="p5_t33" reading_order_no="32" segment_no="4" tag_type="formula">Z t = E t ( x t ) ∼ N ( µ t ( x t ) , σ t ( x t ))</text>
<text top="327" left="551" width="12" height="9" font="font2" id="p5_t34" reading_order_no="33" segment_no="4" tag_type="text">(4)</text>
<text top="349" left="322" width="241" height="9" font="font2" id="p5_t35" reading_order_no="34" segment_no="5" tag_type="text">We use KLD loss function to minimize this discrepancy</text>
<text top="359" left="312" width="251" height="11" font="font2" id="p5_t36" reading_order_no="35" segment_no="5" tag_type="text">along the training. The source data x s and the target data x t</text>
<text top="373" left="312" width="251" height="9" font="font2" id="p5_t37" reading_order_no="36" segment_no="5" tag_type="text">are fed to the source and target auto-encoder respectively. Only</text>
<text top="384" left="312" width="251" height="9" font="font2" id="p5_t38" reading_order_no="37" segment_no="5" tag_type="text">the self reconstruction loss of variation auto-encoder is set to</text>
<text top="396" left="312" width="251" height="9" font="font2" id="p5_t39" reading_order_no="38" segment_no="5" tag_type="text">the source auto-encoder optimization loss added with KLD loss</text>
<text top="407" left="312" width="251" height="10" font="font2" id="p5_t40" reading_order_no="39" segment_no="5" tag_type="text">between the representation space Z s and source reconstruction</text>
<text top="417" left="312" width="15" height="12" font="font15" id="p5_t41" reading_order_no="40" segment_no="5" tag_type="text">x 0 s .</text>
<text top="441" left="400" width="40" height="16" font="font13" id="p5_t42" reading_order_no="41" segment_no="9" tag_type="formula">L s = − 1</text>
<text top="454" left="433" width="8" height="9" font="font6" id="p5_t43" reading_order_no="42" segment_no="9" tag_type="formula">N</text>
<text top="437" left="449" width="6" height="6" font="font11" id="p5_t44" reading_order_no="43" segment_no="9" tag_type="formula">N</text>
<text top="444" left="445" width="14" height="4" font="font18" id="p5_t45" reading_order_no="44" segment_no="9" tag_type="formula">X</text>
<text top="461" left="446" width="13" height="6" font="font11" id="p5_t46" reading_order_no="45" segment_no="9" tag_type="formula">i =1</text>
<text top="445" left="461" width="10" height="11" font="font6" id="p5_t47" reading_order_no="46" segment_no="9" tag_type="formula">x s</text>
<text top="445" left="467" width="47" height="13" font="font13" id="p5_t48" reading_order_no="47" segment_no="9" tag_type="formula">i · log ( p ( x s</text>
<text top="447" left="510" width="12" height="11" font="font12" id="p5_t49" reading_order_no="48" segment_no="9" tag_type="formula">i ))</text>
<text top="447" left="551" width="12" height="9" font="font2" id="p5_t50" reading_order_no="49" segment_no="9" tag_type="text">(5)</text>
<text top="469" left="333" width="38" height="11" font="font12" id="p5_t51" reading_order_no="50" segment_no="9" tag_type="formula">+(1 − x s</text>
<text top="469" left="367" width="69" height="13" font="font12" id="p5_t52" reading_order_no="51" segment_no="9" tag_type="formula">i ) · log (1 − p ( x s</text>
<text top="469" left="432" width="69" height="13" font="font12" id="p5_t53" reading_order_no="52" segment_no="9" tag_type="formula">i ) + α · KLD ( Z s</text>
<text top="469" left="497" width="21" height="13" font="font6" id="p5_t54" reading_order_no="53" segment_no="9" tag_type="formula">i , x 0 s</text>
<text top="471" left="512" width="10" height="11" font="font12" id="p5_t55" reading_order_no="54" segment_no="9" tag_type="formula">i )</text>
<text top="490" left="312" width="51" height="11" font="font2" id="p5_t56" reading_order_no="55" segment_no="10" tag_type="text">Where, p ( x s</text>
<text top="490" left="359" width="64" height="13" font="font12" id="p5_t57" reading_order_no="56" segment_no="10" tag_type="text">i ) = E s ( D s ( x s</text>
<text top="492" left="419" width="144" height="11" font="font12" id="p5_t58" reading_order_no="57" segment_no="10" tag_type="text">i )) is the prediction probability of</text>
<text top="502" left="312" width="251" height="12" font="font6" id="p5_t59" reading_order_no="58" segment_no="10" tag_type="text">x s i by the source encoder and decoder and α is the weighting</text>
<text top="516" left="312" width="251" height="9" font="font2" id="p5_t60" reading_order_no="59" segment_no="10" tag_type="text">parameter for KLD loss. On the other hand, the weighted sum</text>
<text top="528" left="312" width="251" height="9" font="font2" id="p5_t61" reading_order_no="60" segment_no="10" tag_type="text">of self reconstruction loss and the KLD loss function between</text>
<text top="540" left="312" width="251" height="9" font="font2" id="p5_t62" reading_order_no="61" segment_no="10" tag_type="text">the source and the target representation space is set to the</text>
<text top="552" left="312" width="148" height="9" font="font2" id="p5_t63" reading_order_no="62" segment_no="10" tag_type="text">target auto-encoder as loss function.</text>
<text top="572" left="384" width="103" height="10" font="font13" id="p5_t64" reading_order_no="63" segment_no="12" tag_type="formula">L t = L recon + β · L KLD</text>
<text top="573" left="551" width="12" height="9" font="font2" id="p5_t65" reading_order_no="64" segment_no="12" tag_type="text">(6)</text>
<text top="614" left="325" width="87" height="16" font="font2" id="p5_t66" reading_order_no="65" segment_no="14" tag_type="formula">where, L recon = − 1</text>
<text top="628" left="405" width="8" height="9" font="font6" id="p5_t67" reading_order_no="66" segment_no="14" tag_type="formula">N</text>
<text top="610" left="421" width="6" height="6" font="font11" id="p5_t68" reading_order_no="67" segment_no="14" tag_type="formula">N</text>
<text top="618" left="417" width="14" height="4" font="font18" id="p5_t69" reading_order_no="68" segment_no="14" tag_type="formula">X</text>
<text top="635" left="418" width="12" height="6" font="font11" id="p5_t70" reading_order_no="69" segment_no="14" tag_type="formula">i =1</text>
<text top="619" left="433" width="106" height="12" font="font6" id="p5_t71" reading_order_no="70" segment_no="14" tag_type="formula">x t i · log ( p ( x t i )) + (1 − x t i )</text>
<text top="621" left="551" width="12" height="9" font="font2" id="p5_t72" reading_order_no="71" segment_no="14" tag_type="text">(7)</text>
<text top="649" left="312" width="252" height="9" font="font2" id="p5_t73" reading_order_no="72" segment_no="15" tag_type="text">Here β is the weighting parameter in order enforce the</text>
<text top="661" left="312" width="251" height="9" font="font2" id="p5_t74" reading_order_no="73" segment_no="15" tag_type="text">comparative importance of the reconstruction loss and KLD</text>
<text top="673" left="312" width="18" height="9" font="font2" id="p5_t75" reading_order_no="74" segment_no="15" tag_type="text">loss.</text>
<text top="686" left="322" width="241" height="9" font="font2" id="p5_t76" reading_order_no="75" segment_no="17" tag_type="text">The Kullback-Leibler divergence (KLD) [3] is a powerful</text>
<text top="698" left="312" width="251" height="9" font="font2" id="p5_t77" reading_order_no="76" segment_no="17" tag_type="text">metric for determining divergence between two marginal</text>
<text top="710" left="312" width="251" height="9" font="font2" id="p5_t78" reading_order_no="77" segment_no="17" tag_type="text">probability distributions. The main idea of KLD is to calculate</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
<text top="52" left="49" width="251" height="9" font="font2" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="text">the probability divergence between the source and the target</text>
<text top="64" left="49" width="221" height="9" font="font2" id="p6_t2" reading_order_no="1" segment_no="0" tag_type="text">distributions. So, the KLD loss function is defined as,</text>
<text top="84" left="88" width="78" height="16" font="font13" id="p6_t3" reading_order_no="2" segment_no="2" tag_type="formula">L KLD ( x s , x t ) = 1</text>
<text top="98" left="159" width="8" height="9" font="font6" id="p6_t4" reading_order_no="3" segment_no="2" tag_type="formula">N</text>
<text top="81" left="174" width="6" height="6" font="font11" id="p6_t5" reading_order_no="4" segment_no="2" tag_type="formula">N</text>
<text top="88" left="171" width="14" height="4" font="font18" id="p6_t6" reading_order_no="5" segment_no="2" tag_type="formula">X</text>
<text top="105" left="172" width="12" height="6" font="font11" id="p6_t7" reading_order_no="6" segment_no="2" tag_type="formula">i =1</text>
<text top="83" left="188" width="72" height="10" font="font6" id="p6_t8" reading_order_no="7" segment_no="2" tag_type="formula">p ( x s ) · log ( p ( x s ))</text>
<text top="97" left="213" width="22" height="10" font="font6" id="p6_t9" reading_order_no="8" segment_no="2" tag_type="formula">p ( x t )</text>
<text top="91" left="288" width="12" height="9" font="font2" id="p6_t10" reading_order_no="9" segment_no="2" tag_type="text">(8)</text>
<text top="117" left="48" width="252" height="11" font="font2" id="p6_t11" reading_order_no="10" segment_no="3" tag_type="text">Where p ( x s ) and p ( x t ) are the probability distribution of x s</text>
<text top="129" left="49" width="80" height="11" font="font2" id="p6_t12" reading_order_no="11" segment_no="3" tag_type="text">and x t respectively.</text>
<text top="143" left="59" width="241" height="9" font="font7" id="p6_t13" reading_order_no="12" segment_no="4" tag_type="text">2) Training the Classifier: After training the source and the</text>
<text top="155" left="49" width="251" height="9" font="font2" id="p6_t14" reading_order_no="13" segment_no="4" tag_type="text">target auto-encoders, we train a common classifier network</text>
<text top="165" left="49" width="251" height="11" font="font2" id="p6_t15" reading_order_no="14" segment_no="4" tag_type="text">with the learned feature space Z s of the source encoder and</text>
<text top="179" left="49" width="251" height="9" font="font2" id="p6_t16" reading_order_no="15" segment_no="4" tag_type="text">the labels of the source network. We then transfer the learned</text>
<text top="191" left="49" width="251" height="9" font="font2" id="p6_t17" reading_order_no="16" segment_no="4" tag_type="text">classifier to the target network and use to classify the target</text>
<text top="200" left="49" width="252" height="12" font="font2" id="p6_t18" reading_order_no="17" segment_no="4" tag_type="text">feature representation Z t . During the learning of the classifier,</text>
<text top="215" left="49" width="251" height="9" font="font2" id="p6_t19" reading_order_no="18" segment_no="4" tag_type="text">we make the source encoder network frozen. So, the learning</text>
<text top="227" left="49" width="220" height="9" font="font2" id="p6_t20" reading_order_no="19" segment_no="4" tag_type="text">objective function in case of learning the classifier is,</text>
<text top="246" left="137" width="17" height="9" font="font12" id="p6_t21" reading_order_no="20" segment_no="5" tag_type="formula">min</text>
<text top="254" left="142" width="7" height="7" font="font11" id="p6_t22" reading_order_no="21" segment_no="5" tag_type="formula">f c</text>
<text top="244" left="156" width="58" height="11" font="font13" id="p6_t23" reading_order_no="22" segment_no="5" tag_type="formula">L c [ y s , f c ( Z s )]</text>
<text top="246" left="288" width="12" height="9" font="font2" id="p6_t24" reading_order_no="23" segment_no="5" tag_type="text">(9)</text>
<text top="269" left="49" width="251" height="10" font="font2" id="p6_t25" reading_order_no="24" segment_no="6" tag_type="text">Here, f c ( · ) represents the classifier network. We use categorical</text>
<text top="282" left="49" width="252" height="9" font="font2" id="p6_t26" reading_order_no="25" segment_no="6" tag_type="text">cross-entropy loss for classifier network. So, the loss function,</text>
<text top="309" left="117" width="32" height="10" font="font13" id="p6_t27" reading_order_no="26" segment_no="7" tag_type="formula">L c = −</text>
<text top="299" left="154" width="6" height="6" font="font11" id="p6_t28" reading_order_no="27" segment_no="7" tag_type="formula">C</text>
<text top="306" left="150" width="14" height="4" font="font18" id="p6_t29" reading_order_no="28" segment_no="7" tag_type="formula">X</text>
<text top="323" left="151" width="13" height="6" font="font11" id="p6_t30" reading_order_no="29" segment_no="7" tag_type="formula">i =0</text>
<text top="307" left="166" width="10" height="11" font="font6" id="p6_t31" reading_order_no="30" segment_no="7" tag_type="formula">y s</text>
<text top="307" left="171" width="53" height="13" font="font13" id="p6_t32" reading_order_no="31" segment_no="7" tag_type="formula">i · log ( f c ( Z s</text>
<text top="309" left="220" width="13" height="11" font="font12" id="p6_t33" reading_order_no="32" segment_no="7" tag_type="formula">i ))</text>
<text top="309" left="283" width="17" height="9" font="font2" id="p6_t34" reading_order_no="33" segment_no="7" tag_type="text">(10)</text>
<text top="336" left="105" width="140" height="9" font="font2" id="p6_t35" reading_order_no="34" segment_no="8" tag_type="title">V. E XPERIMENTAL E VALUATION</text>
<text top="352" left="59" width="243" height="9" font="font2" id="p6_t36" reading_order_no="35" segment_no="9" tag_type="text">In this section, we describe our hardware system develop-</text>
<text top="364" left="49" width="251" height="9" font="font2" id="p6_t37" reading_order_no="36" segment_no="9" tag_type="text">ment, experiments on real-world collected data and existing</text>
<text top="376" left="49" width="224" height="9" font="font2" id="p6_t38" reading_order_no="37" segment_no="9" tag_type="text">data, and evaluate the performance of our frameworks.</text>
<text top="396" left="49" width="141" height="9" font="font7" id="p6_t39" reading_order_no="38" segment_no="11" tag_type="title">A. Hardware System Development</text>
<text top="411" left="59" width="241" height="9" font="font2" id="p6_t40" reading_order_no="39" segment_no="12" tag_type="text">We used Hypersen 3D Solid-state LiDAR sensor with the</text>
<text top="423" left="49" width="251" height="9" font="font2" id="p6_t41" reading_order_no="40" segment_no="12" tag_type="text">model number HPS-3D160. The detector provided 160 × 60</text>
<text top="435" left="49" width="251" height="9" font="font2" id="p6_t42" reading_order_no="41" segment_no="12" tag_type="text">resolution which translates to 9600 points for each frame. The</text>
<text top="446" left="49" width="251" height="10" font="font2" id="p6_t43" reading_order_no="42" segment_no="12" tag_type="text">detector has a field of view (FOV) of 76 o × 32 o and uses light of</text>
<text top="459" left="49" width="251" height="9" font="font2" id="p6_t44" reading_order_no="43" segment_no="12" tag_type="text">wavelength 850 nm in the near-infrared band. Each frame was</text>
<text top="470" left="49" width="251" height="10" font="font2" id="p6_t45" reading_order_no="44" segment_no="12" tag_type="text">composed of a 160 × 60 grid with each element representing</text>
<text top="483" left="49" width="251" height="9" font="font2" id="p6_t46" reading_order_no="45" segment_no="12" tag_type="text">a distance value. We integrated a TexasInstrument 79-81 GHz</text>
<text top="495" left="49" width="251" height="9" font="font2" id="p6_t47" reading_order_no="46" segment_no="12" tag_type="text">mmWave sensor that provides 1024 PCD on each frame. We</text>
<text top="505" left="49" width="104" height="11" font="font2" id="p6_t48" reading_order_no="47" segment_no="12" tag_type="text">integrated a N V IDIA R</text>
<text top="505" left="160" width="73" height="11" font="font6" id="p6_t49" reading_order_no="48" segment_no="12" tag_type="text">J etson N ano T M</text>
<text top="507" left="240" width="60" height="9" font="font2" id="p6_t50" reading_order_no="49" segment_no="12" tag_type="text">Developer Kit</text>
<text top="519" left="49" width="251" height="9" font="font2" id="p6_t51" reading_order_no="50" segment_no="12" tag_type="text">with the LiDAR using microUSB cable. Jetson Nano delivers</text>
<text top="531" left="49" width="251" height="9" font="font2" id="p6_t52" reading_order_no="51" segment_no="12" tag_type="text">the compute performance to run modern AI workloads at</text>
<text top="543" left="49" width="251" height="9" font="font2" id="p6_t53" reading_order_no="52" segment_no="12" tag_type="text">unprecedented size, power, and cost ( 100 USD). It is also</text>
<text top="555" left="49" width="251" height="9" font="font2" id="p6_t54" reading_order_no="53" segment_no="12" tag_type="text">supported by NVIDIA JetPack, which includes a board support</text>
<text top="567" left="49" width="251" height="9" font="font2" id="p6_t55" reading_order_no="54" segment_no="12" tag_type="text">package (BSP), Linux OS, NVIDIA CUDA, cuDNN, and</text>
<text top="579" left="49" width="253" height="9" font="font2" id="p6_t56" reading_order_no="55" segment_no="12" tag_type="text">TensorRT software libraries for deep learning, computer vision,</text>
<text top="591" left="49" width="251" height="9" font="font2" id="p6_t57" reading_order_no="56" segment_no="12" tag_type="text">GPU computing, multimedia processing, and much more. We</text>
<text top="603" left="49" width="251" height="9" font="font2" id="p6_t58" reading_order_no="57" segment_no="12" tag_type="text">also integrated an 8 MegaPixel USB camera (Sony IMX179</text>
<text top="615" left="49" width="251" height="9" font="font2" id="p6_t59" reading_order_no="58" segment_no="12" tag_type="text">Sensor WebCam) to record the events for ground truth labeling</text>
<text top="627" left="49" width="251" height="9" font="font2" id="p6_t60" reading_order_no="59" segment_no="12" tag_type="text">(Fig 6). We wrote a python USB data reader to read LiDAR</text>
<text top="638" left="49" width="251" height="9" font="font2" id="p6_t61" reading_order_no="60" segment_no="12" tag_type="text">and mmWave data directly thus we can run our algorithms in</text>
<text top="650" left="49" width="38" height="9" font="font2" id="p6_t62" reading_order_no="61" segment_no="12" tag_type="text">real-time.</text>
<text top="670" left="49" width="78" height="9" font="font7" id="p6_t63" reading_order_no="62" segment_no="17" tag_type="title">B. Data Collection</text>
<text top="686" left="59" width="242" height="9" font="font7" id="p6_t64" reading_order_no="63" segment_no="19" tag_type="text">PALMAR Dataset : We employed 6 volunteers (graduate,</text>
<text top="698" left="49" width="253" height="9" font="font2" id="p6_t65" reading_order_no="64" segment_no="19" tag_type="text">undergraduate and high school students) as subjects and col-</text>
<text top="710" left="49" width="253" height="9" font="font2" id="p6_t66" reading_order_no="65" segment_no="19" tag_type="text">lected 7 different activities ("bending", "call", "check_watch",</text>
<text top="52" left="312" width="251" height="9" font="font2" id="p6_t67" reading_order_no="66" segment_no="1" tag_type="text">"single_wave", "two_wave", "walking", "normal_standing") in</text>
<text top="64" left="312" width="253" height="9" font="font2" id="p6_t68" reading_order_no="67" segment_no="1" tag_type="text">3 different rooms as indoor use case and one outdoor use case.</text>
<text top="76" left="312" width="253" height="9" font="font2" id="p6_t69" reading_order_no="68" segment_no="1" tag_type="text">The methodology for each session was designed as follows:</text>
<text top="88" left="312" width="252" height="9" font="font2" id="p6_t70" reading_order_no="69" segment_no="1" tag_type="text">subject would enter frame, subject would perform activity,</text>
<text top="100" left="312" width="252" height="9" font="font2" id="p6_t71" reading_order_no="70" segment_no="1" tag_type="text">subject then walks out of frame. For multiple person tracking,</text>
<text top="112" left="312" width="251" height="9" font="font2" id="p6_t72" reading_order_no="71" segment_no="1" tag_type="text">we use the same above strategies but with a 5 seconds interval</text>
<text top="124" left="312" width="251" height="9" font="font2" id="p6_t73" reading_order_no="72" segment_no="1" tag_type="text">for each person, i.e., the first person entered the field of view</text>
<text top="136" left="312" width="251" height="9" font="font2" id="p6_t74" reading_order_no="73" segment_no="1" tag_type="text">and start performing his/her assigned activity. After 2 seconds</text>
<text top="148" left="312" width="251" height="9" font="font2" id="p6_t75" reading_order_no="74" segment_no="1" tag_type="text">of first person’s entrance to the field of view, second person</text>
<text top="160" left="312" width="251" height="9" font="font2" id="p6_t76" reading_order_no="75" segment_no="1" tag_type="text">entered and started performing activities. We followed the same</text>
<text top="172" left="312" width="251" height="9" font="font2" id="p6_t77" reading_order_no="76" segment_no="1" tag_type="text">strategy for the third person as well. As the first person started</text>
<text top="184" left="312" width="251" height="9" font="font2" id="p6_t78" reading_order_no="77" segment_no="1" tag_type="text">his/her activity first, he/she finished his/her task and left. Then</text>
<text top="196" left="312" width="251" height="9" font="font2" id="p6_t79" reading_order_no="78" segment_no="1" tag_type="text">the second person and finally the third person left the field of</text>
<text top="208" left="312" width="251" height="9" font="font2" id="p6_t80" reading_order_no="79" segment_no="1" tag_type="text">view. We willingly created 20 crossover events (both of the</text>
<text top="220" left="312" width="251" height="9" font="font2" id="p6_t81" reading_order_no="80" segment_no="1" tag_type="text">persons are in the same line of the LiDAR field of view) to</text>
<text top="231" left="312" width="251" height="9" font="font2" id="p6_t82" reading_order_no="81" segment_no="1" tag_type="text">create ambiguity of tracking to validate the performance of our</text>
<text top="243" left="312" width="251" height="9" font="font2" id="p6_t83" reading_order_no="82" segment_no="1" tag_type="text">crossover ambiguity algorithm. In total we have 45 minutes of</text>
<text top="255" left="312" width="251" height="9" font="font2" id="p6_t84" reading_order_no="83" segment_no="1" tag_type="text">data where 25 minutes of data belong to single inhabitant and 20</text>
<text top="267" left="312" width="251" height="9" font="font2" id="p6_t85" reading_order_no="84" segment_no="1" tag_type="text">minutes of data belong to multiple-inhabitants. W used camera</text>
<text top="279" left="312" width="251" height="9" font="font2" id="p6_t86" reading_order_no="85" segment_no="1" tag_type="text">recorded videos as well as as the LiDAR PCD visualization for</text>
<text top="291" left="312" width="251" height="9" font="font2" id="p6_t87" reading_order_no="86" segment_no="1" tag_type="text">activity ground truth labeling. We define the entire LiDAR field</text>
<text top="303" left="312" width="251" height="9" font="font2" id="p6_t88" reading_order_no="87" segment_no="1" tag_type="text">of view as 1000 x 800 resolution and mmWave field of view as</text>
<text top="315" left="312" width="251" height="9" font="font2" id="p6_t89" reading_order_no="88" segment_no="1" tag_type="text">300 x 200 of voxel spaces. While annotating ground truths, we</text>
<text top="327" left="312" width="251" height="9" font="font2" id="p6_t90" reading_order_no="89" segment_no="1" tag_type="text">label the X-Y plane for location annotation where we consider</text>
<text top="339" left="312" width="251" height="9" font="font2" id="p6_t91" reading_order_no="90" segment_no="1" tag_type="text">the head location of each person as centroid ground truth of</text>
<text top="351" left="312" width="84" height="9" font="font2" id="p6_t92" reading_order_no="91" segment_no="1" tag_type="text">multi-person tracker.</text>
<text top="363" left="322" width="241" height="9" font="font10" id="p6_t93" reading_order_no="92" segment_no="10" tag_type="text">Benedek Dataset : To compare our framework’s efficacy, we</text>
<text top="375" left="312" width="251" height="9" font="font2" id="p6_t94" reading_order_no="93" segment_no="10" tag_type="text">also included existing large-scaled high resolution Velodyne</text>
<text top="387" left="312" width="253" height="9" font="font2" id="p6_t95" reading_order_no="94" segment_no="10" tag_type="text">HDL 64-E sensor data [5]. The dataset includes 28 partici-</text>
<text top="399" left="312" width="252" height="9" font="font2" id="p6_t96" reading_order_no="95" segment_no="10" tag_type="text">pants who performed 5 different activities ("bending", "call",</text>
<text top="411" left="312" width="251" height="9" font="font2" id="p6_t97" reading_order_no="96" segment_no="10" tag_type="text">"check_watch", "single_wave", "two_wave") in 6 different</text>
<text top="423" left="312" width="251" height="9" font="font2" id="p6_t98" reading_order_no="97" segment_no="10" tag_type="text">rooms as indoor use case and a thorough study on hundreds</text>
<text top="435" left="312" width="143" height="9" font="font2" id="p6_t99" reading_order_no="98" segment_no="10" tag_type="text">of pedestrians as outdoor use case.</text>
<text top="454" left="312" width="86" height="9" font="font7" id="p6_t100" reading_order_no="99" segment_no="13" tag_type="title">C. Baseline Methods</text>
<text top="469" left="322" width="241" height="9" font="font2" id="p6_t101" reading_order_no="100" segment_no="14" tag_type="text">We implemented the following baseline methods to compare</text>
<text top="481" left="312" width="198" height="9" font="font2" id="p6_t102" reading_order_no="101" segment_no="14" tag_type="text">our multi-person tracking algorithm’s efficiency:</text>
<text top="494" left="322" width="241" height="9" font="font10" id="p6_t103" reading_order_no="102" segment_no="15" tag_type="list">• Tracker 1: mID Method : mID [1] proposed Hungarian</text>
<text top="506" left="332" width="231" height="9" font="font2" id="p6_t104" reading_order_no="103" segment_no="15" tag_type="list">Algorithm assisted Kalman Filter tracking solution to</text>
<text top="518" left="332" width="231" height="9" font="font2" id="p6_t105" reading_order_no="104" segment_no="15" tag_type="list">track multiple cluster centroids produced by DBSCAN</text>
<text top="530" left="332" width="231" height="9" font="font2" id="p6_t106" reading_order_no="105" segment_no="15" tag_type="list">algorithm on mmWave generated PCD. The central</text>
<text top="542" left="332" width="231" height="9" font="font2" id="p6_t107" reading_order_no="106" segment_no="15" tag_type="list">differences between mID and PALMAR provided multiple</text>
<text top="554" left="332" width="231" height="9" font="font2" id="p6_t108" reading_order_no="107" segment_no="15" tag_type="list">person tracking solution are, (i) instead of using only</text>
<text top="566" left="332" width="231" height="9" font="font2" id="p6_t109" reading_order_no="108" segment_no="15" tag_type="list">DBSCAN, we used BIRCH clustering algorithm where</text>
<text top="578" left="332" width="231" height="9" font="font2" id="p6_t110" reading_order_no="109" segment_no="15" tag_type="list">we use DBSCAN algorithm only on each leaf node</text>
<text top="590" left="332" width="231" height="9" font="font2" id="p6_t111" reading_order_no="110" segment_no="15" tag_type="list">clustering, (ii) apart from the clustering, we incorporated</text>
<text top="602" left="332" width="231" height="9" font="font2" id="p6_t112" reading_order_no="111" segment_no="15" tag_type="list">an Adaptive Order HMM model to smooth the person</text>
<text top="614" left="332" width="231" height="9" font="font2" id="p6_t113" reading_order_no="112" segment_no="15" tag_type="list">tracking and CPDA algorithm to disambiguate during</text>
<text top="626" left="332" width="166" height="9" font="font2" id="p6_t114" reading_order_no="113" segment_no="15" tag_type="list">crossover trajectories among inhabitants.</text>
<text top="638" left="322" width="242" height="9" font="font10" id="p6_t115" reading_order_no="114" segment_no="16" tag_type="list">• Tracker 2: PALMAR tracker without BIRCH : In this</text>
<text top="650" left="332" width="231" height="9" font="font2" id="p6_t116" reading_order_no="115" segment_no="16" tag_type="list">method, we incorporated every framework we developed</text>
<text top="662" left="332" width="147" height="9" font="font2" id="p6_t117" reading_order_no="116" segment_no="16" tag_type="list">except BIRCH clustering algorithm.</text>
<text top="674" left="322" width="241" height="9" font="font10" id="p6_t118" reading_order_no="117" segment_no="18" tag_type="list">• Tracker 3: PALMAR tracker without AO-HMM : In this</text>
<text top="686" left="332" width="231" height="9" font="font2" id="p6_t119" reading_order_no="118" segment_no="18" tag_type="list">method, we incorporated every framework we developed</text>
<text top="698" left="332" width="231" height="9" font="font2" id="p6_t120" reading_order_no="119" segment_no="18" tag_type="list">except Adaptive Order HMM algorithm based smoothing</text>
<text top="710" left="332" width="119" height="9" font="font2" id="p6_t121" reading_order_no="120" segment_no="18" tag_type="list">technique of person tracking.</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font19" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font20" size="8" family="CMSY8" color="#000000"/>
<text top="52" left="59" width="241" height="9" font="font10" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="list">• Tracker 4: PALMAR tracker without CPDA : In this</text>
<text top="64" left="69" width="231" height="9" font="font2" id="p7_t2" reading_order_no="1" segment_no="0" tag_type="list">method, we incorporated every framework we developed</text>
<text top="76" left="69" width="231" height="9" font="font2" id="p7_t3" reading_order_no="2" segment_no="0" tag_type="list">except CPDA based disambiguation technique of person</text>
<text top="88" left="69" width="40" height="9" font="font2" id="p7_t4" reading_order_no="3" segment_no="0" tag_type="list">crossover.</text>
<text top="102" left="59" width="241" height="9" font="font2" id="p7_t5" reading_order_no="4" segment_no="2" tag_type="text">To compare the performance improvements of our proposed</text>
<text top="114" left="49" width="251" height="9" font="font2" id="p7_t6" reading_order_no="5" segment_no="2" tag_type="text">domain adaptation model, we implemented 7 stste-of-art</text>
<text top="126" left="49" width="251" height="9" font="font2" id="p7_t7" reading_order_no="6" segment_no="2" tag_type="text">algorithms: 1) DANN (Domain-Adversarial Training of Neural</text>
<text top="138" left="49" width="251" height="9" font="font2" id="p7_t8" reading_order_no="7" segment_no="2" tag_type="text">Networks) [15]; 2) CORAL [17]; 3) ADR (Adversarial Dropout</text>
<text top="150" left="49" width="251" height="9" font="font2" id="p7_t9" reading_order_no="8" segment_no="2" tag_type="text">Regularization) [19]; 4) VADA: A Virtual Adversarial Domain</text>
<text top="162" left="49" width="253" height="9" font="font2" id="p7_t10" reading_order_no="9" segment_no="2" tag_type="text">Adaptation (VADA) [12]; 5) DIRT-T: Decision-boundary It-</text>
<text top="174" left="49" width="252" height="9" font="font2" id="p7_t11" reading_order_no="10" segment_no="2" tag_type="text">erative Refinement Training with a Teacher [13]; 6) ADA:</text>
<text top="186" left="49" width="253" height="9" font="font2" id="p7_t12" reading_order_no="11" segment_no="2" tag_type="text">Associative Domain Adaptation [16]; and 7) SEVDA: Self-</text>
<text top="198" left="49" width="222" height="9" font="font2" id="p7_t13" reading_order_no="12" segment_no="2" tag_type="text">ensembling for visual domain adaptation [18] models.</text>
<text top="217" left="49" width="180" height="9" font="font7" id="p7_t14" reading_order_no="13" segment_no="5" tag_type="title">D. Point-Cloud Processing for Each Sensor</text>
<text top="233" left="59" width="241" height="9" font="font2" id="p7_t15" reading_order_no="14" segment_no="7" tag_type="text">For each of the sensor PCD, we ran the data transformation</text>
<text top="245" left="49" width="251" height="9" font="font2" id="p7_t16" reading_order_no="15" segment_no="7" tag_type="text">and voxel fitting. For 3D LiDAR, we found optimized voxel</text>
<text top="256" left="49" width="251" height="9" font="font2" id="p7_t17" reading_order_no="16" segment_no="7" tag_type="text">grid resolution of 0.3 cm and voxel mapping method AXOR</text>
<text top="268" left="49" width="251" height="9" font="font2" id="p7_t18" reading_order_no="17" segment_no="7" tag_type="text">mapping that provided highest accuracy on each object volume</text>
<text top="280" left="49" width="251" height="9" font="font2" id="p7_t19" reading_order_no="18" segment_no="7" tag_type="text">estimation (average error rate 0.03m) using LiDAR PCD from</text>
<text top="292" left="49" width="251" height="9" font="font2" id="p7_t20" reading_order_no="19" segment_no="7" tag_type="text">all three considered distances which reduces the number of</text>
<text top="304" left="49" width="251" height="9" font="font2" id="p7_t21" reading_order_no="20" segment_no="7" tag_type="text">PCD significantly with maximum 90% and minimum 79% of</text>
<text top="316" left="49" width="251" height="9" font="font2" id="p7_t22" reading_order_no="21" segment_no="7" tag_type="text">PCD. For 79 GHz mmWave, we found voxel grid resolution</text>
<text top="328" left="49" width="251" height="9" font="font2" id="p7_t23" reading_order_no="22" segment_no="7" tag_type="text">0.5 cm, voxel mapping method AXOR mapping with verage</text>
<text top="340" left="49" width="251" height="9" font="font2" id="p7_t24" reading_order_no="23" segment_no="7" tag_type="text">error rate 0.045m that reduces the number of PCD significantly</text>
<text top="352" left="49" width="202" height="9" font="font2" id="p7_t25" reading_order_no="24" segment_no="7" tag_type="text">with maximum 95% and minimum 83% of PCD.</text>
<text top="372" left="49" width="42" height="9" font="font7" id="p7_t26" reading_order_no="25" segment_no="8" tag_type="title">E. Results</text>
<text top="387" left="59" width="241" height="9" font="font7" id="p7_t27" reading_order_no="26" segment_no="10" tag_type="text">1) Person Tracker Performance: Since, Benedek Dataset</text>
<text top="399" left="49" width="252" height="9" font="font2" id="p7_t28" reading_order_no="27" segment_no="10" tag_type="text">does not have any ground truth of multiple person tracking,</text>
<text top="411" left="49" width="251" height="9" font="font2" id="p7_t29" reading_order_no="28" segment_no="10" tag_type="text">we evaluated the performance of our proposed multiple person</text>
<text top="423" left="49" width="251" height="9" font="font2" id="p7_t30" reading_order_no="29" segment_no="10" tag_type="text">tracking framework only on PALMAR dataset. At first, we ran</text>
<text top="435" left="49" width="251" height="9" font="font2" id="p7_t31" reading_order_no="30" segment_no="10" tag_type="text">the PCD transformation, voxelization and clustering (DBSCAN</text>
<text top="447" left="49" width="251" height="9" font="font2" id="p7_t32" reading_order_no="31" segment_no="10" tag_type="text">and BIRCH) algorithms to identify the centroid of each cluster</text>
<text top="459" left="49" width="252" height="9" font="font2" id="p7_t33" reading_order_no="32" segment_no="10" tag_type="text">i.e. the centroid of each inhabitant of the field of view. Then,</text>
<text top="471" left="49" width="251" height="9" font="font2" id="p7_t34" reading_order_no="33" segment_no="10" tag_type="text">we train the AO-HMM and CPDA algorithms using the location</text>
<text top="483" left="49" width="251" height="9" font="font2" id="p7_t35" reading_order_no="34" segment_no="10" tag_type="text">ground truth of each person. We use the Euclidean Distance</text>
<text top="494" left="49" width="251" height="9" font="font2" id="p7_t36" reading_order_no="35" segment_no="10" tag_type="text">(ED) as an error metric between the detected and original</text>
<text top="506" left="49" width="251" height="9" font="font2" id="p7_t37" reading_order_no="36" segment_no="10" tag_type="text">person location to evaluate person tracker’s performance. Our</text>
<text top="518" left="49" width="251" height="9" font="font2" id="p7_t38" reading_order_no="37" segment_no="10" tag_type="text">person tracker provided an overall ED of 9.3 with a minimum</text>
<text top="530" left="49" width="251" height="9" font="font2" id="p7_t39" reading_order_no="38" segment_no="10" tag_type="text">ED as 2.5 and a maximum ED of 11.6 in tracking multiple</text>
<text top="542" left="49" width="251" height="9" font="font2" id="p7_t40" reading_order_no="39" segment_no="10" tag_type="text">persons in different scenarios for 3D LiDAR; and an overall</text>
<text top="554" left="49" width="251" height="9" font="font2" id="p7_t41" reading_order_no="40" segment_no="10" tag_type="text">ED of 11.5 with a minimum ED as 3.1 and a maximum ED</text>
<text top="566" left="49" width="251" height="9" font="font2" id="p7_t42" reading_order_no="41" segment_no="10" tag_type="text">of 13.6 in tracking multiple persons in different scenarios</text>
<text top="578" left="49" width="253" height="9" font="font2" id="p7_t43" reading_order_no="42" segment_no="10" tag_type="text">for mmWave. Table I shows the details of the above results.</text>
<text top="590" left="48" width="252" height="9" font="font2" id="p7_t44" reading_order_no="43" segment_no="10" tag_type="text">We can clearly see that our person tracker outperforms the</text>
<text top="602" left="49" width="251" height="9" font="font2" id="p7_t45" reading_order_no="44" segment_no="10" tag_type="text">existing state-of-art model (Tracker 1: mID Method [1]) for</text>
<text top="614" left="49" width="251" height="9" font="font2" id="p7_t46" reading_order_no="45" segment_no="10" tag_type="text">every case with an overall 63.5% of accuracy improvements for</text>
<text top="626" left="49" width="251" height="9" font="font2" id="p7_t47" reading_order_no="46" segment_no="10" tag_type="text">LiDAR and an overall 57.4% of accuracy improvements for</text>
<text top="638" left="49" width="251" height="9" font="font2" id="p7_t48" reading_order_no="47" segment_no="10" tag_type="text">mmWave. The table also clearly illustrated the importance</text>
<text top="650" left="49" width="251" height="9" font="font2" id="p7_t49" reading_order_no="48" segment_no="10" tag_type="text">of each of our proposed modules as tracker 2, tracker 3</text>
<text top="662" left="49" width="251" height="9" font="font2" id="p7_t50" reading_order_no="49" segment_no="10" tag_type="text">and tracker 4 individually improves the state-of-art method</text>
<text top="674" left="49" width="251" height="9" font="font2" id="p7_t51" reading_order_no="50" segment_no="10" tag_type="text">significantly for single, two and three inhabitants environment</text>
<text top="686" left="49" width="251" height="9" font="font2" id="p7_t52" reading_order_no="51" segment_no="10" tag_type="text">as well as outdoor scenario for both LiDAR and mmWave</text>
<text top="698" left="49" width="251" height="9" font="font2" id="p7_t53" reading_order_no="52" segment_no="10" tag_type="text">sensors. In case of crossover sessions, where we willingly</text>
<text top="710" left="49" width="251" height="9" font="font2" id="p7_t54" reading_order_no="53" segment_no="10" tag_type="text">created some ambiguity of crossing over the field of views, we</text>
<text top="52" left="312" width="251" height="9" font="font2" id="p7_t55" reading_order_no="54" segment_no="1" tag_type="text">can clearly see that without our proposed AO-HMM (Tracker</text>
<text top="64" left="312" width="251" height="9" font="font2" id="p7_t56" reading_order_no="55" segment_no="1" tag_type="text">3) or CPDA (Tracker 4), person tracker significantly failed with</text>
<text top="76" left="312" width="251" height="9" font="font2" id="p7_t57" reading_order_no="56" segment_no="1" tag_type="text">ED. State-of-art multiple person tracker method mID (Tracker</text>
<text top="88" left="311" width="254" height="9" font="font2" id="p7_t58" reading_order_no="57" segment_no="1" tag_type="text">1) also failed significantly in presence of crossover ambiguity.</text>
<text top="100" left="312" width="251" height="9" font="font2" id="p7_t59" reading_order_no="58" segment_no="1" tag_type="text">However, inclusion of AO-HMM and CPDA improved the</text>
<text top="112" left="312" width="251" height="9" font="font2" id="p7_t60" reading_order_no="59" segment_no="1" tag_type="text">performance significantly with improvements of 66.6% and</text>
<text top="124" left="312" width="252" height="9" font="font2" id="p7_t61" reading_order_no="60" segment_no="1" tag_type="text">51.2% for LiDAR and mmWave sensors. On the other hand,</text>
<text top="136" left="312" width="251" height="9" font="font2" id="p7_t62" reading_order_no="61" segment_no="1" tag_type="text">in outdoor scenario, our proposed person tracker performed</text>
<text top="148" left="312" width="251" height="9" font="font2" id="p7_t63" reading_order_no="62" segment_no="1" tag_type="text">significantly better (55.5% and 63.6% for LiDAR and mmWave</text>
<text top="160" left="312" width="253" height="9" font="font2" id="p7_t64" reading_order_no="63" segment_no="1" tag_type="text">improvements) than state-of-art person tracking method as well.</text>
<text top="188" left="422" width="30" height="7" font="font5" id="p7_t65" reading_order_no="64" segment_no="3" tag_type="title">TABLE I</text>
<text top="197" left="327" width="221" height="7" font="font5" id="p7_t66" reading_order_no="65" segment_no="4" tag_type="text">E UCLIDEAN D ISTANCES BETWEEN PERSON TRACKERS PROVIDED</text>
<text top="207" left="335" width="206" height="6" font="font19" id="p7_t67" reading_order_no="66" segment_no="4" tag_type="text">CENTROID AND GROUND TRUTH CENTROID OF EACH PERSON</text>
<text top="223" left="396" width="24" height="7" font="font5" id="p7_t68" reading_order_no="67" segment_no="6" tag_type="table">Tracker</text>
<text top="232" left="395" width="4" height="7" font="font5" id="p7_t69" reading_order_no="68" segment_no="6" tag_type="table">1</text>
<text top="223" left="430" width="24" height="7" font="font5" id="p7_t70" reading_order_no="69" segment_no="6" tag_type="table">Tracker</text>
<text top="232" left="430" width="4" height="7" font="font5" id="p7_t71" reading_order_no="70" segment_no="6" tag_type="table">2</text>
<text top="223" left="465" width="24" height="7" font="font5" id="p7_t72" reading_order_no="71" segment_no="6" tag_type="table">Tracker</text>
<text top="232" left="465" width="4" height="7" font="font5" id="p7_t73" reading_order_no="72" segment_no="6" tag_type="table">3</text>
<text top="223" left="499" width="24" height="7" font="font5" id="p7_t74" reading_order_no="73" segment_no="6" tag_type="table">Tracker</text>
<text top="232" left="499" width="4" height="7" font="font5" id="p7_t75" reading_order_no="74" segment_no="6" tag_type="table">4</text>
<text top="223" left="534" width="15" height="7" font="font5" id="p7_t76" reading_order_no="75" segment_no="6" tag_type="table">Ours</text>
<text top="241" left="324" width="20" height="7" font="font5" id="p7_t77" reading_order_no="76" segment_no="6" tag_type="table">Single</text>
<text top="241" left="358" width="24" height="7" font="font5" id="p7_t78" reading_order_no="77" segment_no="6" tag_type="table">LiDAR</text>
<text top="241" left="395" width="14" height="7" font="font5" id="p7_t79" reading_order_no="78" segment_no="6" tag_type="table">15.3</text>
<text top="241" left="430" width="14" height="7" font="font5" id="p7_t80" reading_order_no="79" segment_no="6" tag_type="table">14.2</text>
<text top="241" left="464" width="14" height="7" font="font5" id="p7_t81" reading_order_no="80" segment_no="6" tag_type="table">11.7</text>
<text top="241" left="499" width="10" height="7" font="font5" id="p7_t82" reading_order_no="81" segment_no="6" tag_type="table">7.3</text>
<text top="241" left="534" width="10" height="7" font="font5" id="p7_t83" reading_order_no="82" segment_no="6" tag_type="table">2.5</text>
<text top="250" left="324" width="22" height="7" font="font5" id="p7_t84" reading_order_no="83" segment_no="6" tag_type="table">Person</text>
<text top="250" left="358" width="29" height="7" font="font5" id="p7_t85" reading_order_no="84" segment_no="6" tag_type="table">mmWave</text>
<text top="250" left="395" width="14" height="7" font="font5" id="p7_t86" reading_order_no="85" segment_no="6" tag_type="table">16.6</text>
<text top="250" left="430" width="14" height="7" font="font5" id="p7_t87" reading_order_no="86" segment_no="6" tag_type="table">16.1</text>
<text top="250" left="464" width="14" height="7" font="font5" id="p7_t88" reading_order_no="87" segment_no="6" tag_type="table">15.3</text>
<text top="250" left="500" width="10" height="7" font="font5" id="p7_t89" reading_order_no="88" segment_no="6" tag_type="table">9.5</text>
<text top="250" left="534" width="10" height="7" font="font5" id="p7_t90" reading_order_no="89" segment_no="6" tag_type="table">6.9</text>
<text top="260" left="323" width="14" height="7" font="font5" id="p7_t91" reading_order_no="90" segment_no="6" tag_type="table">Two</text>
<text top="260" left="358" width="24" height="7" font="font5" id="p7_t92" reading_order_no="91" segment_no="6" tag_type="table">LiDAR</text>
<text top="260" left="396" width="14" height="7" font="font5" id="p7_t93" reading_order_no="92" segment_no="6" tag_type="table">21.8</text>
<text top="260" left="430" width="14" height="7" font="font5" id="p7_t94" reading_order_no="93" segment_no="6" tag_type="table">19.3</text>
<text top="260" left="464" width="14" height="7" font="font5" id="p7_t95" reading_order_no="94" segment_no="6" tag_type="table">17.4</text>
<text top="260" left="499" width="14" height="7" font="font5" id="p7_t96" reading_order_no="95" segment_no="6" tag_type="table">14.7</text>
<text top="260" left="534" width="10" height="7" font="font5" id="p7_t97" reading_order_no="96" segment_no="6" tag_type="table">9.4</text>
<text top="269" left="324" width="24" height="7" font="font5" id="p7_t98" reading_order_no="97" segment_no="6" tag_type="table">Persons</text>
<text top="269" left="358" width="29" height="7" font="font5" id="p7_t99" reading_order_no="98" segment_no="6" tag_type="table">mmWave</text>
<text top="269" left="396" width="14" height="7" font="font5" id="p7_t100" reading_order_no="99" segment_no="6" tag_type="table">26.7</text>
<text top="269" left="430" width="14" height="7" font="font5" id="p7_t101" reading_order_no="100" segment_no="6" tag_type="table">25.3</text>
<text top="269" left="465" width="14" height="7" font="font5" id="p7_t102" reading_order_no="101" segment_no="6" tag_type="table">23.9</text>
<text top="269" left="499" width="14" height="7" font="font5" id="p7_t103" reading_order_no="102" segment_no="6" tag_type="table">16.3</text>
<text top="269" left="534" width="14" height="7" font="font5" id="p7_t104" reading_order_no="103" segment_no="6" tag_type="table">12.5</text>
<text top="278" left="323" width="19" height="7" font="font5" id="p7_t105" reading_order_no="104" segment_no="6" tag_type="table">Three</text>
<text top="278" left="358" width="24" height="7" font="font5" id="p7_t106" reading_order_no="105" segment_no="6" tag_type="table">LiDAR</text>
<text top="278" left="396" width="14" height="7" font="font5" id="p7_t107" reading_order_no="106" segment_no="6" tag_type="table">30.2</text>
<text top="278" left="430" width="14" height="7" font="font5" id="p7_t108" reading_order_no="107" segment_no="6" tag_type="table">23.7</text>
<text top="278" left="464" width="14" height="7" font="font5" id="p7_t109" reading_order_no="108" segment_no="6" tag_type="table">17.3</text>
<text top="278" left="499" width="14" height="7" font="font5" id="p7_t110" reading_order_no="109" segment_no="6" tag_type="table">14.9</text>
<text top="278" left="534" width="14" height="7" font="font5" id="p7_t111" reading_order_no="110" segment_no="6" tag_type="table">10.2</text>
<text top="287" left="324" width="24" height="7" font="font5" id="p7_t112" reading_order_no="111" segment_no="6" tag_type="table">Persons</text>
<text top="287" left="358" width="29" height="7" font="font5" id="p7_t113" reading_order_no="112" segment_no="6" tag_type="table">mmWave</text>
<text top="287" left="396" width="14" height="7" font="font5" id="p7_t114" reading_order_no="113" segment_no="6" tag_type="table">37.8</text>
<text top="287" left="430" width="14" height="7" font="font5" id="p7_t115" reading_order_no="114" segment_no="6" tag_type="table">33.5</text>
<text top="287" left="465" width="14" height="7" font="font5" id="p7_t116" reading_order_no="115" segment_no="6" tag_type="table">30.6</text>
<text top="287" left="500" width="14" height="7" font="font5" id="p7_t117" reading_order_no="116" segment_no="6" tag_type="table">25.5</text>
<text top="287" left="534" width="14" height="7" font="font5" id="p7_t118" reading_order_no="117" segment_no="6" tag_type="table">21.3</text>
<text top="296" left="324" width="58" height="7" font="font5" id="p7_t119" reading_order_no="118" segment_no="6" tag_type="table">Crossover LiDAR</text>
<text top="296" left="396" width="14" height="7" font="font5" id="p7_t120" reading_order_no="119" segment_no="6" tag_type="table">34.8</text>
<text top="296" left="430" width="14" height="7" font="font5" id="p7_t121" reading_order_no="120" segment_no="6" tag_type="table">25.5</text>
<text top="296" left="465" width="14" height="7" font="font5" id="p7_t122" reading_order_no="121" segment_no="6" tag_type="table">30.3</text>
<text top="296" left="500" width="14" height="7" font="font5" id="p7_t123" reading_order_no="122" segment_no="6" tag_type="table">34.2</text>
<text top="296" left="534" width="14" height="7" font="font5" id="p7_t124" reading_order_no="123" segment_no="6" tag_type="table">11.7</text>
<text top="305" left="324" width="27" height="7" font="font5" id="p7_t125" reading_order_no="124" segment_no="6" tag_type="table">Sessions</text>
<text top="305" left="358" width="29" height="7" font="font5" id="p7_t126" reading_order_no="125" segment_no="6" tag_type="table">mmWave</text>
<text top="305" left="396" width="14" height="7" font="font5" id="p7_t127" reading_order_no="126" segment_no="6" tag_type="table">39.6</text>
<text top="305" left="430" width="14" height="7" font="font5" id="p7_t128" reading_order_no="127" segment_no="6" tag_type="table">36.4</text>
<text top="305" left="465" width="14" height="7" font="font5" id="p7_t129" reading_order_no="128" segment_no="6" tag_type="table">35.2</text>
<text top="305" left="500" width="14" height="7" font="font5" id="p7_t130" reading_order_no="129" segment_no="6" tag_type="table">27.0</text>
<text top="305" left="534" width="14" height="7" font="font5" id="p7_t131" reading_order_no="130" segment_no="6" tag_type="table">19.3</text>
<text top="315" left="324" width="26" height="7" font="font5" id="p7_t132" reading_order_no="131" segment_no="6" tag_type="table">Outdoor</text>
<text top="315" left="358" width="24" height="7" font="font5" id="p7_t133" reading_order_no="132" segment_no="6" tag_type="table">LiDAR</text>
<text top="315" left="396" width="14" height="7" font="font5" id="p7_t134" reading_order_no="133" segment_no="6" tag_type="table">23.6</text>
<text top="315" left="430" width="14" height="7" font="font5" id="p7_t135" reading_order_no="134" segment_no="6" tag_type="table">24.9</text>
<text top="315" left="464" width="14" height="7" font="font5" id="p7_t136" reading_order_no="135" segment_no="6" tag_type="table">17.5</text>
<text top="315" left="499" width="14" height="7" font="font5" id="p7_t137" reading_order_no="136" segment_no="6" tag_type="table">14.3</text>
<text top="315" left="534" width="14" height="7" font="font5" id="p7_t138" reading_order_no="137" segment_no="6" tag_type="table">10.5</text>
<text top="324" left="324" width="27" height="7" font="font5" id="p7_t139" reading_order_no="138" segment_no="6" tag_type="table">Sessions</text>
<text top="324" left="358" width="29" height="7" font="font5" id="p7_t140" reading_order_no="139" segment_no="6" tag_type="table">mmWave</text>
<text top="324" left="396" width="14" height="7" font="font5" id="p7_t141" reading_order_no="140" segment_no="6" tag_type="table">28.5</text>
<text top="324" left="430" width="14" height="7" font="font5" id="p7_t142" reading_order_no="141" segment_no="6" tag_type="table">27.4</text>
<text top="324" left="465" width="14" height="7" font="font5" id="p7_t143" reading_order_no="142" segment_no="6" tag_type="table">22.3</text>
<text top="324" left="499" width="14" height="7" font="font5" id="p7_t144" reading_order_no="143" segment_no="6" tag_type="table">16.6</text>
<text top="324" left="534" width="14" height="7" font="font5" id="p7_t145" reading_order_no="144" segment_no="6" tag_type="table">10.3</text>
<text top="333" left="324" width="23" height="7" font="font5" id="p7_t146" reading_order_no="145" segment_no="6" tag_type="table">Overall</text>
<text top="333" left="358" width="24" height="7" font="font5" id="p7_t147" reading_order_no="146" segment_no="6" tag_type="table">LiDAR</text>
<text top="333" left="396" width="14" height="7" font="font5" id="p7_t148" reading_order_no="147" segment_no="6" tag_type="table">25.7</text>
<text top="333" left="430" width="14" height="7" font="font5" id="p7_t149" reading_order_no="148" segment_no="6" tag_type="table">21.2</text>
<text top="333" left="464" width="14" height="7" font="font5" id="p7_t150" reading_order_no="149" segment_no="6" tag_type="table">16.8</text>
<text top="333" left="499" width="14" height="7" font="font5" id="p7_t151" reading_order_no="150" segment_no="6" tag_type="table">12.3</text>
<text top="333" left="534" width="10" height="7" font="font5" id="p7_t152" reading_order_no="151" segment_no="6" tag_type="table">9.3</text>
<text top="342" left="324" width="11" height="7" font="font5" id="p7_t153" reading_order_no="152" segment_no="6" tag_type="table">ED</text>
<text top="342" left="358" width="29" height="7" font="font5" id="p7_t154" reading_order_no="153" segment_no="6" tag_type="table">mmWave</text>
<text top="342" left="396" width="14" height="7" font="font5" id="p7_t155" reading_order_no="154" segment_no="6" tag_type="table">27.5</text>
<text top="342" left="430" width="14" height="7" font="font5" id="p7_t156" reading_order_no="155" segment_no="6" tag_type="table">24.6</text>
<text top="342" left="465" width="14" height="7" font="font5" id="p7_t157" reading_order_no="156" segment_no="6" tag_type="table">20.4</text>
<text top="342" left="499" width="14" height="7" font="font5" id="p7_t158" reading_order_no="157" segment_no="6" tag_type="table">17.5</text>
<text top="342" left="534" width="14" height="7" font="font5" id="p7_t159" reading_order_no="158" segment_no="6" tag_type="table">11.6</text>
<text top="373" left="322" width="241" height="9" font="font7" id="p7_t160" reading_order_no="159" segment_no="9" tag_type="text">2) Activity Recognition Results: After identifying cluster’s</text>
<text top="385" left="312" width="251" height="9" font="font2" id="p7_t161" reading_order_no="160" segment_no="9" tag_type="text">centroids (i.e. each person’s location) and extracting their</text>
<text top="397" left="312" width="251" height="9" font="font2" id="p7_t162" reading_order_no="161" segment_no="9" tag_type="text">related PCD (as stated in Section III.C), we ran the activity</text>
<text top="408" left="312" width="251" height="9" font="font2" id="p7_t163" reading_order_no="162" segment_no="9" tag_type="text">recognition algorithm on each of the person’s extracted PCD</text>
<text top="420" left="312" width="251" height="10" font="font2" id="p7_t164" reading_order_no="163" segment_no="9" tag_type="text">voxels for both PALMAR datasets and Benedek dataset. We</text>
<text top="432" left="312" width="251" height="9" font="font2" id="p7_t165" reading_order_no="164" segment_no="9" tag_type="text">converted the our LiDAR, mmWave and Benedek datasets</text>
<text top="444" left="312" width="251" height="9" font="font2" id="p7_t166" reading_order_no="165" segment_no="9" tag_type="text">as 1000 x 800, 300 x 200 and 3500 x 2000 resolution (as</text>
<text top="456" left="312" width="251" height="9" font="font2" id="p7_t167" reading_order_no="166" segment_no="9" tag_type="text">used in Benedek et. al. [5] experiments) voxels. Then we</text>
<text top="468" left="312" width="251" height="9" font="font2" id="p7_t168" reading_order_no="167" segment_no="9" tag_type="text">trained and test on our Deep Activity Recognizer model on</text>
<text top="479" left="312" width="252" height="10" font="font2" id="p7_t169" reading_order_no="168" segment_no="9" tag_type="text">both of the datasets. We achieved accuracies of 75 . 75% ± 0 . 1 ,</text>
<text top="491" left="312" width="251" height="10" font="font12" id="p7_t170" reading_order_no="169" segment_no="9" tag_type="text">70 . 77% ± 0 . 2 and 73 . 85% ± 0 . 1 for LiDAR, mmWave and</text>
<text top="504" left="312" width="251" height="9" font="font2" id="p7_t171" reading_order_no="170" segment_no="9" tag_type="text">BENEDEK datasets source only respectively. We also achieved</text>
<text top="515" left="312" width="251" height="10" font="font2" id="p7_t172" reading_order_no="171" segment_no="9" tag_type="text">an accuracy of 93 . 81% ± 0 . 1 using only camera videos data</text>
<text top="528" left="312" width="78" height="9" font="font2" id="p7_t173" reading_order_no="172" segment_no="9" tag_type="text">(Computer Vision).</text>
<text top="674" left="312" width="24" height="7" font="font5" id="p7_t174" reading_order_no="173" segment_no="11" tag_type="text">Fig. 7.</text>
<text top="673" left="347" width="26" height="8" font="font5" id="p7_t175" reading_order_no="174" segment_no="11" tag_type="text">Video −</text>
<text top="673" left="366" width="65" height="8" font="font20" id="p7_t176" reading_order_no="175" segment_no="11" tag_type="text">→ LiDAR Domain</text>
<text top="683" left="312" width="119" height="7" font="font5" id="p7_t177" reading_order_no="176" segment_no="11" tag_type="text">Adaptation Accuracy Validation and</text>
<text top="692" left="312" width="34" height="7" font="font5" id="p7_t178" reading_order_no="177" segment_no="11" tag_type="text">Sensitivity</text>
<text top="677" left="434" width="23" height="7" font="font5" id="p7_t179" reading_order_no="178" segment_no="12" tag_type="text">Fig. 8.</text>
<text top="676" left="466" width="45" height="8" font="font5" id="p7_t180" reading_order_no="179" segment_no="12" tag_type="text">BENEDEK −</text>
<text top="676" left="504" width="49" height="8" font="font20" id="p7_t181" reading_order_no="180" segment_no="12" tag_type="text">→ LiDAR Do-</text>
<text top="686" left="434" width="118" height="7" font="font5" id="p7_t182" reading_order_no="181" segment_no="12" tag_type="text">main Adaptation Accuracy Validation</text>
<text top="695" left="434" width="48" height="7" font="font5" id="p7_t183" reading_order_no="182" segment_no="12" tag_type="text">and Sensitivity</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font21" size="8" family="CMR8" color="#000000"/>
	<fontspec id="font22" size="8" family="CMMI8" color="#000000"/>
	<fontspec id="font23" size="8" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font24" size="8" family="CMBX8" color="#000000"/>
<text top="54" left="289" width="33" height="7" font="font5" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="title">TABLE II</text>
<text top="62" left="143" width="326" height="8" font="font5" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="text">A CTIVITY RECOGNITION PERFORMANCES (% A CCURACY ON TARGET ) OF DOMAIN ADAPTATION</text>
<text top="81" left="58" width="25" height="7" font="font5" id="p8_t3" reading_order_no="2" segment_no="2" tag_type="table">Method</text>
<text top="81" left="113" width="38" height="7" font="font5" id="p8_t4" reading_order_no="3" segment_no="2" tag_type="table">BENEDEK</text>
<text top="89" left="113" width="7" height="8" font="font20" id="p8_t5" reading_order_no="4" segment_no="2" tag_type="table">−</text>
<text top="89" left="113" width="35" height="8" font="font20" id="p8_t6" reading_order_no="5" segment_no="2" tag_type="table">→ LiDAR</text>
<text top="80" left="164" width="36" height="8" font="font5" id="p8_t7" reading_order_no="6" segment_no="2" tag_type="table">LiDAR −</text>
<text top="80" left="193" width="8" height="8" font="font20" id="p8_t8" reading_order_no="7" segment_no="2" tag_type="table">→</text>
<text top="90" left="164" width="37" height="7" font="font5" id="p8_t9" reading_order_no="8" segment_no="2" tag_type="table">BENEDEK</text>
<text top="81" left="213" width="38" height="7" font="font5" id="p8_t10" reading_order_no="9" segment_no="2" tag_type="table">BENEDEK</text>
<text top="89" left="213" width="7" height="8" font="font20" id="p8_t11" reading_order_no="10" segment_no="2" tag_type="table">−</text>
<text top="89" left="213" width="41" height="8" font="font20" id="p8_t12" reading_order_no="11" segment_no="2" tag_type="table">→ mmWave</text>
<text top="80" left="267" width="41" height="8" font="font5" id="p8_t13" reading_order_no="12" segment_no="2" tag_type="table">mmWave −</text>
<text top="80" left="301" width="8" height="8" font="font20" id="p8_t14" reading_order_no="13" segment_no="2" tag_type="table">→</text>
<text top="90" left="267" width="37" height="7" font="font5" id="p8_t15" reading_order_no="14" segment_no="2" tag_type="table">BENEDEK</text>
<text top="80" left="322" width="35" height="8" font="font5" id="p8_t16" reading_order_no="15" segment_no="2" tag_type="table">LiDAR −</text>
<text top="80" left="350" width="8" height="8" font="font20" id="p8_t17" reading_order_no="16" segment_no="2" tag_type="table">→</text>
<text top="90" left="322" width="30" height="7" font="font5" id="p8_t18" reading_order_no="17" segment_no="2" tag_type="table">mmWave</text>
<text top="81" left="371" width="31" height="7" font="font5" id="p8_t19" reading_order_no="18" segment_no="2" tag_type="table">mmWave</text>
<text top="89" left="371" width="7" height="8" font="font20" id="p8_t20" reading_order_no="19" segment_no="2" tag_type="table">−</text>
<text top="89" left="371" width="35" height="8" font="font20" id="p8_t21" reading_order_no="20" segment_no="2" tag_type="table">→ LiDAR</text>
<text top="81" left="419" width="19" height="7" font="font5" id="p8_t22" reading_order_no="21" segment_no="2" tag_type="table">Video</text>
<text top="80" left="448" width="7" height="8" font="font20" id="p8_t23" reading_order_no="22" segment_no="2" tag_type="table">−</text>
<text top="80" left="448" width="8" height="8" font="font20" id="p8_t24" reading_order_no="23" segment_no="2" tag_type="table">→</text>
<text top="90" left="420" width="24" height="7" font="font5" id="p8_t25" reading_order_no="24" segment_no="2" tag_type="table">LiDAR</text>
<text top="81" left="468" width="19" height="7" font="font5" id="p8_t26" reading_order_no="25" segment_no="2" tag_type="table">Video</text>
<text top="80" left="497" width="7" height="8" font="font20" id="p8_t27" reading_order_no="26" segment_no="2" tag_type="table">−</text>
<text top="80" left="497" width="8" height="8" font="font20" id="p8_t28" reading_order_no="27" segment_no="2" tag_type="table">→</text>
<text top="90" left="468" width="30" height="7" font="font5" id="p8_t29" reading_order_no="28" segment_no="2" tag_type="table">mmWave</text>
<text top="81" left="517" width="19" height="7" font="font5" id="p8_t30" reading_order_no="29" segment_no="2" tag_type="table">Video</text>
<text top="80" left="546" width="7" height="8" font="font20" id="p8_t31" reading_order_no="30" segment_no="2" tag_type="table">−</text>
<text top="80" left="546" width="8" height="8" font="font20" id="p8_t32" reading_order_no="31" segment_no="2" tag_type="table">→</text>
<text top="90" left="517" width="37" height="7" font="font5" id="p8_t33" reading_order_no="32" segment_no="2" tag_type="table">BENEDEK</text>
<text top="99" left="58" width="22" height="7" font="font5" id="p8_t34" reading_order_no="33" segment_no="2" tag_type="table">Source</text>
<text top="99" left="112" width="36" height="8" font="font21" id="p8_t35" reading_order_no="34" segment_no="2" tag_type="table">73 . 85 ± . 1</text>
<text top="99" left="164" width="36" height="8" font="font21" id="p8_t36" reading_order_no="35" segment_no="2" tag_type="table">75 . 75 ± . 1</text>
<text top="99" left="213" width="36" height="8" font="font21" id="p8_t37" reading_order_no="36" segment_no="2" tag_type="table">73 . 85 ± . 1</text>
<text top="99" left="267" width="36" height="8" font="font21" id="p8_t38" reading_order_no="37" segment_no="2" tag_type="table">70 . 77 ± . 2</text>
<text top="99" left="322" width="36" height="8" font="font21" id="p8_t39" reading_order_no="38" segment_no="2" tag_type="table">75 . 75 ± . 1</text>
<text top="99" left="370" width="36" height="8" font="font21" id="p8_t40" reading_order_no="39" segment_no="2" tag_type="table">70 . 77 ± . 2</text>
<text top="99" left="420" width="36" height="8" font="font21" id="p8_t41" reading_order_no="40" segment_no="2" tag_type="table">93 . 81 ± . 1</text>
<text top="99" left="468" width="36" height="8" font="font21" id="p8_t42" reading_order_no="41" segment_no="2" tag_type="table">93 . 81 ± . 1</text>
<text top="99" left="517" width="36" height="8" font="font21" id="p8_t43" reading_order_no="42" segment_no="2" tag_type="table">93 . 81 ± . 1</text>
<text top="109" left="58" width="39" height="7" font="font5" id="p8_t44" reading_order_no="43" segment_no="2" tag_type="table">DANN [15]</text>
<text top="108" left="113" width="35" height="8" font="font21" id="p8_t45" reading_order_no="44" segment_no="2" tag_type="table">82 . 83 ± . 3</text>
<text top="108" left="164" width="36" height="8" font="font21" id="p8_t46" reading_order_no="45" segment_no="2" tag_type="table">83 . 89 ± . 1</text>
<text top="108" left="213" width="36" height="8" font="font21" id="p8_t47" reading_order_no="46" segment_no="2" tag_type="table">83 . 88 ± . 1</text>
<text top="108" left="267" width="36" height="8" font="font21" id="p8_t48" reading_order_no="47" segment_no="2" tag_type="table">85 . 34 ± . 2</text>
<text top="108" left="322" width="36" height="8" font="font21" id="p8_t49" reading_order_no="48" segment_no="2" tag_type="table">82 . 43 ± . 3</text>
<text top="108" left="371" width="36" height="8" font="font21" id="p8_t50" reading_order_no="49" segment_no="2" tag_type="table">89 . 65 ± . 2</text>
<text top="108" left="420" width="36" height="8" font="font21" id="p8_t51" reading_order_no="50" segment_no="2" tag_type="table">89 . 66 ± . 3</text>
<text top="108" left="468" width="36" height="8" font="font21" id="p8_t52" reading_order_no="51" segment_no="2" tag_type="table">91 . 56 ± . 2</text>
<text top="108" left="517" width="36" height="8" font="font21" id="p8_t53" reading_order_no="52" segment_no="2" tag_type="table">90 . 34 ± . 2</text>
<text top="118" left="58" width="43" height="7" font="font5" id="p8_t54" reading_order_no="53" segment_no="2" tag_type="table">CORAL [17]</text>
<text top="117" left="113" width="35" height="8" font="font21" id="p8_t55" reading_order_no="54" segment_no="2" tag_type="table">86 . 03 ± . 3</text>
<text top="117" left="164" width="36" height="8" font="font21" id="p8_t56" reading_order_no="55" segment_no="2" tag_type="table">84 . 57 ± . 2</text>
<text top="117" left="213" width="36" height="8" font="font21" id="p8_t57" reading_order_no="56" segment_no="2" tag_type="table">81 . 70 ± . 1</text>
<text top="117" left="267" width="36" height="8" font="font21" id="p8_t58" reading_order_no="57" segment_no="2" tag_type="table">83 . 63 ± . 2</text>
<text top="117" left="322" width="36" height="8" font="font21" id="p8_t59" reading_order_no="58" segment_no="2" tag_type="table">81 . 75 ± . 3</text>
<text top="117" left="371" width="36" height="8" font="font21" id="p8_t60" reading_order_no="59" segment_no="2" tag_type="table">86 . 39 ± . 2</text>
<text top="117" left="420" width="36" height="8" font="font21" id="p8_t61" reading_order_no="60" segment_no="2" tag_type="table">87 . 44 ± . 3</text>
<text top="117" left="468" width="36" height="8" font="font21" id="p8_t62" reading_order_no="61" segment_no="2" tag_type="table">88 . 60 ± . 2</text>
<text top="117" left="517" width="36" height="8" font="font21" id="p8_t63" reading_order_no="62" segment_no="2" tag_type="table">89 . 56 ± . 2</text>
<text top="127" left="58" width="33" height="7" font="font5" id="p8_t64" reading_order_no="63" segment_no="2" tag_type="table">ADR [19]</text>
<text top="127" left="113" width="35" height="8" font="font21" id="p8_t65" reading_order_no="64" segment_no="2" tag_type="table">89 . 03 ± . 4</text>
<text top="127" left="164" width="36" height="8" font="font21" id="p8_t66" reading_order_no="65" segment_no="2" tag_type="table">84 . 66 ± . 4</text>
<text top="127" left="213" width="36" height="8" font="font21" id="p8_t67" reading_order_no="66" segment_no="2" tag_type="table">85 . 92 ± . 1</text>
<text top="127" left="267" width="36" height="8" font="font21" id="p8_t68" reading_order_no="67" segment_no="2" tag_type="table">79 . 66 ± . 2</text>
<text top="127" left="322" width="36" height="8" font="font21" id="p8_t69" reading_order_no="68" segment_no="2" tag_type="table">80 . 80 ± . 3</text>
<text top="127" left="371" width="36" height="8" font="font21" id="p8_t70" reading_order_no="69" segment_no="2" tag_type="table">87 . 32 ± . 2</text>
<text top="127" left="420" width="31" height="8" font="font21" id="p8_t71" reading_order_no="70" segment_no="2" tag_type="table">86 . 3 ± . 3</text>
<text top="127" left="468" width="36" height="8" font="font21" id="p8_t72" reading_order_no="71" segment_no="2" tag_type="table">87 . 59 ± . 2</text>
<text top="127" left="517" width="36" height="8" font="font21" id="p8_t73" reading_order_no="72" segment_no="2" tag_type="table">87 . 53 ± . 2</text>
<text top="137" left="58" width="38" height="7" font="font5" id="p8_t74" reading_order_no="73" segment_no="2" tag_type="table">VADA [12]</text>
<text top="136" left="113" width="35" height="8" font="font21" id="p8_t75" reading_order_no="74" segment_no="2" tag_type="table">91 . 51 ± . 2</text>
<text top="136" left="164" width="36" height="8" font="font21" id="p8_t76" reading_order_no="75" segment_no="2" tag_type="table">85 . 05 ± . 2</text>
<text top="136" left="213" width="36" height="8" font="font21" id="p8_t77" reading_order_no="76" segment_no="2" tag_type="table">87 . 44 ± . 1</text>
<text top="136" left="267" width="36" height="8" font="font21" id="p8_t78" reading_order_no="77" segment_no="2" tag_type="table">82 . 62 ± . 2</text>
<text top="136" left="322" width="36" height="8" font="font21" id="p8_t79" reading_order_no="78" segment_no="2" tag_type="table">79 . 51 ± . 3</text>
<text top="136" left="371" width="36" height="8" font="font21" id="p8_t80" reading_order_no="79" segment_no="2" tag_type="table">84 . 98 ± . 2</text>
<text top="136" left="420" width="36" height="8" font="font21" id="p8_t81" reading_order_no="80" segment_no="2" tag_type="table">87 . 21 ± . 3</text>
<text top="136" left="468" width="36" height="8" font="font21" id="p8_t82" reading_order_no="81" segment_no="2" tag_type="table">88 . 45 ± . 2</text>
<text top="136" left="517" width="36" height="8" font="font21" id="p8_t83" reading_order_no="82" segment_no="2" tag_type="table">87 . 61 ± . 2</text>
<text top="146" left="58" width="34" height="7" font="font5" id="p8_t84" reading_order_no="83" segment_no="2" tag_type="table">DIRT [13]</text>
<text top="146" left="113" width="35" height="8" font="font21" id="p8_t85" reading_order_no="84" segment_no="2" tag_type="table">92 . 65 ± . 2</text>
<text top="146" left="164" width="36" height="8" font="font21" id="p8_t86" reading_order_no="85" segment_no="2" tag_type="table">85 . 43 ± . 2</text>
<text top="146" left="213" width="36" height="8" font="font21" id="p8_t87" reading_order_no="86" segment_no="2" tag_type="table">85 . 34 ± . 1</text>
<text top="146" left="267" width="36" height="8" font="font21" id="p8_t88" reading_order_no="87" segment_no="2" tag_type="table">86 . 98 ± . 2</text>
<text top="146" left="322" width="36" height="8" font="font21" id="p8_t89" reading_order_no="88" segment_no="2" tag_type="table">83 . 58 ± . 3</text>
<text top="146" left="371" width="36" height="8" font="font21" id="p8_t90" reading_order_no="89" segment_no="2" tag_type="table">87 . 87 ± . 2</text>
<text top="146" left="420" width="36" height="8" font="font21" id="p8_t91" reading_order_no="90" segment_no="2" tag_type="table">91 . 44 ± . 3</text>
<text top="146" left="468" width="36" height="8" font="font21" id="p8_t92" reading_order_no="91" segment_no="2" tag_type="table">91 . 56 ± . 2</text>
<text top="146" left="517" width="36" height="8" font="font21" id="p8_t93" reading_order_no="92" segment_no="2" tag_type="table">91 . 56 ± . 2</text>
<text top="156" left="58" width="33" height="7" font="font5" id="p8_t94" reading_order_no="93" segment_no="2" tag_type="table">ADA [16]</text>
<text top="155" left="113" width="35" height="8" font="font21" id="p8_t95" reading_order_no="94" segment_no="2" tag_type="table">92 . 78 ± . 2</text>
<text top="155" left="164" width="36" height="8" font="font21" id="p8_t96" reading_order_no="95" segment_no="2" tag_type="table">86 . 03 ± . 2</text>
<text top="155" left="213" width="36" height="8" font="font21" id="p8_t97" reading_order_no="96" segment_no="2" tag_type="table">84 . 56 ± . 1</text>
<text top="155" left="267" width="36" height="8" font="font21" id="p8_t98" reading_order_no="97" segment_no="2" tag_type="table">88 . 50 ± . 2</text>
<text top="155" left="322" width="36" height="8" font="font21" id="p8_t99" reading_order_no="98" segment_no="2" tag_type="table">85 . 78 ± . 3</text>
<text top="155" left="371" width="36" height="8" font="font21" id="p8_t100" reading_order_no="99" segment_no="2" tag_type="table">86 . 98 ± . 2</text>
<text top="155" left="420" width="36" height="8" font="font21" id="p8_t101" reading_order_no="100" segment_no="2" tag_type="table">92 . 45 ± . 3</text>
<text top="155" left="468" width="36" height="8" font="font21" id="p8_t102" reading_order_no="101" segment_no="2" tag_type="table">92 . 56 ± . 2</text>
<text top="155" left="517" width="36" height="8" font="font21" id="p8_t103" reading_order_no="102" segment_no="2" tag_type="table">90 . 89 ± . 2</text>
<text top="165" left="58" width="42" height="7" font="font5" id="p8_t104" reading_order_no="103" segment_no="2" tag_type="table">SEVDA [18]</text>
<text top="164" left="113" width="35" height="8" font="font21" id="p8_t105" reading_order_no="104" segment_no="2" tag_type="table">92 . 13 ± . 3</text>
<text top="164" left="164" width="32" height="8" font="font21" id="p8_t106" reading_order_no="105" segment_no="2" tag_type="table">86 . 7 ± . 3</text>
<text top="164" left="213" width="36" height="8" font="font21" id="p8_t107" reading_order_no="106" segment_no="2" tag_type="table">83 . 58 ± . 1</text>
<text top="164" left="267" width="36" height="8" font="font21" id="p8_t108" reading_order_no="107" segment_no="2" tag_type="table">88 . 45 ± . 2</text>
<text top="164" left="322" width="36" height="8" font="font21" id="p8_t109" reading_order_no="108" segment_no="2" tag_type="table">86 . 34 ± . 3</text>
<text top="164" left="371" width="36" height="8" font="font21" id="p8_t110" reading_order_no="109" segment_no="2" tag_type="table">87 . 86 ± . 2</text>
<text top="164" left="420" width="36" height="8" font="font21" id="p8_t111" reading_order_no="110" segment_no="2" tag_type="table">93 . 56 ± . 3</text>
<text top="164" left="468" width="36" height="8" font="font21" id="p8_t112" reading_order_no="111" segment_no="2" tag_type="table">92 . 87 ± . 2</text>
<text top="164" left="517" width="36" height="8" font="font21" id="p8_t113" reading_order_no="112" segment_no="2" tag_type="table">91 . 56 ± . 2</text>
<text top="174" left="58" width="17" height="7" font="font23" id="p8_t114" reading_order_no="113" segment_no="2" tag_type="table">Ours</text>
<text top="174" left="113" width="38" height="8" font="font24" id="p8_t115" reading_order_no="114" segment_no="2" tag_type="table">94 . 63 ± . 3</text>
<text top="174" left="164" width="37" height="8" font="font24" id="p8_t116" reading_order_no="115" segment_no="2" tag_type="table">89 . 38 ± . 2</text>
<text top="174" left="213" width="39" height="8" font="font24" id="p8_t117" reading_order_no="116" segment_no="2" tag_type="table">85 . 39 ± . 1</text>
<text top="174" left="267" width="39" height="8" font="font24" id="p8_t118" reading_order_no="117" segment_no="2" tag_type="table">87 . 52 ± . 2</text>
<text top="174" left="322" width="37" height="8" font="font24" id="p8_t119" reading_order_no="118" segment_no="2" tag_type="table">90 . 64 ± . 3</text>
<text top="174" left="371" width="36" height="8" font="font24" id="p8_t120" reading_order_no="119" segment_no="2" tag_type="table">91 . 88 ± . 2</text>
<text top="174" left="420" width="36" height="8" font="font24" id="p8_t121" reading_order_no="120" segment_no="2" tag_type="table">96 . 71 ± . 3</text>
<text top="174" left="468" width="37" height="8" font="font24" id="p8_t122" reading_order_no="121" segment_no="2" tag_type="table">95 . 55 ± . 2</text>
<text top="174" left="517" width="37" height="8" font="font24" id="p8_t123" reading_order_no="122" segment_no="2" tag_type="table">94 . 38 ± . 2</text>
<text top="206" left="59" width="241" height="10" font="font7" id="p8_t124" reading_order_no="123" segment_no="3" tag_type="text">3) Domain Adaptation Results: The aim of our domain</text>
<text top="218" left="49" width="253" height="9" font="font2" id="p8_t125" reading_order_no="124" segment_no="3" tag_type="text">adaptation is to utilize more accurate models to improve less ac-</text>
<text top="230" left="49" width="251" height="9" font="font2" id="p8_t126" reading_order_no="125" segment_no="3" tag_type="text">curate models as stated above. In this regard, we experimented</text>
<text top="242" left="49" width="251" height="9" font="font2" id="p8_t127" reading_order_no="126" segment_no="3" tag_type="text">on all of our developed models which performances wise</text>
<text top="254" left="49" width="251" height="9" font="font2" id="p8_t128" reading_order_no="127" segment_no="3" tag_type="text">can be ranked as follows: ComputerV ision &gt; LiDAR &gt;</text>
<text top="266" left="49" width="251" height="9" font="font6" id="p8_t129" reading_order_no="128" segment_no="3" tag_type="text">BEN EDEK &gt; mmW ave . While selecting source and target</text>
<text top="278" left="49" width="251" height="9" font="font2" id="p8_t130" reading_order_no="129" segment_no="3" tag_type="text">dataset pair, we consider only the common classes between</text>
<text top="290" left="49" width="251" height="9" font="font2" id="p8_t131" reading_order_no="130" segment_no="3" tag_type="text">them to avoid class heterogeneity. According to the training</text>
<text top="302" left="49" width="251" height="9" font="font2" id="p8_t132" reading_order_no="131" segment_no="3" tag_type="text">method presented, we first trained the source autoencoder</text>
<text top="314" left="49" width="251" height="9" font="font2" id="p8_t133" reading_order_no="132" segment_no="3" tag_type="text">until convergence, froze the source encoder, trained the target</text>
<text top="326" left="49" width="251" height="9" font="font2" id="p8_t134" reading_order_no="133" segment_no="3" tag_type="text">autoencoder using the target labels by optimizing the Equation</text>
<text top="338" left="49" width="251" height="9" font="font2" id="p8_t135" reading_order_no="134" segment_no="3" tag_type="text">8 and finally train the classifier network using the objective</text>
<text top="350" left="49" width="251" height="9" font="font2" id="p8_t136" reading_order_no="135" segment_no="3" tag_type="text">function presented in Equation 9. Table II presented the details</text>
<text top="362" left="49" width="251" height="9" font="font2" id="p8_t137" reading_order_no="136" segment_no="3" tag_type="text">results of domain adaptation using state-of-art methods as well</text>
<text top="374" left="49" width="251" height="9" font="font2" id="p8_t138" reading_order_no="137" segment_no="3" tag_type="text">as our proposed method. We clearly can see that our proposed</text>
<text top="386" left="49" width="251" height="9" font="font2" id="p8_t139" reading_order_no="138" segment_no="3" tag_type="text">framework outperforms all baseline domain adaptation methods</text>
<text top="398" left="49" width="251" height="9" font="font2" id="p8_t140" reading_order_no="139" segment_no="3" tag_type="text">for every pair of source-target datasets. From these figures</text>
<text top="409" left="49" width="221" height="10" font="font2" id="p8_t141" reading_order_no="140" segment_no="3" tag_type="text">Fig 7 and Fig 8, we clearly can identify that Video −</text>
<text top="409" left="262" width="38" height="10" font="font13" id="p8_t142" reading_order_no="141" segment_no="3" tag_type="text">→ LiDAR</text>
<text top="422" left="49" width="251" height="9" font="font2" id="p8_t143" reading_order_no="142" segment_no="3" tag_type="text">domain adaptation converges much faster and less sensitive</text>
<text top="433" left="49" width="73" height="10" font="font2" id="p8_t144" reading_order_no="143" segment_no="3" tag_type="text">than BENEDEK −</text>
<text top="433" left="114" width="186" height="10" font="font13" id="p8_t145" reading_order_no="144" segment_no="3" tag_type="text">→ LiDAR. As we know, video data has lesser</text>
<text top="446" left="49" width="251" height="9" font="font2" id="p8_t146" reading_order_no="145" segment_no="3" tag_type="text">noises than LiDAR PCD data that depicts the faster convergence</text>
<text top="458" left="49" width="251" height="9" font="font2" id="p8_t147" reading_order_no="146" segment_no="3" tag_type="text">with lesser noises (sensitivity) for domain adaptation than</text>
<text top="469" left="49" width="52" height="10" font="font7" id="p8_t148" reading_order_no="147" segment_no="3" tag_type="text">BENEDEK −</text>
<text top="469" left="93" width="207" height="9" font="font13" id="p8_t149" reading_order_no="148" segment_no="3" tag_type="text">→ LiDAR. While trying to transfer knowledge from</text>
<text top="481" left="49" width="252" height="10" font="font2" id="p8_t150" reading_order_no="149" segment_no="3" tag_type="text">higher quality (Benedek) dataset to lower quality ( LiDAR ) data,</text>
<text top="493" left="49" width="252" height="9" font="font2" id="p8_t151" reading_order_no="150" segment_no="3" tag_type="text">we see significant improvements of accuracy as well as faster</text>
<text top="505" left="49" width="252" height="9" font="font2" id="p8_t152" reading_order_no="151" segment_no="3" tag_type="text">convergence of domain adaptation (Fig 7). On the other hand,</text>
<text top="517" left="49" width="251" height="9" font="font2" id="p8_t153" reading_order_no="152" segment_no="3" tag_type="text">while trying to transfer knowledge from lower quality data</text>
<text top="529" left="49" width="251" height="9" font="font2" id="p8_t154" reading_order_no="153" segment_no="3" tag_type="text">( LiDAR ) to higher quality data (Benedek), the improvement</text>
<text top="541" left="49" width="251" height="9" font="font2" id="p8_t155" reading_order_no="154" segment_no="3" tag_type="text">of domain adaptation is not that much significant and domain</text>
<text top="553" left="49" width="208" height="9" font="font2" id="p8_t156" reading_order_no="155" segment_no="3" tag_type="text">convergence takes more epochs than the prior one.</text>
<text top="566" left="59" width="241" height="9" font="font7" id="p8_t157" reading_order_no="156" segment_no="12" tag_type="text">4) Edge Computing Device Performance: To evaluate the</text>
<text top="578" left="49" width="251" height="9" font="font2" id="p8_t158" reading_order_no="157" segment_no="12" tag_type="text">edge computing system performance, we ran LAMAR system</text>
<text top="590" left="49" width="251" height="9" font="font2" id="p8_t159" reading_order_no="158" segment_no="12" tag_type="text">in real-time crowd environment (hallway) and recorded videos</text>
<text top="602" left="49" width="251" height="9" font="font2" id="p8_t160" reading_order_no="159" segment_no="12" tag_type="text">along with the output of framework (number of person and</text>
<text top="614" left="49" width="251" height="9" font="font2" id="p8_t161" reading_order_no="160" segment_no="12" tag_type="text">each person’s activities). We considered average time-delay</text>
<text top="626" left="49" width="251" height="9" font="font2" id="p8_t162" reading_order_no="161" segment_no="12" tag_type="text">(in seconds) of detected activity’s start-end points comparing</text>
<text top="638" left="49" width="251" height="9" font="font2" id="p8_t163" reading_order_no="162" segment_no="12" tag_type="text">to the ground truth. Table III shows that for single person</text>
<text top="650" left="49" width="252" height="9" font="font2" id="p8_t164" reading_order_no="163" segment_no="12" tag_type="text">scenario, the time delay is almost same to state-of-art (T1)</text>
<text top="662" left="49" width="251" height="9" font="font2" id="p8_t165" reading_order_no="164" segment_no="12" tag_type="text">but for multiple person scenario, our method produced higher</text>
<text top="674" left="49" width="251" height="9" font="font2" id="p8_t166" reading_order_no="165" segment_no="12" tag_type="text">time delays in recognizing activities with an overall time delay</text>
<text top="685" left="49" width="251" height="10" font="font2" id="p8_t167" reading_order_no="166" segment_no="12" tag_type="text">of 4 seconds which is ≈ 0 . 5 seconds degradation of activity</text>
<text top="698" left="49" width="251" height="9" font="font2" id="p8_t168" reading_order_no="167" segment_no="12" tag_type="text">recognition time which is tolerable in terms of real-time HAR</text>
<text top="710" left="49" width="30" height="9" font="font2" id="p8_t169" reading_order_no="168" segment_no="12" tag_type="text">system.</text>
<text top="208" left="420" width="36" height="7" font="font5" id="p8_t170" reading_order_no="169" segment_no="4" tag_type="title">TABLE III</text>
<text top="217" left="333" width="209" height="7" font="font5" id="p8_t171" reading_order_no="170" segment_no="5" tag_type="text">T IME - DELAYS ( IN SECONDS ) OF PALMAR SYSTEM ACTIVITY</text>
<text top="226" left="320" width="234" height="7" font="font5" id="p8_t172" reading_order_no="171" segment_no="5" tag_type="text">RECOGNITION WITH DIFFERENT BASELINE MULTI - PERSON TRACKING</text>
<text top="235" left="312" width="251" height="7" font="font5" id="p8_t173" reading_order_no="172" segment_no="5" tag_type="text">MODELS (+T1 MEANS PROPOSED DOMAIN ADAPTATION FRAMEWORK PLUS</text>
<text top="244" left="417" width="42" height="7" font="font5" id="p8_t174" reading_order_no="173" segment_no="6" tag_type="text">T RACKER 1)</text>
<text top="262" left="415" width="13" height="7" font="font5" id="p8_t175" reading_order_no="174" segment_no="7" tag_type="table">+T1</text>
<text top="262" left="441" width="13" height="7" font="font5" id="p8_t176" reading_order_no="175" segment_no="7" tag_type="table">+T2</text>
<text top="262" left="467" width="13" height="7" font="font5" id="p8_t177" reading_order_no="176" segment_no="7" tag_type="table">+T3</text>
<text top="262" left="493" width="13" height="7" font="font5" id="p8_t178" reading_order_no="177" segment_no="7" tag_type="table">+T4</text>
<text top="262" left="519" width="20" height="7" font="font5" id="p8_t179" reading_order_no="178" segment_no="7" tag_type="table">+Ours</text>
<text top="272" left="339" width="20" height="7" font="font5" id="p8_t180" reading_order_no="179" segment_no="7" tag_type="table">Single</text>
<text top="272" left="376" width="24" height="7" font="font5" id="p8_t181" reading_order_no="180" segment_no="7" tag_type="table">LiDAR</text>
<text top="272" left="415" width="14" height="7" font="font5" id="p8_t182" reading_order_no="181" segment_no="7" tag_type="table">3.35</text>
<text top="272" left="441" width="14" height="7" font="font5" id="p8_t183" reading_order_no="182" segment_no="7" tag_type="table">3.37</text>
<text top="272" left="469" width="10" height="7" font="font5" id="p8_t184" reading_order_no="183" segment_no="7" tag_type="table">3.4</text>
<text top="272" left="495" width="10" height="7" font="font5" id="p8_t185" reading_order_no="184" segment_no="7" tag_type="table">3.4</text>
<text top="272" left="522" width="14" height="7" font="font5" id="p8_t186" reading_order_no="185" segment_no="7" tag_type="table">3.51</text>
<text top="281" left="338" width="22" height="7" font="font5" id="p8_t187" reading_order_no="186" segment_no="7" tag_type="table">Person</text>
<text top="281" left="373" width="30" height="7" font="font5" id="p8_t188" reading_order_no="187" segment_no="7" tag_type="table">mmWave</text>
<text top="281" left="415" width="14" height="7" font="font5" id="p8_t189" reading_order_no="188" segment_no="7" tag_type="table">2.67</text>
<text top="281" left="441" width="14" height="7" font="font5" id="p8_t190" reading_order_no="189" segment_no="7" tag_type="table">2.69</text>
<text top="281" left="467" width="14" height="7" font="font5" id="p8_t191" reading_order_no="190" segment_no="7" tag_type="table">2.67</text>
<text top="281" left="493" width="14" height="7" font="font5" id="p8_t192" reading_order_no="191" segment_no="7" tag_type="table">2.67</text>
<text top="281" left="522" width="14" height="7" font="font5" id="p8_t193" reading_order_no="192" segment_no="7" tag_type="table">2.78</text>
<text top="290" left="342" width="14" height="7" font="font5" id="p8_t194" reading_order_no="193" segment_no="7" tag_type="table">Two</text>
<text top="290" left="376" width="24" height="7" font="font5" id="p8_t195" reading_order_no="194" segment_no="7" tag_type="table">LiDAR</text>
<text top="290" left="415" width="14" height="7" font="font5" id="p8_t196" reading_order_no="195" segment_no="7" tag_type="table">4.15</text>
<text top="290" left="441" width="14" height="7" font="font5" id="p8_t197" reading_order_no="196" segment_no="7" tag_type="table">4.48</text>
<text top="290" left="467" width="14" height="7" font="font5" id="p8_t198" reading_order_no="197" segment_no="7" tag_type="table">4.47</text>
<text top="290" left="493" width="14" height="7" font="font5" id="p8_t199" reading_order_no="198" segment_no="7" tag_type="table">4.40</text>
<text top="290" left="522" width="14" height="7" font="font5" id="p8_t200" reading_order_no="199" segment_no="7" tag_type="table">4.55</text>
<text top="299" left="336" width="25" height="7" font="font5" id="p8_t201" reading_order_no="200" segment_no="7" tag_type="table">Persons</text>
<text top="299" left="373" width="30" height="7" font="font5" id="p8_t202" reading_order_no="201" segment_no="7" tag_type="table">mmWave</text>
<text top="299" left="415" width="14" height="7" font="font5" id="p8_t203" reading_order_no="202" segment_no="7" tag_type="table">3.58</text>
<text top="299" left="441" width="14" height="7" font="font5" id="p8_t204" reading_order_no="203" segment_no="7" tag_type="table">3.80</text>
<text top="299" left="467" width="14" height="7" font="font5" id="p8_t205" reading_order_no="204" segment_no="7" tag_type="table">3.81</text>
<text top="299" left="493" width="14" height="7" font="font5" id="p8_t206" reading_order_no="205" segment_no="7" tag_type="table">3.89</text>
<text top="299" left="522" width="14" height="7" font="font5" id="p8_t207" reading_order_no="206" segment_no="7" tag_type="table">4.10</text>
<text top="308" left="339" width="19" height="7" font="font5" id="p8_t208" reading_order_no="207" segment_no="7" tag_type="table">Three</text>
<text top="308" left="376" width="24" height="7" font="font5" id="p8_t209" reading_order_no="208" segment_no="7" tag_type="table">LiDAR</text>
<text top="308" left="415" width="14" height="7" font="font5" id="p8_t210" reading_order_no="209" segment_no="7" tag_type="table">5.22</text>
<text top="308" left="441" width="14" height="7" font="font5" id="p8_t211" reading_order_no="210" segment_no="7" tag_type="table">5.49</text>
<text top="308" left="467" width="14" height="7" font="font5" id="p8_t212" reading_order_no="211" segment_no="7" tag_type="table">5.51</text>
<text top="308" left="493" width="14" height="7" font="font5" id="p8_t213" reading_order_no="212" segment_no="7" tag_type="table">5.55</text>
<text top="308" left="522" width="14" height="7" font="font5" id="p8_t214" reading_order_no="213" segment_no="7" tag_type="table">6.10</text>
<text top="317" left="336" width="25" height="7" font="font5" id="p8_t215" reading_order_no="214" segment_no="7" tag_type="table">Persons</text>
<text top="317" left="373" width="30" height="7" font="font5" id="p8_t216" reading_order_no="215" segment_no="7" tag_type="table">mmWave</text>
<text top="317" left="415" width="14" height="7" font="font5" id="p8_t217" reading_order_no="216" segment_no="7" tag_type="table">4.39</text>
<text top="317" left="441" width="14" height="7" font="font5" id="p8_t218" reading_order_no="217" segment_no="7" tag_type="table">4.65</text>
<text top="317" left="467" width="14" height="7" font="font5" id="p8_t219" reading_order_no="218" segment_no="7" tag_type="table">4.68</text>
<text top="317" left="493" width="14" height="7" font="font5" id="p8_t220" reading_order_no="219" segment_no="7" tag_type="table">4.60</text>
<text top="317" left="522" width="14" height="7" font="font5" id="p8_t221" reading_order_no="220" segment_no="7" tag_type="table">4.75</text>
<text top="327" left="337" width="24" height="7" font="font5" id="p8_t222" reading_order_no="221" segment_no="7" tag_type="table">Overall</text>
<text top="327" left="376" width="24" height="7" font="font5" id="p8_t223" reading_order_no="222" segment_no="7" tag_type="table">LiDAR</text>
<text top="327" left="415" width="14" height="7" font="font5" id="p8_t224" reading_order_no="223" segment_no="7" tag_type="table">3.95</text>
<text top="327" left="441" width="14" height="7" font="font5" id="p8_t225" reading_order_no="224" segment_no="7" tag_type="table">4.21</text>
<text top="327" left="467" width="14" height="7" font="font5" id="p8_t226" reading_order_no="225" segment_no="7" tag_type="table">4.21</text>
<text top="327" left="493" width="14" height="7" font="font5" id="p8_t227" reading_order_no="226" segment_no="7" tag_type="table">4.33</text>
<text top="327" left="522" width="14" height="7" font="font5" id="p8_t228" reading_order_no="227" segment_no="7" tag_type="table">4.38</text>
<text top="336" left="339" width="19" height="7" font="font5" id="p8_t229" reading_order_no="228" segment_no="7" tag_type="table">Delay</text>
<text top="336" left="373" width="30" height="7" font="font5" id="p8_t230" reading_order_no="229" segment_no="7" tag_type="table">mmWave</text>
<text top="336" left="415" width="14" height="7" font="font5" id="p8_t231" reading_order_no="230" segment_no="7" tag_type="table">3.51</text>
<text top="336" left="441" width="14" height="7" font="font5" id="p8_t232" reading_order_no="231" segment_no="7" tag_type="table">3.79</text>
<text top="336" left="467" width="14" height="7" font="font5" id="p8_t233" reading_order_no="232" segment_no="7" tag_type="table">3.80</text>
<text top="336" left="493" width="14" height="7" font="font5" id="p8_t234" reading_order_no="233" segment_no="7" tag_type="table">3.84</text>
<text top="336" left="522" width="14" height="7" font="font5" id="p8_t235" reading_order_no="234" segment_no="7" tag_type="table">4.01</text>
<text top="374" left="390" width="95" height="9" font="font2" id="p8_t236" reading_order_no="235" segment_no="8" tag_type="title">VI. R ELATED W ORKS</text>
<text top="389" left="322" width="241" height="9" font="font2" id="p8_t237" reading_order_no="236" segment_no="9" tag_type="text">This paper builds on previous works on human activity</text>
<text top="401" left="312" width="251" height="9" font="font2" id="p8_t238" reading_order_no="237" segment_no="9" tag_type="text">recognition using machine learning, deep learning, domain</text>
<text top="413" left="312" width="251" height="9" font="font2" id="p8_t239" reading_order_no="238" segment_no="9" tag_type="text">adaptation, multiple inhabitant tracking and LiDAR PCD</text>
<text top="425" left="312" width="251" height="9" font="font2" id="p8_t240" reading_order_no="239" segment_no="9" tag_type="text">processing techniques. Here we compare and contrast our</text>
<text top="437" left="312" width="224" height="9" font="font2" id="p8_t241" reading_order_no="240" segment_no="9" tag_type="text">contributions with the most relevant existing literature.</text>
<text top="456" left="312" width="213" height="9" font="font7" id="p8_t242" reading_order_no="241" segment_no="10" tag_type="title">A. Multiple Person Tracking using Ambient Sensors</text>
<text top="471" left="322" width="241" height="9" font="font2" id="p8_t243" reading_order_no="242" segment_no="11" tag_type="text">Multiple person tracking has been a popular problem in</text>
<text top="483" left="312" width="251" height="9" font="font2" id="p8_t244" reading_order_no="243" segment_no="11" tag_type="text">computer vision where video streams have been processed</text>
<text top="494" left="312" width="252" height="9" font="font2" id="p8_t245" reading_order_no="244" segment_no="11" tag_type="text">to track by detection of human using supervised [20], [21],</text>
<text top="506" left="312" width="253" height="9" font="font2" id="p8_t246" reading_order_no="245" segment_no="11" tag_type="text">[22], [35], [48], [49], [50] or unsupervised methods [23], [24].</text>
<text top="518" left="312" width="251" height="9" font="font2" id="p8_t247" reading_order_no="246" segment_no="11" tag_type="text">However, tracking by detection is not applicable in case of</text>
<text top="530" left="312" width="251" height="9" font="font2" id="p8_t248" reading_order_no="247" segment_no="11" tag_type="text">LiDAR PCD where different body parts are not clearly visible</text>
<text top="542" left="312" width="251" height="9" font="font2" id="p8_t249" reading_order_no="248" segment_no="11" tag_type="text">by the light detection and ranging time of flight sensor like</text>
<text top="554" left="312" width="251" height="9" font="font2" id="p8_t250" reading_order_no="249" segment_no="11" tag_type="text">LiDAR. On the other hand, multiple densely implanted ambient</text>
<text top="566" left="312" width="251" height="9" font="font2" id="p8_t251" reading_order_no="250" segment_no="11" tag_type="text">sensors assisted multiple person tracking has also been proposed</text>
<text top="578" left="312" width="251" height="9" font="font2" id="p8_t252" reading_order_no="251" segment_no="11" tag_type="text">by many researchers [6], [26] who proposed probabilistic path</text>
<text top="590" left="312" width="251" height="9" font="font2" id="p8_t253" reading_order_no="252" segment_no="11" tag_type="text">tracking of each sensor node firings using a pretrained model</text>
<text top="602" left="312" width="251" height="9" font="font2" id="p8_t254" reading_order_no="253" segment_no="11" tag_type="text">for supervised learning [6] or unknown number of inhabitants</text>
<text top="614" left="312" width="251" height="9" font="font2" id="p8_t255" reading_order_no="254" segment_no="11" tag_type="text">for unsupervised person tracking [26]. On the other hand, using</text>
<text top="626" left="312" width="251" height="9" font="font2" id="p8_t256" reading_order_no="255" segment_no="11" tag_type="text">LiDAR or LiDAR like RF technologies (such as millimeter</text>
<text top="638" left="312" width="252" height="9" font="font2" id="p8_t257" reading_order_no="256" segment_no="11" tag_type="text">wave, RF sensor) for multiple person tracking is relatively</text>
<text top="650" left="312" width="251" height="9" font="font2" id="p8_t258" reading_order_no="257" segment_no="11" tag_type="text">new area of research [27], [28], [5], [1]. Most of the PCD</text>
<text top="662" left="312" width="251" height="9" font="font2" id="p8_t259" reading_order_no="258" segment_no="11" tag_type="text">technologies (sensors that create PCD of object) utilize sensor</text>
<text top="674" left="312" width="251" height="9" font="font2" id="p8_t260" reading_order_no="259" segment_no="11" tag_type="text">generated high dimensional points-clouds in certain pipeline</text>
<text top="686" left="312" width="251" height="9" font="font2" id="p8_t261" reading_order_no="260" segment_no="11" tag_type="text">consists of PCD processing, representation, clustering and</text>
<text top="698" left="312" width="251" height="9" font="font2" id="p8_t262" reading_order_no="261" segment_no="11" tag_type="text">tracking. [27] propsoed a CNN network based people’s leg</text>
<text top="710" left="312" width="251" height="9" font="font2" id="p8_t263" reading_order_no="262" segment_no="11" tag_type="text">identification and tracking the leg motion using Kalman filter</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
<text top="52" left="49" width="251" height="9" font="font2" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="text">method, which requires the total view of human body work</text>
<text top="64" left="49" width="251" height="9" font="font2" id="p9_t2" reading_order_no="1" segment_no="0" tag_type="text">efficiently. [28] proposed to use Kalman filter and LSTM</text>
<text top="76" left="49" width="252" height="9" font="font2" id="p9_t3" reading_order_no="2" segment_no="0" tag_type="text">(Long Short Term Memory) deep learning model to track</text>
<text top="88" left="49" width="251" height="9" font="font2" id="p9_t4" reading_order_no="3" segment_no="0" tag_type="text">multiple inhabitants in indoor scenario for Robots using 2D</text>
<text top="100" left="49" width="251" height="9" font="font2" id="p9_t5" reading_order_no="4" segment_no="0" tag_type="text">LiDAR, which fails in presence of furniture as well as in</text>
<text top="112" left="49" width="251" height="9" font="font2" id="p9_t6" reading_order_no="5" segment_no="0" tag_type="text">crossover ambiguity. [5] proposed multiple persons tracking</text>
<text top="124" left="49" width="252" height="9" font="font2" id="p9_t7" reading_order_no="6" segment_no="0" tag_type="text">using Kalman Filter and Gait Pattern to track pedestrians,</text>
<text top="136" left="49" width="251" height="9" font="font2" id="p9_t8" reading_order_no="7" segment_no="0" tag_type="text">however, it fails to address crossover ambiguity which is</text>
<text top="148" left="49" width="251" height="9" font="font2" id="p9_t9" reading_order_no="8" segment_no="0" tag_type="text">present in multiple inhabitant smart homes. [1] presented People</text>
<text top="160" left="49" width="251" height="9" font="font2" id="p9_t10" reading_order_no="9" segment_no="0" tag_type="text">Tracker package, aka PeTra, which uses a convolutional neural</text>
<text top="172" left="49" width="251" height="9" font="font2" id="p9_t11" reading_order_no="10" segment_no="0" tag_type="text">network to identify person legs in complex environments and</text>
<text top="184" left="49" width="251" height="9" font="font2" id="p9_t12" reading_order_no="11" segment_no="0" tag_type="text">develop a correlation technique to estimate temporal location</text>
<text top="196" left="49" width="251" height="9" font="font2" id="p9_t13" reading_order_no="12" segment_no="0" tag_type="text">of people using a Kalman filter, but, this method also fails in</text>
<text top="208" left="49" width="130" height="9" font="font2" id="p9_t14" reading_order_no="13" segment_no="0" tag_type="text">addressing crossover ambiguity.</text>
<text top="228" left="49" width="194" height="9" font="font7" id="p9_t15" reading_order_no="14" segment_no="4" tag_type="title">B. Domain Adaptation for Activity Recognition</text>
<text top="243" left="59" width="241" height="9" font="font2" id="p9_t16" reading_order_no="15" segment_no="5" tag_type="text">Among all of the domain adaptation techniques in activity</text>
<text top="255" left="49" width="251" height="9" font="font2" id="p9_t17" reading_order_no="16" segment_no="5" tag_type="text">recognition in computer vision, the most successful one is is</text>
<text top="267" left="49" width="251" height="9" font="font2" id="p9_t18" reading_order_no="17" segment_no="5" tag_type="text">the problem of cross-viewpoint (or viewpoint-invariant) action</text>
<text top="279" left="49" width="251" height="9" font="font2" id="p9_t19" reading_order_no="18" segment_no="5" tag_type="text">recognition [30], [31], [32], [36], [37]. These works focus on</text>
<text top="291" left="49" width="251" height="9" font="font2" id="p9_t20" reading_order_no="19" segment_no="5" tag_type="text">adapting to the geometric transformations of a camera but</text>
<text top="303" left="49" width="251" height="9" font="font2" id="p9_t21" reading_order_no="20" segment_no="5" tag_type="text">do little to combat other shifts, like changes in environment</text>
<text top="315" left="49" width="251" height="9" font="font2" id="p9_t22" reading_order_no="21" segment_no="5" tag_type="text">such as indoor or outdoor. Works utilise supervisory signals</text>
<text top="327" left="49" width="251" height="9" font="font2" id="p9_t23" reading_order_no="22" segment_no="5" tag_type="text">such as skeleton or pose [32] and corresponding frames from</text>
<text top="339" left="49" width="251" height="9" font="font2" id="p9_t24" reading_order_no="23" segment_no="5" tag_type="text">multiple viewpoints [30], [37]. Recent works have used GRLs</text>
<text top="351" left="49" width="251" height="9" font="font2" id="p9_t25" reading_order_no="24" segment_no="5" tag_type="text">to create a view-invariant representation [31]. Though several</text>
<text top="363" left="49" width="251" height="9" font="font2" id="p9_t26" reading_order_no="25" segment_no="5" tag_type="text">modalities (RGB, flow and depth) have been investigated, these</text>
<text top="375" left="49" width="253" height="9" font="font2" id="p9_t27" reading_order_no="26" segment_no="5" tag_type="text">were aligned and evaluated independently. On the other hand,</text>
<text top="387" left="49" width="252" height="9" font="font2" id="p9_t28" reading_order_no="27" segment_no="5" tag_type="text">before deep-learning, heterogenous domain adaptation (HDA)</text>
<text top="399" left="49" width="251" height="9" font="font2" id="p9_t29" reading_order_no="28" segment_no="5" tag_type="text">for action recognition used shallow models to align source</text>
<text top="411" left="49" width="253" height="9" font="font2" id="p9_t30" reading_order_no="29" segment_no="5" tag_type="text">and target distributions of handcrafted features [38], [39], [40].</text>
<text top="423" left="49" width="251" height="9" font="font2" id="p9_t31" reading_order_no="30" segment_no="5" tag_type="text">Three recent works attempted deep HDA [41], [42], [43]. These</text>
<text top="435" left="49" width="251" height="9" font="font2" id="p9_t32" reading_order_no="31" segment_no="5" tag_type="text">apply GRL adversarial training to C3D [44], TRN [45] or both</text>
<text top="447" left="49" width="251" height="9" font="font2" id="p9_t33" reading_order_no="32" segment_no="5" tag_type="text">[43] architectures. Jamal et al.’s approach [42] outperforms</text>
<text top="459" left="49" width="251" height="9" font="font2" id="p9_t34" reading_order_no="33" segment_no="5" tag_type="text">shallow methods that use subspace alignment. Chen et al. [41]</text>
<text top="471" left="49" width="251" height="9" font="font2" id="p9_t35" reading_order_no="34" segment_no="5" tag_type="text">show that attending to the temporal dynamics of videos can</text>
<text top="483" left="49" width="251" height="9" font="font2" id="p9_t36" reading_order_no="35" segment_no="5" tag_type="text">improve alignment. Pan et al. [43] use a crossdomain attention</text>
<text top="494" left="49" width="251" height="9" font="font2" id="p9_t37" reading_order_no="36" segment_no="5" tag_type="text">module, to avoid uninformative frames. Two of these works</text>
<text top="506" left="49" width="251" height="9" font="font2" id="p9_t38" reading_order_no="37" segment_no="5" tag_type="text">use RGB only [41], [42] while [43] reports results on RGB</text>
<text top="518" left="49" width="251" height="9" font="font2" id="p9_t39" reading_order_no="38" segment_no="5" tag_type="text">and Flow, however, modalities are aligned independently and</text>
<text top="530" left="49" width="251" height="9" font="font2" id="p9_t40" reading_order_no="39" segment_no="5" tag_type="text">only fused during inference. The approaches [41], [42], [43]</text>
<text top="542" left="49" width="253" height="9" font="font2" id="p9_t41" reading_order_no="40" segment_no="5" tag_type="text">are evaluated on 5-7 pairs of domains from subsets of coarse-</text>
<text top="554" left="49" width="251" height="9" font="font2" id="p9_t42" reading_order_no="41" segment_no="5" tag_type="text">grained action recognition and gesture datasets, for example</text>
<text top="566" left="49" width="251" height="9" font="font2" id="p9_t43" reading_order_no="42" segment_no="5" tag_type="text">aligning UCF [46] to Olympics [33]. We evaluate on 6 pairs of</text>
<text top="578" left="49" width="251" height="9" font="font2" id="p9_t44" reading_order_no="43" segment_no="5" tag_type="text">domains. Compared to [42], we use 3.8x more training and 2x</text>
<text top="590" left="49" width="253" height="9" font="font2" id="p9_t45" reading_order_no="44" segment_no="5" tag_type="text">more testing videos. The EPIC-Kitchens [47] dataset for fine-</text>
<text top="602" left="49" width="251" height="9" font="font2" id="p9_t46" reading_order_no="45" segment_no="5" tag_type="text">grained action recognition released two distinct test sets—one</text>
<text top="614" left="49" width="251" height="9" font="font2" id="p9_t47" reading_order_no="46" segment_no="5" tag_type="text">with seen and another with unseen/novel kitchens. In the 2019</text>
<text top="626" left="49" width="251" height="9" font="font2" id="p9_t48" reading_order_no="47" segment_no="5" tag_type="text">challenges report, all participating entries exhibit a drop in</text>
<text top="638" left="49" width="251" height="9" font="font2" id="p9_t49" reading_order_no="48" segment_no="5" tag_type="text">action recognition accuracy of 12-20% when testing their</text>
<text top="650" left="49" width="253" height="9" font="font2" id="p9_t50" reading_order_no="49" segment_no="5" tag_type="text">models on novel environments compared to seen environments.</text>
<text top="670" left="49" width="146" height="9" font="font7" id="p9_t51" reading_order_no="50" segment_no="11" tag_type="title">C. PCD based Activity Recognition</text>
<text top="686" left="59" width="241" height="9" font="font2" id="p9_t52" reading_order_no="51" segment_no="12" tag_type="text">LiDAR and mmWave based activity recognition has been</text>
<text top="698" left="49" width="253" height="9" font="font2" id="p9_t53" reading_order_no="52" segment_no="12" tag_type="text">explored by many researchers [5], [51]. Benedek et. al.</text>
<text top="710" left="49" width="251" height="9" font="font2" id="p9_t54" reading_order_no="53" segment_no="12" tag_type="text">proposed to use DBSCAN clustering algorithm and Kalman</text>
<text top="52" left="312" width="251" height="9" font="font2" id="p9_t55" reading_order_no="54" segment_no="1" tag_type="text">Filtering method to identify number of people and track their</text>
<text top="64" left="312" width="251" height="9" font="font2" id="p9_t56" reading_order_no="55" segment_no="1" tag_type="text">movements [5]. Apart from that, [5] also proposed to utilize</text>
<text top="76" left="312" width="253" height="9" font="font2" id="p9_t57" reading_order_no="56" segment_no="1" tag_type="text">CNN based supervised method to detect multiple persons’</text>
<text top="88" left="312" width="251" height="9" font="font2" id="p9_t58" reading_order_no="57" segment_no="1" tag_type="text">5 different activities. As, this is one of the rare investigated</text>
<text top="100" left="312" width="251" height="9" font="font2" id="p9_t59" reading_order_no="58" segment_no="1" tag_type="text">activity recognition framework using LiDAR, it does have many</text>
<text top="112" left="312" width="251" height="9" font="font2" id="p9_t60" reading_order_no="59" segment_no="1" tag_type="text">limitations that includes extremely poor accuracy of detecting</text>
<text top="124" left="312" width="253" height="9" font="font2" id="p9_t61" reading_order_no="60" segment_no="1" tag_type="text">activities (75%) as well as person tracking (89%). Wenjun et.</text>
<text top="136" left="312" width="251" height="9" font="font2" id="p9_t62" reading_order_no="61" segment_no="1" tag_type="text">al. proposed adversarial domain adaptation technique to detect</text>
<text top="148" left="312" width="251" height="9" font="font2" id="p9_t63" reading_order_no="62" segment_no="1" tag_type="text">HAR in presence of environmental diversity but did not cover</text>
<text top="160" left="312" width="191" height="9" font="font2" id="p9_t64" reading_order_no="63" segment_no="1" tag_type="text">the multiple inhabitant tracking problem [51] .</text>
<text top="178" left="397" width="81" height="9" font="font2" id="p9_t65" reading_order_no="64" segment_no="2" tag_type="title">VII. C ONCLUSION</text>
<text top="193" left="322" width="241" height="9" font="font7" id="p9_t66" reading_order_no="65" segment_no="3" tag_type="text">PALMAR is the first of its kind adaptive activity recognition</text>
<text top="205" left="312" width="253" height="9" font="font2" id="p9_t67" reading_order_no="66" segment_no="3" tag_type="text">technique in multiple inhabitant point-cloud image generat-</text>
<text top="217" left="312" width="251" height="9" font="font2" id="p9_t68" reading_order_no="67" segment_no="3" tag_type="text">ing technology-assisted environment with best accuracy ever</text>
<text top="229" left="312" width="253" height="9" font="font2" id="p9_t69" reading_order_no="68" segment_no="3" tag_type="text">achieved in indoor environment and outdoor environment.</text>
<text top="241" left="312" width="252" height="9" font="font2" id="p9_t70" reading_order_no="69" segment_no="3" tag_type="text">Although, we establish the state-of-art of PCD-based activity</text>
<text top="253" left="312" width="251" height="9" font="font2" id="p9_t71" reading_order_no="70" segment_no="3" tag_type="text">recognition, we have certain limitations. The data collection</text>
<text top="264" left="312" width="251" height="9" font="font2" id="p9_t72" reading_order_no="71" segment_no="3" tag_type="text">part was one of the most challenging part of this project due to</text>
<text top="276" left="312" width="253" height="9" font="font2" id="p9_t73" reading_order_no="72" segment_no="3" tag_type="text">the on-going COVID-19 pandemic related campus lock-down.</text>
<text top="288" left="312" width="252" height="9" font="font2" id="p9_t74" reading_order_no="73" segment_no="3" tag_type="text">To accommodate appropriate data collection, we recruited only</text>
<text top="300" left="312" width="251" height="9" font="font2" id="p9_t75" reading_order_no="74" segment_no="3" tag_type="text">our lab members by shipping the PALMAR system to their</text>
<text top="312" left="312" width="251" height="9" font="font2" id="p9_t76" reading_order_no="75" segment_no="3" tag_type="text">house. For multi-person activity data collection, participants</text>
<text top="324" left="312" width="251" height="9" font="font2" id="p9_t77" reading_order_no="76" segment_no="3" tag_type="text">were requested to engage their family members in home</text>
<text top="336" left="312" width="253" height="9" font="font2" id="p9_t78" reading_order_no="77" segment_no="3" tag_type="text">environment without exposing themselves to outdoor people.</text>
<text top="348" left="312" width="251" height="9" font="font2" id="p9_t79" reading_order_no="78" segment_no="3" tag_type="text">Due to these limitations, we were able to engage only 6 unique</text>
<text top="360" left="312" width="251" height="9" font="font2" id="p9_t80" reading_order_no="79" segment_no="3" tag_type="text">users with a maximum occupancy of 3 persons. However, we</text>
<text top="372" left="312" width="251" height="9" font="font2" id="p9_t81" reading_order_no="80" segment_no="3" tag_type="text">also could not collect significant amount of outdoor activity</text>
<text top="384" left="312" width="253" height="9" font="font2" id="p9_t82" reading_order_no="81" segment_no="3" tag_type="text">recognition data for multiple persons due to the on-going lock-</text>
<text top="396" left="312" width="251" height="9" font="font2" id="p9_t83" reading_order_no="82" segment_no="3" tag_type="text">down in the campus. In this project, we collected the gait</text>
<text top="408" left="312" width="251" height="9" font="font2" id="p9_t84" reading_order_no="83" segment_no="3" tag_type="text">information and breathing rate (participants worn respiratory</text>
<text top="420" left="312" width="251" height="9" font="font2" id="p9_t85" reading_order_no="84" segment_no="3" tag_type="text">belt) of each participants. However, due to the limited number</text>
<text top="432" left="312" width="251" height="9" font="font2" id="p9_t86" reading_order_no="85" segment_no="3" tag_type="text">of users, we could not propose improved models for gait</text>
<text top="444" left="312" width="251" height="9" font="font2" id="p9_t87" reading_order_no="86" segment_no="3" tag_type="text">pattern based person identification/reidentification which we</text>
<text top="456" left="312" width="251" height="9" font="font2" id="p9_t88" reading_order_no="87" segment_no="3" tag_type="text">would significantly improve the accuracy of person tracking</text>
<text top="468" left="312" width="251" height="9" font="font2" id="p9_t89" reading_order_no="88" segment_no="3" tag_type="text">in case of multiple persons. In future, we aim to develop a</text>
<text top="480" left="312" width="251" height="9" font="font2" id="p9_t90" reading_order_no="89" segment_no="3" tag_type="text">breathing rate detection framework along with an experimental</text>
<text top="492" left="312" width="252" height="9" font="font2" id="p9_t91" reading_order_no="90" segment_no="3" tag_type="text">evaluation of PALMAR framework’s time -complexity and real-</text>
<text top="504" left="312" width="251" height="9" font="font2" id="p9_t92" reading_order_no="91" segment_no="3" tag_type="text">time system performance. In terms of application, we aim</text>
<text top="516" left="312" width="251" height="9" font="font2" id="p9_t93" reading_order_no="92" segment_no="3" tag_type="text">to address multiple scenarios, such as older adults, dementia</text>
<text top="528" left="312" width="251" height="9" font="font2" id="p9_t94" reading_order_no="93" segment_no="3" tag_type="text">population and skilled-nursing facility inhabitants monitoring</text>
<text top="539" left="312" width="253" height="9" font="font2" id="p9_t95" reading_order_no="94" segment_no="3" tag_type="text">technique to accommodate more real-time solution for well-</text>
<text top="551" left="312" width="116" height="9" font="font2" id="p9_t96" reading_order_no="95" segment_no="3" tag_type="text">being of in need population.</text>
<text top="569" left="392" width="92" height="9" font="font2" id="p9_t97" reading_order_no="96" segment_no="6" tag_type="title">A CKNOWLEDGEMENT</text>
<text top="584" left="322" width="241" height="9" font="font2" id="p9_t98" reading_order_no="97" segment_no="7" tag_type="text">We thank Fernando Mazzoni and Mohammad Haerinia for</text>
<text top="596" left="312" width="106" height="9" font="font2" id="p9_t99" reading_order_no="98" segment_no="7" tag_type="text">helping in data collection.</text>
<text top="614" left="410" width="56" height="9" font="font2" id="p9_t100" reading_order_no="99" segment_no="8" tag_type="title">R EFERENCES</text>
<text top="630" left="312" width="251" height="7" font="font5" id="p9_t101" reading_order_no="100" segment_no="9" tag_type="text">[1] P. Zhao et al., "mID: Tracking and Identifying People with Millimeter Wave</text>
<text top="639" left="326" width="237" height="7" font="font5" id="p9_t102" reading_order_no="101" segment_no="9" tag_type="text">Radar," 2019 15th International Conference on Distributed Computing in</text>
<text top="648" left="326" width="238" height="7" font="font5" id="p9_t103" reading_order_no="102" segment_no="9" tag_type="text">Sensor Systems (DCOSS), Santorini Island, Greece, 2019, pp. 33-40, doi:</text>
<text top="657" left="326" width="96" height="7" font="font5" id="p9_t104" reading_order_no="103" segment_no="9" tag_type="text">10.1109/DCOSS.2019.00028.</text>
<text top="666" left="312" width="251" height="7" font="font5" id="p9_t105" reading_order_no="104" segment_no="10" tag_type="text">[2] F.R. Lopez-Serrano et. al., Site and weather effects in allometries: A</text>
<text top="675" left="326" width="237" height="7" font="font5" id="p9_t106" reading_order_no="105" segment_no="10" tag_type="text">simple approach to climate change effect on pines, Forest Ecology and</text>
<text top="684" left="326" width="198" height="7" font="font5" id="p9_t107" reading_order_no="106" segment_no="10" tag_type="text">Management, Volume 215, Issues 1–3, 2005, Pages 251-270</text>
<text top="693" left="312" width="251" height="7" font="font5" id="p9_t108" reading_order_no="107" segment_no="13" tag_type="text">[3] T. van Erven and P. Harremos, "Rényi Divergence and Kullback-Leibler</text>
<text top="702" left="326" width="238" height="7" font="font5" id="p9_t109" reading_order_no="108" segment_no="13" tag_type="text">Divergence," in IEEE Transactions on Information Theory, vol. 60, no. 7,</text>
<text top="711" left="326" width="84" height="7" font="font5" id="p9_t110" reading_order_no="109" segment_no="13" tag_type="text">pp. 3797-3820, July 2014</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="792" width="612">
<text top="54" left="49" width="251" height="7" font="font5" id="p10_t1" reading_order_no="0" segment_no="0" tag_type="text">[4] Fumiki Hosoi, Yohei Nakai, Kenji Omasa, 3-D voxel-based solid modeling</text>
<text top="62" left="63" width="237" height="7" font="font5" id="p10_t2" reading_order_no="1" segment_no="0" tag_type="text">of a broad-leaved tree for accurate volume estimation using portable</text>
<text top="71" left="63" width="238" height="7" font="font5" id="p10_t3" reading_order_no="2" segment_no="0" tag_type="text">scanning lidar, ISPRS Journal of Photogrammetry and Remote Sensing,</text>
<text top="80" left="63" width="160" height="7" font="font5" id="p10_t4" reading_order_no="3" segment_no="0" tag_type="text">Volume 82, 2013, Pages 41-48, ISSN 0924-2716</text>
<text top="89" left="49" width="251" height="7" font="font5" id="p10_t5" reading_order_no="4" segment_no="2" tag_type="text">[5] C. Benedek, B. Galai, B. Nagy and Z. Janko, "Lidar-Based Gait</text>
<text top="98" left="63" width="237" height="7" font="font5" id="p10_t6" reading_order_no="5" segment_no="2" tag_type="text">Analysis and Activity Recognition in a 4D Surveillance System," in</text>
<text top="107" left="63" width="238" height="7" font="font5" id="p10_t7" reading_order_no="6" segment_no="2" tag_type="text">IEEE Transactions on Circuits and Systems for Video Technology, vol.</text>
<text top="116" left="63" width="110" height="7" font="font5" id="p10_t8" reading_order_no="7" segment_no="2" tag_type="text">28, no. 1, pp. 101-113, Jan. 2018</text>
<text top="125" left="49" width="252" height="7" font="font5" id="p10_t9" reading_order_no="8" segment_no="5" tag_type="text">[6] D. De, W. Song, M. Xu, C. Wang, D. Cook and X. Huo, "FindingHuMo:</text>
<text top="134" left="63" width="238" height="7" font="font5" id="p10_t10" reading_order_no="9" segment_no="5" tag_type="text">Real-Time Tracking of Motion Trajectories from Anonymous Binary Sens-</text>
<text top="143" left="63" width="237" height="7" font="font5" id="p10_t11" reading_order_no="10" segment_no="5" tag_type="text">ing in Smart Environments," 2012 IEEE 32nd International Conference</text>
<text top="152" left="63" width="206" height="7" font="font5" id="p10_t12" reading_order_no="11" segment_no="5" tag_type="text">on Distributed Computing Systems, Macau, 2012, pp. 163-172</text>
<text top="161" left="49" width="251" height="7" font="font5" id="p10_t13" reading_order_no="12" segment_no="7" tag_type="text">[7] L. R. Rabinder. A tutorial on hidden Markov models and selected</text>
<text top="170" left="63" width="238" height="7" font="font5" id="p10_t14" reading_order_no="13" segment_no="7" tag_type="text">applications in speech recognition. Proceedings of the IEEE, 77(2):257–</text>
<text top="179" left="63" width="51" height="7" font="font5" id="p10_t15" reading_order_no="14" segment_no="7" tag_type="text">286, Feb. 1989.</text>
<text top="189" left="49" width="251" height="7" font="font5" id="p10_t16" reading_order_no="15" segment_no="9" tag_type="text">[8] S. M. Thede and M. P. Harper. A second-order Hidden Markov Model for</text>
<text top="197" left="63" width="237" height="7" font="font5" id="p10_t17" reading_order_no="16" segment_no="9" tag_type="text">part-of-speech tagging. In Proceedings of the 37th annual meeting of the</text>
<text top="206" left="63" width="237" height="7" font="font5" id="p10_t18" reading_order_no="17" segment_no="9" tag_type="text">Association for Computational Linguistics on Computational Linguistics</text>
<text top="215" left="63" width="132" height="7" font="font5" id="p10_t19" reading_order_no="18" segment_no="9" tag_type="text">(ACL’99), Stroudsburg, PA, USA, 1999.</text>
<text top="224" left="49" width="251" height="7" font="font5" id="p10_t20" reading_order_no="19" segment_no="11" tag_type="text">[9] S. Patel. A lower-complexity Viterbi algorithm. In International Conference</text>
<text top="233" left="63" width="238" height="7" font="font5" id="p10_t21" reading_order_no="20" segment_no="11" tag_type="text">on Acoustics, Speech, and Signal Processing (ICASSP’95), Detroit, USA,</text>
<text top="242" left="63" width="16" height="7" font="font5" id="p10_t22" reading_order_no="21" segment_no="11" tag_type="text">1995</text>
<text top="252" left="49" width="251" height="7" font="font5" id="p10_t23" reading_order_no="22" segment_no="13" tag_type="text">[10] Zhang T, Ramakrishnan R and Livny M (1997), "BIRCH: A new data</text>
<text top="260" left="63" width="237" height="7" font="font5" id="p10_t24" reading_order_no="23" segment_no="13" tag_type="text">clustering algorithm and its applications", Data Mining and Knowledge</text>
<text top="269" left="63" width="112" height="7" font="font5" id="p10_t25" reading_order_no="24" segment_no="13" tag_type="text">Discovery. Vol. 1(2), pp. 141-182.</text>
<text top="279" left="49" width="251" height="7" font="font5" id="p10_t26" reading_order_no="25" segment_no="15" tag_type="text">[11] S. Schneider, A. S. Ecker, J. H. Macke, and M. Bethge Salad: A Toolbox</text>
<text top="288" left="63" width="237" height="7" font="font5" id="p10_t27" reading_order_no="26" segment_no="15" tag_type="text">for Semi-supervised Adaptive Learning Across Domains NeurIPS Machine</text>
<text top="296" left="63" width="161" height="7" font="font5" id="p10_t28" reading_order_no="27" segment_no="15" tag_type="text">Learning Open Source Software Workshop, 2018</text>
<text top="306" left="49" width="251" height="7" font="font5" id="p10_t29" reading_order_no="28" segment_no="17" tag_type="text">[12] Rui Shu, Hung H. Bui, Hirokazu Narui, Stefano Ermon: A DIRT-T</text>
<text top="315" left="63" width="223" height="7" font="font5" id="p10_t30" reading_order_no="29" segment_no="17" tag_type="text">Approach to Unsupervised Domain Adaptation. ICLR (Poster) 2018</text>
<text top="324" left="49" width="251" height="7" font="font5" id="p10_t31" reading_order_no="30" segment_no="19" tag_type="text">[13] Rui Shu, Hung H. Bui, Hirokazu Narui, Stefano Ermon: A DIRT-T</text>
<text top="333" left="63" width="237" height="7" font="font5" id="p10_t32" reading_order_no="31" segment_no="19" tag_type="text">Approach to Unsupervised Domain Adaptation. CoRR abs/1802.08735</text>
<text top="342" left="63" width="42" height="7" font="font5" id="p10_t33" reading_order_no="32" segment_no="19" tag_type="text">(2018) 2017.</text>
<text top="351" left="49" width="251" height="7" font="font5" id="p10_t34" reading_order_no="33" segment_no="21" tag_type="text">[14] E. Rehder et. al., “Head detection and orientation estimation for pedestrian</text>
<text top="360" left="63" width="237" height="7" font="font5" id="p10_t35" reading_order_no="34" segment_no="21" tag_type="text">safety,” in 2014 17th IEEE International Conference on Intelligent</text>
<text top="369" left="63" width="191" height="7" font="font5" id="p10_t36" reading_order_no="35" segment_no="21" tag_type="text">Transportation Systems, ITSC 2014, 2014, pp. 2292–2297</text>
<text top="378" left="49" width="251" height="7" font="font5" id="p10_t37" reading_order_no="36" segment_no="23" tag_type="text">[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo</text>
<text top="387" left="63" width="238" height="7" font="font5" id="p10_t38" reading_order_no="37" segment_no="23" tag_type="text">Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky.</text>
<text top="396" left="63" width="238" height="7" font="font5" id="p10_t39" reading_order_no="38" segment_no="23" tag_type="text">2016. Domain-adversarial training of neural networks. J. Mach. Learn.</text>
<text top="405" left="63" width="128" height="7" font="font5" id="p10_t40" reading_order_no="39" segment_no="23" tag_type="text">Res. 17, 1 (January 2016), 2096–2030.</text>
<text top="414" left="49" width="252" height="7" font="font5" id="p10_t41" reading_order_no="40" segment_no="25" tag_type="text">[16] Philip Häusser, Thomas Frerix, Alexander Mordvintsev, Daniel Cremers:</text>
<text top="423" left="63" width="188" height="7" font="font5" id="p10_t42" reading_order_no="41" segment_no="25" tag_type="text">Associative Domain Adaptation. ICCV 2017: 2784-2792.</text>
<text top="432" left="49" width="251" height="7" font="font5" id="p10_t43" reading_order_no="42" segment_no="27" tag_type="text">[17] Baochen Sun, Kate Saenko: Deep CORAL: Correlation Alignment for</text>
<text top="441" left="63" width="212" height="7" font="font5" id="p10_t44" reading_order_no="43" segment_no="27" tag_type="text">Deep Domain Adaptation. ECCV Workshops (3) 2016: 443-450.</text>
<text top="450" left="49" width="251" height="7" font="font5" id="p10_t45" reading_order_no="44" segment_no="28" tag_type="text">[18] Geoffrey French, Michal Mackiewicz, Mark H. Fisher: Self-ensembling</text>
<text top="459" left="63" width="137" height="7" font="font5" id="p10_t46" reading_order_no="45" segment_no="28" tag_type="text">for visual domain adaptation. ICLR 2018.</text>
<text top="468" left="49" width="252" height="7" font="font5" id="p10_t47" reading_order_no="46" segment_no="30" tag_type="text">[19] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Kate Saenko: Adver-</text>
<text top="477" left="63" width="138" height="7" font="font5" id="p10_t48" reading_order_no="47" segment_no="30" tag_type="text">sarial Dropout Regularization. ICLR 2018</text>
<text top="486" left="49" width="251" height="7" font="font5" id="p10_t49" reading_order_no="48" segment_no="32" tag_type="text">[20] H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, “Globally-optimal greedy</text>
<text top="495" left="63" width="232" height="7" font="font5" id="p10_t50" reading_order_no="49" segment_no="32" tag_type="text">algorithms for tracking a variable number of objects,” in CVPR, 2011.</text>
<text top="504" left="49" width="252" height="7" font="font5" id="p10_t51" reading_order_no="50" segment_no="33" tag_type="text">[21] A. Andriyenko, K. Schindler, and S. Roth, “Discrete-continuous opti-</text>
<text top="513" left="63" width="169" height="7" font="font5" id="p10_t52" reading_order_no="51" segment_no="33" tag_type="text">mization for multi-target tracking,” in CVPR, 2012.</text>
<text top="522" left="49" width="251" height="7" font="font5" id="p10_t53" reading_order_no="52" segment_no="35" tag_type="text">[22] C. Huang, B. Wu, and R. Nevatia, “Robust object tracking by hierarchical</text>
<text top="531" left="63" width="170" height="7" font="font5" id="p10_t54" reading_order_no="53" segment_no="35" tag_type="text">association of detection responses,” in ECCV, 2008.</text>
<text top="540" left="49" width="251" height="7" font="font5" id="p10_t55" reading_order_no="54" segment_no="37" tag_type="text">[23] K. Fragkiadaki, W. Zhang, G. Zhang, and J. Shi, “Two-granularity</text>
<text top="549" left="63" width="237" height="7" font="font5" id="p10_t56" reading_order_no="55" segment_no="37" tag_type="text">tracking: Mediating trajectory and detection graphs for tracking under</text>
<text top="558" left="63" width="93" height="7" font="font5" id="p10_t57" reading_order_no="56" segment_no="37" tag_type="text">occlusions,” in ECCV, 2012.</text>
<text top="567" left="49" width="251" height="7" font="font5" id="p10_t58" reading_order_no="57" segment_no="39" tag_type="text">[24] X. Wang, E. Turetken, F. Fleuret, and P. Fua, “Tracking interacting</text>
<text top="576" left="63" width="207" height="7" font="font5" id="p10_t59" reading_order_no="58" segment_no="39" tag_type="text">objects optimally using integer programming,” in ECCV, 2014.</text>
<text top="585" left="49" width="251" height="7" font="font5" id="p10_t60" reading_order_no="59" segment_no="41" tag_type="text">[25] R. Henschel, Y. Zou, and B. Rosenhahn, “Multiple people tracking using</text>
<text top="594" left="63" width="237" height="7" font="font5" id="p10_t61" reading_order_no="60" segment_no="41" tag_type="text">body and joint detections,” in Proceedings of the IEEE Conference on</text>
<text top="603" left="63" width="227" height="7" font="font5" id="p10_t62" reading_order_no="61" segment_no="41" tag_type="text">Computer Vision and Pattern Recognition Workshops, 2019, pp. 0–0.</text>
<text top="612" left="49" width="252" height="7" font="font5" id="p10_t63" reading_order_no="62" segment_no="43" tag_type="text">[26] Lei Yang, Qiongzheng Lin, Xiangyang Li, Tianci Liu, and Yunhao Liu.</text>
<text top="621" left="63" width="237" height="7" font="font5" id="p10_t64" reading_order_no="63" segment_no="43" tag_type="text">2015. See Through Walls with COTS RFID System! In Proceedings</text>
<text top="630" left="63" width="237" height="7" font="font5" id="p10_t65" reading_order_no="64" segment_no="43" tag_type="text">of the 21st Annual International Conference on Mobile Computing and</text>
<text top="639" left="63" width="237" height="7" font="font5" id="p10_t66" reading_order_no="65" segment_no="43" tag_type="text">Networking (MobiCom ’15). Association for Computing Machinery, New</text>
<text top="648" left="63" width="86" height="7" font="font5" id="p10_t67" reading_order_no="66" segment_no="43" tag_type="text">York, NY, USA, 487–499.</text>
<text top="657" left="49" width="251" height="7" font="font5" id="p10_t68" reading_order_no="67" segment_no="45" tag_type="text">[27] Alvarez-Aparicio et. al., People Detection and Tracking Using LIDAR</text>
<text top="666" left="63" width="100" height="7" font="font5" id="p10_t69" reading_order_no="68" segment_no="45" tag_type="text">Sensors. Robotics 2019, 8, 75.</text>
<text top="675" left="49" width="252" height="7" font="font5" id="p10_t70" reading_order_no="69" segment_no="47" tag_type="text">[28] Guerrero-Higueras, A.M.; Alvarez-Aparicio, C.; Calvo-Olivera, M.C.;</text>
<text top="684" left="63" width="238" height="7" font="font5" id="p10_t71" reading_order_no="70" segment_no="47" tag_type="text">Rodriguez-Lera, F.J.; Fernandez-Llamas, C.; Martín, F.; Matellan, V.</text>
<text top="693" left="63" width="237" height="7" font="font5" id="p10_t72" reading_order_no="71" segment_no="47" tag_type="text">Tracking People in a Mobile Robot From 2D LIDAR Scans Using Full</text>
<text top="702" left="63" width="238" height="7" font="font5" id="p10_t73" reading_order_no="72" segment_no="47" tag_type="text">Convolutional Neural Networks for Security in Cluttered Environments.</text>
<text top="711" left="63" width="105" height="7" font="font5" id="p10_t74" reading_order_no="73" segment_no="47" tag_type="text">Front. Neurorobot. 2019, 12, 85</text>
<text top="54" left="312" width="251" height="7" font="font5" id="p10_t75" reading_order_no="74" segment_no="1" tag_type="text">[29] F. Diederichs, T. Schuttke, and D. Spath, “Driver Intention Algorithm</text>
<text top="62" left="326" width="238" height="7" font="font5" id="p10_t76" reading_order_no="75" segment_no="1" tag_type="text">for Pedestrian Protection and Automated Emergency Braking Systems,”</text>
<text top="71" left="326" width="238" height="7" font="font5" id="p10_t77" reading_order_no="76" segment_no="1" tag_type="text">in IEEE Conference on Intelligent Transportation Systems, Proceedings,</text>
<text top="80" left="326" width="152" height="7" font="font5" id="p10_t78" reading_order_no="77" segment_no="1" tag_type="text">ITSC, 2015, vol. 2015–Octob, pp. 1049–1054.</text>
<text top="89" left="312" width="252" height="7" font="font5" id="p10_t79" reading_order_no="78" segment_no="3" tag_type="text">[30] Yu Kong, Zhengming Ding, Jun Li, and Yun Fu. Deeply learned view-</text>
<text top="98" left="326" width="237" height="7" font="font5" id="p10_t80" reading_order_no="79" segment_no="3" tag_type="text">invariant features for cross-view action recognition. Transactions on Image</text>
<text top="107" left="326" width="79" height="7" font="font5" id="p10_t81" reading_order_no="80" segment_no="3" tag_type="text">Processing, 26(6), 2017.</text>
<text top="116" left="312" width="252" height="7" font="font5" id="p10_t82" reading_order_no="81" segment_no="4" tag_type="text">[31] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan Kankanhalli. Unsu-</text>
<text top="125" left="326" width="237" height="7" font="font5" id="p10_t83" reading_order_no="82" segment_no="4" tag_type="text">pervised learning of view-invariant action representations. In Advances in</text>
<text top="134" left="326" width="185" height="7" font="font5" id="p10_t84" reading_order_no="83" segment_no="4" tag_type="text">Neural Information Processing Systems (Neurips), 2018.</text>
<text top="143" left="312" width="252" height="7" font="font5" id="p10_t85" reading_order_no="84" segment_no="6" tag_type="text">[32] Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skele- ton visual-</text>
<text top="152" left="326" width="238" height="7" font="font5" id="p10_t86" reading_order_no="85" segment_no="6" tag_type="text">ization for view invariant human action recognition. Pattern Recognition,</text>
<text top="161" left="326" width="59" height="7" font="font5" id="p10_t87" reading_order_no="86" segment_no="6" tag_type="text">68:346–362, 2017</text>
<text top="170" left="312" width="251" height="7" font="font5" id="p10_t88" reading_order_no="87" segment_no="8" tag_type="text">[33] Juan Carlos Niebles, Chih-Wei Chen, and Li Fei-Fei. Modeling temporal</text>
<text top="179" left="326" width="237" height="7" font="font5" id="p10_t89" reading_order_no="88" segment_no="8" tag_type="text">structure of decomposable motion segments for activity classification. In</text>
<text top="188" left="326" width="191" height="7" font="font5" id="p10_t90" reading_order_no="89" segment_no="8" tag_type="text">European Conference on Computer Vision (ECCV), 2010.</text>
<text top="197" left="312" width="252" height="7" font="font5" id="p10_t91" reading_order_no="90" segment_no="10" tag_type="text">[34] Mohammad Arif Ul Alam, Md Mahmudur Rahman, Fernando Mazzoni,</text>
<text top="206" left="326" width="238" height="7" font="font5" id="p10_t92" reading_order_no="91" segment_no="10" tag_type="text">Jared Widberg, LAMAR: Lidar based Multi-inhabitant Activity Recog-</text>
<text top="215" left="326" width="237" height="7" font="font5" id="p10_t93" reading_order_no="92" segment_no="10" tag_type="text">nition, 17th EAI International Conference on Mobile and Ubiquitous</text>
<text top="224" left="326" width="222" height="7" font="font5" id="p10_t94" reading_order_no="93" segment_no="10" tag_type="text">Systems: Computing, Networking and Services (Mobiquitous 2020)</text>
<text top="233" left="312" width="251" height="7" font="font5" id="p10_t95" reading_order_no="94" segment_no="12" tag_type="text">[35] Mohammad Arif Ul Alam, Nirmalya Roy, Archan Misra, Tracking and</text>
<text top="242" left="326" width="237" height="7" font="font5" id="p10_t96" reading_order_no="95" segment_no="12" tag_type="text">Behavior Augmented Activity Recognition for Multiple Inhabitants, IEEE</text>
<text top="251" left="326" width="134" height="7" font="font5" id="p10_t97" reading_order_no="96" segment_no="12" tag_type="text">Transactions on Mobile Computing 2019</text>
<text top="260" left="312" width="251" height="7" font="font5" id="p10_t98" reading_order_no="97" segment_no="14" tag_type="text">[36] Hossein Rahmani and Ajmal Mian. Learning a non-linear knowledge</text>
<text top="269" left="326" width="237" height="7" font="font5" id="p10_t99" reading_order_no="98" segment_no="14" tag_type="text">transfer model for cross-view action recognition. In Computer Vision and</text>
<text top="278" left="326" width="114" height="7" font="font5" id="p10_t100" reading_order_no="99" segment_no="14" tag_type="text">Pattern Recognition (CVPR), 2015</text>
<text top="287" left="312" width="252" height="7" font="font5" id="p10_t101" reading_order_no="100" segment_no="16" tag_type="text">[37] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi,</text>
<text top="296" left="326" width="237" height="7" font="font5" id="p10_t102" reading_order_no="101" segment_no="16" tag_type="text">and Karteek Alahari. Actor and observer: Joint modeling of first and</text>
<text top="305" left="326" width="238" height="7" font="font5" id="p10_t103" reading_order_no="102" segment_no="16" tag_type="text">third-person videos. In Computer Vision and Pattern Recognition (CVPR),</text>
<text top="314" left="326" width="18" height="7" font="font5" id="p10_t104" reading_order_no="103" segment_no="16" tag_type="text">2018.</text>
<text top="322" left="312" width="251" height="7" font="font5" id="p10_t105" reading_order_no="104" segment_no="18" tag_type="text">[38] Liangliang Cao, Zicheng Liu, and Thomas S Huang. Crossdataset action</text>
<text top="331" left="326" width="230" height="7" font="font5" id="p10_t106" reading_order_no="105" segment_no="18" tag_type="text">detection. In Computer Vision and Pattern Recognition (CVPR), 2010</text>
<text top="340" left="312" width="251" height="7" font="font5" id="p10_t107" reading_order_no="106" segment_no="20" tag_type="text">[39] N Faraji Davar, Teofilo de Campos, David Windridge, Josef Kittler, and</text>
<text top="349" left="326" width="237" height="7" font="font5" id="p10_t108" reading_order_no="107" segment_no="20" tag_type="text">William Christmas. Domain adaptation in the context of sport video action</text>
<text top="358" left="326" width="238" height="7" font="font5" id="p10_t109" reading_order_no="108" segment_no="20" tag_type="text">recognition. In Domain Adaptation Workshop, in conjunction with NIPS,</text>
<text top="367" left="326" width="16" height="7" font="font5" id="p10_t110" reading_order_no="109" segment_no="20" tag_type="text">2011</text>
<text top="376" left="312" width="251" height="7" font="font5" id="p10_t111" reading_order_no="110" segment_no="22" tag_type="text">[40] Fan Zhu and Ling Shao. Enhancing action recognition by cross-domain</text>
<text top="385" left="326" width="238" height="7" font="font5" id="p10_t112" reading_order_no="111" segment_no="22" tag_type="text">dictionary learning. In British Machine Vision Conference (BMVC), 2013.</text>
<text top="394" left="312" width="251" height="7" font="font5" id="p10_t113" reading_order_no="112" segment_no="24" tag_type="text">[41] Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin</text>
<text top="403" left="326" width="237" height="7" font="font5" id="p10_t114" reading_order_no="113" segment_no="24" tag_type="text">Chen, and Jian Zheng. Temporal attentive alignment for large-scale video</text>
<text top="412" left="326" width="237" height="7" font="font5" id="p10_t115" reading_order_no="114" segment_no="24" tag_type="text">domain adaptation. In International Conference on Computer Vision</text>
<text top="421" left="326" width="76" height="7" font="font5" id="p10_t116" reading_order_no="115" segment_no="24" tag_type="text">(ICCV), October 2019.</text>
<text top="430" left="312" width="252" height="7" font="font5" id="p10_t117" reading_order_no="116" segment_no="26" tag_type="text">[42] Arshad Jamal, Vinay P Namboodiri, Dipti Deodhare, and KS Venkatesh.</text>
<text top="439" left="326" width="237" height="7" font="font5" id="p10_t118" reading_order_no="117" segment_no="26" tag_type="text">Deep domain adaptation in action space. In British Machine Vision</text>
<text top="448" left="326" width="89" height="7" font="font5" id="p10_t119" reading_order_no="118" segment_no="26" tag_type="text">Conference (BMVC), 2018</text>
<text top="457" left="312" width="252" height="7" font="font5" id="p10_t120" reading_order_no="119" segment_no="29" tag_type="text">[43] Boxiao Pan, Zhangjie Cao, Ehsan Adeli, and Juan Carlos Niebles.</text>
<text top="466" left="326" width="237" height="7" font="font5" id="p10_t121" reading_order_no="120" segment_no="29" tag_type="text">Adversarial cross-domain action recognition with co-attention. AAAI</text>
<text top="475" left="326" width="140" height="7" font="font5" id="p10_t122" reading_order_no="121" segment_no="29" tag_type="text">Conference on Artificial Intelligence, 2020</text>
<text top="484" left="312" width="251" height="7" font="font5" id="p10_t123" reading_order_no="122" segment_no="31" tag_type="text">[44] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar</text>
<text top="493" left="326" width="238" height="7" font="font5" id="p10_t124" reading_order_no="123" segment_no="31" tag_type="text">Paluri. Learning spatiotemporal features with 3D convolutional networks.</text>
<text top="502" left="326" width="198" height="7" font="font5" id="p10_t125" reading_order_no="124" segment_no="31" tag_type="text">In Computer Vision and Pattern Recognition (CVPR), 2015.</text>
<text top="511" left="312" width="251" height="7" font="font5" id="p10_t126" reading_order_no="125" segment_no="34" tag_type="text">[45] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal</text>
<text top="520" left="326" width="237" height="7" font="font5" id="p10_t127" reading_order_no="126" segment_no="34" tag_type="text">relational reasoning in videos. In European Conference on Computer Vision</text>
<text top="529" left="326" width="49" height="7" font="font5" id="p10_t128" reading_order_no="127" segment_no="34" tag_type="text">(ECCV), 2018.</text>
<text top="538" left="312" width="251" height="7" font="font5" id="p10_t129" reading_order_no="128" segment_no="36" tag_type="text">[46] Kishore K Reddy and Mubarak Shah. Recognizing 50 human action</text>
<text top="547" left="326" width="216" height="7" font="font5" id="p10_t130" reading_order_no="129" segment_no="36" tag_type="text">categories of web videos. Machine Vision and Applications, 2013</text>
<text top="556" left="312" width="252" height="7" font="font5" id="p10_t131" reading_order_no="130" segment_no="38" tag_type="text">[47] Dima Damen et. al. Scaling egocentric vision: The EPIC-Kitchens dataset.</text>
<text top="565" left="326" width="201" height="7" font="font5" id="p10_t132" reading_order_no="131" segment_no="38" tag_type="text">In European Conference on Computer Vision (ECCV), 2018.</text>
<text top="574" left="312" width="252" height="7" font="font5" id="p10_t133" reading_order_no="132" segment_no="40" tag_type="text">[48] Mohammad Arif Ul Alam, Aliza Heching, Nicola Palmarini, Scaling Lon-</text>
<text top="583" left="326" width="238" height="7" font="font5" id="p10_t134" reading_order_no="133" segment_no="40" tag_type="text">gitudinal Functional Health Assessment in Multi-Inhabitant Smarthome,</text>
<text top="591" left="326" width="237" height="7" font="font5" id="p10_t135" reading_order_no="134" segment_no="40" tag_type="text">IEEE International Conference on Distributed Computing Systems (ICDCS</text>
<text top="600" left="326" width="19" height="7" font="font5" id="p10_t136" reading_order_no="135" segment_no="40" tag_type="text">2019)</text>
<text top="609" left="312" width="251" height="7" font="font5" id="p10_t137" reading_order_no="136" segment_no="42" tag_type="text">[49] Mohammad Arif Ul Alam, Context-Aware Multi-Inhabitant Functional</text>
<text top="618" left="326" width="237" height="7" font="font5" id="p10_t138" reading_order_no="137" segment_no="42" tag_type="text">and Physiological Health Assessment in Smart Home Environment, Ph.D</text>
<text top="627" left="326" width="71" height="7" font="font5" id="p10_t139" reading_order_no="138" segment_no="42" tag_type="text">Forum, Percom 2017.</text>
<text top="636" left="312" width="252" height="7" font="font5" id="p10_t140" reading_order_no="139" segment_no="44" tag_type="text">[50] Mohammad Arif Ul Alam, Nirmalya Roy, Archan Misra, Joseph Taylor,</text>
<text top="645" left="326" width="238" height="7" font="font5" id="p10_t141" reading_order_no="140" segment_no="44" tag_type="text">CACE: Exploiting Behavioral Interactions for Improved Activity Recog-</text>
<text top="654" left="326" width="237" height="7" font="font5" id="p10_t142" reading_order_no="141" segment_no="44" tag_type="text">nition in Multi-Inhabitant Smart Homes, 36th International Conference</text>
<text top="663" left="326" width="205" height="7" font="font5" id="p10_t143" reading_order_no="142" segment_no="44" tag_type="text">on Distributed Computing Systems, ICDCS 2016, Nara, Japan</text>
<text top="672" left="312" width="252" height="7" font="font5" id="p10_t144" reading_order_no="143" segment_no="46" tag_type="text">[51] Wenjun Jiang et. al. Towards Environment Independent Device Free Hu-</text>
<text top="681" left="326" width="237" height="7" font="font5" id="p10_t145" reading_order_no="144" segment_no="46" tag_type="text">man Activity Recognition. In Proceedings of the 24th Annual International</text>
<text top="690" left="326" width="221" height="7" font="font5" id="p10_t146" reading_order_no="145" segment_no="46" tag_type="text">Conference on Mobile Computing and Networking (MobiCom ’18)</text>
</page>
</pdf2xml>
