<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="14" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font1" size="10" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font2" size="7" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="12" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font4" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font5" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font6" size="10" family="NimbusRomNo9L-Regu" color="#001472"/>
	<fontspec id="font7" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font8" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font9" size="7" family="CMMI7" color="#000000"/>
	<fontspec id="font10" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font11" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font12" size="7" family="CMBSY7" color="#000000"/>
	<fontspec id="font13" size="5" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font14" size="10" family="CMBSY10" color="#000000"/>
	<fontspec id="font15" size="6" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font16" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font17" size="9" family="NimbusRomNo9L-ReguItal" color="#000000"/>
	<fontspec id="font18" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font19" size="20" family="Times" color="#7f7f7f"/>
<text top="91" left="98" width="401" height="13" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="142" left="197" width="72" height="11" font="font1" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="text">Andreas Kirsch 1</text>
<text top="142" left="276" width="68" height="11" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="text">Tom Rainforth 2</text>
<text top="142" left="351" width="47" height="11" font="font1" id="p1_t4" reading_order_no="4" segment_no="1" tag_type="text">Yarin Gal 1</text>
<text top="175" left="150" width="44" height="11" font="font3" id="p1_t5" reading_order_no="5" segment_no="2" tag_type="title">Abstract</text>
<text top="193" left="75" width="196" height="9" font="font4" id="p1_t6" reading_order_no="6" segment_no="4" tag_type="text">Active Learning is essential for more label-</text>
<text top="205" left="75" width="194" height="9" font="font4" id="p1_t7" reading_order_no="7" segment_no="4" tag_type="text">efficient deep learning. Bayesian Active Learning</text>
<text top="217" left="75" width="194" height="9" font="font4" id="p1_t8" reading_order_no="8" segment_no="4" tag_type="text">has focused on BALD, which reduces model</text>
<text top="229" left="75" width="194" height="9" font="font4" id="p1_t9" reading_order_no="9" segment_no="4" tag_type="text">parameter uncertainty. However, we show that</text>
<text top="241" left="75" width="194" height="9" font="font4" id="p1_t10" reading_order_no="10" segment_no="4" tag_type="text">BALD gets stuck on out-of-distribution or junk</text>
<text top="253" left="75" width="194" height="9" font="font4" id="p1_t11" reading_order_no="11" segment_no="4" tag_type="text">data that is not relevant for the task. We examine</text>
<text top="265" left="75" width="194" height="9" font="font4" id="p1_t12" reading_order_no="12" segment_no="4" tag_type="text">a novel Expected Predictive Information Gain</text>
<text top="277" left="75" width="194" height="9" font="font5" id="p1_t13" reading_order_no="13" segment_no="4" tag_type="text">(EPIG) to deal with distribution shifts of the pool</text>
<text top="289" left="75" width="194" height="9" font="font4" id="p1_t14" reading_order_no="14" segment_no="4" tag_type="text">set. EPIG reduces the uncertainty of predictions</text>
<text top="301" left="75" width="195" height="9" font="font4" id="p1_t15" reading_order_no="15" segment_no="4" tag_type="text">on an unlabelled evaluation set sampled from the</text>
<text top="313" left="75" width="194" height="9" font="font4" id="p1_t16" reading_order_no="16" segment_no="4" tag_type="text">test data distribution whose distribution might be</text>
<text top="325" left="75" width="194" height="9" font="font4" id="p1_t17" reading_order_no="17" segment_no="4" tag_type="text">different to the pool set distribution. Based on</text>
<text top="337" left="75" width="194" height="9" font="font4" id="p1_t18" reading_order_no="18" segment_no="4" tag_type="text">this, our new EPIG-BALD acquisition function</text>
<text top="349" left="75" width="194" height="9" font="font4" id="p1_t19" reading_order_no="19" segment_no="4" tag_type="text">for Bayesian Neural Networks selects samples</text>
<text top="361" left="75" width="194" height="9" font="font4" id="p1_t20" reading_order_no="20" segment_no="4" tag_type="text">to improve the performance on the test data</text>
<text top="373" left="75" width="194" height="9" font="font4" id="p1_t21" reading_order_no="21" segment_no="4" tag_type="text">distribution instead of selecting samples that</text>
<text top="385" left="75" width="194" height="9" font="font4" id="p1_t22" reading_order_no="22" segment_no="4" tag_type="text">reduce model uncertainty everywhere, including</text>
<text top="397" left="75" width="194" height="9" font="font4" id="p1_t23" reading_order_no="23" segment_no="4" tag_type="text">for out-of-distribution regions with low density in</text>
<text top="409" left="75" width="194" height="9" font="font4" id="p1_t24" reading_order_no="24" segment_no="4" tag_type="text">the test data distribution. Our method outperforms</text>
<text top="421" left="75" width="194" height="9" font="font4" id="p1_t25" reading_order_no="25" segment_no="4" tag_type="text">state-of-the-art Bayesian active learning methods</text>
<text top="433" left="75" width="196" height="9" font="font4" id="p1_t26" reading_order_no="26" segment_no="4" tag_type="text">on high-dimensional datasets and avoids out-of-</text>
<text top="444" left="75" width="196" height="9" font="font4" id="p1_t27" reading_order_no="27" segment_no="4" tag_type="text">distribution junk data in cases where current state-</text>
<text top="456" left="75" width="92" height="9" font="font4" id="p1_t28" reading_order_no="28" segment_no="4" tag_type="text">of-the-art methods fail.</text>
<text top="491" left="55" width="77" height="11" font="font3" id="p1_t29" reading_order_no="29" segment_no="8" tag_type="title">1. Introduction</text>
<text top="512" left="55" width="234" height="9" font="font4" id="p1_t30" reading_order_no="30" segment_no="9" tag_type="text">Active learning is essential for increasing label- and</text>
<text top="524" left="55" width="234" height="9" font="font4" id="p1_t31" reading_order_no="31" segment_no="9" tag_type="text">thus cost-efficiency in real-world machine learning</text>
<text top="536" left="55" width="236" height="9" font="font4" id="p1_t32" reading_order_no="32" segment_no="9" tag_type="text">applications, especially when they use deep learning.</text>
<text top="548" left="55" width="234" height="9" font="font4" id="p1_t33" reading_order_no="33" segment_no="9" tag_type="text">Similarly, quantifying uncertainty using Bayesian methods</text>
<text top="560" left="55" width="234" height="9" font="font4" id="p1_t34" reading_order_no="34" segment_no="9" tag_type="text">is important for safety-critical systems. Combining active</text>
<text top="572" left="55" width="234" height="9" font="font4" id="p1_t35" reading_order_no="35" segment_no="9" tag_type="text">learning with Bayesian methods for deep neural networks</text>
<text top="584" left="55" width="216" height="9" font="font4" id="p1_t36" reading_order_no="36" segment_no="9" tag_type="text">has been an important research avenue for this reason.</text>
<text top="602" left="55" width="236" height="9" font="font4" id="p1_t37" reading_order_no="37" segment_no="12" tag_type="text">In active learning ( Atlas et al. , 1990 ; Settles , 2009 ),</text>
<text top="614" left="55" width="234" height="9" font="font4" id="p1_t38" reading_order_no="38" segment_no="12" tag_type="text">we have access to a huge reservoir of unlabelled data</text>
<text top="623" left="55" width="23" height="11" font="font7" id="p1_t39" reading_order_no="39" segment_no="12" tag_type="text">{ x pool</text>
<text top="630" left="66" width="3" height="6" font="font9" id="p1_t40" reading_order_no="40" segment_no="12" tag_type="text">i</text>
<text top="623" left="79" width="210" height="13" font="font7" id="p1_t41" reading_order_no="41" segment_no="12" tag_type="text">} i ∈{ 1 ,..., | D pool |} in a pool set D pool . We iteratively use</text>
<text top="638" left="55" width="235" height="9" font="font4" id="p1_t42" reading_order_no="42" segment_no="12" tag_type="text">an acquisition function to score and select samples from this</text>
<text top="649" left="55" width="234" height="9" font="font4" id="p1_t43" reading_order_no="43" segment_no="12" tag_type="text">pool set to be labeled by an oracle (e.g. human experts) and</text>
<text top="667" left="69" width="220" height="10" font="font16" id="p1_t44" reading_order_no="87" segment_no="13" tag_type="footnote">1 OATML, Department of Computer Science, 2 Department</text>
<text top="679" left="55" width="82" height="8" font="font16" id="p1_t45" reading_order_no="88" segment_no="13" tag_type="footnote">of Statistics, Oxford.</text>
<text top="679" left="148" width="142" height="8" font="font16" id="p1_t46" reading_order_no="89" segment_no="13" tag_type="footnote">Correspondence to: Andreas Kirsch</text>
<text top="689" left="54" width="112" height="8" font="font16" id="p1_t47" reading_order_no="90" segment_no="13" tag_type="footnote">&lt;andreas.kirsch@cs.ox.ac.uk&gt;.<a href="deeplearning_paper2.html#7">(</a></text>
<text top="709" left="55" width="186" height="8" font="font17" id="p1_t48" reading_order_no="91" segment_no="15" tag_type="footnote">Preliminary work. Copyright 2021 by the author(s).<a href="deeplearning_paper2.html#7">Atlas et al.</a></text>
<text top="177" left="307" width="234" height="9" font="font4" id="p1_t49" reading_order_no="44" segment_no="3" tag_type="text">added to the training set. Ideally, the selected samples are<a href="deeplearning_paper2.html#7">,</a></text>
<text top="188" left="307" width="234" height="9" font="font4" id="p1_t50" reading_order_no="45" segment_no="3" tag_type="text">informative and increase the performance of the machine<a href="deeplearning_paper2.html#7">1990</a></text>
<text top="200" left="307" width="234" height="10" font="font4" id="p1_t51" reading_order_no="46" segment_no="3" tag_type="text">learning model faster than a random acquisition of samples<a href="deeplearning_paper2.html#7">;</a></text>
<text top="212" left="307" width="27" height="9" font="font4" id="p1_t52" reading_order_no="47" segment_no="3" tag_type="text">would.<a href="deeplearning_paper2.html#7">Settles</a></text>
<text top="230" left="307" width="234" height="9" font="font5" id="p1_t53" reading_order_no="48" segment_no="5" tag_type="text">Bayesian neural networks treat the model parameters Ω<a href="deeplearning_paper2.html#7">,</a></text>
<text top="242" left="307" width="201" height="9" font="font4" id="p1_t54" reading_order_no="49" segment_no="5" tag_type="text">as a random variable with a distribution p( ω ) .<a href="deeplearning_paper2.html#7">2009</a></text>
<text top="242" left="517" width="24" height="9" font="font4" id="p1_t55" reading_order_no="50" segment_no="5" tag_type="text">Using<a href="deeplearning_paper2.html#7">),</a></text>
<text top="252" left="307" width="236" height="11" font="font4" id="p1_t56" reading_order_no="51" segment_no="5" tag_type="text">training data D train , a posterior distribution p( ω | D train )</text>
<text top="266" left="307" width="234" height="9" font="font4" id="p1_t57" reading_order_no="52" segment_no="5" tag_type="text">is inferred. This contrasts with regular deep learning, which</text>
<text top="278" left="307" width="234" height="9" font="font4" id="p1_t58" reading_order_no="53" segment_no="5" tag_type="text">only learns a maximum-likelihood point estimate of the</text>
<text top="290" left="307" width="74" height="9" font="font4" id="p1_t59" reading_order_no="54" segment_no="5" tag_type="text">model parameters.</text>
<text top="308" left="307" width="234" height="9" font="font4" id="p1_t60" reading_order_no="55" segment_no="6" tag_type="text">Conventionally, Bayesian Active Learning selects samples</text>
<text top="319" left="307" width="235" height="10" font="font8" id="p1_t61" reading_order_no="56" segment_no="6" tag_type="text">x which maximize the expected information gain I[Ω; Y |</text>
<text top="330" left="307" width="234" height="11" font="font8" id="p1_t62" reading_order_no="57" segment_no="6" tag_type="text">x, D train ] between the model parameters Ω and the</text>
<text top="344" left="307" width="234" height="9" font="font4" id="p1_t63" reading_order_no="58" segment_no="6" tag_type="text">prediction Y of the model for x , which is also referred</text>
<text top="356" left="307" width="235" height="9" font="font4" id="p1_t64" reading_order_no="59" segment_no="6" tag_type="text">to as BALD (Bayesian Active Learning by Disagreement)</text>
<text top="368" left="307" width="234" height="9" font="font4" id="p1_t65" reading_order_no="60" segment_no="6" tag_type="text">acquisition function ( Houlsby et al. , 2011 ). Using this</text>
<text top="380" left="307" width="234" height="9" font="font4" id="p1_t66" reading_order_no="61" segment_no="6" tag_type="text">acquisition function ensures that the model posterior</text>
<text top="389" left="307" width="235" height="12" font="font5" id="p1_t67" reading_order_no="62" segment_no="6" tag_type="text">uncertainty H[Ω | D train ] is reduced as quickly as possible</text>
<text top="404" left="307" width="226" height="9" font="font4" id="p1_t68" reading_order_no="63" segment_no="6" tag_type="text">with the aim of converging to the true model parameters.</text>
<text top="422" left="307" width="234" height="9" font="font4" id="p1_t69" reading_order_no="64" segment_no="7" tag_type="text">This approach suffers from several issues: firstly, unlike</text>
<text top="434" left="307" width="234" height="9" font="font4" id="p1_t70" reading_order_no="65" segment_no="7" tag_type="text">in experiment design and statistics, where the model</text>
<text top="446" left="307" width="235" height="9" font="font4" id="p1_t71" reading_order_no="66" segment_no="7" tag_type="text">parameters are sought as the main goal, in active learning,</text>
<text top="457" left="307" width="234" height="9" font="font4" id="p1_t72" reading_order_no="67" segment_no="7" tag_type="text">the task is to minimize an empirical risk objective in</text>
<text top="469" left="307" width="90" height="9" font="font4" id="p1_t73" reading_order_no="68" segment_no="7" tag_type="text">a supervised setting.</text>
<text top="469" left="412" width="129" height="9" font="font4" id="p1_t74" reading_order_no="69" segment_no="7" tag_type="text">We are interested in making</text>
<text top="481" left="307" width="235" height="9" font="font4" id="p1_t75" reading_order_no="70" segment_no="7" tag_type="text">correct predictions for samples form the test distribution,</text>
<text top="493" left="307" width="234" height="9" font="font4" id="p1_t76" reading_order_no="71" segment_no="7" tag_type="text">not necessarily getting a good estimate of the posterior</text>
<text top="505" left="307" width="234" height="9" font="font4" id="p1_t77" reading_order_no="72" segment_no="7" tag_type="text">distribution. Secondly, the models are of limited capacity</text>
<text top="517" left="307" width="234" height="9" font="font4" id="p1_t78" reading_order_no="73" segment_no="7" tag_type="text">and do not contain the data-generating model, which means</text>
<text top="529" left="307" width="234" height="9" font="font4" id="p1_t79" reading_order_no="74" segment_no="7" tag_type="text">that there will be trade-offs in the performance of the model</text>
<text top="541" left="307" width="195" height="9" font="font4" id="p1_t80" reading_order_no="75" segment_no="7" tag_type="text">depending on what we focus ( Cobb et al. , 2018 ).</text>
<text top="559" left="307" width="234" height="9" font="font4" id="p1_t81" reading_order_no="76" segment_no="11" tag_type="text">Moreover, the pool set might not follow the distribution</text>
<text top="571" left="307" width="234" height="9" font="font4" id="p1_t82" reading_order_no="77" segment_no="11" tag_type="text">of the test set the model should generalize to: it is easy to</text>
<text top="583" left="307" width="235" height="9" font="font4" id="p1_t83" reading_order_no="78" segment_no="11" tag_type="text">collect unlabelled data in a pool set, but it is hard to clean,</text>
<text top="595" left="307" width="234" height="9" font="font4" id="p1_t84" reading_order_no="79" segment_no="11" tag_type="text">filter and resample the pool set to make it follow the test</text>
<text top="607" left="307" width="234" height="9" font="font4" id="p1_t85" reading_order_no="80" segment_no="11" tag_type="text">set distribution one is interested in. For example, for big</text>
<text top="619" left="307" width="234" height="9" font="font4" id="p1_t86" reading_order_no="81" segment_no="11" tag_type="text">text datasets that are created by crawling the internet, the</text>
<text top="631" left="307" width="234" height="9" font="font4" id="p1_t87" reading_order_no="82" segment_no="11" tag_type="text">crawled data might not follow the distribution which we</text>
<text top="643" left="307" width="234" height="9" font="font4" id="p1_t88" reading_order_no="83" segment_no="11" tag_type="text">want to predict on. This is not covered by the usual active</text>
<text top="655" left="307" width="208" height="9" font="font4" id="p1_t89" reading_order_no="84" segment_no="11" tag_type="text">learning setting which assumes no distribution shift.</text>
<text top="673" left="307" width="234" height="9" font="font4" id="p1_t90" reading_order_no="85" segment_no="14" tag_type="text">We introduce a novel acquisition function which selects</text>
<text top="685" left="307" width="235" height="9" font="font4" id="p1_t91" reading_order_no="86" segment_no="14" tag_type="text">samples that improve the performance on an evaluation</text>
<text top="546" left="32" width="0" height="18" font="font19" id="p1_t92" reading_order_no="0" segment_no="10" tag_type="title">arXiv:2106.11719v1  [cs.LG]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font20" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font21" size="9" family="NimbusRomNo9L-Regu-Slant_167" color="#000000"/>
	<fontspec id="font22" size="6" family="DejaVuSans" color="#262626"/>
	<fontspec id="font23" size="6" family="DejaVuSans" color="#262626"/>
	<fontspec id="font24" size="7" family="NimbusRomNo9L-Regu" color="#001472"/>
	<fontspec id="font25" size="9" family="CMBSY9" color="#000000"/>
	<fontspec id="font26" size="9" family="NimbusRomNo9L-Regu" color="#001472"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="156" left="55" width="113" height="8" font="font21" id="p2_t2" reading_order_no="19" segment_no="3" tag_type="text">Figure 1. Expected Information</text>
<text top="167" left="55" width="18" height="8" font="font17" id="p2_t3" reading_order_no="20" segment_no="3" tag_type="text">Gain</text>
<text top="167" left="82" width="29" height="8" font="font17" id="p2_t4" reading_order_no="21" segment_no="3" tag_type="text">(BALD)</text>
<text top="167" left="118" width="8" height="8" font="font17" id="p2_t5" reading_order_no="22" segment_no="3" tag_type="text">vs</text>
<text top="167" left="134" width="34" height="8" font="font17" id="p2_t6" reading_order_no="23" segment_no="3" tag_type="text">Expected</text>
<text top="178" left="55" width="114" height="8" font="font17" id="p2_t7" reading_order_no="24" segment_no="3" tag_type="text">Predictive Information Gain.</text>
<text top="189" left="55" width="112" height="8" font="font16" id="p2_t8" reading_order_no="25" segment_no="3" tag_type="text">EPIG acquires samples near the</text>
<text top="200" left="55" width="51" height="8" font="font16" id="p2_t9" reading_order_no="26" segment_no="3" tag_type="text">evaluation set.</text>
<text top="124" left="204" width="4" height="7" font="font22" id="p2_t10" reading_order_no="9" segment_no="2" tag_type="figure">50</text>
<text top="124" left="219" width="7" height="7" font="font22" id="p2_t11" reading_order_no="10" segment_no="2" tag_type="figure">100</text>
<text top="124" left="234" width="7" height="7" font="font22" id="p2_t12" reading_order_no="11" segment_no="2" tag_type="figure">150</text>
<text top="124" left="250" width="7" height="7" font="font22" id="p2_t13" reading_order_no="12" segment_no="2" tag_type="figure">200</text>
<text top="124" left="266" width="7" height="7" font="font22" id="p2_t14" reading_order_no="13" segment_no="2" tag_type="figure">250</text>
<text top="124" left="282" width="7" height="7" font="font22" id="p2_t15" reading_order_no="14" segment_no="2" tag_type="figure">300</text>
<text top="129" left="226" width="30" height="7" font="font22" id="p2_t16" reading_order_no="15" segment_no="2" tag_type="figure">Training Set Size</text>
<text top="115" left="182" width="8" height="7" font="font22" id="p2_t17" reading_order_no="8" segment_no="2" tag_type="figure">0.65</text>
<text top="107" left="182" width="8" height="7" font="font22" id="p2_t18" reading_order_no="7" segment_no="2" tag_type="figure">0.70</text>
<text top="99" left="182" width="8" height="7" font="font22" id="p2_t19" reading_order_no="6" segment_no="2" tag_type="figure">0.75</text>
<text top="92" left="182" width="8" height="7" font="font22" id="p2_t20" reading_order_no="5" segment_no="2" tag_type="figure">0.80</text>
<text top="84" left="182" width="8" height="7" font="font22" id="p2_t21" reading_order_no="4" segment_no="2" tag_type="figure">0.85</text>
<text top="76" left="182" width="8" height="7" font="font22" id="p2_t22" reading_order_no="3" segment_no="2" tag_type="figure">0.90</text>
<text top="68" left="182" width="8" height="7" font="font22" id="p2_t23" reading_order_no="2" segment_no="2" tag_type="figure">0.95</text>
<text top="98" left="180" width="0" height="7" font="font23" id="p2_t24" reading_order_no="1" segment_no="2" tag_type="figure">Accuracy</text>
<text top="136" left="203" width="51" height="12" font="font22" id="p2_t25" reading_order_no="16" segment_no="2" tag_type="figure">Acquisition Function EPIG-BALD</text>
<text top="141" left="239" width="14" height="7" font="font22" id="p2_t26" reading_order_no="17" segment_no="2" tag_type="figure">Uniform</text>
<text top="141" left="270" width="8" height="7" font="font22" id="p2_t27" reading_order_no="18" segment_no="2" tag_type="figure">EPIG</text>
<text top="156" left="177" width="69" height="8" font="font21" id="p2_t28" reading_order_no="27" segment_no="4" tag_type="text">Figure 2. Ablation:</text>
<text top="156" left="255" width="34" height="8" font="font17" id="p2_t29" reading_order_no="28" segment_no="4" tag_type="text">EPIG vs</text>
<text top="167" left="177" width="47" height="8" font="font17" id="p2_t30" reading_order_no="29" segment_no="4" tag_type="text">EPIG-BALD</text>
<text top="167" left="232" width="16" height="8" font="font17" id="p2_t31" reading_order_no="30" segment_no="4" tag_type="text">with</text>
<text top="167" left="256" width="34" height="8" font="font17" id="p2_t32" reading_order_no="31" segment_no="4" tag_type="text">Bayesian</text>
<text top="178" left="177" width="114" height="8" font="font17" id="p2_t33" reading_order_no="32" segment_no="4" tag_type="text">Neural Networks on MNIST.</text>
<text top="189" left="177" width="112" height="8" font="font16" id="p2_t34" reading_order_no="33" segment_no="4" tag_type="text">EPIG does not perform better</text>
<text top="200" left="177" width="90" height="8" font="font16" id="p2_t35" reading_order_no="34" segment_no="4" tag_type="text">than uniform acquisition.</text>
<text top="221" left="55" width="11" height="9" font="font5" id="p2_t36" reading_order_no="35" segment_no="6" tag_type="text">set</text>
<text top="219" left="74" width="120" height="11" font="font14" id="p2_t37" reading_order_no="36" segment_no="6" tag_type="text">D eval , which is unlabeled 1</text>
<text top="221" left="202" width="87" height="9" font="font4" id="p2_t38" reading_order_no="37" segment_no="6" tag_type="text">and whose samples</text>
<text top="231" left="55" width="22" height="12" font="font7" id="p2_t39" reading_order_no="38" segment_no="6" tag_type="text">{ x eval i</text>
<text top="232" left="78" width="211" height="12" font="font7" id="p2_t40" reading_order_no="39" segment_no="6" tag_type="text">} i ∈{ 1 ,..., | D eval |} follow the test data distribution: the</text>
<text top="245" left="55" width="210" height="12" font="font5" id="p2_t41" reading_order_no="40" segment_no="6" tag_type="text">Expected Predictive Information Gain (EPIG) I[ { Y eval i</text>
<text top="246" left="266" width="24" height="10" font="font7" id="p2_t42" reading_order_no="41" segment_no="6" tag_type="text">} i ; Y |<a href="deeplearning_paper2.html#2">unlabeled</a></text>
<text top="257" left="55" width="32" height="12" font="font8" id="p2_t43" reading_order_no="42" segment_no="6" tag_type="text">x, { x eval i<a href="deeplearning_paper2.html#2">1</a></text>
<text top="256" left="88" width="202" height="12" font="font7" id="p2_t44" reading_order_no="43" segment_no="6" tag_type="text">} i , D train ] between the evaluation set D eval and a</text>
<text top="270" left="55" width="234" height="9" font="font4" id="p2_t45" reading_order_no="44" segment_no="6" tag_type="text">candidate sample x . We show that EPIG avoids acquiring</text>
<text top="282" left="55" width="236" height="9" font="font4" id="p2_t46" reading_order_no="45" segment_no="6" tag_type="text">samples that only reduce the model uncertainty in out-</text>
<text top="294" left="55" width="234" height="9" font="font4" id="p2_t47" reading_order_no="46" segment_no="6" tag_type="text">of-distribution regions with low density in the test data</text>
<text top="306" left="55" width="234" height="9" font="font4" id="p2_t48" reading_order_no="47" segment_no="6" tag_type="text">distribution and that EPIG is directly connected to selecting</text>
<text top="318" left="55" width="201" height="9" font="font4" id="p2_t49" reading_order_no="48" segment_no="6" tag_type="text">samples that help minimze the generalization loss.</text>
<text top="336" left="55" width="235" height="9" font="font4" id="p2_t50" reading_order_no="49" segment_no="8" tag_type="text">In fig. 1 , we show a toy example which visualizes the</text>
<text top="348" left="55" width="234" height="9" font="font4" id="p2_t51" reading_order_no="50" segment_no="8" tag_type="text">difference between the Expected Information Gain and</text>
<text top="360" left="55" width="234" height="9" font="font4" id="p2_t52" reading_order_no="51" segment_no="8" tag_type="text">our new Predictive Information Gain. Whereas BALD</text>
<text top="372" left="55" width="234" height="9" font="font4" id="p2_t53" reading_order_no="52" segment_no="8" tag_type="text">acquires samples outside the evaluation set, EPIG correctly</text>
<text top="384" left="55" width="234" height="9" font="font4" id="p2_t54" reading_order_no="53" segment_no="8" tag_type="text">focuses on acquiring samples that meaningfully reduce the</text>
<text top="396" left="55" width="133" height="9" font="font4" id="p2_t55" reading_order_no="54" segment_no="8" tag_type="text">uncertainty for the evaluation set.</text>
<text top="414" left="55" width="234" height="9" font="font4" id="p2_t56" reading_order_no="55" segment_no="9" tag_type="text">The expected predictive information gain can be viewed as</text>
<text top="423" left="55" width="235" height="12" font="font4" id="p2_t57" reading_order_no="56" segment_no="9" tag_type="text">a version of the expected information gain I[Ω; Y | x, D train ]</text>
<text top="438" left="55" width="234" height="9" font="font4" id="p2_t58" reading_order_no="57" segment_no="9" tag_type="text">(BALD) where the predictions on the evaluation set take</text>
<text top="449" left="55" width="234" height="9" font="font4" id="p2_t59" reading_order_no="58" segment_no="9" tag_type="text">the place of the model parameters. The model parameters</text>
<text top="461" left="55" width="234" height="9" font="font4" id="p2_t60" reading_order_no="59" segment_no="9" tag_type="text">become nuisance variables, as we do not care about learning</text>
<text top="473" left="55" width="234" height="9" font="font4" id="p2_t61" reading_order_no="60" segment_no="9" tag_type="text">the true model parameters in this setting: we only care about</text>
<text top="485" left="55" width="234" height="9" font="font4" id="p2_t62" reading_order_no="61" segment_no="9" tag_type="text">learning the model parameters insofar as they allow us to</text>
<text top="497" left="55" width="234" height="9" font="font4" id="p2_t63" reading_order_no="62" segment_no="9" tag_type="text">make correct future predictions on the data distribution of</text>
<text top="507" left="55" width="139" height="11" font="font4" id="p2_t64" reading_order_no="63" segment_no="9" tag_type="text">the unlabelled evaluation set D eval .</text>
<text top="527" left="55" width="234" height="9" font="font4" id="p2_t65" reading_order_no="64" segment_no="11" tag_type="text">To expand EPIG to high-dimensional datasets and deep</text>
<text top="539" left="55" width="90" height="9" font="font4" id="p2_t66" reading_order_no="65" segment_no="11" tag_type="text">learning, we show that</text>
<text top="556" left="90" width="31" height="11" font="font18" id="p2_t67" reading_order_no="66" segment_no="12" tag_type="formula">I[ { Y eval</text>
<text top="563" left="108" width="3" height="6" font="font9" id="p2_t68" reading_order_no="67" segment_no="12" tag_type="formula">i</text>
<text top="556" left="122" width="59" height="12" font="font7" id="p2_t69" reading_order_no="68" segment_no="12" tag_type="formula">} i ; Y | x, { x eval</text>
<text top="563" left="170" width="3" height="6" font="font9" id="p2_t70" reading_order_no="69" segment_no="12" tag_type="formula">i</text>
<text top="556" left="182" width="48" height="12" font="font7" id="p2_t71" reading_order_no="70" segment_no="12" tag_type="formula">} i , D train ] =</text>
<text top="573" left="102" width="40" height="12" font="font18" id="p2_t72" reading_order_no="71" segment_no="12" tag_type="formula">= I[ { Y eval i</text>
<text top="573" left="143" width="72" height="12" font="font7" id="p2_t73" reading_order_no="72" segment_no="12" tag_type="formula">} i ; Y ; Ω | x, { x eval i</text>
<text top="573" left="215" width="41" height="11" font="font7" id="p2_t74" reading_order_no="73" segment_no="12" tag_type="formula">} i , D train ] ,</text>
<text top="593" left="55" width="195" height="12" font="font4" id="p2_t75" reading_order_no="74" segment_no="13" tag_type="text">and use the triple mutual information I[ { Y eval i</text>
<text top="594" left="251" width="39" height="10" font="font7" id="p2_t76" reading_order_no="75" segment_no="13" tag_type="text">} i ; Y ; Ω |</text>
<text top="605" left="55" width="32" height="12" font="font8" id="p2_t77" reading_order_no="76" segment_no="13" tag_type="text">x, { x eval i</text>
<text top="604" left="88" width="201" height="12" font="font7" id="p2_t78" reading_order_no="77" segment_no="13" tag_type="text">} i , D train ] (EPIG-BALD) to evaluate EPIG in</text>
<text top="618" left="55" width="234" height="9" font="font4" id="p2_t79" reading_order_no="78" segment_no="13" tag_type="text">a tractable way for Bayesian parametric models with</text>
<text top="628" left="55" width="196" height="11" font="font4" id="p2_t80" reading_order_no="79" segment_no="13" tag_type="text">approximate posteriors. We decompose I[ { Y eval</text>
<text top="635" left="238" width="3" height="6" font="font9" id="p2_t81" reading_order_no="80" segment_no="13" tag_type="text">i</text>
<text top="629" left="252" width="38" height="10" font="font7" id="p2_t82" reading_order_no="81" segment_no="13" tag_type="text">} i ; Y ; Ω |</text>
<text top="640" left="55" width="32" height="11" font="font8" id="p2_t83" reading_order_no="82" segment_no="13" tag_type="text">x, { x eval</text>
<text top="647" left="76" width="3" height="6" font="font9" id="p2_t84" reading_order_no="83" segment_no="13" tag_type="text">i</text>
<text top="640" left="88" width="166" height="11" font="font7" id="p2_t85" reading_order_no="84" segment_no="13" tag_type="text">} i , D train ] into two tractable BALD terms:</text>
<text top="659" left="57" width="30" height="11" font="font18" id="p2_t86" reading_order_no="85" segment_no="15" tag_type="formula">I[ { Y eval</text>
<text top="666" left="74" width="3" height="6" font="font9" id="p2_t87" reading_order_no="86" segment_no="15" tag_type="formula">i</text>
<text top="659" left="88" width="72" height="12" font="font7" id="p2_t88" reading_order_no="87" segment_no="15" tag_type="formula">} i ; Y ; Ω | x, { x eval</text>
<text top="666" left="149" width="3" height="6" font="font9" id="p2_t89" reading_order_no="88" segment_no="15" tag_type="formula">i</text>
<text top="659" left="160" width="39" height="12" font="font7" id="p2_t90" reading_order_no="89" segment_no="15" tag_type="formula">} i , D train ]</text>
<text top="676" left="68" width="159" height="12" font="font18" id="p2_t91" reading_order_no="90" segment_no="15" tag_type="formula">= I[Ω; Y | x, D train ] − I[Ω; Y | x, { Y eval i</text>
<text top="676" left="227" width="35" height="12" font="font7" id="p2_t92" reading_order_no="91" segment_no="15" tag_type="formula">} i , { x eval i</text>
<text top="676" left="263" width="41" height="11" font="font7" id="p2_t93" reading_order_no="92" segment_no="15" tag_type="formula">} i , D train ] .</text>
<text top="697" left="68" width="221" height="10" font="font16" id="p2_t94" reading_order_no="162" segment_no="16" tag_type="footnote">1 We distinguish the evaluation set D eval from a validation set<a href="deeplearning_paper2.html#2">1</a></text>
<text top="709" left="55" width="185" height="8" font="font16" id="p2_t95" reading_order_no="163" segment_no="16" tag_type="footnote">as the latter is commonly understood to be labelled.<a href="deeplearning_paper2.html#2">, </a>we show a toy example which visualizes the</text>
<text top="147" left="333" width="4" height="7" font="font22" id="p2_t96" reading_order_no="93" segment_no="1" tag_type="figure">50</text>
<text top="147" left="350" width="6" height="7" font="font22" id="p2_t97" reading_order_no="94" segment_no="1" tag_type="figure">100</text>
<text top="147" left="368" width="6" height="7" font="font22" id="p2_t98" reading_order_no="95" segment_no="1" tag_type="figure">150</text>
<text top="147" left="385" width="6" height="7" font="font22" id="p2_t99" reading_order_no="96" segment_no="1" tag_type="figure">200</text>
<text top="147" left="403" width="6" height="7" font="font22" id="p2_t100" reading_order_no="97" segment_no="1" tag_type="figure">250</text>
<text top="147" left="421" width="6" height="7" font="font22" id="p2_t101" reading_order_no="98" segment_no="1" tag_type="figure">300</text>
<text top="151" left="360" width="29" height="7" font="font22" id="p2_t102" reading_order_no="99" segment_no="1" tag_type="figure">Training Set Size</text>
<text top="138" left="312" width="5" height="7" font="font22" id="p2_t103" reading_order_no="100" segment_no="1" tag_type="figure">0.5</text>
<text top="123" left="312" width="5" height="7" font="font22" id="p2_t104" reading_order_no="101" segment_no="1" tag_type="figure">0.6</text>
<text top="109" left="312" width="5" height="7" font="font22" id="p2_t105" reading_order_no="102" segment_no="1" tag_type="figure">0.7</text>
<text top="94" left="312" width="5" height="7" font="font22" id="p2_t106" reading_order_no="103" segment_no="1" tag_type="figure">0.8</text>
<text top="80" left="312" width="5" height="7" font="font22" id="p2_t107" reading_order_no="104" segment_no="1" tag_type="figure">0.9</text>
<text top="112" left="310" width="0" height="7" font="font23" id="p2_t108" reading_order_no="105" segment_no="1" tag_type="figure">Accuracy</text>
<text top="64" left="357" width="34" height="7" font="font22" id="p2_t109" reading_order_no="106" segment_no="1" tag_type="figure">Acquisition Size = 5</text>
<text top="147" left="446" width="4" height="7" font="font22" id="p2_t110" reading_order_no="107" segment_no="1" tag_type="figure">50</text>
<text top="147" left="462" width="6" height="7" font="font22" id="p2_t111" reading_order_no="108" segment_no="1" tag_type="figure">100</text>
<text top="147" left="480" width="6" height="7" font="font22" id="p2_t112" reading_order_no="109" segment_no="1" tag_type="figure">150</text>
<text top="147" left="498" width="6" height="7" font="font22" id="p2_t113" reading_order_no="110" segment_no="1" tag_type="figure">200</text>
<text top="147" left="516" width="6" height="7" font="font22" id="p2_t114" reading_order_no="111" segment_no="1" tag_type="figure">250</text>
<text top="147" left="533" width="6" height="7" font="font22" id="p2_t115" reading_order_no="112" segment_no="1" tag_type="figure">300</text>
<text top="151" left="473" width="29" height="7" font="font22" id="p2_t116" reading_order_no="113" segment_no="1" tag_type="figure">Training Set Size</text>
<text top="64" left="469" width="36" height="7" font="font22" id="p2_t117" reading_order_no="114" segment_no="1" tag_type="figure">Acquisition Size = 10</text>
<text top="152" left="420" width="34" height="7" font="font22" id="p2_t118" reading_order_no="115" segment_no="1" tag_type="figure">Acquisition Function</text>
<text top="157" left="354" width="39" height="7" font="font22" id="p2_t119" reading_order_no="116" segment_no="1" tag_type="figure">BatchEPIG-BALD (ours)</text>
<text top="157" left="410" width="19" height="7" font="font22" id="p2_t120" reading_order_no="117" segment_no="1" tag_type="figure">BatchBALD</text>
<text top="157" left="445" width="29" height="7" font="font22" id="p2_t121" reading_order_no="118" segment_no="1" tag_type="figure">EPIG-BALD (ours)</text>
<text top="157" left="490" width="14" height="7" font="font22" id="p2_t122" reading_order_no="119" segment_no="1" tag_type="figure">Uniform</text>
<text top="157" left="520" width="9" height="7" font="font22" id="p2_t123" reading_order_no="120" segment_no="1" tag_type="figure">BALD</text>
<text top="171" left="307" width="235" height="8" font="font21" id="p2_t124" reading_order_no="121" segment_no="5" tag_type="text">Figure 3. (Batch)BALD vs (Batch)EPIG-BALD on RepeatedMNIST</text>
<text top="182" left="307" width="171" height="8" font="font17" id="p2_t125" reading_order_no="122" segment_no="5" tag_type="text">(MNISTx2) with batch acquisition size 5.</text>
<text top="182" left="493" width="48" height="8" font="font16" id="p2_t126" reading_order_no="123" segment_no="5" tag_type="text">EPIG-BALD</text>
<text top="193" left="307" width="234" height="8" font="font16" id="p2_t127" reading_order_no="124" segment_no="5" tag_type="text">outperforms BALD. However, EPIG does not perform well with</text>
<text top="204" left="307" width="99" height="8" font="font16" id="p2_t128" reading_order_no="125" segment_no="5" tag_type="text">BNNs as explained in § 4.2 .</text>
<text top="225" left="307" width="234" height="9" font="font4" id="p2_t129" reading_order_no="126" segment_no="7" tag_type="text">The first BALD term measures the epistemic uncertainty of</text>
<text top="237" left="307" width="235" height="9" font="font4" id="p2_t130" reading_order_no="127" segment_no="7" tag_type="text">the model at x . The second BALD term is a conditional</text>
<text top="248" left="307" width="225" height="12" font="font4" id="p2_t131" reading_order_no="128" segment_no="7" tag_type="text">mutual information over pseudo-label variables { Y eval i</text>
<text top="248" left="533" width="8" height="11" font="font7" id="p2_t132" reading_order_no="129" segment_no="7" tag_type="text">} i</text>
<text top="260" left="307" width="118" height="12" font="font4" id="p2_t133" reading_order_no="130" segment_no="7" tag_type="text">for the evaluation set { x eval i</text>
<text top="260" left="426" width="11" height="11" font="font7" id="p2_t134" reading_order_no="131" segment_no="7" tag_type="text">} i .</text>
<text top="261" left="446" width="96" height="9" font="font4" id="p2_t135" reading_order_no="132" segment_no="7" tag_type="text">We will show that we</text>
<text top="273" left="307" width="234" height="9" font="font4" id="p2_t136" reading_order_no="133" segment_no="7" tag_type="text">can approximate this term tractably by training a separate</text>
<text top="283" left="307" width="234" height="11" font="font4" id="p2_t137" reading_order_no="134" segment_no="7" tag_type="text">model using self-distillation 2 . As BALD captures epistemic</text>
<text top="297" left="307" width="234" height="9" font="font4" id="p2_t138" reading_order_no="135" segment_no="7" tag_type="text">uncertainty, intuitively, the first term is large when the model</text>
<text top="309" left="307" width="236" height="9" font="font4" id="p2_t139" reading_order_no="136" segment_no="7" tag_type="text">has high epistemic uncertainty about its prediction at x ,</text>
<text top="321" left="307" width="234" height="9" font="font4" id="p2_t140" reading_order_no="137" segment_no="7" tag_type="text">and learning the true label would thus be informative for</text>
<text top="333" left="307" width="234" height="9" font="font4" id="p2_t141" reading_order_no="138" segment_no="7" tag_type="text">the model, while the second term captures the epistemic</text>
<text top="345" left="307" width="234" height="9" font="font4" id="p2_t142" reading_order_no="139" segment_no="7" tag_type="text">uncertainty about the model’s prediction at x assuming we</text>
<text top="357" left="307" width="234" height="9" font="font4" id="p2_t143" reading_order_no="140" segment_no="7" tag_type="text">had obtained labels for the evaluation set. This second term</text>
<text top="369" left="307" width="234" height="9" font="font4" id="p2_t144" reading_order_no="141" segment_no="7" tag_type="text">is small when x is similar to the evaluation set and the</text>
<text top="381" left="307" width="234" height="9" font="font4" id="p2_t145" reading_order_no="142" segment_no="7" tag_type="text">model can explain it well given the pseudo-labels. Together</text>
<text top="393" left="307" width="234" height="9" font="font4" id="p2_t146" reading_order_no="143" segment_no="7" tag_type="text">EPIG-BALD (and thus EPIG) is large when the first term is</text>
<text top="404" left="307" width="234" height="10" font="font4" id="p2_t147" reading_order_no="144" segment_no="7" tag_type="text">large and the second term is small, so the sample x is both</text>
<text top="417" left="307" width="234" height="9" font="font4" id="p2_t148" reading_order_no="145" segment_no="7" tag_type="text">informative for the model and similar to an element in the</text>
<text top="428" left="307" width="57" height="9" font="font4" id="p2_t149" reading_order_no="146" segment_no="7" tag_type="text">evaluation set.</text>
<text top="446" left="307" width="234" height="9" font="font4" id="p2_t150" reading_order_no="147" segment_no="10" tag_type="text">Beyond introducing a novel and intuitive acquisition</text>
<text top="458" left="307" width="234" height="9" font="font4" id="p2_t151" reading_order_no="148" segment_no="10" tag_type="text">function, we show that we outperform BALD and other</text>
<text top="470" left="307" width="234" height="9" font="font4" id="p2_t152" reading_order_no="149" segment_no="10" tag_type="text">acquisition functions both in standard experimental settings</text>
<text top="482" left="307" width="234" height="9" font="font4" id="p2_t153" reading_order_no="150" segment_no="10" tag_type="text">with high-dimensional data as well as new experimental</text>
<text top="494" left="307" width="234" height="9" font="font4" id="p2_t154" reading_order_no="151" segment_no="10" tag_type="text">settings in which there is a distribution shift between the</text>
<text top="506" left="307" width="234" height="9" font="font4" id="p2_t155" reading_order_no="152" segment_no="10" tag_type="text">pool set and test set distribution. When the pool set is</text>
<text top="518" left="307" width="236" height="9" font="font4" id="p2_t156" reading_order_no="153" segment_no="10" tag_type="text">imbalanced or contains junk or noisy data, state-of-the-</text>
<text top="530" left="307" width="234" height="9" font="font4" id="p2_t157" reading_order_no="154" segment_no="10" tag_type="text">art acquisition functions repeatedly select junk samples</text>
<text top="542" left="307" width="234" height="9" font="font4" id="p2_t158" reading_order_no="155" segment_no="10" tag_type="text">and waste acquisitions on OoD or noisy data, while our</text>
<text top="554" left="307" width="234" height="9" font="font4" id="p2_t159" reading_order_no="156" segment_no="10" tag_type="text">method performs well and follows the distribution of the</text>
<text top="566" left="307" width="234" height="9" font="font4" id="p2_t160" reading_order_no="157" segment_no="10" tag_type="text">evaluation set. This allows EPIG-BALD to work directly</text>
<text top="578" left="307" width="234" height="9" font="font4" id="p2_t161" reading_order_no="158" segment_no="10" tag_type="text">from unfiltered pool set data as long as an unlabelled curated</text>
<text top="590" left="307" width="234" height="9" font="font4" id="p2_t162" reading_order_no="159" segment_no="10" tag_type="text">evaluation set of smaller size can be provided. This is of</text>
<text top="602" left="307" width="234" height="9" font="font4" id="p2_t163" reading_order_no="160" segment_no="10" tag_type="text">particular interest for practical applications as data cleaning</text>
<text top="614" left="307" width="102" height="9" font="font4" id="p2_t164" reading_order_no="161" segment_no="10" tag_type="text">and filtering is expensive.</text>
<text top="632" left="320" width="222" height="9" font="font16" id="p2_t165" reading_order_no="164" segment_no="14" tag_type="footnote">2 If we had labels for the evaluation set, we could train with</text>
<text top="643" left="307" width="234" height="8" font="font16" id="p2_t166" reading_order_no="165" segment_no="14" tag_type="footnote">those. However, this would reduce the label efficiency of our active</text>
<text top="653" left="307" width="69" height="8" font="font16" id="p2_t167" reading_order_no="166" segment_no="14" tag_type="footnote">learning algorithm.</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font27" size="10" family="CMEX10" color="#000000"/>
	<fontspec id="font28" size="10" family="MSBM10" color="#000000"/>
	<fontspec id="font29" size="5" family="CMMI5" color="#000000"/>
	<fontspec id="font30" size="5" family="CMSY5" color="#000000"/>
	<fontspec id="font31" size="5" family="CMR5" color="#000000"/>
	<fontspec id="font32" size="10" family="CMMIB10" color="#000000"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="69" left="55" width="83" height="11" font="font3" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="title">2. Related Work</text>
<text top="90" left="55" width="234" height="9" font="font4" id="p3_t3" reading_order_no="2" segment_no="4" tag_type="text">The Expected Information Gain was introduced in Bayesian</text>
<text top="102" left="55" width="199" height="9" font="font4" id="p3_t4" reading_order_no="3" segment_no="4" tag_type="text">optimal experiment design by Lindley ( 1956 ).</text>
<text top="102" left="265" width="25" height="9" font="font4" id="p3_t5" reading_order_no="4" segment_no="4" tag_type="text">While<a href="deeplearning_paper2.html#7">Lindley</a></text>
<text top="114" left="55" width="234" height="9" font="font4" id="p3_t6" reading_order_no="5" segment_no="4" tag_type="text">Expected Information Gain focuses on the formulation<a href="deeplearning_paper2.html#7">(</a></text>
<text top="126" left="55" width="234" height="9" font="font4" id="p3_t7" reading_order_no="6" segment_no="4" tag_type="text">of the mutual information term as a reduction in model<a href="deeplearning_paper2.html#7">1956</a></text>
<text top="137" left="55" width="234" height="9" font="font4" id="p3_t8" reading_order_no="7" segment_no="4" tag_type="text">posterior uncertainty given a potential sample, in active<a href="deeplearning_paper2.html#7">).</a></text>
<text top="149" left="55" width="234" height="9" font="font4" id="p3_t9" reading_order_no="8" segment_no="4" tag_type="text">learning, BALD was introduced as a tractable formulation</text>
<text top="161" left="55" width="234" height="9" font="font4" id="p3_t10" reading_order_no="9" segment_no="4" tag_type="text">of the same term, with the focus on measuring the level</text>
<text top="173" left="55" width="234" height="9" font="font4" id="p3_t11" reading_order_no="10" segment_no="4" tag_type="text">of model disagreement for a given sample as a proxy of</text>
<text top="185" left="55" width="236" height="9" font="font4" id="p3_t12" reading_order_no="11" segment_no="4" tag_type="text">epistemic uncertainty ( Houlsby et al. , 2011 ). For high-</text>
<text top="197" left="55" width="234" height="9" font="font4" id="p3_t13" reading_order_no="12" segment_no="4" tag_type="text">dimensional data, BALD has been extended to Bayesian</text>
<text top="209" left="55" width="235" height="9" font="font4" id="p3_t14" reading_order_no="13" segment_no="4" tag_type="text">deep learning models using Monte-Carlo dropout ( Gal</text>
<text top="221" left="55" width="235" height="9" font="font6" id="p3_t15" reading_order_no="14" segment_no="4" tag_type="text">et al. , 2017 ). BALD was further extended to BatchBALD</text>
<text top="233" left="55" width="234" height="9" font="font4" id="p3_t16" reading_order_no="15" segment_no="4" tag_type="text">to correctly capture redundancies in the batch acquisition<a href="deeplearning_paper2.html#7">(</a></text>
<text top="245" left="55" width="112" height="9" font="font4" id="p3_t17" reading_order_no="16" segment_no="4" tag_type="text">setting ( Kirsch et al. , 2019 ).<a href="deeplearning_paper2.html#7">Houlsby et al.</a></text>
<text top="263" left="55" width="234" height="9" font="font4" id="p3_t18" reading_order_no="17" segment_no="8" tag_type="text">The predictive information as mutual information between<a href="deeplearning_paper2.html#7">,</a></text>
<text top="275" left="55" width="234" height="9" font="font4" id="p3_t19" reading_order_no="18" segment_no="8" tag_type="text">the past and future was introduced by Bialek and Tishby<a href="deeplearning_paper2.html#7">2011</a></text>
<text top="287" left="55" width="235" height="9" font="font4" id="p3_t20" reading_order_no="19" segment_no="8" tag_type="text">( 1999 ) and has been used in reinforcement learning to<a href="deeplearning_paper2.html#7">). </a>For high-</text>
<text top="299" left="55" width="235" height="9" font="font4" id="p3_t21" reading_order_no="20" segment_no="8" tag_type="text">increase sample efficienc ( Lee et al. , 2020 ). In Bayesian</text>
<text top="311" left="55" width="234" height="9" font="font4" id="p3_t22" reading_order_no="21" segment_no="8" tag_type="text">optimization with Gaussian Processes, an information gain<a href="deeplearning_paper2.html#7">(</a></text>
<text top="323" left="55" width="234" height="9" font="font4" id="p3_t23" reading_order_no="22" segment_no="8" tag_type="text">is usually computed between potential query candidates and<a href="deeplearning_paper2.html#7">Gal</a></text>
<text top="335" left="55" width="199" height="9" font="font4" id="p3_t24" reading_order_no="23" segment_no="8" tag_type="text">the already acquired points ( Srinivas et al. , 2009 ).<a href="deeplearning_paper2.html#7">et al.</a></text>
<text top="353" left="55" width="234" height="9" font="font4" id="p3_t25" reading_order_no="24" segment_no="11" tag_type="text">To our knowledge, within (Bayesian) Active Learning, EPIG<a href="deeplearning_paper2.html#7">,</a></text>
<text top="365" left="55" width="235" height="9" font="font4" id="p3_t26" reading_order_no="25" segment_no="11" tag_type="text">and EPIG-BALD are novel and have not been examined,<a href="deeplearning_paper2.html#7">2017</a></text>
<text top="377" left="55" width="44" height="9" font="font4" id="p3_t27" reading_order_no="26" segment_no="11" tag_type="text">previously.<a href="deeplearning_paper2.html#7">). </a>BALD was further extended to BatchBALD</text>
<text top="404" left="55" width="75" height="11" font="font3" id="p3_t28" reading_order_no="27" segment_no="13" tag_type="title">3. Background</text>
<text top="425" left="55" width="234" height="9" font="font4" id="p3_t29" reading_order_no="28" segment_no="14" tag_type="text">In this section, we revisit Bayesian deep learning and active<a href="deeplearning_paper2.html#7">(</a></text>
<text top="436" left="55" width="35" height="9" font="font4" id="p3_t30" reading_order_no="29" segment_no="14" tag_type="text">learning.<a href="deeplearning_paper2.html#7">Kirsch et al.</a></text>
<text top="454" left="55" width="123" height="9" font="font1" id="p3_t31" reading_order_no="30" segment_no="15" tag_type="text">Bayesian Neural Networks.<a href="deeplearning_paper2.html#7">,</a></text>
<text top="454" left="192" width="98" height="9" font="font4" id="p3_t32" reading_order_no="31" segment_no="15" tag_type="text">The model parameters<a href="deeplearning_paper2.html#7">2019</a></text>
<text top="466" left="55" width="12" height="9" font="font4" id="p3_t33" reading_order_no="32" segment_no="15" tag_type="text">are<a href="deeplearning_paper2.html#7">).</a></text>
<text top="466" left="77" width="28" height="9" font="font4" id="p3_t34" reading_order_no="33" segment_no="15" tag_type="text">treated</text>
<text top="466" left="114" width="8" height="9" font="font4" id="p3_t35" reading_order_no="34" segment_no="15" tag_type="text">as</text>
<text top="466" left="131" width="5" height="9" font="font4" id="p3_t36" reading_order_no="35" segment_no="15" tag_type="text">a<a href="deeplearning_paper2.html#7">Bialek and Tishby</a></text>
<text top="466" left="145" width="31" height="9" font="font4" id="p3_t37" reading_order_no="36" segment_no="15" tag_type="text">random<a href="deeplearning_paper2.html#7">(</a></text>
<text top="466" left="185" width="32" height="9" font="font4" id="p3_t38" reading_order_no="37" segment_no="15" tag_type="text">variable<a href="deeplearning_paper2.html#7">1999</a></text>
<text top="466" left="226" width="7" height="9" font="font18" id="p3_t39" reading_order_no="38" segment_no="15" tag_type="text">Ω<a href="deeplearning_paper2.html#7">) </a>and has been used in reinforcement learning to</text>
<text top="466" left="243" width="18" height="9" font="font4" id="p3_t40" reading_order_no="39" segment_no="15" tag_type="text">with<a href="deeplearning_paper2.html#7">(</a></text>
<text top="466" left="270" width="20" height="9" font="font4" id="p3_t41" reading_order_no="40" segment_no="15" tag_type="text">prior<a href="deeplearning_paper2.html#7">Lee et al.</a></text>
<text top="476" left="55" width="235" height="11" font="font4" id="p3_t42" reading_order_no="41" segment_no="15" tag_type="text">distribution p( ω ) . We denote the training set D train =<a href="deeplearning_paper2.html#7">,</a></text>
<text top="489" left="55" width="28" height="12" font="font7" id="p3_t43" reading_order_no="42" segment_no="15" tag_type="text">{ ( x train i<a href="deeplearning_paper2.html#7">2020</a></text>
<text top="489" left="83" width="23" height="12" font="font8" id="p3_t44" reading_order_no="43" segment_no="15" tag_type="text">, y train i<a href="deeplearning_paper2.html#7">). </a>In Bayesian</text>
<text top="490" left="106" width="58" height="11" font="font18" id="p3_t45" reading_order_no="44" segment_no="15" tag_type="text">) } 1 ,...,i ∈| D train | ,</text>
<text top="490" left="173" width="25" height="9" font="font4" id="p3_t46" reading_order_no="45" segment_no="15" tag_type="text">where</text>
<text top="489" left="206" width="24" height="12" font="font7" id="p3_t47" reading_order_no="46" segment_no="15" tag_type="text">{ x train i<a href="deeplearning_paper2.html#7">(</a></text>
<text top="490" left="230" width="58" height="11" font="font7" id="p3_t48" reading_order_no="47" segment_no="15" tag_type="text">} i ∈{ 1 ,..., | D train |}<a href="deeplearning_paper2.html#7">Srinivas et al.</a></text>
<text top="502" left="55" width="132" height="12" font="font4" id="p3_t49" reading_order_no="48" segment_no="15" tag_type="text">are the input samples and { y train i<a href="deeplearning_paper2.html#7">,</a></text>
<text top="503" left="188" width="101" height="11" font="font7" id="p3_t50" reading_order_no="49" segment_no="15" tag_type="text">} i ∈{ 1 ,..., | D train |} the labels<a href="deeplearning_paper2.html#7">2009</a></text>
<text top="515" left="55" width="235" height="10" font="font4" id="p3_t51" reading_order_no="50" segment_no="15" tag_type="text">or targets. The probabilistic model is p( y, x, ω ) = p( y |<a href="deeplearning_paper2.html#7">).</a></text>
<text top="527" left="55" width="234" height="10" font="font8" id="p3_t52" reading_order_no="51" segment_no="15" tag_type="text">x, ω ) p( ω ) p( x ) , where x and y are outcomes for the</text>
<text top="539" left="55" width="235" height="9" font="font4" id="p3_t53" reading_order_no="52" segment_no="15" tag_type="text">random variables X and Y denoting the input and label,</text>
<text top="551" left="55" width="51" height="9" font="font4" id="p3_t54" reading_order_no="53" segment_no="15" tag_type="text">respectively.</text>
<text top="551" left="116" width="173" height="9" font="font4" id="p3_t55" reading_order_no="54" segment_no="15" tag_type="text">We are only interested in discriminative</text>
<text top="563" left="55" width="235" height="9" font="font4" id="p3_t56" reading_order_no="55" segment_no="15" tag_type="text">models and do not explicitly model p( x ) but use empirical</text>
<text top="575" left="55" width="114" height="9" font="font4" id="p3_t57" reading_order_no="56" segment_no="15" tag_type="text">sample distributions instead.</text>
<text top="593" left="55" width="234" height="9" font="font4" id="p3_t58" reading_order_no="57" segment_no="18" tag_type="text">To include multiple labels and inputs, we expand the model</text>
<text top="605" left="55" width="235" height="10" font="font4" id="p3_t59" reading_order_no="58" segment_no="18" tag_type="text">to joints of random variables { x i } i ∈ I and { y i } i ∈ I obtaining</text>
<text top="624" left="74" width="99" height="13" font="font18" id="p3_t60" reading_order_no="59" segment_no="20" tag_type="formula">p( { y i } i , { x i } i , ω ) = Y</text>
<text top="641" left="160" width="12" height="7" font="font9" id="p3_t61" reading_order_no="60" segment_no="20" tag_type="formula">i ∈ I</text>
<text top="626" left="174" width="97" height="11" font="font18" id="p3_t62" reading_order_no="61" segment_no="20" tag_type="formula">p( y i | x i , ω ) p( x i ) p( ω ) .</text>
<text top="662" left="55" width="235" height="12" font="font4" id="p3_t63" reading_order_no="62" segment_no="21" tag_type="text">The posterior parameter distribution p( ω | D train ) is</text>
<text top="674" left="55" width="236" height="11" font="font4" id="p3_t64" reading_order_no="63" segment_no="21" tag_type="text">determined via Bayesian inference. We obtain p( ω | D train )</text>
<text top="688" left="55" width="90" height="9" font="font4" id="p3_t65" reading_order_no="64" segment_no="21" tag_type="text">using Bayes’ theorem:</text>
<text top="706" left="81" width="96" height="11" font="font18" id="p3_t66" reading_order_no="65" segment_no="22" tag_type="formula">p( ω | D train ) ∝ p( { y train</text>
<text top="713" left="163" width="3" height="6" font="font9" id="p3_t67" reading_order_no="66" segment_no="22" tag_type="formula">i</text>
<text top="706" left="177" width="39" height="12" font="font7" id="p3_t68" reading_order_no="67" segment_no="22" tag_type="formula">} i | { x train</text>
<text top="713" left="203" width="3" height="6" font="font9" id="p3_t69" reading_order_no="68" segment_no="22" tag_type="formula">i</text>
<text top="708" left="216" width="48" height="10" font="font7" id="p3_t70" reading_order_no="69" segment_no="22" tag_type="formula">} i , ω ) p( ω ) .</text>
<text top="70" left="307" width="216" height="9" font="font4" id="p3_t71" reading_order_no="70" segment_no="2" tag_type="text">which allows for predictions by marginalizing over Ω :</text>
<text top="87" left="338" width="173" height="13" font="font18" id="p3_t72" reading_order_no="71" segment_no="3" tag_type="formula">p( y | x, D train ) = E ω ∼ p( ω | D train ) p( y | x, ω ) .</text>
<text top="109" left="307" width="235" height="9" font="font4" id="p3_t73" reading_order_no="72" segment_no="5" tag_type="text">Exact Bayesian inference is intractable in deep learning,</text>
<text top="121" left="307" width="234" height="9" font="font4" id="p3_t74" reading_order_no="73" segment_no="5" tag_type="text">and we use variational inference for approximate inference</text>
<text top="133" left="307" width="235" height="9" font="font4" id="p3_t75" reading_order_no="74" segment_no="5" tag_type="text">using a variational distribution q( ω ) . We determine q( ω ) by</text>
<text top="145" left="307" width="166" height="9" font="font4" id="p3_t76" reading_order_no="75" segment_no="5" tag_type="text">minimizing the following KL divergence:</text>
<text top="165" left="319" width="116" height="11" font="font18" id="p3_t77" reading_order_no="76" segment_no="6" tag_type="formula">D KL (q( ω ) k p( ω | D train )) =</text>
<text top="181" left="340" width="95" height="12" font="font18" id="p3_t78" reading_order_no="77" segment_no="6" tag_type="formula">= E q( ω ) [ − log p(( { y train</text>
<text top="188" left="421" width="3" height="6" font="font9" id="p3_t79" reading_order_no="78" segment_no="6" tag_type="formula">i</text>
<text top="181" left="435" width="47" height="12" font="font7" id="p3_t80" reading_order_no="79" segment_no="6" tag_type="formula">} i ) | ( { x train</text>
<text top="188" left="469" width="3" height="6" font="font9" id="p3_t81" reading_order_no="80" segment_no="6" tag_type="formula">i</text>
<text top="183" left="482" width="27" height="10" font="font7" id="p3_t82" reading_order_no="81" segment_no="6" tag_type="formula">} i ) , ω )</text>
<text top="196" left="384" width="4" height="4" font="font27" id="p3_t83" reading_order_no="83" segment_no="6" tag_type="formula">|</text>
<text top="196" left="442" width="9" height="4" font="font27" id="p3_t84" reading_order_no="84" segment_no="6" tag_type="formula">{z</text>
<text top="196" left="505" width="4" height="4" font="font27" id="p3_t85" reading_order_no="85" segment_no="6" tag_type="formula">}</text>
<text top="201" left="432" width="28" height="6" font="font2" id="p3_t86" reading_order_no="86" segment_no="6" tag_type="formula">likelihood</text>
<text top="183" left="509" width="3" height="9" font="font18" id="p3_t87" reading_order_no="82" segment_no="6" tag_type="formula">]</text>
<text top="212" left="363" width="86" height="10" font="font18" id="p3_t88" reading_order_no="87" segment_no="6" tag_type="formula">+ D KL (q( ω ) k p( ω ))</text>
<text top="226" left="373" width="4" height="4" font="font27" id="p3_t89" reading_order_no="88" segment_no="6" tag_type="formula">|</text>
<text top="226" left="407" width="9" height="4" font="font27" id="p3_t90" reading_order_no="89" segment_no="6" tag_type="formula">{z</text>
<text top="226" left="445" width="4" height="4" font="font27" id="p3_t91" reading_order_no="90" segment_no="6" tag_type="formula">}</text>
<text top="230" left="384" width="54" height="6" font="font2" id="p3_t92" reading_order_no="91" segment_no="6" tag_type="formula">prior regularization</text>
<text top="210" left="451" width="60" height="12" font="font18" id="p3_t93" reading_order_no="92" segment_no="6" tag_type="formula">+ log p( D train )</text>
<text top="226" left="460" width="4" height="4" font="font27" id="p3_t94" reading_order_no="93" segment_no="6" tag_type="formula">|</text>
<text top="226" left="481" width="9" height="4" font="font27" id="p3_t95" reading_order_no="94" segment_no="6" tag_type="formula">{z</text>
<text top="226" left="463" width="47" height="10" font="font27" id="p3_t96" reading_order_no="95" segment_no="6" tag_type="formula">} model evidence</text>
<text top="212" left="513" width="19" height="10" font="font7" id="p3_t97" reading_order_no="96" segment_no="6" tag_type="formula">≥ 0 .</text>
<text top="245" left="307" width="236" height="9" font="font4" id="p3_t98" reading_order_no="97" segment_no="7" tag_type="text">We can use the local reparameterization trick and Monte-</text>
<text top="257" left="307" width="200" height="9" font="font4" id="p3_t99" reading_order_no="98" segment_no="7" tag_type="text">Carlo dropout for q( ω ) in a deep learning context.</text>
<text top="275" left="307" width="234" height="9" font="font1" id="p3_t100" reading_order_no="99" segment_no="9" tag_type="text">Batch Active Learning. Generally, samples are acquired in</text>
<text top="287" left="307" width="234" height="9" font="font4" id="p3_t101" reading_order_no="100" segment_no="9" tag_type="text">batches to avoid retraining the model all the time. We score</text>
<text top="298" left="307" width="234" height="10" font="font4" id="p3_t102" reading_order_no="101" segment_no="9" tag_type="text">possible candidate batches x batch of acquisition batch size b</text>
<text top="310" left="307" width="151" height="12" font="font4" id="p3_t103" reading_order_no="102" segment_no="9" tag_type="text">using an acquisition function a ( { x batch i</text>
<text top="309" left="459" width="82" height="12" font="font7" id="p3_t104" reading_order_no="103" segment_no="9" tag_type="text">} i , p( ω | D train )) and</text>
<text top="323" left="307" width="122" height="9" font="font4" id="p3_t105" reading_order_no="104" segment_no="9" tag_type="text">pick the highest scoring batch:</text>
<text top="342" left="357" width="34" height="9" font="font18" id="p3_t106" reading_order_no="105" segment_no="10" tag_type="formula">arg max</text>
<text top="353" left="334" width="19" height="8" font="font10" id="p3_t107" reading_order_no="106" segment_no="10" tag_type="formula">{ x batch i</text>
<text top="353" left="354" width="59" height="8" font="font10" id="p3_t108" reading_order_no="107" segment_no="10" tag_type="formula">} i ∈{ 1 ,...,b } ⊆ D pool</text>
<text top="340" left="414" width="34" height="11" font="font8" id="p3_t109" reading_order_no="108" segment_no="10" tag_type="formula">a ( { x batch</text>
<text top="347" left="433" width="3" height="6" font="font9" id="p3_t110" reading_order_no="109" segment_no="10" tag_type="formula">i</text>
<text top="340" left="449" width="66" height="12" font="font7" id="p3_t111" reading_order_no="110" segment_no="10" tag_type="formula">} i , p( ω | D train ))</text>
<text top="376" left="307" width="234" height="9" font="font4" id="p3_t112" reading_order_no="111" segment_no="12" tag_type="text">BALD was introduced as a one-sample acquisition function</text>
<text top="388" left="307" width="234" height="9" font="font4" id="p3_t113" reading_order_no="112" segment_no="12" tag_type="text">of the expected information gain between the prediction</text>
<text top="399" left="307" width="23" height="10" font="font8" id="p3_t114" reading_order_no="113" segment_no="12" tag_type="text">Y batch</text>
<text top="399" left="338" width="83" height="10" font="font4" id="p3_t115" reading_order_no="114" segment_no="12" tag_type="text">for an input x batch</text>
<text top="400" left="429" width="113" height="9" font="font4" id="p3_t116" reading_order_no="115" segment_no="12" tag_type="text">and the stochastic model</text>
<text top="410" left="307" width="235" height="11" font="font4" id="p3_t117" reading_order_no="116" segment_no="12" tag_type="text">parameters Ω : I[ Y batch ; Ω | x batch , D train ] . This was trivially</text>
<text top="424" left="307" width="234" height="9" font="font4" id="p3_t118" reading_order_no="117" segment_no="12" tag_type="text">extended to batch acquisition by selecting the top-k highest</text>
<text top="436" left="307" width="160" height="9" font="font4" id="p3_t119" reading_order_no="118" segment_no="12" tag_type="text">scorers as a batch ( Gal et al. , 2017 ).</text>
<text top="436" left="477" width="66" height="9" font="font4" id="p3_t120" reading_order_no="119" segment_no="12" tag_type="text">In Kirsch et al.</text>
<text top="448" left="307" width="235" height="9" font="font4" id="p3_t121" reading_order_no="120" segment_no="12" tag_type="text">( 2019 ), this approach was shown to lead to the selection</text>
<text top="460" left="307" width="234" height="9" font="font4" id="p3_t122" reading_order_no="121" segment_no="12" tag_type="text">of redundant samples, and instead the one-sample case was</text>
<text top="472" left="307" width="234" height="9" font="font4" id="p3_t123" reading_order_no="122" segment_no="12" tag_type="text">canonically extended to the batch acquisition case using</text>
<text top="484" left="307" width="234" height="9" font="font4" id="p3_t124" reading_order_no="123" segment_no="12" tag_type="text">the expected information gain between the joint of the</text>
<text top="494" left="307" width="75" height="11" font="font4" id="p3_t125" reading_order_no="124" segment_no="12" tag_type="text">predictions { Y batch</text>
<text top="501" left="365" width="3" height="6" font="font9" id="p3_t126" reading_order_no="125" segment_no="12" tag_type="text">i</text>
<text top="494" left="382" width="134" height="11" font="font7" id="p3_t127" reading_order_no="126" segment_no="12" tag_type="text">} i for the batch candidates { x batch</text>
<text top="501" left="501" width="3" height="6" font="font9" id="p3_t128" reading_order_no="127" segment_no="12" tag_type="text">i</text>
<text top="495" left="516" width="25" height="10" font="font7" id="p3_t129" reading_order_no="128" segment_no="12" tag_type="text">} i and</text>
<text top="508" left="307" width="155" height="9" font="font4" id="p3_t130" reading_order_no="129" segment_no="12" tag_type="text">the model parameters Ω ( BatchBALD ):</text>
<text top="528" left="317" width="71" height="12" font="font8" id="p3_t131" reading_order_no="130" segment_no="16" tag_type="formula">a BatchBALD ( { x batch</text>
<text top="535" left="373" width="3" height="6" font="font9" id="p3_t132" reading_order_no="131" segment_no="16" tag_type="formula">i</text>
<text top="528" left="388" width="80" height="12" font="font7" id="p3_t133" reading_order_no="132" segment_no="16" tag_type="formula">} i , p(Ω | D train )) :=</text>
<text top="545" left="357" width="45" height="12" font="font18" id="p3_t134" reading_order_no="133" segment_no="16" tag_type="formula">= I[ { Y batch i</text>
<text top="545" left="403" width="53" height="12" font="font7" id="p3_t135" reading_order_no="134" segment_no="16" tag_type="formula">} i ; Ω | { x batch i</text>
<text top="545" left="456" width="38" height="11" font="font7" id="p3_t136" reading_order_no="135" segment_no="16" tag_type="formula">} i , D train ]</text>
<text top="561" left="329" width="40" height="11" font="font18" id="p3_t137" reading_order_no="136" segment_no="16" tag_type="formula">= I[ Y batch</text>
<text top="568" left="352" width="4" height="6" font="font11" id="p3_t138" reading_order_no="137" segment_no="16" tag_type="formula">1</text>
<text top="561" left="370" width="45" height="11" font="font8" id="p3_t139" reading_order_no="138" segment_no="16" tag_type="formula">, . . . , Y batch</text>
<text top="568" left="398" width="4" height="6" font="font9" id="p3_t140" reading_order_no="139" segment_no="16" tag_type="formula">b</text>
<text top="561" left="415" width="40" height="11" font="font18" id="p3_t141" reading_order_no="140" segment_no="16" tag_type="formula">; Ω | x batch</text>
<text top="568" left="440" width="4" height="6" font="font11" id="p3_t142" reading_order_no="141" segment_no="16" tag_type="formula">1</text>
<text top="561" left="456" width="42" height="11" font="font8" id="p3_t143" reading_order_no="142" segment_no="16" tag_type="formula">, . . . , x batch</text>
<text top="568" left="483" width="4" height="6" font="font9" id="p3_t144" reading_order_no="143" segment_no="16" tag_type="formula">b</text>
<text top="561" left="499" width="33" height="11" font="font8" id="p3_t145" reading_order_no="144" segment_no="16" tag_type="formula">, D train ] .</text>
<text top="583" left="307" width="234" height="9" font="font4" id="p3_t146" reading_order_no="145" segment_no="17" tag_type="text">In practice, samples and batches are selected greedily</text>
<text top="595" left="307" width="234" height="9" font="font4" id="p3_t147" reading_order_no="146" segment_no="17" tag_type="text">because the information gain is submodular, and greedy</text>
<text top="605" left="307" width="70" height="11" font="font4" id="p3_t148" reading_order_no="147" segment_no="17" tag_type="text">selection is 1 − 1</text>
<text top="607" left="373" width="155" height="11" font="font4" id="p3_t149" reading_order_no="148" segment_no="17" tag_type="text">e optimal ( Krause and Golovin , 2014 ).</text>
<text top="623" left="307" width="108" height="12" font="font1" id="p3_t150" reading_order_no="149" segment_no="19" tag_type="text">Notation. Instead of { Y eval i</text>
<text top="623" left="416" width="35" height="12" font="font7" id="p3_t151" reading_order_no="150" segment_no="19" tag_type="text">} i , { x eval i</text>
<text top="623" left="452" width="90" height="11" font="font7" id="p3_t152" reading_order_no="151" segment_no="19" tag_type="text">} i , we will write Y eval ,</text>
<text top="635" left="307" width="234" height="11" font="font32" id="p3_t153" reading_order_no="152" segment_no="19" tag_type="text">x eval and so on to to cut down on notation. Like above, all</text>
<text top="649" left="307" width="234" height="9" font="font4" id="p3_t154" reading_order_no="153" segment_no="19" tag_type="text">terms can be canonically extended to sets by substituting the</text>
<text top="660" left="307" width="234" height="9" font="font4" id="p3_t155" reading_order_no="154" segment_no="19" tag_type="text">joint. We provide the full derivations in the appendix. Also</text>
<text top="671" left="307" width="235" height="10" font="font4" id="p3_t156" reading_order_no="155" segment_no="19" tag_type="text">note again that lower-case variables like y eval are outcomes</text>
<text top="683" left="307" width="236" height="10" font="font4" id="p3_t157" reading_order_no="156" segment_no="19" tag_type="text">and upper-case variables like Y eval are random variables,</text>
<text top="694" left="307" width="235" height="11" font="font4" id="p3_t158" reading_order_no="157" segment_no="19" tag_type="text">with the exception of the datasets D pool , D train , etc, which</text>
<text top="708" left="307" width="84" height="9" font="font4" id="p3_t159" reading_order_no="158" segment_no="19" tag_type="text">are sets of outcomes.</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font33" size="7" family="CMMIB7" color="#000000"/>
	<fontspec id="font34" size="7" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="69" left="55" width="52" height="11" font="font3" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="title">4. Method</text>
<text top="90" left="55" width="234" height="9" font="font4" id="p4_t3" reading_order_no="2" segment_no="3" tag_type="text">We introduce the novel Expected Predictive Information</text>
<text top="102" left="55" width="235" height="9" font="font5" id="p4_t4" reading_order_no="3" segment_no="3" tag_type="text">Gain (EPIG) acquisition function, provide intuitions, and</text>
<text top="114" left="55" width="234" height="9" font="font4" id="p4_t5" reading_order_no="4" segment_no="3" tag_type="text">show how it can be computed. However, we also argue that</text>
<text top="126" left="55" width="234" height="9" font="font4" id="p4_t6" reading_order_no="5" segment_no="3" tag_type="text">is not feasible to compute EPIG with approximate Bayesian</text>
<text top="137" left="55" width="235" height="9" font="font4" id="p4_t7" reading_order_no="6" segment_no="3" tag_type="text">posteriors in practice, and hence introduce EPIG-BALD,</text>
<text top="149" left="55" width="236" height="9" font="font4" id="p4_t8" reading_order_no="7" segment_no="3" tag_type="text">which works for approximate Bayesian posteriors. Finally,</text>
<text top="161" left="55" width="236" height="9" font="font4" id="p4_t9" reading_order_no="8" segment_no="3" tag_type="text">we present an approximation that allows us to evaluate EPIG-</text>
<text top="173" left="55" width="189" height="9" font="font4" id="p4_t10" reading_order_no="9" segment_no="3" tag_type="text">BALD on Bayesian neural networks efficiently.</text>
<text top="198" left="55" width="180" height="9" font="font1" id="p4_t11" reading_order_no="10" segment_no="5" tag_type="title">4.1. Expected Predictive Information Gain</text>
<text top="217" left="55" width="103" height="9" font="font4" id="p4_t12" reading_order_no="11" segment_no="7" tag_type="text">In supervised learning,</text>
<text top="217" left="168" width="121" height="9" font="font4" id="p4_t13" reading_order_no="12" segment_no="7" tag_type="text">we want to minimize the</text>
<text top="228" left="55" width="237" height="12" font="font4" id="p4_t14" reading_order_no="13" segment_no="7" tag_type="text">generalization loss: H(p data ( X, Y ) k p( Y | X )) , the cross-</text>
<text top="241" left="55" width="234" height="9" font="font4" id="p4_t15" reading_order_no="14" segment_no="7" tag_type="text">entropy between the true test data distribution and our</text>
<text top="253" left="55" width="124" height="9" font="font4" id="p4_t16" reading_order_no="15" segment_no="7" tag_type="text">model’s predictive distribution:</text>
<text top="274" left="67" width="215" height="12" font="font18" id="p4_t17" reading_order_no="16" segment_no="10" tag_type="formula">H(p data ( Y, X ) k p( Y | X )) := E p data ( y,x ) [ − log p( y | x )] .</text>
<text top="294" left="55" width="234" height="9" font="font4" id="p4_t18" reading_order_no="17" segment_no="12" tag_type="text">The data distribution is available to us as the empirical</text>
<text top="306" left="55" width="159" height="11" font="font4" id="p4_t19" reading_order_no="18" segment_no="12" tag_type="text">sample distribution ˆ p test ( x ) , the test set.</text>
<text top="324" left="55" width="234" height="9" font="font4" id="p4_t20" reading_order_no="19" segment_no="13" tag_type="text">The main idea of EPIG is to reduce the uncertainty of</text>
<text top="336" left="55" width="234" height="9" font="font4" id="p4_t21" reading_order_no="20" segment_no="13" tag_type="text">our model’s predictions for samples that follow the test</text>
<text top="348" left="55" width="72" height="9" font="font4" id="p4_t22" reading_order_no="21" segment_no="13" tag_type="text">data distribution.</text>
<text top="348" left="139" width="150" height="9" font="font4" id="p4_t23" reading_order_no="22" segment_no="13" tag_type="text">Hence, we introduce an unlabeled</text>
<text top="357" left="55" width="116" height="13" font="font5" id="p4_t24" reading_order_no="23" segment_no="13" tag_type="text">evaluation set D eval = { x eval i</text>
<text top="359" left="171" width="118" height="12" font="font7" id="p4_t25" reading_order_no="24" segment_no="13" tag_type="text">} i ∈{ 1 ,..., | D eval |} which follows</text>
<text top="372" left="55" width="234" height="9" font="font4" id="p4_t26" reading_order_no="25" segment_no="13" tag_type="text">the distribution of the test set and provides us with</text>
<text top="384" left="55" width="175" height="10" font="font4" id="p4_t27" reading_order_no="26" segment_no="13" tag_type="text">an empirical sample distribution ˆ p eval ( x ) .</text>
<text top="384" left="239" width="50" height="9" font="font4" id="p4_t28" reading_order_no="27" segment_no="13" tag_type="text">We use this</text>
<text top="396" left="55" width="228" height="9" font="font4" id="p4_t29" reading_order_no="28" segment_no="13" tag_type="text">distribution to determine our active learning acquisitions.</text>
<text top="414" left="55" width="234" height="9" font="font4" id="p4_t30" reading_order_no="29" segment_no="16" tag_type="text">EPIG is a natural objective that is connected to the</text>
<text top="426" left="55" width="234" height="9" font="font4" id="p4_t31" reading_order_no="30" segment_no="16" tag_type="text">generalization loss on the test distribution. Assuming we</text>
<text top="436" left="55" width="188" height="11" font="font4" id="p4_t32" reading_order_no="31" segment_no="16" tag_type="text">had a label y eval for every x eval , we could write:</text>
<text top="456" left="58" width="229" height="11" font="font18" id="p4_t33" reading_order_no="32" segment_no="17" tag_type="formula">H(p test ( Y, X ) k p( Y | X )) ≈ H(ˆ p eval ( Y, X ) k p( Y | X )) ,</text>
<text top="476" left="55" width="235" height="9" font="font4" id="p4_t34" reading_order_no="33" segment_no="19" tag_type="text">for any model distribution p( ω ) . In active learning, we</text>
<text top="486" left="55" width="235" height="11" font="font4" id="p4_t35" reading_order_no="34" segment_no="19" tag_type="text">want to acquire a set of new training samples x batch from</text>
<text top="500" left="55" width="234" height="9" font="font4" id="p4_t36" reading_order_no="35" segment_no="19" tag_type="text">the pool set which minimize the generalization loss, i.e. the</text>
<text top="512" left="55" width="149" height="9" font="font4" id="p4_t37" reading_order_no="36" segment_no="19" tag_type="text">cross-entropy on the test distribution:</text>
<text top="530" left="58" width="32" height="9" font="font18" id="p4_t38" reading_order_no="37" segment_no="20" tag_type="formula">arg min</text>
<text top="541" left="55" width="37" height="7" font="font33" id="p4_t39" reading_order_no="38" segment_no="20" tag_type="formula">x batch ⊆ x pool</text>
<text top="528" left="100" width="186" height="13" font="font18" id="p4_t40" reading_order_no="39" segment_no="20" tag_type="formula">H(p test ( X, Y ) k p( Y | X, y batch , x batch , D train ))</text>
<text top="544" left="100" width="4" height="4" font="font27" id="p4_t41" reading_order_no="40" segment_no="20" tag_type="formula">|</text>
<text top="544" left="188" width="9" height="4" font="font27" id="p4_t42" reading_order_no="41" segment_no="20" tag_type="formula">{z</text>
<text top="544" left="281" width="4" height="4" font="font27" id="p4_t43" reading_order_no="42" segment_no="20" tag_type="formula">}</text>
<text top="548" left="94" width="198" height="13" font="font7" id="p4_t44" reading_order_no="43" segment_no="20" tag_type="formula">≈ H(ˆ p eval ( X, Y ) k p( Y | X, y batch , x batch , D train ))</text>
<text top="530" left="291" width="3" height="9" font="font8" id="p4_t45" reading_order_no="44" segment_no="20" tag_type="formula">,</text>
<text top="569" left="55" width="237" height="10" font="font4" id="p4_t46" reading_order_no="45" segment_no="21" tag_type="text">where we have assumed that we know the labels y batch .</text>
<text top="581" left="55" width="235" height="10" font="font4" id="p4_t47" reading_order_no="46" segment_no="21" tag_type="text">However, we do not know the labels y batch a-priori. Instead</text>
<text top="594" left="55" width="234" height="9" font="font4" id="p4_t48" reading_order_no="47" segment_no="21" tag_type="text">we approximate this by taking the expectation over the</text>
<text top="604" left="55" width="235" height="11" font="font4" id="p4_t49" reading_order_no="48" segment_no="21" tag_type="text">predicted labels for x batch using the current model p( y batch |</text>
<text top="616" left="55" width="234" height="11" font="font32" id="p4_t50" reading_order_no="49" segment_no="21" tag_type="text">x batch , D train ) . Similarly, we also do not know the labels for</text>
<text top="628" left="55" width="235" height="11" font="font32" id="p4_t51" reading_order_no="50" segment_no="21" tag_type="text">y eval and can compute the expectation over their predictions</text>
<text top="642" left="55" width="31" height="9" font="font4" id="p4_t52" reading_order_no="51" segment_no="21" tag_type="text">instead.</text>
<text top="657" left="57" width="188" height="13" font="font18" id="p4_t53" reading_order_no="52" segment_no="23" tag_type="formula">H(ˆ p eval ( X, Y ) k p( Y | X, y batch , x batch , D train ))</text>
<text top="673" left="58" width="236" height="14" font="font18" id="p4_t54" reading_order_no="53" segment_no="23" tag_type="formula">= E y eval ,x eval ∼ ˆ eval ( y eval ,x eval ) H[ y eval | x eval , y batch , x batch , D train ]</text>
<text top="690" left="58" width="212" height="14" font="font7" id="p4_t55" reading_order_no="54" segment_no="23" tag_type="formula">≈ E x eval ∼ ˆ eval ( x eval ) H[ Y eval | x eval , Y batch , x batch , D train ]</text>
<text top="707" left="58" width="156" height="12" font="font7" id="p4_t56" reading_order_no="55" segment_no="23" tag_type="formula">≥ H[ Y eval | x eval , Y batch , x batch , D train ] .</text>
<text top="70" left="307" width="234" height="9" font="font4" id="p4_t57" reading_order_no="56" segment_no="2" tag_type="text">Minimizing this conditional entropy is equivalent to</text>
<text top="82" left="307" width="234" height="9" font="font4" id="p4_t58" reading_order_no="57" segment_no="2" tag_type="text">maximizing the expected predictive information gain</text>
<text top="92" left="307" width="234" height="11" font="font18" id="p4_t59" reading_order_no="58" segment_no="2" tag_type="text">I[ Y eval ; Y batch | x eval , x batch , D train ] . The bound is not tight</text>
<text top="106" left="307" width="234" height="9" font="font4" id="p4_t60" reading_order_no="59" segment_no="2" tag_type="text">when there is redundancy between the samples according</text>
<text top="117" left="307" width="84" height="12" font="font4" id="p4_t61" reading_order_no="60" segment_no="2" tag_type="text">to the model: I[ Y eval i</text>
<text top="117" left="391" width="24" height="12" font="font18" id="p4_t62" reading_order_no="61" segment_no="2" tag_type="text">; Y eval j</text>
<text top="117" left="418" width="22" height="12" font="font7" id="p4_t63" reading_order_no="62" segment_no="2" tag_type="text">| x eval i</text>
<text top="117" left="441" width="21" height="12" font="font8" id="p4_t64" reading_order_no="63" segment_no="2" tag_type="text">, x eval j</text>
<text top="116" left="462" width="79" height="11" font="font8" id="p4_t65" reading_order_no="64" segment_no="2" tag_type="text">, train ] 6 = 0 . In the D</text>
<text top="130" left="307" width="184" height="9" font="font4" id="p4_t66" reading_order_no="65" segment_no="2" tag_type="text">infinite data limit, the bound is tight, however.</text>
<text top="145" left="307" width="234" height="10" font="font1" id="p4_t67" reading_order_no="66" segment_no="4" tag_type="text">Definition 4.1. The Expected Predictive Information Gain</text>
<text top="158" left="307" width="234" height="9" font="font4" id="p4_t68" reading_order_no="67" segment_no="4" tag_type="text">(EPIG) is the expected information gain between the</text>
<text top="168" left="307" width="234" height="11" font="font5" id="p4_t69" reading_order_no="68" segment_no="4" tag_type="text">predictions Y batch for the batch candidate samples and the</text>
<text top="180" left="307" width="200" height="11" font="font5" id="p4_t70" reading_order_no="69" segment_no="4" tag_type="text">predictions Y eval for the unlabeled evaluation set:</text>
<text top="200" left="353" width="143" height="11" font="font18" id="p4_t71" reading_order_no="70" segment_no="6" tag_type="formula">I[ Y eval ; Y batch | x eval , x batch , D train ] .</text>
<text top="202" left="530" width="12" height="9" font="font4" id="p4_t72" reading_order_no="71" segment_no="6" tag_type="text">(1)</text>
<text top="223" left="307" width="139" height="9" font="font1" id="p4_t73" reading_order_no="72" segment_no="8" tag_type="text">Lemma 4.2. EPIG is submodular.</text>
<text top="248" left="307" width="234" height="9" font="font5" id="p4_t74" reading_order_no="73" segment_no="9" tag_type="text">Proof. This follows analogously to the proof for the</text>
<text top="260" left="307" width="217" height="9" font="font4" id="p4_t75" reading_order_no="74" segment_no="9" tag_type="text">submodularity of (Batch)BALD in Kirsch et al. ( 2019 ).</text>
<text top="285" left="307" width="234" height="9" font="font4" id="p4_t76" reading_order_no="75" segment_no="11" tag_type="text">This mutual information does not directly depend on the</text>
<text top="297" left="307" width="234" height="9" font="font4" id="p4_t77" reading_order_no="76" segment_no="11" tag_type="text">model parameters: the model parameters are marginalized</text>
<text top="309" left="307" width="235" height="9" font="font4" id="p4_t78" reading_order_no="77" segment_no="11" tag_type="text">out as a nuisance variables. Because EPIG is submodular,</text>
<text top="319" left="307" width="135" height="11" font="font4" id="p4_t79" reading_order_no="78" segment_no="11" tag_type="text">greedy sample acquisition is 1 − 1</text>
<text top="321" left="438" width="103" height="11" font="font4" id="p4_t80" reading_order_no="79" segment_no="11" tag_type="text">e optimal. Note that while</text>
<text top="332" left="307" width="175" height="11" font="font4" id="p4_t81" reading_order_no="80" segment_no="11" tag_type="text">the unlabeled evaluation set D eval = { x eval</text>
<text top="339" left="471" width="3" height="6" font="font9" id="p4_t82" reading_order_no="81" segment_no="11" tag_type="text">i</text>
<text top="333" left="483" width="57" height="12" font="font7" id="p4_t83" reading_order_no="82" segment_no="11" tag_type="text">} i ∈{ 1 ,..., | D eval |}</text>
<text top="346" left="307" width="234" height="9" font="font4" id="p4_t84" reading_order_no="83" segment_no="11" tag_type="text">follows the distribution of the test set, the pool set might</text>
<text top="358" left="307" width="120" height="9" font="font4" id="p4_t85" reading_order_no="84" segment_no="11" tag_type="text">follow a different distribution.</text>
<text top="376" left="307" width="234" height="9" font="font1" id="p4_t86" reading_order_no="85" segment_no="14" tag_type="text">Expected Reduction of Predictive Uncertainty for the</text>
<text top="388" left="307" width="163" height="9" font="font1" id="p4_t87" reading_order_no="86" segment_no="14" tag_type="text">Evaluation Set. We can rewrite EPIG as</text>
<text top="405" left="324" width="149" height="11" font="font18" id="p4_t88" reading_order_no="87" segment_no="15" tag_type="formula">I[ Y eval ; Y batch | x eval , x batch , D train ] =</text>
<text top="423" left="336" width="97" height="11" font="font18" id="p4_t89" reading_order_no="88" segment_no="15" tag_type="formula">= H[ Y eval | x eval , D train ]</text>
<text top="425" left="447" width="5" height="9" font="font4" id="p4_t90" reading_order_no="89" segment_no="15" tag_type="formula">1</text>
<text top="441" left="335" width="153" height="11" font="font7" id="p4_t91" reading_order_no="90" segment_no="15" tag_type="formula">− H[ Y eval | x eval , Y batch , x batch , D train ]</text>
<text top="443" left="502" width="13" height="9" font="font4" id="p4_t92" reading_order_no="91" segment_no="15" tag_type="formula">2 ,</text>
<text top="425" left="530" width="12" height="9" font="font4" id="p4_t93" reading_order_no="92" segment_no="15" tag_type="text">(2)</text>
<text top="468" left="307" width="234" height="9" font="font4" id="p4_t94" reading_order_no="93" segment_no="18" tag_type="text">where 2 is the conditional entropy over the (joint) R.V.s</text>
<text top="479" left="307" width="235" height="11" font="font8" id="p4_t95" reading_order_no="94" segment_no="18" tag_type="text">Y eval and Y batch . The difference between the two terms</text>
<text top="493" left="307" width="234" height="9" font="font4" id="p4_t96" reading_order_no="95" segment_no="18" tag_type="text">then measures the reduction in prediction uncertainty for</text>
<text top="505" left="307" width="235" height="9" font="font4" id="p4_t97" reading_order_no="96" segment_no="18" tag_type="text">the evaluation set when taking into account the batch set:</text>
<text top="517" left="307" width="234" height="9" font="font4" id="p4_t98" reading_order_no="97" segment_no="18" tag_type="text">when it is high, it means that the batch set tells us a lot about</text>
<text top="529" left="307" width="234" height="9" font="font4" id="p4_t99" reading_order_no="98" segment_no="18" tag_type="text">the evaluation set. Intuitively, this reduction will be larger</text>
<text top="541" left="307" width="234" height="9" font="font4" id="p4_t100" reading_order_no="99" segment_no="18" tag_type="text">when batch candidate samples are similar to the samples</text>
<text top="552" left="307" width="234" height="10" font="font4" id="p4_t101" reading_order_no="100" segment_no="18" tag_type="text">in the evaluation set. Conversely, it is = 0 exactly when</text>
<text top="562" left="307" width="137" height="11" font="font8" id="p4_t102" reading_order_no="101" segment_no="18" tag_type="text">Y eval ⊥ ⊥ Y batch | x eval , x batch , D train .</text>
<text top="582" left="307" width="201" height="9" font="font1" id="p4_t103" reading_order_no="102" segment_no="22" tag_type="text">Evaluating the Predictive Information Gain.</text>
<text top="582" left="521" width="21" height="9" font="font4" id="p4_t104" reading_order_no="103" segment_no="22" tag_type="text">Even</text>
<text top="594" left="307" width="234" height="11" font="font4" id="p4_t105" reading_order_no="104" segment_no="22" tag_type="text">though 1 in eq. ( 2 ) above is constant for fixed D train</text>
<text top="610" left="307" width="235" height="10" font="font4" id="p4_t106" reading_order_no="105" segment_no="22" tag_type="text">and x eval , it is impractical to reevaluate 2 and perform</text>
<text top="623" left="307" width="235" height="9" font="font4" id="p4_t107" reading_order_no="106" segment_no="22" tag_type="text">a Bayesian inference step for each potential batch. Luckily,</text>
<text top="635" left="307" width="234" height="9" font="font4" id="p4_t108" reading_order_no="107" segment_no="22" tag_type="text">the mutual information is symmetric in its arguments, and</text>
<text top="647" left="307" width="181" height="9" font="font4" id="p4_t109" reading_order_no="108" segment_no="22" tag_type="text">we can expand it the other way, which yields:</text>
<text top="666" left="323" width="149" height="11" font="font18" id="p4_t110" reading_order_no="109" segment_no="24" tag_type="formula">I[ Y eval ; Y batch | x eval , x batch , D train ] =</text>
<text top="685" left="334" width="105" height="11" font="font18" id="p4_t111" reading_order_no="110" segment_no="24" tag_type="formula">= H[ Y batch | x batch , D train ]</text>
<text top="688" left="453" width="8" height="9" font="font4" id="p4_t112" reading_order_no="111" segment_no="24" tag_type="formula">1’</text>
<text top="706" left="334" width="152" height="11" font="font7" id="p4_t113" reading_order_no="112" segment_no="24" tag_type="formula">− H[ Y batch | x batch , Y eval , x eval , D train ]</text>
<text top="708" left="500" width="16" height="9" font="font4" id="p4_t114" reading_order_no="113" segment_no="24" tag_type="formula">2’ ,</text>
<text top="689" left="530" width="12" height="9" font="font4" id="p4_t115" reading_order_no="114" segment_no="24" tag_type="text">(3)</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font35" size="10" family="CMSY10" color="#ff0000"/>
	<fontspec id="font36" size="10" family="CMR10" color="#ff0000"/>
	<fontspec id="font37" size="7" family="NimbusRomNo9L-Medi" color="#000000"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="72" left="60" width="230" height="9" font="font4" id="p5_t2" reading_order_no="1" segment_no="2" tag_type="text">1’ is simply the conditional entropy with the Bayesian</text>
<text top="87" left="55" width="234" height="12" font="font4" id="p5_t3" reading_order_no="2" segment_no="2" tag_type="text">model p( ω | D train ) . To see how we can evaluate 2’ , we</text>
<text top="102" left="55" width="216" height="9" font="font4" id="p5_t4" reading_order_no="3" segment_no="2" tag_type="text">expand using the definition of the conditional entropy:</text>
<text top="120" left="67" width="153" height="11" font="font18" id="p5_t5" reading_order_no="4" segment_no="4" tag_type="formula">H[ Y batch | x batch , Y eval , x eval , D train ] =</text>
<text top="137" left="59" width="221" height="13" font="font18" id="p5_t6" reading_order_no="5" segment_no="4" tag_type="formula">= E p( y eval | x eval , D train ) H[ Y batch | x batch , y eval , x eval , D train ] .</text>
<text top="156" left="55" width="234" height="11" font="font4" id="p5_t7" reading_order_no="6" segment_no="6" tag_type="text">We take an expectation over labels y eval and the conditional</text>
<text top="168" left="55" width="235" height="11" font="font4" id="p5_t8" reading_order_no="7" segment_no="6" tag_type="text">entropy using the Bayesian model p( ω | y eval , x eval , D train ) .</text>
<text top="185" left="55" width="234" height="12" font="font4" id="p5_t9" reading_order_no="8" segment_no="7" tag_type="text">In other words, we train a model on D train , then sample</text>
<text top="198" left="55" width="234" height="11" font="font4" id="p5_t10" reading_order_no="9" segment_no="7" tag_type="text">possible joint predictions y eval for the evaluation set x eval</text>
<text top="212" left="55" width="234" height="9" font="font4" id="p5_t11" reading_order_no="10" segment_no="7" tag_type="text">and then for each such joint prediction, we evaluate the</text>
<text top="222" left="55" width="235" height="11" font="font4" id="p5_t12" reading_order_no="11" segment_no="7" tag_type="text">conditional entropy H[ Y batch | x batch , Ω] using ω ∼ p( ω |</text>
<text top="233" left="55" width="234" height="12" font="font32" id="p5_t13" reading_order_no="12" segment_no="7" tag_type="text">y eval , x eval , D train ) , which we can approximate using one</text>
<text top="248" left="55" width="234" height="9" font="font4" id="p5_t14" reading_order_no="13" segment_no="7" tag_type="text">variational distribution per sampled joint prediction for</text>
<text top="262" left="60" width="230" height="9" font="font4" id="p5_t15" reading_order_no="14" segment_no="7" tag_type="text">2’ instead of one per potential batch and sampled joint</text>
<text top="279" left="55" width="115" height="9" font="font4" id="p5_t16" reading_order_no="15" segment_no="7" tag_type="text">prediction in the case of 2 .</text>
<text top="304" left="55" width="193" height="9" font="font1" id="p5_t17" reading_order_no="16" segment_no="11" tag_type="title">4.2. Triple Mutual Information: EPIG-BALD</text>
<text top="323" left="55" width="234" height="9" font="font4" id="p5_t18" reading_order_no="17" segment_no="13" tag_type="text">When trying to evaluate EPIG with approximate Bayesian</text>
<text top="335" left="55" width="234" height="9" font="font4" id="p5_t19" reading_order_no="18" segment_no="13" tag_type="text">models like commonly used Bayesian neural network</text>
<text top="347" left="55" width="234" height="9" font="font4" id="p5_t20" reading_order_no="19" segment_no="13" tag_type="text">approximations, we empirically find that for variational</text>
<text top="356" left="55" width="235" height="13" font="font4" id="p5_t21" reading_order_no="20" segment_no="13" tag_type="text">approximations q 1 ( ω ) ≈ p( ω | D train ) and q 2 ( ω ) ≈ p( ω |</text>
<text top="368" left="55" width="78" height="12" font="font32" id="p5_t22" reading_order_no="21" segment_no="13" tag_type="text">Y eval , x eval , D train ) :</text>
<text top="388" left="101" width="143" height="11" font="font18" id="p5_t23" reading_order_no="22" segment_no="15" tag_type="formula">H(q 1 ( Y | x, Ω)) 6 = H(q 2 ( Y | x, Ω)) ,</text>
<text top="406" left="55" width="49" height="9" font="font4" id="p5_t24" reading_order_no="23" segment_no="16" tag_type="text">even though</text>
<text top="421" left="74" width="197" height="11" font="font18" id="p5_t25" reading_order_no="24" segment_no="17" tag_type="formula">H[ Y | x, Ω , D train ] = H[ Y | x, Ω , Y eval , x eval , D train ]</text>
<text top="439" left="55" width="234" height="11" font="font4" id="p5_t26" reading_order_no="25" segment_no="19" tag_type="text">as Y ⊥ ⊥ Y eval | x, x eval , ω . We can take this into account</text>
<text top="453" left="55" width="234" height="9" font="font4" id="p5_t27" reading_order_no="26" segment_no="19" tag_type="text">with the following triple mutual information which includes</text>
<text top="465" left="55" width="129" height="9" font="font4" id="p5_t28" reading_order_no="27" segment_no="19" tag_type="text">the model parameters explicitly:</text>
<text top="479" left="55" width="234" height="9" font="font1" id="p5_t29" reading_order_no="28" segment_no="20" tag_type="text">Definition 4.3. We define EPIG-BALD as the triple mutual</text>
<text top="491" left="55" width="236" height="9" font="font5" id="p5_t30" reading_order_no="29" segment_no="20" tag_type="text">information between predictions on the evaluation set,</text>
<text top="503" left="55" width="207" height="9" font="font5" id="p5_t31" reading_order_no="30" segment_no="20" tag_type="text">predictions on the batch set, and model parameters:</text>
<text top="518" left="95" width="155" height="12" font="font18" id="p5_t32" reading_order_no="31" segment_no="22" tag_type="formula">I[ Y eval ; Y batch ; Ω | x batch , x eval , D train ] .</text>
<text top="521" left="278" width="12" height="9" font="font4" id="p5_t33" reading_order_no="32" segment_no="22" tag_type="text">(4)</text>
<text top="538" left="55" width="92" height="9" font="font1" id="p5_t34" reading_order_no="33" segment_no="24" tag_type="text">Lemma 4.4. We have:</text>
<text top="557" left="67" width="162" height="11" font="font18" id="p5_t35" reading_order_no="34" segment_no="26" tag_type="formula">I[ Y eval ; Y batch ; Ω | x batch , x eval , D train ] =</text>
<text top="573" left="126" width="154" height="11" font="font18" id="p5_t36" reading_order_no="35" segment_no="26" tag_type="formula">= I[ Y eval ; Y batch | x batch , x eval , D train ] ,</text>
<text top="593" left="55" width="230" height="9" font="font5" id="p5_t37" reading_order_no="36" segment_no="28" tag_type="text">so EPIG-BALD = EPIG, and EPIG-BALD is submodular.</text>
<text top="617" left="55" width="234" height="10" font="font5" id="p5_t38" reading_order_no="37" segment_no="29" tag_type="text">Proof. We can expand this term using the properties of a</text>
<text top="629" left="55" width="114" height="9" font="font4" id="p5_t39" reading_order_no="38" segment_no="29" tag_type="text">triple mutual information to:</text>
<text top="645" left="83" width="160" height="11" font="font18" id="p5_t40" reading_order_no="39" segment_no="31" tag_type="formula">I[ Y eval ; Y batch ; Ω | x batch , x eval , D train ] =</text>
<text top="662" left="94" width="149" height="11" font="font18" id="p5_t41" reading_order_no="40" segment_no="31" tag_type="formula">= I[ Y eval ; Y batch | x batch , x eval , D train ]</text>
<text top="678" left="103" width="161" height="11" font="font7" id="p5_t42" reading_order_no="41" segment_no="31" tag_type="formula">− I[ Y eval ; Y batch | x batch , x eval , Ω , D train ]</text>
<text top="694" left="113" width="4" height="4" font="font27" id="p5_t43" reading_order_no="42" segment_no="31" tag_type="formula">|</text>
<text top="694" left="184" width="9" height="4" font="font27" id="p5_t44" reading_order_no="43" segment_no="31" tag_type="formula">{z</text>
<text top="694" left="259" width="4" height="4" font="font27" id="p5_t45" reading_order_no="44" segment_no="31" tag_type="formula">}</text>
<text top="698" left="183" width="10" height="6" font="font11" id="p5_t46" reading_order_no="45" segment_no="31" tag_type="formula">=0</text>
<text top="707" left="94" width="152" height="12" font="font18" id="p5_t47" reading_order_no="46" segment_no="31" tag_type="formula">= I[ Y eval ; Y batch | x batch , x eval , D train ] ,</text>
<text top="69" left="307" width="235" height="10" font="font4" id="p5_t48" reading_order_no="47" segment_no="1" tag_type="text">where we have used the independence Y eval ⊥ ⊥ Y batch |</text>
<text top="81" left="307" width="59" height="10" font="font32" id="p5_t49" reading_order_no="48" segment_no="1" tag_type="text">x eval , x batch , Ω .</text>
<text top="107" left="307" width="235" height="9" font="font1" id="p5_t50" reading_order_no="49" segment_no="3" tag_type="text">Evaluating EPIG-BALD. We obtain a more tractable</text>
<text top="117" left="307" width="202" height="11" font="font4" id="p5_t51" reading_order_no="50" segment_no="3" tag_type="text">version by expanding while conditioning on Y eval :</text>
<text top="139" left="334" width="150" height="11" font="font18" id="p5_t52" reading_order_no="51" segment_no="5" tag_type="formula">I[ Y eval ; Y batch ; Ω | x batch , x eval , D train ]</text>
<text top="155" left="345" width="112" height="12" font="font18" id="p5_t53" reading_order_no="52" segment_no="5" tag_type="formula">= I[ Y batch ; Ω | x batch , D train ]</text>
<text top="172" left="354" width="163" height="11" font="font7" id="p5_t54" reading_order_no="53" segment_no="5" tag_type="formula">− I[ Y batch ; Ω | x batch , Y eval , x eval , D train ] .</text>
<text top="196" left="307" width="234" height="9" font="font4" id="p5_t55" reading_order_no="54" segment_no="8" tag_type="text">Both terms are BALD terms now, except that the second</text>
<text top="206" left="307" width="236" height="11" font="font4" id="p5_t56" reading_order_no="55" segment_no="8" tag_type="text">one is a conditional mutual information over Y eval . Again,</text>
<text top="219" left="307" width="234" height="9" font="font4" id="p5_t57" reading_order_no="56" segment_no="8" tag_type="text">using the definition of the conditional entropy, we expand</text>
<text top="231" left="307" width="150" height="9" font="font4" id="p5_t58" reading_order_no="57" segment_no="8" tag_type="text">this term to clarify how to evaluate it:</text>
<text top="251" left="309" width="150" height="11" font="font18" id="p5_t59" reading_order_no="58" segment_no="9" tag_type="formula">I[ Y batch ; Ω | x batch , Y eval , x eval , D train ]</text>
<text top="268" left="320" width="229" height="13" font="font18" id="p5_t60" reading_order_no="59" segment_no="9" tag_type="formula">= E p( y eval | x eval , D train ) I[ Y batch ; Ω | x batch , y eval , x eval , D train ] .</text>
<text top="292" left="307" width="235" height="9" font="font4" id="p5_t61" reading_order_no="60" segment_no="10" tag_type="text">This an expectation of BALD scores for models p( ω |</text>
<text top="302" left="307" width="211" height="11" font="font32" id="p5_t62" reading_order_no="61" segment_no="10" tag_type="text">y eval , x eval , D train ) over y eval ∼ p( y eval | x eval , D train ) .</text>
<text top="322" left="307" width="234" height="9" font="font1" id="p5_t63" reading_order_no="62" segment_no="12" tag_type="text">Expected Reduction of Epistemic Uncertainty of the</text>
<text top="334" left="307" width="64" height="9" font="font1" id="p5_t64" reading_order_no="63" segment_no="12" tag_type="text">Evaluation set.</text>
<text top="334" left="379" width="163" height="9" font="font4" id="p5_t65" reading_order_no="64" segment_no="12" tag_type="text">In addition to the intuition provided in</text>
<text top="345" left="307" width="234" height="10" font="font4" id="p5_t66" reading_order_no="65" segment_no="12" tag_type="text">§ 1 , we see that by conditioning on Y batch instead of Y eval</text>
<text top="358" left="307" width="42" height="9" font="font4" id="p5_t67" reading_order_no="66" segment_no="12" tag_type="text">we obtain:</text>
<text top="376" left="334" width="160" height="11" font="font18" id="p5_t68" reading_order_no="67" segment_no="14" tag_type="formula">I[ Y eval ; Y batch ; Ω | x batch , x eval , D train ] =</text>
<text top="393" left="345" width="105" height="11" font="font18" id="p5_t69" reading_order_no="68" segment_no="14" tag_type="formula">= I[ Y eval ; Ω | x eval , D train ]</text>
<text top="409" left="354" width="163" height="11" font="font7" id="p5_t70" reading_order_no="69" segment_no="14" tag_type="formula">− I[ Y eval ; Ω | x eval , Y batch , x batch , D train ] .</text>
<text top="433" left="307" width="234" height="9" font="font4" id="p5_t71" reading_order_no="70" segment_no="18" tag_type="text">Since BALD is known to measure epistemic uncertainty</text>
<text top="445" left="307" width="234" height="9" font="font4" id="p5_t72" reading_order_no="71" segment_no="18" tag_type="text">( Smith and Gal , 2018 ), maximizing EPIG-BALD (and</text>
<text top="457" left="307" width="234" height="9" font="font4" id="p5_t73" reading_order_no="72" segment_no="18" tag_type="text">thus EPIG) selects a batch which reduces the epistemic</text>
<text top="469" left="307" width="234" height="9" font="font4" id="p5_t74" reading_order_no="73" segment_no="18" tag_type="text">uncertainty for predictions on the evaluation set the most (as</text>
<text top="478" left="307" width="229" height="11" font="font18" id="p5_t75" reading_order_no="74" segment_no="18" tag_type="text">I[ Y eval ; Ω | x eval , D train ] = const independently of x batch ).</text>
<text top="503" left="307" width="223" height="11" font="font1" id="p5_t76" reading_order_no="75" segment_no="21" tag_type="text">4.3. Estimating I[ Y batch ; Ω | x batch , Y eval , x eval , D train ]</text>
<text top="522" left="307" width="234" height="11" font="font4" id="p5_t77" reading_order_no="76" segment_no="23" tag_type="text">We want to find a model approximation ˆ Ω with distribution</text>
<text top="536" left="307" width="204" height="9" font="font18" id="p5_t78" reading_order_no="77" segment_no="23" tag_type="text">q(ˆ ω ) , such that for all possible batch sets, we have:</text>
<text top="556" left="307" width="235" height="11" font="font18" id="p5_t79" reading_order_no="78" segment_no="25" tag_type="formula">I[ Y batch ; Ω | x batch , Y eval , x eval , D train ] ≈ I[ Y batch ; ˆ Ω | x batch ] .</text>
<text top="580" left="307" width="234" height="9" font="font4" id="p5_t80" reading_order_no="79" segment_no="27" tag_type="text">We note two properties of this conditional mutual</text>
<text top="592" left="307" width="48" height="9" font="font4" id="p5_t81" reading_order_no="80" segment_no="27" tag_type="text">information</text>
<text top="592" left="367" width="15" height="9" font="font4" id="p5_t82" reading_order_no="81" segment_no="27" tag_type="text">and</text>
<text top="592" left="393" width="12" height="9" font="font4" id="p5_t83" reading_order_no="82" segment_no="27" tag_type="text">the</text>
<text top="592" left="417" width="44" height="9" font="font4" id="p5_t84" reading_order_no="83" segment_no="27" tag_type="text">underlying</text>
<text top="592" left="472" width="29" height="9" font="font4" id="p5_t85" reading_order_no="84" segment_no="27" tag_type="text">models</text>
<text top="592" left="513" width="17" height="9" font="font18" id="p5_t86" reading_order_no="85" segment_no="27" tag_type="text">p(Ω</text>
<text top="591" left="539" width="3" height="9" font="font7" id="p5_t87" reading_order_no="86" segment_no="27" tag_type="text">|</text>
<text top="601" left="307" width="220" height="12" font="font32" id="p5_t88" reading_order_no="87" segment_no="27" tag_type="text">y eval , x eval , D train ) for different y eval ∼ p( y eval | D train ) :</text>
<text top="619" left="307" width="235" height="12" font="font4" id="p5_t89" reading_order_no="88" segment_no="30" tag_type="text">1. marginalizing p(Ω | y eval , x eval , D train ) over all possible</text>
<text top="632" left="320" width="222" height="11" font="font32" id="p5_t90" reading_order_no="89" segment_no="30" tag_type="text">y eval yields the predictions of the original posterior p(Ω |</text>
<text top="643" left="320" width="222" height="14" font="font14" id="p5_t91" reading_order_no="90" segment_no="30" tag_type="text">D train ) , so we would like E q( ˆ Ω) p( y | x, ˆ ω ) = p( y |</text>
<text top="658" left="320" width="56" height="11" font="font8" id="p5_t92" reading_order_no="91" segment_no="30" tag_type="text">x, D train ) ; and</text>
<text top="670" left="307" width="135" height="11" font="font4" id="p5_t93" reading_order_no="92" segment_no="32" tag_type="text">2. I[ Y ; Ω | x, Y eval , x eval , D train ]</text>
<text top="672" left="450" width="8" height="9" font="font7" id="p5_t94" reading_order_no="93" segment_no="32" tag_type="text">≤</text>
<text top="670" left="467" width="76" height="11" font="font18" id="p5_t95" reading_order_no="94" segment_no="32" tag_type="text">I[ Y ; Ω | x, D train ] ,</text>
<text top="684" left="320" width="15" height="9" font="font4" id="p5_t96" reading_order_no="95" segment_no="32" tag_type="text">and</text>
<text top="684" left="344" width="22" height="9" font="font4" id="p5_t97" reading_order_no="96" segment_no="32" tag_type="text">when</text>
<text top="684" left="375" width="6" height="9" font="font8" id="p5_t98" reading_order_no="97" segment_no="32" tag_type="text">x</text>
<text top="684" left="396" width="7" height="9" font="font7" id="p5_t99" reading_order_no="98" segment_no="32" tag_type="text">∈</text>
<text top="683" left="417" width="22" height="10" font="font32" id="p5_t100" reading_order_no="99" segment_no="32" tag_type="text">x eval ,</text>
<text top="684" left="449" width="12" height="9" font="font4" id="p5_t101" reading_order_no="100" segment_no="32" tag_type="text">we</text>
<text top="684" left="470" width="26" height="9" font="font4" id="p5_t102" reading_order_no="101" segment_no="32" tag_type="text">expect</text>
<text top="684" left="506" width="36" height="9" font="font18" id="p5_t103" reading_order_no="102" segment_no="32" tag_type="text">I[ Y ; Ω |</text>
<text top="694" left="320" width="84" height="11" font="font8" id="p5_t104" reading_order_no="103" segment_no="32" tag_type="text">x, Y eval , x eval , D train ]</text>
<text top="694" left="437" width="79" height="11" font="font18" id="p5_t105" reading_order_no="104" segment_no="32" tag_type="text">I[ Y ; Ω | x, D train ] .</text>
<text top="696" left="533" width="8" height="9" font="font4" id="p5_t106" reading_order_no="105" segment_no="32" tag_type="text">In</text>
<text top="708" left="320" width="222" height="9" font="font4" id="p5_t107" reading_order_no="106" segment_no="32" tag_type="text">other words, the epistemic uncertainty of evaluation</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font38" size="5" family="DejaVuSans" color="#262626"/>
	<fontspec id="font39" size="5" family="DejaVuSans" color="#262626"/>
	<fontspec id="font40" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="107" left="84" width="4" height="6" font="font38" id="p6_t2" reading_order_no="6" segment_no="1" tag_type="figure">50</text>
<text top="107" left="100" width="5" height="6" font="font38" id="p6_t3" reading_order_no="7" segment_no="1" tag_type="figure">100</text>
<text top="107" left="118" width="5" height="6" font="font38" id="p6_t4" reading_order_no="8" segment_no="1" tag_type="figure">150</text>
<text top="107" left="136" width="5" height="6" font="font38" id="p6_t5" reading_order_no="9" segment_no="1" tag_type="figure">200</text>
<text top="99" left="68" width="4" height="6" font="font38" id="p6_t6" reading_order_no="5" segment_no="1" tag_type="figure">0.7</text>
<text top="87" left="68" width="4" height="6" font="font38" id="p6_t7" reading_order_no="4" segment_no="1" tag_type="figure">0.8</text>
<text top="76" left="68" width="4" height="6" font="font38" id="p6_t8" reading_order_no="3" segment_no="1" tag_type="figure">0.9</text>
<text top="91" left="67" width="0" height="6" font="font39" id="p6_t9" reading_order_no="2" segment_no="1" tag_type="figure">Accuracy</text>
<text top="65" left="72" width="70" height="6" font="font38" id="p6_t10" reading_order_no="1" segment_no="1" tag_type="figure">OoD Exposure: MNIST (iD) + FashionMNIST (OoD)</text>
<text top="107" left="160" width="4" height="6" font="font38" id="p6_t11" reading_order_no="14" segment_no="1" tag_type="figure">50</text>
<text top="107" left="177" width="5" height="6" font="font38" id="p6_t12" reading_order_no="15" segment_no="1" tag_type="figure">100</text>
<text top="107" left="195" width="5" height="6" font="font38" id="p6_t13" reading_order_no="16" segment_no="1" tag_type="figure">150</text>
<text top="107" left="213" width="5" height="6" font="font38" id="p6_t14" reading_order_no="17" segment_no="1" tag_type="figure">200</text>
<text top="98" left="145" width="4" height="6" font="font38" id="p6_t15" reading_order_no="13" segment_no="1" tag_type="figure">0.7</text>
<text top="87" left="145" width="4" height="6" font="font38" id="p6_t16" reading_order_no="12" segment_no="1" tag_type="figure">0.8</text>
<text top="76" left="145" width="4" height="6" font="font38" id="p6_t17" reading_order_no="11" segment_no="1" tag_type="figure">0.9</text>
<text top="65" left="149" width="70" height="6" font="font38" id="p6_t18" reading_order_no="10" segment_no="1" tag_type="figure">OoD Rejection: MNIST (iD) + FashionMNIST (OoD)</text>
<text top="154" left="84" width="4" height="6" font="font38" id="p6_t19" reading_order_no="24" segment_no="1" tag_type="figure">50</text>
<text top="154" left="100" width="5" height="6" font="font38" id="p6_t20" reading_order_no="25" segment_no="1" tag_type="figure">100</text>
<text top="154" left="118" width="5" height="6" font="font38" id="p6_t21" reading_order_no="26" segment_no="1" tag_type="figure">150</text>
<text top="154" left="136" width="5" height="6" font="font38" id="p6_t22" reading_order_no="27" segment_no="1" tag_type="figure">200</text>
<text top="157" left="94" width="25" height="6" font="font38" id="p6_t23" reading_order_no="28" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="148" left="67" width="6" height="6" font="font38" id="p6_t24" reading_order_no="23" segment_no="1" tag_type="figure">0.60</text>
<text top="138" left="67" width="6" height="6" font="font38" id="p6_t25" reading_order_no="22" segment_no="1" tag_type="figure">0.65</text>
<text top="128" left="67" width="6" height="6" font="font38" id="p6_t26" reading_order_no="21" segment_no="1" tag_type="figure">0.70</text>
<text top="118" left="67" width="6" height="6" font="font38" id="p6_t27" reading_order_no="20" segment_no="1" tag_type="figure">0.75</text>
<text top="138" left="65" width="0" height="6" font="font39" id="p6_t28" reading_order_no="19" segment_no="1" tag_type="figure">Accuracy</text>
<text top="112" left="72" width="70" height="6" font="font38" id="p6_t29" reading_order_no="18" segment_no="1" tag_type="figure">OoD Exposure: FashionMNIST (iD) + MNIST (OoD)</text>
<text top="154" left="160" width="4" height="6" font="font38" id="p6_t30" reading_order_no="34" segment_no="1" tag_type="figure">50</text>
<text top="154" left="177" width="5" height="6" font="font38" id="p6_t31" reading_order_no="35" segment_no="1" tag_type="figure">100</text>
<text top="154" left="195" width="5" height="6" font="font38" id="p6_t32" reading_order_no="36" segment_no="1" tag_type="figure">150</text>
<text top="154" left="213" width="5" height="6" font="font38" id="p6_t33" reading_order_no="37" segment_no="1" tag_type="figure">200</text>
<text top="157" left="171" width="25" height="6" font="font38" id="p6_t34" reading_order_no="38" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="146" left="142" width="8" height="6" font="font38" id="p6_t35" reading_order_no="33" segment_no="1" tag_type="figure">0.600</text>
<text top="139" left="142" width="8" height="6" font="font38" id="p6_t36" reading_order_no="32" segment_no="1" tag_type="figure">0.625</text>
<text top="132" left="142" width="8" height="6" font="font38" id="p6_t37" reading_order_no="31" segment_no="1" tag_type="figure">0.650</text>
<text top="125" left="142" width="8" height="6" font="font38" id="p6_t38" reading_order_no="30" segment_no="1" tag_type="figure">0.675</text>
<text top="112" left="142" width="77" height="12" font="font38" id="p6_t39" reading_order_no="29" segment_no="1" tag_type="figure">OoD Rejection: FashionMNIST (iD) + MNIST (OoD) 0.700</text>
<text top="159" left="133" width="28" height="6" font="font38" id="p6_t40" reading_order_no="39" segment_no="1" tag_type="figure">Acquisition Function</text>
<text top="163" left="125" width="23" height="6" font="font38" id="p6_t41" reading_order_no="40" segment_no="1" tag_type="figure">BatchEPIG-BALD</text>
<text top="163" left="162" width="16" height="6" font="font38" id="p6_t42" reading_order_no="41" segment_no="1" tag_type="figure">BatchBALD</text>
<text top="176" left="117" width="46" height="8" font="font16" id="p6_t43" reading_order_no="42" segment_no="1" tag_type="figure">(a) Accuracy</text>
<text top="106" left="240" width="4" height="6" font="font38" id="p6_t44" reading_order_no="51" segment_no="1" tag_type="figure">50</text>
<text top="106" left="257" width="5" height="6" font="font38" id="p6_t45" reading_order_no="52" segment_no="1" tag_type="figure">100</text>
<text top="106" left="276" width="5" height="6" font="font38" id="p6_t46" reading_order_no="53" segment_no="1" tag_type="figure">150</text>
<text top="106" left="294" width="5" height="6" font="font38" id="p6_t47" reading_order_no="54" segment_no="1" tag_type="figure">200</text>
<text top="103" left="225" width="5" height="6" font="font38" id="p6_t48" reading_order_no="50" segment_no="1" tag_type="figure">0.0</text>
<text top="96" left="225" width="5" height="6" font="font38" id="p6_t49" reading_order_no="49" segment_no="1" tag_type="figure">0.2</text>
<text top="89" left="225" width="5" height="6" font="font38" id="p6_t50" reading_order_no="48" segment_no="1" tag_type="figure">0.4</text>
<text top="82" left="225" width="5" height="6" font="font38" id="p6_t51" reading_order_no="47" segment_no="1" tag_type="figure">0.6</text>
<text top="75" left="225" width="5" height="6" font="font38" id="p6_t52" reading_order_no="46" segment_no="1" tag_type="figure">0.8</text>
<text top="68" left="225" width="5" height="6" font="font38" id="p6_t53" reading_order_no="45" segment_no="1" tag_type="figure">1.0</text>
<text top="100" left="223" width="0" height="6" font="font39" id="p6_t54" reading_order_no="44" segment_no="1" tag_type="figure">Acquired OoD Fraction</text>
<text top="65" left="229" width="71" height="6" font="font38" id="p6_t55" reading_order_no="43" segment_no="1" tag_type="figure">OoD Exposure: MNIST (iD) + FashionMNIST (OoD)</text>
<text top="106" left="317" width="4" height="6" font="font38" id="p6_t56" reading_order_no="61" segment_no="1" tag_type="figure">50</text>
<text top="106" left="334" width="5" height="6" font="font38" id="p6_t57" reading_order_no="62" segment_no="1" tag_type="figure">100</text>
<text top="106" left="353" width="5" height="6" font="font38" id="p6_t58" reading_order_no="63" segment_no="1" tag_type="figure">150</text>
<text top="106" left="371" width="5" height="6" font="font38" id="p6_t59" reading_order_no="64" segment_no="1" tag_type="figure">200</text>
<text top="103" left="301" width="5" height="6" font="font38" id="p6_t60" reading_order_no="60" segment_no="1" tag_type="figure">0.0</text>
<text top="96" left="301" width="5" height="6" font="font38" id="p6_t61" reading_order_no="59" segment_no="1" tag_type="figure">0.2</text>
<text top="89" left="301" width="5" height="6" font="font38" id="p6_t62" reading_order_no="58" segment_no="1" tag_type="figure">0.4</text>
<text top="82" left="301" width="5" height="6" font="font38" id="p6_t63" reading_order_no="57" segment_no="1" tag_type="figure">0.6</text>
<text top="75" left="301" width="5" height="6" font="font38" id="p6_t64" reading_order_no="56" segment_no="1" tag_type="figure">0.8</text>
<text top="68" left="301" width="75" height="6" font="font38" id="p6_t65" reading_order_no="55" segment_no="1" tag_type="figure">1.0OoD Rejection: MNIST (iD) + FashionMNIST (OoD)</text>
<text top="154" left="240" width="4" height="6" font="font38" id="p6_t66" reading_order_no="73" segment_no="1" tag_type="figure">50</text>
<text top="154" left="257" width="5" height="6" font="font38" id="p6_t67" reading_order_no="74" segment_no="1" tag_type="figure">100</text>
<text top="154" left="276" width="5" height="6" font="font38" id="p6_t68" reading_order_no="75" segment_no="1" tag_type="figure">150</text>
<text top="154" left="294" width="5" height="6" font="font38" id="p6_t69" reading_order_no="76" segment_no="1" tag_type="figure">200</text>
<text top="158" left="252" width="25" height="6" font="font38" id="p6_t70" reading_order_no="77" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="151" left="225" width="5" height="6" font="font38" id="p6_t71" reading_order_no="72" segment_no="1" tag_type="figure">0.0</text>
<text top="144" left="225" width="5" height="6" font="font38" id="p6_t72" reading_order_no="71" segment_no="1" tag_type="figure">0.2</text>
<text top="137" left="225" width="5" height="6" font="font38" id="p6_t73" reading_order_no="70" segment_no="1" tag_type="figure">0.4</text>
<text top="129" left="225" width="5" height="6" font="font38" id="p6_t74" reading_order_no="69" segment_no="1" tag_type="figure">0.6</text>
<text top="122" left="225" width="5" height="6" font="font38" id="p6_t75" reading_order_no="68" segment_no="1" tag_type="figure">0.8</text>
<text top="115" left="225" width="5" height="6" font="font38" id="p6_t76" reading_order_no="67" segment_no="1" tag_type="figure">1.0</text>
<text top="148" left="223" width="0" height="6" font="font39" id="p6_t77" reading_order_no="66" segment_no="1" tag_type="figure">Acquired OoD Fraction</text>
<text top="112" left="229" width="71" height="6" font="font38" id="p6_t78" reading_order_no="65" segment_no="1" tag_type="figure">OoD Exposure: FashionMNIST (iD) + MNIST (OoD)</text>
<text top="154" left="317" width="4" height="6" font="font38" id="p6_t79" reading_order_no="84" segment_no="1" tag_type="figure">50</text>
<text top="154" left="334" width="5" height="6" font="font38" id="p6_t80" reading_order_no="85" segment_no="1" tag_type="figure">100</text>
<text top="154" left="353" width="5" height="6" font="font38" id="p6_t81" reading_order_no="86" segment_no="1" tag_type="figure">150</text>
<text top="154" left="371" width="5" height="6" font="font38" id="p6_t82" reading_order_no="87" segment_no="1" tag_type="figure">200</text>
<text top="158" left="328" width="25" height="6" font="font38" id="p6_t83" reading_order_no="88" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="151" left="301" width="5" height="6" font="font38" id="p6_t84" reading_order_no="83" segment_no="1" tag_type="figure">0.0</text>
<text top="144" left="301" width="5" height="6" font="font38" id="p6_t85" reading_order_no="82" segment_no="1" tag_type="figure">0.2</text>
<text top="137" left="301" width="5" height="6" font="font38" id="p6_t86" reading_order_no="81" segment_no="1" tag_type="figure">0.4</text>
<text top="129" left="301" width="5" height="6" font="font38" id="p6_t87" reading_order_no="80" segment_no="1" tag_type="figure">0.6</text>
<text top="122" left="301" width="5" height="6" font="font38" id="p6_t88" reading_order_no="79" segment_no="1" tag_type="figure">0.8</text>
<text top="115" left="301" width="75" height="6" font="font38" id="p6_t89" reading_order_no="78" segment_no="1" tag_type="figure">1.0OoD Rejection: FashionMNIST (iD) + MNIST (OoD)</text>
<text top="160" left="291" width="29" height="6" font="font38" id="p6_t90" reading_order_no="89" segment_no="1" tag_type="figure">Acquisition Function</text>
<text top="164" left="282" width="23" height="6" font="font38" id="p6_t91" reading_order_no="90" segment_no="1" tag_type="figure">BatchEPIG-BALD</text>
<text top="164" left="319" width="16" height="6" font="font38" id="p6_t92" reading_order_no="91" segment_no="1" tag_type="figure">BatchBALD</text>
<text top="177" left="257" width="83" height="8" font="font16" id="p6_t93" reading_order_no="92" segment_no="1" tag_type="figure">(b) Aquired OoD Ratio</text>
<text top="107" left="400" width="4" height="6" font="font38" id="p6_t94" reading_order_no="99" segment_no="1" tag_type="figure">50</text>
<text top="107" left="416" width="5" height="6" font="font38" id="p6_t95" reading_order_no="100" segment_no="1" tag_type="figure">100</text>
<text top="107" left="434" width="5" height="6" font="font38" id="p6_t96" reading_order_no="101" segment_no="1" tag_type="figure">150</text>
<text top="107" left="452" width="5" height="6" font="font38" id="p6_t97" reading_order_no="102" segment_no="1" tag_type="figure">200</text>
<text top="97" left="383" width="6" height="6" font="font38" id="p6_t98" reading_order_no="98" segment_no="1" tag_type="figure">0.60</text>
<text top="88" left="383" width="6" height="6" font="font38" id="p6_t99" reading_order_no="97" segment_no="1" tag_type="figure">0.65</text>
<text top="79" left="383" width="6" height="6" font="font38" id="p6_t100" reading_order_no="96" segment_no="1" tag_type="figure">0.70</text>
<text top="71" left="383" width="6" height="6" font="font38" id="p6_t101" reading_order_no="95" segment_no="1" tag_type="figure">0.75</text>
<text top="91" left="381" width="0" height="6" font="font39" id="p6_t102" reading_order_no="94" segment_no="1" tag_type="figure">Accuracy</text>
<text top="65" left="388" width="70" height="6" font="font38" id="p6_t103" reading_order_no="93" segment_no="1" tag_type="figure">OoD Exposure: FashionMNIST (iD) + MNIST (OoD)</text>
<text top="107" left="476" width="4" height="6" font="font38" id="p6_t104" reading_order_no="109" segment_no="1" tag_type="figure">50</text>
<text top="107" left="493" width="5" height="6" font="font38" id="p6_t105" reading_order_no="110" segment_no="1" tag_type="figure">100</text>
<text top="107" left="511" width="5" height="6" font="font38" id="p6_t106" reading_order_no="111" segment_no="1" tag_type="figure">150</text>
<text top="107" left="529" width="5" height="6" font="font38" id="p6_t107" reading_order_no="112" segment_no="1" tag_type="figure">200</text>
<text top="101" left="458" width="8" height="6" font="font38" id="p6_t108" reading_order_no="108" segment_no="1" tag_type="figure">0.575</text>
<text top="95" left="458" width="8" height="6" font="font38" id="p6_t109" reading_order_no="107" segment_no="1" tag_type="figure">0.600</text>
<text top="89" left="458" width="8" height="6" font="font38" id="p6_t110" reading_order_no="106" segment_no="1" tag_type="figure">0.625</text>
<text top="83" left="458" width="8" height="6" font="font38" id="p6_t111" reading_order_no="105" segment_no="1" tag_type="figure">0.650</text>
<text top="77" left="458" width="8" height="6" font="font38" id="p6_t112" reading_order_no="104" segment_no="1" tag_type="figure">0.675</text>
<text top="65" left="458" width="77" height="12" font="font38" id="p6_t113" reading_order_no="103" segment_no="1" tag_type="figure">OoD Rejection: FashionMNIST (iD) + MNIST (OoD) 0.700</text>
<text top="154" left="400" width="4" height="6" font="font38" id="p6_t114" reading_order_no="118" segment_no="1" tag_type="figure">50</text>
<text top="154" left="416" width="5" height="6" font="font38" id="p6_t115" reading_order_no="119" segment_no="1" tag_type="figure">100</text>
<text top="154" left="434" width="5" height="6" font="font38" id="p6_t116" reading_order_no="120" segment_no="1" tag_type="figure">150</text>
<text top="154" left="452" width="5" height="6" font="font38" id="p6_t117" reading_order_no="121" segment_no="1" tag_type="figure">200</text>
<text top="157" left="410" width="25" height="6" font="font38" id="p6_t118" reading_order_no="122" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="144" left="384" width="4" height="6" font="font38" id="p6_t119" reading_order_no="117" segment_no="1" tag_type="figure">0.7</text>
<text top="134" left="384" width="4" height="6" font="font38" id="p6_t120" reading_order_no="116" segment_no="1" tag_type="figure">0.8</text>
<text top="123" left="384" width="4" height="6" font="font38" id="p6_t121" reading_order_no="115" segment_no="1" tag_type="figure">0.9</text>
<text top="138" left="383" width="0" height="6" font="font39" id="p6_t122" reading_order_no="114" segment_no="1" tag_type="figure">Accuracy</text>
<text top="112" left="388" width="70" height="6" font="font38" id="p6_t123" reading_order_no="113" segment_no="1" tag_type="figure">OoD Exposure: MNIST (iD) + FashionMNIST (OoD)</text>
<text top="154" left="476" width="4" height="6" font="font38" id="p6_t124" reading_order_no="127" segment_no="1" tag_type="figure">50</text>
<text top="154" left="493" width="5" height="6" font="font38" id="p6_t125" reading_order_no="128" segment_no="1" tag_type="figure">100</text>
<text top="154" left="511" width="5" height="6" font="font38" id="p6_t126" reading_order_no="129" segment_no="1" tag_type="figure">150</text>
<text top="154" left="529" width="5" height="6" font="font38" id="p6_t127" reading_order_no="130" segment_no="1" tag_type="figure">200</text>
<text top="157" left="487" width="25" height="6" font="font38" id="p6_t128" reading_order_no="131" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="144" left="461" width="4" height="6" font="font38" id="p6_t129" reading_order_no="126" segment_no="1" tag_type="figure">0.7</text>
<text top="134" left="461" width="4" height="6" font="font38" id="p6_t130" reading_order_no="125" segment_no="1" tag_type="figure">0.8</text>
<text top="123" left="461" width="4" height="6" font="font38" id="p6_t131" reading_order_no="124" segment_no="1" tag_type="figure">0.9</text>
<text top="112" left="465" width="70" height="6" font="font38" id="p6_t132" reading_order_no="123" segment_no="1" tag_type="figure">OoD Rejection: MNIST (iD) + FashionMNIST (OoD)</text>
<text top="159" left="444" width="31" height="10" font="font38" id="p6_t133" reading_order_no="132" segment_no="1" tag_type="figure">Evaluation Set Size 10</text>
<text top="163" left="461" width="5" height="6" font="font38" id="p6_t134" reading_order_no="133" segment_no="1" tag_type="figure">250</text>
<text top="163" left="480" width="7" height="6" font="font38" id="p6_t135" reading_order_no="134" segment_no="1" tag_type="figure">2000</text>
<text top="176" left="398" width="116" height="8" font="font16" id="p6_t136" reading_order_no="135" segment_no="1" tag_type="figure">(c) Evaluation Set Size Ablation</text>
<text top="197" left="55" width="465" height="8" font="font21" id="p6_t137" reading_order_no="136" segment_no="2" tag_type="text">Figure 4. MNIST and FashionMNIST pairings with OoD rejection or exposure. EPIG-BALD performs better than BALD. 5 trials.</text>
<text top="226" left="68" width="221" height="11" font="font4" id="p6_t138" reading_order_no="137" segment_no="3" tag_type="text">samples x eval ought to decrease when we also train on</text>
<text top="238" left="68" width="222" height="11" font="font4" id="p6_t139" reading_order_no="138" segment_no="3" tag_type="text">the evaluation set (using pseudo-labels y eval ), and we</text>
<text top="249" left="68" width="116" height="12" font="font4" id="p6_t140" reading_order_no="139" segment_no="3" tag_type="text">would expect the same for ˆ Ω .</text>
<text top="270" left="55" width="234" height="9" font="font4" id="p6_t141" reading_order_no="140" segment_no="6" tag_type="text">Hence, as a tractable approximation for Ω , we choose to</text>
<text top="279" left="55" width="234" height="12" font="font4" id="p6_t142" reading_order_no="141" segment_no="6" tag_type="text">use self-distillation , where we train a model with D train and</text>
<text top="291" left="55" width="234" height="12" font="font4" id="p6_t143" reading_order_no="142" segment_no="6" tag_type="text">the predictions of the original model p(Ω | D train ) on x eval</text>
<text top="306" left="55" width="234" height="9" font="font4" id="p6_t144" reading_order_no="143" segment_no="6" tag_type="text">using a KL-divergence loss. The resulting model posterior</text>
<text top="315" left="55" width="168" height="12" font="font18" id="p6_t145" reading_order_no="144" segment_no="6" tag_type="text">Ω ˆ fulfills both properties described above.</text>
<text top="345" left="55" width="119" height="11" font="font3" id="p6_t146" reading_order_no="145" segment_no="7" tag_type="title">5. Empirical Validation</text>
<text top="366" left="55" width="234" height="9" font="font4" id="p6_t147" reading_order_no="146" segment_no="8" tag_type="text">We evaluate the performance of EPIG and EPIG-BALD</text>
<text top="378" left="55" width="234" height="9" font="font4" id="p6_t148" reading_order_no="147" segment_no="8" tag_type="text">using self-distillation in a regular active learning setting and</text>
<text top="389" left="55" width="234" height="9" font="font4" id="p6_t149" reading_order_no="148" segment_no="8" tag_type="text">under distribution shift. We also provide an ablation with</text>
<text top="401" left="55" width="115" height="9" font="font4" id="p6_t150" reading_order_no="149" segment_no="8" tag_type="text">different evaluation set sizes.</text>
<text top="419" left="55" width="234" height="9" font="font1" id="p6_t151" reading_order_no="150" segment_no="10" tag_type="text">Setup. We compare various Bayesian acquisition functions</text>
<text top="431" left="55" width="234" height="9" font="font4" id="p6_t152" reading_order_no="151" segment_no="10" tag_type="text">under batch acquisition with acquisition size 5, both using</text>
<text top="443" left="55" width="234" height="9" font="font4" id="p6_t153" reading_order_no="152" segment_no="10" tag_type="text">the top-k individual scores ( Gal et al. , 2017 ) and using</text>
<text top="455" left="55" width="235" height="9" font="font4" id="p6_t154" reading_order_no="153" segment_no="10" tag_type="text">the joint density ( Kirsch et al. , 2019 ). On MNIST and</text>
<text top="467" left="55" width="234" height="9" font="font4" id="p6_t155" reading_order_no="154" segment_no="10" tag_type="text">MNISTx2 (RepeatedMNIST), we use a LeNet-5 model</text>
<text top="479" left="55" width="234" height="9" font="font4" id="p6_t156" reading_order_no="155" segment_no="10" tag_type="text">( LeCun et al. , 1998 ), which we train as described in Kirsch</text>
<text top="491" left="55" width="234" height="9" font="font6" id="p6_t157" reading_order_no="156" segment_no="10" tag_type="text">et al. ( 2019 ). We use MC dropout models with 100 dropout</text>
<text top="503" left="55" width="192" height="9" font="font4" id="p6_t158" reading_order_no="157" segment_no="10" tag_type="text">samples when computing the acquisition scores.</text>
<text top="521" left="55" width="234" height="9" font="font1" id="p6_t159" reading_order_no="158" segment_no="12" tag_type="text">Performance in Regular Active Learning. We compare</text>
<text top="533" left="55" width="234" height="9" font="font4" id="p6_t160" reading_order_no="159" segment_no="12" tag_type="text">BALD and EPIG-BALD in fig. 3 on MNISTx2 and EPIG</text>
<text top="545" left="55" width="234" height="9" font="font4" id="p6_t161" reading_order_no="160" segment_no="12" tag_type="text">and EPIG-BALD in fig. 2 on MNIST. In the regular active</text>
<text top="557" left="55" width="234" height="9" font="font4" id="p6_t162" reading_order_no="161" segment_no="12" tag_type="text">learning setup, there is no distribution shift between the pool</text>
<text top="569" left="55" width="234" height="9" font="font4" id="p6_t163" reading_order_no="162" segment_no="12" tag_type="text">set and test set, so we use all of the unlabeled pool set as</text>
<text top="581" left="55" width="57" height="9" font="font4" id="p6_t164" reading_order_no="163" segment_no="12" tag_type="text">evaluation set.</text>
<text top="599" left="55" width="234" height="9" font="font4" id="p6_t165" reading_order_no="164" segment_no="15" tag_type="text">Both in the top-k and the batch variant, EPIG-BALD</text>
<text top="611" left="55" width="235" height="9" font="font4" id="p6_t166" reading_order_no="165" segment_no="15" tag_type="text">outperforms BALD on MNISTx2 (and also MNIST,</text>
<text top="623" left="55" width="50" height="9" font="font4" id="p6_t167" reading_order_no="166" segment_no="15" tag_type="text">not shown).</text>
<text top="623" left="116" width="174" height="9" font="font4" id="p6_t168" reading_order_no="167" segment_no="15" tag_type="text">Yet, EPIG does not perform better than</text>
<text top="635" left="55" width="234" height="9" font="font4" id="p6_t169" reading_order_no="168" segment_no="15" tag_type="text">uniform acquisition even on MNIST. This is because the</text>
<text top="647" left="55" width="234" height="9" font="font4" id="p6_t170" reading_order_no="169" segment_no="15" tag_type="text">two approximate Bayesian models are separately trained</text>
<text top="658" left="55" width="85" height="9" font="font4" id="p6_t171" reading_order_no="170" segment_no="15" tag_type="text">and not compatible.</text>
<text top="658" left="151" width="138" height="9" font="font4" id="p6_t172" reading_order_no="171" segment_no="15" tag_type="text">EPIG-BALD which is based on</text>
<text top="670" left="55" width="236" height="9" font="font4" id="p6_t173" reading_order_no="172" segment_no="15" tag_type="text">a difference of epistemic uncertainties works, however.</text>
<text top="682" left="55" width="234" height="9" font="font4" id="p6_t174" reading_order_no="173" segment_no="15" tag_type="text">We conclude that the difference in informativeness and</text>
<text top="694" left="55" width="236" height="9" font="font4" id="p6_t175" reading_order_no="174" segment_no="15" tag_type="text">epistemic uncertainty (via mutual information) in EPIG-</text>
<text top="706" left="55" width="234" height="9" font="font4" id="p6_t176" reading_order_no="175" segment_no="15" tag_type="text">BALD is more meaningful than the difference in overall</text>
<text top="228" left="307" width="184" height="9" font="font4" id="p6_t177" reading_order_no="176" segment_no="4" tag_type="text">uncertainty (via conditional entropy) in EPIG.</text>
<text top="246" left="307" width="234" height="9" font="font1" id="p6_t178" reading_order_no="177" segment_no="5" tag_type="text">Performance in Active Learning under Distribution</text>
<text top="258" left="307" width="234" height="9" font="font1" id="p6_t179" reading_order_no="178" segment_no="5" tag_type="text">Shift with MNIST and FashionMNIST. We compare</text>
<text top="270" left="307" width="210" height="9" font="font4" id="p6_t180" reading_order_no="179" segment_no="5" tag_type="text">BALD and EPIG-BALD under distribution shift.</text>
<text top="270" left="528" width="14" height="9" font="font4" id="p6_t181" reading_order_no="180" segment_no="5" tag_type="text">For</text>
<text top="282" left="307" width="234" height="9" font="font4" id="p6_t182" reading_order_no="181" segment_no="5" tag_type="text">this, we add junk out-of-distribution data to the pool</text>
<text top="294" left="307" width="14" height="9" font="font4" id="p6_t183" reading_order_no="182" segment_no="5" tag_type="text">set.</text>
<text top="294" left="331" width="211" height="9" font="font4" id="p6_t184" reading_order_no="183" segment_no="5" tag_type="text">In this experiment, the pool set contains MNIST</text>
<text top="306" left="307" width="234" height="9" font="font4" id="p6_t185" reading_order_no="184" segment_no="5" tag_type="text">and FashionMNIST ( Xiao et al. , 2017 ), while the test set<a href="deeplearning_paper2.html#7">(</a></text>
<text top="318" left="307" width="234" height="9" font="font4" id="p6_t186" reading_order_no="185" segment_no="5" tag_type="text">contains one or the other. We deal with an acquisition<a href="deeplearning_paper2.html#7">Gal et al.</a></text>
<text top="330" left="307" width="234" height="9" font="font4" id="p6_t187" reading_order_no="186" segment_no="5" tag_type="text">function attempting to acquire OoD data in two different<a href="deeplearning_paper2.html#7">,</a></text>
<text top="342" left="307" width="234" height="9" font="font4" id="p6_t188" reading_order_no="187" segment_no="5" tag_type="text">modes: OoD rejection rejects OoD data from the batch and<a href="deeplearning_paper2.html#7">2017</a></text>
<text top="353" left="307" width="234" height="10" font="font4" id="p6_t189" reading_order_no="188" segment_no="5" tag_type="text">does not acquire it; while OoD exposure acquires OoD data<a href="deeplearning_paper2.html#7">) </a>and using</text>
<text top="365" left="307" width="234" height="9" font="font4" id="p6_t190" reading_order_no="189" segment_no="5" tag_type="text">with uniform targets, similar to outlier exposure methods<a href="deeplearning_paper2.html#7">(</a></text>
<text top="377" left="307" width="235" height="9" font="font4" id="p6_t191" reading_order_no="190" segment_no="5" tag_type="text">in OoD detection ( Hendrycks et al. , 2018 ). We use an<a href="deeplearning_paper2.html#7">Kirsch et al.</a></text>
<text top="389" left="307" width="175" height="9" font="font4" id="p6_t192" reading_order_no="191" segment_no="5" tag_type="text">evaluation set with 2000 unlabeled samples.<a href="deeplearning_paper2.html#7">,</a></text>
<text top="407" left="307" width="235" height="9" font="font4" id="p6_t193" reading_order_no="192" segment_no="9" tag_type="text">EPIG-BALD outperforms BALD on in all combinations,<a href="deeplearning_paper2.html#7">2019</a></text>
<text top="419" left="307" width="234" height="9" font="font4" id="p6_t194" reading_order_no="193" segment_no="9" tag_type="text">see fig. 4a . In all cases but one, EPIG-BALD acquires<a href="deeplearning_paper2.html#7">). </a>On MNIST and</text>
<text top="431" left="307" width="234" height="9" font="font4" id="p6_t195" reading_order_no="194" segment_no="9" tag_type="text">fewer junk/OoD samples, see fig. 4b . The ablation in fig. 4c</text>
<text top="443" left="307" width="234" height="9" font="font4" id="p6_t196" reading_order_no="195" segment_no="9" tag_type="text">shows that larger evaluation sets are beneficial. Note that the<a href="deeplearning_paper2.html#7">(</a></text>
<text top="455" left="307" width="234" height="9" font="font4" id="p6_t197" reading_order_no="196" segment_no="9" tag_type="text">evaluation set is unlabeled and thus does not count towards<a href="deeplearning_paper2.html#7">LeCun et al.</a></text>
<text top="467" left="307" width="81" height="9" font="font4" id="p6_t198" reading_order_no="197" segment_no="9" tag_type="text">sample acquisitions.<a href="deeplearning_paper2.html#7">,</a></text>
<text top="485" left="307" width="235" height="9" font="font4" id="p6_t199" reading_order_no="198" segment_no="11" tag_type="text">We provide results for active learning on CIFAR-10 in § A.1<a href="deeplearning_paper2.html#7">1998</a></text>
<text top="497" left="307" width="236" height="9" font="font4" id="p6_t200" reading_order_no="199" segment_no="11" tag_type="text">and for active learning under distribution shift with CIFAR-<a href="deeplearning_paper2.html#7">), </a>which we train as described in</text>
<text top="509" left="307" width="91" height="9" font="font4" id="p6_t201" reading_order_no="200" segment_no="11" tag_type="text">10 and SVHN in § A.2 .<a href="deeplearning_paper2.html#7">Kirsch</a></text>
<text top="536" left="307" width="144" height="11" font="font3" id="p6_t202" reading_order_no="201" segment_no="13" tag_type="title">6. Conclusion &amp; Limitations<a href="deeplearning_paper2.html#7">et al.</a></text>
<text top="557" left="307" width="234" height="9" font="font4" id="p6_t203" reading_order_no="202" segment_no="14" tag_type="text">We have introduced a new Bayesian acquisition function<a href="deeplearning_paper2.html#7">(</a></text>
<text top="569" left="307" width="234" height="9" font="font4" id="p6_t204" reading_order_no="203" segment_no="14" tag_type="text">which is grounded in information theory and shown that<a href="deeplearning_paper2.html#7">2019</a></text>
<text top="581" left="307" width="234" height="9" font="font4" id="p6_t205" reading_order_no="204" segment_no="14" tag_type="text">it outperforms BALD in both the regular active learning<a href="deeplearning_paper2.html#7">). </a>We use MC dropout models with 100 dropout</text>
<text top="593" left="307" width="234" height="9" font="font4" id="p6_t206" reading_order_no="205" segment_no="14" tag_type="text">setup as well as active learning under distribution shift</text>
<text top="605" left="307" width="234" height="9" font="font4" id="p6_t207" reading_order_no="206" segment_no="14" tag_type="text">of the pool set. While EPIG-BALD selects fewer OoD</text>
<text top="617" left="307" width="234" height="9" font="font4" id="p6_t208" reading_order_no="207" segment_no="14" tag_type="text">junk samples than BALD, we would like it to select even</text>
<text top="629" left="307" width="234" height="9" font="font4" id="p6_t209" reading_order_no="208" segment_no="14" tag_type="text">fewer. Moreover, we need to examine additional dataset</text>
<text top="641" left="307" width="234" height="9" font="font4" id="p6_t210" reading_order_no="209" segment_no="14" tag_type="text">combinations and other sources of distribution shift, for<a href="deeplearning_paper2.html#2">3</a></text>
<text top="652" left="307" width="168" height="9" font="font4" id="p6_t211" reading_order_no="210" segment_no="14" tag_type="text">example class imbalances and label noise.</text>
<text top="676" left="308" width="56" height="9" font="font4" id="p6_t212" reading_order_no="211" segment_no="16" tag_type="title">R EFERENCES</text>
<text top="694" left="307" width="52" height="9" font="font4" id="p6_t213" reading_order_no="212" segment_no="17" tag_type="text">Anonymous.<a href="deeplearning_paper2.html#2">2</a></text>
<text top="694" left="376" width="165" height="9" font="font4" id="p6_t214" reading_order_no="213" segment_no="17" tag_type="text">Batch active learning with stochastic</text>
<text top="706" left="317" width="226" height="9" font="font4" id="p6_t215" reading_order_no="214" segment_no="17" tag_type="text">acquisition functions. ICML Workshop Submission , 2021.</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font41" size="10" family="NimbusMonL-Regu" color="#001472"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="70" left="55" width="234" height="9" font="font4" id="p7_t2" reading_order_no="1" segment_no="1" tag_type="text">Les E Atlas, David A Cohn, and Richard E Ladner. Training</text>
<text top="82" left="65" width="224" height="9" font="font4" id="p7_t3" reading_order_no="2" segment_no="1" tag_type="text">connectionist networks with queries and selective</text>
<text top="94" left="65" width="225" height="9" font="font4" id="p7_t4" reading_order_no="3" segment_no="1" tag_type="text">sampling. In Advances in neural information processing</text>
<text top="106" left="65" width="161" height="9" font="font5" id="p7_t5" reading_order_no="4" segment_no="1" tag_type="text">systems , pages 566–573. Citeseer, 1990.</text>
<text top="126" left="55" width="236" height="9" font="font4" id="p7_t6" reading_order_no="5" segment_no="3" tag_type="text">William Bialek and Naftali Tishby. Predictive information.</text>
<text top="138" left="65" width="162" height="9" font="font5" id="p7_t7" reading_order_no="6" segment_no="3" tag_type="text">arXiv preprint cond-mat/9902341 , 1999.</text>
<text top="157" left="55" width="236" height="9" font="font4" id="p7_t8" reading_order_no="7" segment_no="6" tag_type="text">Adam D Cobb, Stephen J Roberts, and Yarin Gal. Loss-</text>
<text top="169" left="65" width="224" height="9" font="font4" id="p7_t9" reading_order_no="8" segment_no="6" tag_type="text">calibrated approximate inference in bayesian neural</text>
<text top="181" left="65" width="202" height="9" font="font4" id="p7_t10" reading_order_no="9" segment_no="6" tag_type="text">networks. arXiv preprint arXiv:1805.03901 , 2018.</text>
<text top="201" left="55" width="234" height="9" font="font4" id="p7_t11" reading_order_no="10" segment_no="8" tag_type="text">Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep</text>
<text top="213" left="65" width="225" height="9" font="font4" id="p7_t12" reading_order_no="11" segment_no="8" tag_type="text">bayesian active learning with image data. In International</text>
<text top="225" left="65" width="226" height="9" font="font5" id="p7_t13" reading_order_no="12" segment_no="8" tag_type="text">Conference on Machine Learning , pages 1183–1192.</text>
<text top="237" left="65" width="55" height="9" font="font4" id="p7_t14" reading_order_no="13" segment_no="8" tag_type="text">PMLR, 2017.</text>
<text top="256" left="55" width="234" height="9" font="font4" id="p7_t15" reading_order_no="14" segment_no="10" tag_type="text">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian</text>
<text top="268" left="65" width="224" height="9" font="font4" id="p7_t16" reading_order_no="15" segment_no="10" tag_type="text">Sun. Deep residual learning for image recognition. In</text>
<text top="280" left="65" width="224" height="9" font="font5" id="p7_t17" reading_order_no="16" segment_no="10" tag_type="text">Proceedings of the IEEE conference on computer vision</text>
<text top="292" left="65" width="187" height="9" font="font5" id="p7_t18" reading_order_no="17" segment_no="10" tag_type="text">and pattern recognition , pages 770–778, 2016.</text>
<text top="312" left="55" width="236" height="9" font="font4" id="p7_t19" reading_order_no="18" segment_no="11" tag_type="text">Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.</text>
<text top="324" left="65" width="225" height="9" font="font4" id="p7_t20" reading_order_no="19" segment_no="11" tag_type="text">Deep anomaly detection with outlier exposure. arXiv</text>
<text top="336" left="65" width="135" height="9" font="font5" id="p7_t21" reading_order_no="20" segment_no="11" tag_type="text">preprint arXiv:1812.04606 , 2018.</text>
<text top="356" left="55" width="234" height="9" font="font4" id="p7_t22" reading_order_no="21" segment_no="12" tag_type="text">Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and</text>
<text top="368" left="65" width="224" height="9" font="font4" id="p7_t23" reading_order_no="22" segment_no="12" tag_type="text">Máté Lengyel. Bayesian active learning for classification</text>
<text top="379" left="65" width="225" height="10" font="font4" id="p7_t24" reading_order_no="23" segment_no="12" tag_type="text">and preference learning. arXiv preprint arXiv:1112.5745 ,</text>
<text top="391" left="65" width="22" height="9" font="font4" id="p7_t25" reading_order_no="24" segment_no="12" tag_type="text">2011.</text>
<text top="411" left="55" width="236" height="9" font="font4" id="p7_t26" reading_order_no="25" segment_no="13" tag_type="text">Andreas Kirsch, Joost van Amersfoort, and Yarin Gal.</text>
<text top="423" left="65" width="224" height="9" font="font4" id="p7_t27" reading_order_no="26" segment_no="13" tag_type="text">Batchbald: Efficient and diverse batch acquisition for</text>
<text top="435" left="65" width="225" height="9" font="font4" id="p7_t28" reading_order_no="27" segment_no="13" tag_type="text">deep bayesian active learning. In Advances in Neural</text>
<text top="447" left="65" width="226" height="9" font="font5" id="p7_t29" reading_order_no="28" segment_no="13" tag_type="text">Information Processing Systems , pages 7024–7035, 2019.</text>
<text top="467" left="55" width="234" height="9" font="font4" id="p7_t30" reading_order_no="29" segment_no="14" tag_type="text">Andreas Krause and Daniel Golovin. Submodular function</text>
<text top="479" left="65" width="178" height="9" font="font4" id="p7_t31" reading_order_no="30" segment_no="14" tag_type="text">maximization. Tractability , 3:71–104, 2014.</text>
<text top="498" left="55" width="234" height="9" font="font4" id="p7_t32" reading_order_no="31" segment_no="15" tag_type="text">Alex Krizhevsky et al. Learning multiple layers of features</text>
<text top="510" left="65" width="96" height="9" font="font4" id="p7_t33" reading_order_no="32" segment_no="15" tag_type="text">from tiny images. 2009.</text>
<text top="530" left="55" width="234" height="9" font="font4" id="p7_t34" reading_order_no="33" segment_no="16" tag_type="text">Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick</text>
<text top="542" left="65" width="224" height="9" font="font4" id="p7_t35" reading_order_no="34" segment_no="16" tag_type="text">Haffner. Gradient-based learning applied to document</text>
<text top="554" left="65" width="226" height="9" font="font4" id="p7_t36" reading_order_no="35" segment_no="16" tag_type="text">recognition. Proceedings of the IEEE , 86(11):2278–2324,</text>
<text top="566" left="65" width="22" height="9" font="font4" id="p7_t37" reading_order_no="36" segment_no="16" tag_type="text">1998.</text>
<text top="585" left="55" width="235" height="9" font="font4" id="p7_t38" reading_order_no="37" segment_no="17" tag_type="text">Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo,</text>
<text top="597" left="65" width="226" height="9" font="font4" id="p7_t39" reading_order_no="38" segment_no="17" tag_type="text">Honglak Lee, John Canny, and Sergio Guadarrama.</text>
<text top="609" left="65" width="225" height="9" font="font4" id="p7_t40" reading_order_no="39" segment_no="17" tag_type="text">Predictive information accelerates learning in rl. arXiv</text>
<text top="621" left="65" width="135" height="9" font="font5" id="p7_t41" reading_order_no="40" segment_no="17" tag_type="text">preprint arXiv:2007.12401 , 2020.</text>
<text top="641" left="55" width="234" height="9" font="font4" id="p7_t42" reading_order_no="41" segment_no="18" tag_type="text">Dennis V Lindley. On a measure of the information provided</text>
<text top="653" left="65" width="225" height="9" font="font4" id="p7_t43" reading_order_no="42" segment_no="18" tag_type="text">by an experiment. The Annals of Mathematical Statistics ,</text>
<text top="665" left="65" width="92" height="9" font="font4" id="p7_t44" reading_order_no="43" segment_no="18" tag_type="text">pages 986–1005, 1956.</text>
<text top="684" left="55" width="234" height="9" font="font4" id="p7_t45" reading_order_no="44" segment_no="19" tag_type="text">Yuval Netzer, Tao Wang, Adam Coates, Alessandro</text>
<text top="696" left="65" width="175" height="9" font="font4" id="p7_t46" reading_order_no="45" segment_no="19" tag_type="text">Bissacco, Bo Wu, and Andrew Y. Ng.</text>
<text top="696" left="256" width="34" height="9" font="font4" id="p7_t47" reading_order_no="46" segment_no="19" tag_type="text">Reading</text>
<text top="708" left="65" width="224" height="9" font="font4" id="p7_t48" reading_order_no="47" segment_no="19" tag_type="text">digits in natural images with unsupervised feature</text>
<text top="70" left="317" width="36" height="9" font="font4" id="p7_t49" reading_order_no="48" segment_no="2" tag_type="text">learning.</text>
<text top="70" left="362" width="180" height="9" font="font4" id="p7_t50" reading_order_no="49" segment_no="2" tag_type="text">In NIPS Workshop on Deep Learning and</text>
<text top="82" left="317" width="228" height="9" font="font5" id="p7_t51" reading_order_no="50" segment_no="2" tag_type="text">Unsupervised Feature Learning 2011 , 2011. URL http:</text>
<text top="95" left="317" width="203" height="8" font="font41" id="p7_t52" reading_order_no="51" segment_no="2" tag_type="text">//ufldl.stanford.edu/housenumbers/</text>
<text top="106" left="317" width="152" height="9" font="font41" id="p7_t53" reading_order_no="52" segment_no="2" tag_type="text">nips2011_housenumbers.pdf .</text>
<text top="126" left="307" width="210" height="9" font="font4" id="p7_t54" reading_order_no="53" segment_no="4" tag_type="text">Burr Settles. Active learning literature survey. 2009.</text>
<text top="146" left="307" width="234" height="9" font="font4" id="p7_t55" reading_order_no="54" segment_no="5" tag_type="text">Lewis Smith and Yarin Gal. Understanding measures of</text>
<text top="158" left="317" width="193" height="9" font="font4" id="p7_t56" reading_order_no="55" segment_no="5" tag_type="text">uncertainty for adversarial example detection.</text>
<text top="158" left="519" width="23" height="9" font="font5" id="p7_t57" reading_order_no="56" segment_no="5" tag_type="text">arXiv</text>
<text top="170" left="317" width="135" height="9" font="font5" id="p7_t58" reading_order_no="57" segment_no="5" tag_type="text">preprint arXiv:1803.08533 , 2018.</text>
<text top="190" left="307" width="234" height="9" font="font4" id="p7_t59" reading_order_no="58" segment_no="7" tag_type="text">Niranjan Srinivas, Andreas Krause, Sham M Kakade, and</text>
<text top="202" left="317" width="224" height="9" font="font4" id="p7_t60" reading_order_no="59" segment_no="7" tag_type="text">Matthias Seeger. Gaussian process optimization in the</text>
<text top="214" left="317" width="224" height="9" font="font4" id="p7_t61" reading_order_no="60" segment_no="7" tag_type="text">bandit setting: No regret and experimental design. arXiv</text>
<text top="226" left="317" width="130" height="9" font="font5" id="p7_t62" reading_order_no="61" segment_no="7" tag_type="text">preprint arXiv:0912.3995 , 2009.</text>
<text top="246" left="307" width="236" height="9" font="font4" id="p7_t63" reading_order_no="62" segment_no="9" tag_type="text">Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-</text>
<text top="258" left="317" width="224" height="9" font="font4" id="p7_t64" reading_order_no="63" segment_no="9" tag_type="text">mnist: a novel image dataset for benchmarking machine</text>
<text top="270" left="317" width="226" height="9" font="font4" id="p7_t65" reading_order_no="64" segment_no="9" tag_type="text">learning algorithms. arXiv preprint arXiv:1708.07747 ,</text>
<text top="281" left="317" width="22" height="9" font="font4" id="p7_t66" reading_order_no="65" segment_no="9" tag_type="text">2017.</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
<text top="48" left="173" width="251" height="8" font="font20" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="69" left="55" width="137" height="11" font="font3" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="title">A. Additional Experiments</text>
<text top="90" left="55" width="196" height="9" font="font1" id="p8_t3" reading_order_no="2" segment_no="2" tag_type="title">A.1. Performance in Regular Active Learning.</text>
<text top="108" left="55" width="235" height="9" font="font1" id="p8_t4" reading_order_no="3" segment_no="3" tag_type="text">CIFAR-10. EPIG-BALD outperforms SoftmaxBALD, as</text>
<text top="120" left="55" width="236" height="9" font="font4" id="p8_t5" reading_order_no="4" segment_no="3" tag_type="text">depicted in fig. 6 . For CIFAR-10 ( Krizhevsky et al. , 2009 ),</text>
<text top="132" left="55" width="235" height="9" font="font4" id="p8_t6" reading_order_no="5" segment_no="3" tag_type="text">we use a ResNet18 model ( He et al. , 2016 ) which was</text>
<text top="144" left="55" width="235" height="9" font="font4" id="p8_t7" reading_order_no="6" segment_no="3" tag_type="text">modified as described in Kirsch et al. ( 2019 ) to add MC<a href="deeplearning_paper2.html#9">6</a></text>
<text top="156" left="55" width="234" height="9" font="font4" id="p8_t8" reading_order_no="7" segment_no="3" tag_type="text">dropout to the classifier head and also follows the described<a href="deeplearning_paper2.html#9">. </a>For CIFAR-10 <a href="deeplearning_paper2.html#7">(</a></text>
<text top="168" left="55" width="234" height="9" font="font4" id="p8_t9" reading_order_no="8" segment_no="3" tag_type="text">training regime. We train with an acquisition batch size of<a href="deeplearning_paper2.html#7">Krizhevsky et al.</a></text>
<text top="180" left="55" width="234" height="9" font="font4" id="p8_t10" reading_order_no="9" segment_no="3" tag_type="text">250 and an intial training set size of 1000. We use stochastic<a href="deeplearning_paper2.html#7">,</a></text>
<text top="192" left="55" width="234" height="9" font="font4" id="p8_t11" reading_order_no="10" segment_no="3" tag_type="text">acquisition functions (Softmax*) instead of BatchBALD<a href="deeplearning_paper2.html#7">2009</a></text>
<text top="204" left="55" width="234" height="9" font="font4" id="p8_t12" reading_order_no="11" segment_no="3" tag_type="text">and variants, which samples without replacement from the<a href="deeplearning_paper2.html#7">),</a></text>
<text top="216" left="55" width="234" height="9" font="font4" id="p8_t13" reading_order_no="12" segment_no="3" tag_type="text">pool set using the Softmax of the acquisition scores with<a href="deeplearning_paper2.html#7">(</a></text>
<text top="228" left="55" width="141" height="9" font="font4" id="p8_t14" reading_order_no="13" segment_no="3" tag_type="text">temperature 8 ( Anonymous , 2021 ).<a href="deeplearning_paper2.html#7">He et al.</a></text>
<text top="253" left="55" width="184" height="9" font="font1" id="p8_t15" reading_order_no="14" segment_no="4" tag_type="title">A.2. Performance in Active Learning under<a href="deeplearning_paper2.html#7">,</a></text>
<text top="265" left="75" width="77" height="9" font="font1" id="p8_t16" reading_order_no="15" segment_no="4" tag_type="title">Distribution Shift.<a href="deeplearning_paper2.html#7">2016</a></text>
<text top="283" left="55" width="235" height="10" font="font1" id="p8_t17" reading_order_no="16" segment_no="5" tag_type="text">CIFAR-10 and SVHN. EPIG-BALD outperforms BALD<a href="deeplearning_paper2.html#7">) </a>which was</text>
<text top="296" left="55" width="234" height="9" font="font4" id="p8_t18" reading_order_no="17" segment_no="5" tag_type="text">under distribution shift with CIFAR-10 and SVHN ( Netzer</text>
<text top="307" left="55" width="234" height="9" font="font6" id="p8_t19" reading_order_no="18" segment_no="5" tag_type="text">et al. , 2011 ) in all but one combination and selects fewer<a href="deeplearning_paper2.html#7">Kirsch et al.</a></text>
<text top="319" left="55" width="235" height="9" font="font4" id="p8_t20" reading_order_no="19" segment_no="5" tag_type="text">OoD samples, see fig. 5 . We use an evaluation set with 1000<a href="deeplearning_paper2.html#7">(</a></text>
<text top="331" left="55" width="76" height="9" font="font4" id="p8_t21" reading_order_no="20" segment_no="5" tag_type="text">unlabeled samples.<a href="deeplearning_paper2.html#7">2019</a></text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font42" size="7" family="DejaVuSans" color="#262626"/>
	<fontspec id="font43" size="7" family="DejaVuSans" color="#262626"/>
	<fontspec id="font44" size="9" family="DejaVuSans" color="#262626"/>
	<fontspec id="font45" size="9" family="DejaVuSans" color="#262626"/>
<text top="48" left="173" width="251" height="8" font="font20" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="title">Active Learning under Pool Set Distribution Shift and Noisy Data</text>
<text top="126" left="91" width="5" height="8" font="font42" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="figure">50</text>
<text top="126" left="117" width="8" height="8" font="font42" id="p9_t3" reading_order_no="2" segment_no="1" tag_type="figure">100</text>
<text top="126" left="144" width="8" height="8" font="font42" id="p9_t4" reading_order_no="3" segment_no="1" tag_type="figure">150</text>
<text top="126" left="171" width="8" height="8" font="font42" id="p9_t5" reading_order_no="4" segment_no="1" tag_type="figure">200</text>
<text top="119" left="65" width="9" height="8" font="font42" id="p9_t6" reading_order_no="5" segment_no="1" tag_type="figure">0.10</text>
<text top="107" left="65" width="9" height="8" font="font42" id="p9_t7" reading_order_no="6" segment_no="1" tag_type="figure">0.15</text>
<text top="95" left="65" width="9" height="8" font="font42" id="p9_t8" reading_order_no="7" segment_no="1" tag_type="figure">0.20</text>
<text top="83" left="65" width="9" height="8" font="font42" id="p9_t9" reading_order_no="8" segment_no="1" tag_type="figure">0.25</text>
<text top="71" left="65" width="9" height="8" font="font42" id="p9_t10" reading_order_no="9" segment_no="1" tag_type="figure">0.30</text>
<text top="103" left="62" width="0" height="8" font="font43" id="p9_t11" reading_order_no="10" segment_no="1" tag_type="figure">Accuracy</text>
<text top="64" left="79" width="94" height="8" font="font42" id="p9_t12" reading_order_no="11" segment_no="1" tag_type="figure">OoD Rejection: SVHN (iD) vs CIFAR-10 (ooD)</text>
<text top="126" left="209" width="5" height="8" font="font42" id="p9_t13" reading_order_no="12" segment_no="1" tag_type="figure">50</text>
<text top="126" left="235" width="8" height="8" font="font42" id="p9_t14" reading_order_no="13" segment_no="1" tag_type="figure">100</text>
<text top="126" left="262" width="8" height="8" font="font42" id="p9_t15" reading_order_no="14" segment_no="1" tag_type="figure">150</text>
<text top="126" left="289" width="8" height="8" font="font42" id="p9_t16" reading_order_no="15" segment_no="1" tag_type="figure">200</text>
<text top="118" left="183" width="9" height="8" font="font42" id="p9_t17" reading_order_no="16" segment_no="1" tag_type="figure">0.10</text>
<text top="107" left="183" width="9" height="8" font="font42" id="p9_t18" reading_order_no="17" segment_no="1" tag_type="figure">0.15</text>
<text top="97" left="183" width="9" height="8" font="font42" id="p9_t19" reading_order_no="18" segment_no="1" tag_type="figure">0.20</text>
<text top="86" left="183" width="9" height="8" font="font42" id="p9_t20" reading_order_no="19" segment_no="1" tag_type="figure">0.25</text>
<text top="76" left="183" width="9" height="8" font="font42" id="p9_t21" reading_order_no="20" segment_no="1" tag_type="figure">0.30</text>
<text top="64" left="197" width="94" height="8" font="font42" id="p9_t22" reading_order_no="21" segment_no="1" tag_type="figure">OoD Exposure: SVHN (iD) vs CIFAR-10 (ooD)</text>
<text top="197" left="91" width="5" height="8" font="font42" id="p9_t23" reading_order_no="22" segment_no="1" tag_type="figure">50</text>
<text top="197" left="117" width="8" height="8" font="font42" id="p9_t24" reading_order_no="28" segment_no="1" tag_type="figure">100</text>
<text top="197" left="144" width="8" height="8" font="font42" id="p9_t25" reading_order_no="29" segment_no="1" tag_type="figure">150</text>
<text top="197" left="171" width="8" height="8" font="font42" id="p9_t26" reading_order_no="30" segment_no="1" tag_type="figure">200</text>
<text top="203" left="108" width="38" height="8" font="font42" id="p9_t27" reading_order_no="31" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="180" left="65" width="9" height="8" font="font42" id="p9_t28" reading_order_no="23" segment_no="1" tag_type="figure">0.20</text>
<text top="166" left="65" width="9" height="8" font="font42" id="p9_t29" reading_order_no="24" segment_no="1" tag_type="figure">0.25</text>
<text top="152" left="65" width="9" height="8" font="font42" id="p9_t30" reading_order_no="25" segment_no="1" tag_type="figure">0.30</text>
<text top="174" left="62" width="0" height="8" font="font43" id="p9_t31" reading_order_no="26" segment_no="1" tag_type="figure">Accuracy</text>
<text top="135" left="79" width="94" height="8" font="font42" id="p9_t32" reading_order_no="27" segment_no="1" tag_type="figure">OoD Rejection: CIFAR-10 (iD) vs SVHN (ooD)</text>
<text top="197" left="209" width="5" height="8" font="font42" id="p9_t33" reading_order_no="36" segment_no="1" tag_type="figure">50</text>
<text top="197" left="235" width="8" height="8" font="font42" id="p9_t34" reading_order_no="37" segment_no="1" tag_type="figure">100</text>
<text top="197" left="262" width="8" height="8" font="font42" id="p9_t35" reading_order_no="38" segment_no="1" tag_type="figure">150</text>
<text top="197" left="289" width="8" height="8" font="font42" id="p9_t36" reading_order_no="39" segment_no="1" tag_type="figure">200</text>
<text top="203" left="226" width="38" height="8" font="font42" id="p9_t37" reading_order_no="40" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="179" left="183" width="9" height="8" font="font42" id="p9_t38" reading_order_no="35" segment_no="1" tag_type="figure">0.20</text>
<text top="165" left="183" width="9" height="8" font="font42" id="p9_t39" reading_order_no="34" segment_no="1" tag_type="figure">0.25</text>
<text top="151" left="183" width="9" height="8" font="font42" id="p9_t40" reading_order_no="33" segment_no="1" tag_type="figure">0.30</text>
<text top="135" left="197" width="94" height="8" font="font42" id="p9_t41" reading_order_no="32" segment_no="1" tag_type="figure">OoD Exposure: CIFAR-10 (iD) vs SVHN (ooD)</text>
<text top="205" left="166" width="43" height="8" font="font42" id="p9_t42" reading_order_no="41" segment_no="1" tag_type="figure">Acquisition Function</text>
<text top="212" left="154" width="35" height="8" font="font42" id="p9_t43" reading_order_no="42" segment_no="1" tag_type="figure">BatchEPIG-BALD</text>
<text top="212" left="209" width="24" height="8" font="font42" id="p9_t44" reading_order_no="43" segment_no="1" tag_type="figure">BatchBALD</text>
<text top="228" left="155" width="46" height="8" font="font16" id="p9_t45" reading_order_no="44" segment_no="1" tag_type="figure">(a) Accuracy</text>
<text top="126" left="329" width="5" height="8" font="font42" id="p9_t46" reading_order_no="54" segment_no="1" tag_type="figure">50</text>
<text top="126" left="355" width="8" height="8" font="font42" id="p9_t47" reading_order_no="55" segment_no="1" tag_type="figure">100</text>
<text top="126" left="383" width="8" height="8" font="font42" id="p9_t48" reading_order_no="56" segment_no="1" tag_type="figure">150</text>
<text top="126" left="410" width="8" height="8" font="font42" id="p9_t49" reading_order_no="57" segment_no="1" tag_type="figure">200</text>
<text top="121" left="305" width="7" height="8" font="font42" id="p9_t50" reading_order_no="53" segment_no="1" tag_type="figure">0.0</text>
<text top="112" left="305" width="7" height="8" font="font42" id="p9_t51" reading_order_no="52" segment_no="1" tag_type="figure">0.1</text>
<text top="103" left="305" width="7" height="8" font="font42" id="p9_t52" reading_order_no="51" segment_no="1" tag_type="figure">0.2</text>
<text top="94" left="305" width="7" height="8" font="font42" id="p9_t53" reading_order_no="50" segment_no="1" tag_type="figure">0.3</text>
<text top="86" left="305" width="7" height="8" font="font42" id="p9_t54" reading_order_no="49" segment_no="1" tag_type="figure">0.4</text>
<text top="77" left="305" width="7" height="8" font="font42" id="p9_t55" reading_order_no="48" segment_no="1" tag_type="figure">0.5</text>
<text top="68" left="305" width="7" height="8" font="font42" id="p9_t56" reading_order_no="47" segment_no="1" tag_type="figure">0.6</text>
<text top="117" left="303" width="0" height="8" font="font43" id="p9_t57" reading_order_no="46" segment_no="1" tag_type="figure">Acquired OoD Fraction</text>
<text top="64" left="318" width="94" height="8" font="font42" id="p9_t58" reading_order_no="45" segment_no="1" tag_type="figure">OoD Rejection: SVHN (iD) vs CIFAR-10 (ooD)</text>
<text top="126" left="448" width="5" height="8" font="font42" id="p9_t59" reading_order_no="65" segment_no="1" tag_type="figure">50</text>
<text top="126" left="475" width="8" height="8" font="font42" id="p9_t60" reading_order_no="66" segment_no="1" tag_type="figure">100</text>
<text top="126" left="502" width="8" height="8" font="font42" id="p9_t61" reading_order_no="67" segment_no="1" tag_type="figure">150</text>
<text top="126" left="530" width="8" height="8" font="font42" id="p9_t62" reading_order_no="68" segment_no="1" tag_type="figure">200</text>
<text top="121" left="425" width="7" height="8" font="font42" id="p9_t63" reading_order_no="64" segment_no="1" tag_type="figure">0.0</text>
<text top="112" left="425" width="7" height="8" font="font42" id="p9_t64" reading_order_no="63" segment_no="1" tag_type="figure">0.1</text>
<text top="103" left="425" width="7" height="8" font="font42" id="p9_t65" reading_order_no="62" segment_no="1" tag_type="figure">0.2</text>
<text top="94" left="425" width="7" height="8" font="font42" id="p9_t66" reading_order_no="61" segment_no="1" tag_type="figure">0.3</text>
<text top="86" left="425" width="7" height="8" font="font42" id="p9_t67" reading_order_no="60" segment_no="1" tag_type="figure">0.4</text>
<text top="77" left="425" width="7" height="8" font="font42" id="p9_t68" reading_order_no="59" segment_no="1" tag_type="figure">0.5</text>
<text top="68" left="425" width="107" height="8" font="font42" id="p9_t69" reading_order_no="58" segment_no="1" tag_type="figure">0.6 OoD Exposure: SVHN (iD) vs CIFAR-10 (ooD)</text>
<text top="197" left="329" width="5" height="8" font="font42" id="p9_t70" reading_order_no="78" segment_no="1" tag_type="figure">50</text>
<text top="197" left="355" width="8" height="8" font="font42" id="p9_t71" reading_order_no="79" segment_no="1" tag_type="figure">100</text>
<text top="197" left="383" width="8" height="8" font="font42" id="p9_t72" reading_order_no="80" segment_no="1" tag_type="figure">150</text>
<text top="197" left="410" width="8" height="8" font="font42" id="p9_t73" reading_order_no="81" segment_no="1" tag_type="figure">200</text>
<text top="203" left="346" width="38" height="8" font="font42" id="p9_t74" reading_order_no="82" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="192" left="305" width="7" height="8" font="font42" id="p9_t75" reading_order_no="77" segment_no="1" tag_type="figure">0.0</text>
<text top="184" left="305" width="7" height="8" font="font42" id="p9_t76" reading_order_no="76" segment_no="1" tag_type="figure">0.1</text>
<text top="175" left="305" width="7" height="8" font="font42" id="p9_t77" reading_order_no="75" segment_no="1" tag_type="figure">0.2</text>
<text top="166" left="305" width="7" height="8" font="font42" id="p9_t78" reading_order_no="74" segment_no="1" tag_type="figure">0.3</text>
<text top="157" left="305" width="7" height="8" font="font42" id="p9_t79" reading_order_no="73" segment_no="1" tag_type="figure">0.4</text>
<text top="148" left="305" width="7" height="8" font="font42" id="p9_t80" reading_order_no="72" segment_no="1" tag_type="figure">0.5</text>
<text top="139" left="305" width="7" height="8" font="font42" id="p9_t81" reading_order_no="71" segment_no="1" tag_type="figure">0.6</text>
<text top="188" left="303" width="0" height="8" font="font43" id="p9_t82" reading_order_no="70" segment_no="1" tag_type="figure">Acquired OoD Fraction</text>
<text top="135" left="318" width="94" height="8" font="font42" id="p9_t83" reading_order_no="69" segment_no="1" tag_type="figure">OoD Rejection: CIFAR-10 (iD) vs SVHN (ooD)</text>
<text top="197" left="448" width="5" height="8" font="font42" id="p9_t84" reading_order_no="90" segment_no="1" tag_type="figure">50</text>
<text top="197" left="475" width="8" height="8" font="font42" id="p9_t85" reading_order_no="91" segment_no="1" tag_type="figure">100</text>
<text top="197" left="502" width="8" height="8" font="font42" id="p9_t86" reading_order_no="92" segment_no="1" tag_type="figure">150</text>
<text top="197" left="530" width="8" height="8" font="font42" id="p9_t87" reading_order_no="93" segment_no="1" tag_type="figure">200</text>
<text top="203" left="466" width="38" height="8" font="font42" id="p9_t88" reading_order_no="94" segment_no="1" tag_type="figure">Total Acquisitions</text>
<text top="192" left="425" width="7" height="8" font="font42" id="p9_t89" reading_order_no="89" segment_no="1" tag_type="figure">0.0</text>
<text top="184" left="425" width="7" height="8" font="font42" id="p9_t90" reading_order_no="88" segment_no="1" tag_type="figure">0.1</text>
<text top="175" left="425" width="7" height="8" font="font42" id="p9_t91" reading_order_no="87" segment_no="1" tag_type="figure">0.2</text>
<text top="166" left="425" width="7" height="8" font="font42" id="p9_t92" reading_order_no="86" segment_no="1" tag_type="figure">0.3</text>
<text top="157" left="425" width="7" height="8" font="font42" id="p9_t93" reading_order_no="85" segment_no="1" tag_type="figure">0.4</text>
<text top="148" left="425" width="7" height="8" font="font42" id="p9_t94" reading_order_no="84" segment_no="1" tag_type="figure">0.5</text>
<text top="139" left="425" width="107" height="8" font="font42" id="p9_t95" reading_order_no="83" segment_no="1" tag_type="figure">0.6 OoD Exposure: CIFAR-10 (iD) vs SVHN (ooD)</text>
<text top="205" left="407" width="43" height="8" font="font42" id="p9_t96" reading_order_no="95" segment_no="1" tag_type="figure">Acquisition Function</text>
<text top="212" left="395" width="35" height="8" font="font42" id="p9_t97" reading_order_no="96" segment_no="1" tag_type="figure">BatchEPIG-BALD</text>
<text top="212" left="450" width="24" height="8" font="font42" id="p9_t98" reading_order_no="97" segment_no="1" tag_type="figure">BatchBALD</text>
<text top="228" left="377" width="83" height="8" font="font16" id="p9_t99" reading_order_no="98" segment_no="1" tag_type="figure">(b) Aquired OoD Ratio</text>
<text top="248" left="55" width="487" height="8" font="font21" id="p9_t100" reading_order_no="99" segment_no="2" tag_type="text">Figure 5. CIFAR-10 and SVHN pairings with OoD rejection or exposure. EPIG-BALD performs better than BALD. 5 trials. Acquisition</text>
<text top="259" left="55" width="102" height="8" font="font16" id="p9_t101" reading_order_no="100" segment_no="2" tag_type="text">size 5. Initial training size 5.</text>
<text top="518" left="88" width="14" height="11" font="font44" id="p9_t102" reading_order_no="108" segment_no="3" tag_type="figure">2500</text>
<text top="518" left="114" width="14" height="11" font="font44" id="p9_t103" reading_order_no="109" segment_no="3" tag_type="figure">5000</text>
<text top="518" left="141" width="14" height="11" font="font44" id="p9_t104" reading_order_no="110" segment_no="3" tag_type="figure">7500</text>
<text top="518" left="166" width="124" height="11" font="font44" id="p9_t105" reading_order_no="111" segment_no="3" tag_type="figure">10000 12500 15000 17500 20000</text>
<text top="526" left="157" width="46" height="11" font="font44" id="p9_t106" reading_order_no="112" segment_no="3" tag_type="figure">Training Set Size</text>
<text top="510" left="63" width="12" height="11" font="font44" id="p9_t107" reading_order_no="107" segment_no="3" tag_type="figure">0.50</text>
<text top="486" left="63" width="12" height="11" font="font44" id="p9_t108" reading_order_no="106" segment_no="3" tag_type="figure">0.55</text>
<text top="462" left="63" width="12" height="11" font="font44" id="p9_t109" reading_order_no="105" segment_no="3" tag_type="figure">0.60</text>
<text top="439" left="63" width="12" height="11" font="font44" id="p9_t110" reading_order_no="104" segment_no="3" tag_type="figure">0.65</text>
<text top="415" left="63" width="12" height="11" font="font44" id="p9_t111" reading_order_no="103" segment_no="3" tag_type="figure">0.70</text>
<text top="391" left="63" width="12" height="11" font="font44" id="p9_t112" reading_order_no="102" segment_no="3" tag_type="figure">0.75</text>
<text top="459" left="60" width="0" height="11" font="font45" id="p9_t113" reading_order_no="101" segment_no="3" tag_type="figure">Accuracy</text>
<text top="535" left="142" width="55" height="11" font="font44" id="p9_t114" reading_order_no="113" segment_no="3" tag_type="figure">Acquisition Function</text>
<text top="543" left="87" width="69" height="11" font="font44" id="p9_t115" reading_order_no="114" segment_no="3" tag_type="figure">SoftmaxEPIG-BALD (ours)</text>
<text top="543" left="182" width="37" height="11" font="font44" id="p9_t116" reading_order_no="115" segment_no="3" tag_type="figure">SoftmaxBALD</text>
<text top="543" left="245" width="22" height="11" font="font44" id="p9_t117" reading_order_no="116" segment_no="3" tag_type="figure">Uniform</text>
<text top="569" left="55" width="178" height="8" font="font21" id="p9_t118" reading_order_no="117" segment_no="4" tag_type="text">Figure 6. BALD vs EPIG-BALD on CIFAR-10.</text>
<text top="569" left="241" width="48" height="8" font="font16" id="p9_t119" reading_order_no="118" segment_no="4" tag_type="text">EPIG-BALD</text>
<text top="580" left="55" width="235" height="8" font="font16" id="p9_t120" reading_order_no="119" segment_no="4" tag_type="text">outperforms BALD. 5 trials each. With batch acquisition size 250,</text>
<text top="591" left="55" width="234" height="8" font="font16" id="p9_t121" reading_order_no="120" segment_no="4" tag_type="text">and initial training size 1000. Median accuracy after smoothing</text>
<text top="602" left="55" width="231" height="8" font="font16" id="p9_t122" reading_order_no="121" segment_no="4" tag_type="text">with a Parzen window filter over 30 acquisition steps to denoise.</text>
</page>
</pdf2xml>
