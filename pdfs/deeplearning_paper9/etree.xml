<!DOCTYPE pdf2xml SYSTEM "pdf2xml.dtd">
<pdf2xml producer="poppler" version="23.04.0">
<page number="1" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font0" size="7" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font1" size="24" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font2" size="11" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font3" size="9" family="NimbusRomNo9L-MediItal" color="#000000"/>
	<fontspec id="font4" size="9" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font5" size="10" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font6" size="8" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font7" size="20" family="Times" color="#7f7f7f"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p1_t1" reading_order_no="1" segment_no="0" tag_type="text">1</text>
<text top="61" left="79" width="454" height="21" font="font1" id="p1_t2" reading_order_no="2" segment_no="1" tag_type="title">Understanding the Perceived Quality of Video</text>
<text top="88" left="252" width="108" height="21" font="font1" id="p1_t3" reading_order_no="3" segment_no="1" tag_type="title">Predictions</text>
<text top="121" left="128" width="356" height="10" font="font2" id="p1_t4" reading_order_no="4" segment_no="2" tag_type="text">Nagabhushan Somraj, Manoj Surya Kashi, S. P. Arun and Rajiv Soundararajan</text>
<text top="178" left="59" width="31" height="8" font="font3" id="p1_t5" reading_order_no="5" segment_no="4" tag_type="text">Abstract</text>
<text top="178" left="90" width="210" height="8" font="font4" id="p1_t6" reading_order_no="6" segment_no="4" tag_type="text">—The study of video prediction models is believed</text>
<text top="188" left="49" width="251" height="8" font="font4" id="p1_t7" reading_order_no="7" segment_no="4" tag_type="text">to be a fundamental approach to representation learning for</text>
<text top="198" left="49" width="251" height="8" font="font4" id="p1_t8" reading_order_no="8" segment_no="4" tag_type="text">videos. While a plethora of generative models for predicting</text>
<text top="208" left="49" width="251" height="8" font="font4" id="p1_t9" reading_order_no="9" segment_no="4" tag_type="text">the future frame pixel values given the past few frames exist,</text>
<text top="218" left="49" width="251" height="8" font="font4" id="p1_t10" reading_order_no="10" segment_no="4" tag_type="text">the quantitative evaluation of the predicted frames has been</text>
<text top="228" left="49" width="251" height="8" font="font4" id="p1_t11" reading_order_no="11" segment_no="4" tag_type="text">found to be extremely challenging. In this context, we study the</text>
<text top="238" left="49" width="251" height="8" font="font4" id="p1_t12" reading_order_no="12" segment_no="4" tag_type="text">problem of quality assessment of predicted videos. We create the</text>
<text top="248" left="49" width="251" height="8" font="font4" id="p1_t13" reading_order_no="13" segment_no="4" tag_type="text">Indian Institute of Science Predicted Videos Quality Assessment</text>
<text top="258" left="49" width="251" height="8" font="font4" id="p1_t14" reading_order_no="14" segment_no="4" tag_type="text">(IISc PVQA) Database consisting of 300 videos, obtained by</text>
<text top="267" left="49" width="251" height="8" font="font4" id="p1_t15" reading_order_no="15" segment_no="4" tag_type="text">applying different prediction models on different datasets, and</text>
<text top="277" left="49" width="251" height="8" font="font4" id="p1_t16" reading_order_no="16" segment_no="4" tag_type="text">accompanying human opinion scores. We collected subjective</text>
<text top="287" left="49" width="251" height="8" font="font4" id="p1_t17" reading_order_no="17" segment_no="4" tag_type="text">ratings of quality from 50 human participants for these videos.</text>
<text top="297" left="49" width="251" height="8" font="font4" id="p1_t18" reading_order_no="18" segment_no="4" tag_type="text">Our subjective study reveals that human observers were highly</text>
<text top="307" left="49" width="251" height="8" font="font4" id="p1_t19" reading_order_no="19" segment_no="4" tag_type="text">consistent in their judgments of quality of predicted videos.</text>
<text top="317" left="49" width="251" height="8" font="font4" id="p1_t20" reading_order_no="20" segment_no="4" tag_type="text">We benchmark several popularly used measures for evaluating</text>
<text top="327" left="49" width="251" height="8" font="font4" id="p1_t21" reading_order_no="21" segment_no="4" tag_type="text">video prediction and show that they do not adequately correlate</text>
<text top="337" left="49" width="251" height="8" font="font4" id="p1_t22" reading_order_no="22" segment_no="4" tag_type="text">with these subjective scores. We introduce two new features</text>
<text top="347" left="49" width="251" height="8" font="font4" id="p1_t23" reading_order_no="23" segment_no="4" tag_type="text">to effectively capture the quality of predicted videos, motion-</text>
<text top="357" left="49" width="251" height="8" font="font4" id="p1_t24" reading_order_no="24" segment_no="4" tag_type="text">compensated cosine similarities of deep features of predicted</text>
<text top="367" left="49" width="251" height="8" font="font4" id="p1_t25" reading_order_no="25" segment_no="4" tag_type="text">frames with past frames, and deep features extracted from</text>
<text top="377" left="49" width="251" height="8" font="font4" id="p1_t26" reading_order_no="26" segment_no="4" tag_type="text">rescaled frame differences. We show that our feature design</text>
<text top="387" left="49" width="251" height="8" font="font4" id="p1_t27" reading_order_no="27" segment_no="4" tag_type="text">leads to state of the art quality prediction in accordance</text>
<text top="397" left="49" width="251" height="8" font="font4" id="p1_t28" reading_order_no="28" segment_no="4" tag_type="text">with human judgments on our IISc PVQA Database. The</text>
<text top="407" left="49" width="251" height="8" font="font4" id="p1_t29" reading_order_no="29" segment_no="4" tag_type="text">database and code are publicly available on our project website:</text>
<text top="417" left="49" width="223" height="8" font="font4" id="p1_t30" reading_order_no="30" segment_no="4" tag_type="text">https://nagabhushansn95.github.io/publications/2020/pvqa</text>
<text top="433" left="59" width="51" height="8" font="font3" id="p1_t31" reading_order_no="31" segment_no="7" tag_type="text">Index Terms</text>
<text top="433" left="110" width="190" height="8" font="font4" id="p1_t32" reading_order_no="32" segment_no="7" tag_type="text">—Video quality assessment, video prediction,</text>
<text top="443" left="49" width="240" height="8" font="font4" id="p1_t33" reading_order_no="33" segment_no="7" tag_type="text">database, perceptual quality, neural networks, deep learning.</text>
<text top="473" left="136" width="15" height="9" font="font5" id="p1_t34" reading_order_no="34" segment_no="8" tag_type="title">I. I</text>
<text top="475" left="151" width="62" height="7" font="font6" id="p1_t35" reading_order_no="35" segment_no="8" tag_type="title">NTRODUCTION</text>
<text top="491" left="59" width="241" height="9" font="font5" id="p1_t36" reading_order_no="36" segment_no="9" tag_type="text">Video prediction refers to the problem of generating pixels</text>
<text top="503" left="49" width="251" height="9" font="font5" id="p1_t37" reading_order_no="37" segment_no="9" tag_type="text">of future frames given context information in the form of</text>
<text top="515" left="49" width="251" height="9" font="font5" id="p1_t38" reading_order_no="38" segment_no="9" tag_type="text">past frames. The problem has attracted a lot of attention in</text>
<text top="527" left="49" width="251" height="9" font="font5" id="p1_t39" reading_order_no="39" segment_no="9" tag_type="text">the context of generative video models. The ability to predict</text>
<text top="539" left="49" width="251" height="9" font="font5" id="p1_t40" reading_order_no="40" segment_no="9" tag_type="text">the future accurately has applications in various domains,</text>
<text top="551" left="49" width="251" height="9" font="font5" id="p1_t41" reading_order_no="41" segment_no="9" tag_type="text">including robotics for path planning, self-driving cars, anomaly</text>
<text top="563" left="49" width="251" height="9" font="font5" id="p1_t42" reading_order_no="42" segment_no="9" tag_type="text">detection <a href="deeplearning_paper9.html#12">[1], </a>and video compression. It is also shown that</text>
<text top="574" left="49" width="251" height="9" font="font5" id="p1_t43" reading_order_no="43" segment_no="9" tag_type="text">solving this problem offers a fundamental approach to learning</text>
<text top="586" left="49" width="251" height="9" font="font5" id="p1_t44" reading_order_no="44" segment_no="9" tag_type="text">internal representations of videos <a href="deeplearning_paper9.html#12">[2], [3]. </a>Further, the problem</text>
<text top="598" left="49" width="251" height="9" font="font5" id="p1_t45" reading_order_no="45" segment_no="9" tag_type="text">also helps in understanding interactions of physical objects in</text>
<text top="610" left="49" width="251" height="9" font="font5" id="p1_t46" reading_order_no="46" segment_no="9" tag_type="text">the real world <a href="deeplearning_paper9.html#12">[4], [5]. </a>Although, there may be applications</text>
<text top="622" left="49" width="251" height="9" font="font5" id="p1_t47" reading_order_no="47" segment_no="9" tag_type="text">where we are only interested in predicting features in future</text>
<text top="634" left="49" width="210" height="9" font="font5" id="p1_t48" reading_order_no="48" segment_no="9" tag_type="text">frames <a href="deeplearning_paper9.html#12">[6], </a>predicting all pixels in future frames</text>
<text top="634" left="269" width="31" height="9" font="font5" id="p1_t49" reading_order_no="49" segment_no="9" tag_type="text"><a href="deeplearning_paper9.html#12">[2], [7]</a></text>
<text top="646" left="49" width="251" height="9" font="font5" id="p1_t50" reading_order_no="50" segment_no="9" tag_type="text">allows for rich self-supervision, a visual interpretation of the</text>
<text top="669" left="57" width="243" height="7" font="font6" id="p1_t51" reading_order_no="99" segment_no="12" tag_type="footnote">N. Somraj, M. S. Kashi and R. Soundararajan are with the Department</text>
<text top="678" left="49" width="251" height="7" font="font6" id="p1_t52" reading_order_no="100" segment_no="12" tag_type="footnote">of Electrical Communication Engineering, Indian Institute of Science, Ben-</text>
<text top="687" left="49" width="251" height="7" font="font6" id="p1_t53" reading_order_no="101" segment_no="12" tag_type="footnote">galuru 560012, India (email: nagabhushans@iisc.ac.in; manojsk@iisc.ac.in;</text>
<text top="696" left="49" width="251" height="7" font="font6" id="p1_t54" reading_order_no="102" segment_no="12" tag_type="footnote">rajivs@iisc.ac.in). S. P. Arun is with the Centre for Neuroscience, Indian</text>
<text top="705" left="49" width="251" height="7" font="font6" id="p1_t55" reading_order_no="103" segment_no="12" tag_type="footnote">Institute of Science, Bengaluru 560012, India (email: sparun@iisc.ac.in). This</text>
<text top="714" left="49" width="251" height="7" font="font6" id="p1_t56" reading_order_no="104" segment_no="12" tag_type="footnote">research was supported in part by Pratiksha Trust grants (FG/PTCH-19-</text>
<text top="723" left="49" width="251" height="7" font="font6" id="p1_t57" reading_order_no="105" segment_no="12" tag_type="footnote">2068 and FG/PTCH-20-2068). The first author was supported by the Prime</text>
<text top="732" left="49" width="251" height="7" font="font6" id="p1_t58" reading_order_no="106" segment_no="12" tag_type="footnote">Minister’s Research Fellowship awarded by the Ministry of Human Resources</text>
<text top="740" left="49" width="118" height="7" font="font6" id="p1_t59" reading_order_no="107" segment_no="12" tag_type="footnote">Development, Government of India.</text>
<text top="177" left="312" width="251" height="9" font="font5" id="p1_t60" reading_order_no="51" segment_no="3" tag_type="text">predicted frames, and a more generic approach to learning</text>
<text top="189" left="312" width="251" height="9" font="font5" id="p1_t61" reading_order_no="52" segment_no="3" tag_type="text">across different applications. The video prediction problem</text>
<text top="201" left="312" width="251" height="9" font="font5" id="p1_t62" reading_order_no="53" segment_no="3" tag_type="text">leads to an important question of how to generically evaluate</text>
<text top="213" left="312" width="251" height="9" font="font5" id="p1_t63" reading_order_no="54" segment_no="3" tag_type="text">the quality of the predicted videos in a task free viewing</text>
<text top="225" left="312" width="40" height="9" font="font5" id="p1_t64" reading_order_no="55" segment_no="3" tag_type="text">condition.</text>
<text top="237" left="322" width="241" height="9" font="font5" id="p1_t65" reading_order_no="56" segment_no="5" tag_type="text">While there exists a rich body of work on video prediction</text>
<text top="249" left="312" width="251" height="9" font="font5" id="p1_t66" reading_order_no="57" segment_no="5" tag_type="text">using generative models, the design of methods for evaluating</text>
<text top="261" left="312" width="251" height="9" font="font5" id="p1_t67" reading_order_no="58" segment_no="5" tag_type="text">the quality of the videos has received much less attention.</text>
<text top="273" left="312" width="251" height="9" font="font5" id="p1_t68" reading_order_no="59" segment_no="5" tag_type="text">Simple signal fidelity measures such as mean squared error</text>
<text top="285" left="312" width="251" height="9" font="font5" id="p1_t69" reading_order_no="60" segment_no="5" tag_type="text">(MSE) or the structural similarity (SSIM) index <a href="deeplearning_paper9.html#12">[8] </a>can be</text>
<text top="297" left="312" width="251" height="9" font="font5" id="p1_t70" reading_order_no="61" segment_no="5" tag_type="text">computed in scenarios where a reference future video sequence</text>
<text top="309" left="312" width="251" height="9" font="font5" id="p1_t71" reading_order_no="62" segment_no="5" tag_type="text">is available. However, for a given context, there might exist a</text>
<text top="321" left="312" width="251" height="9" font="font5" id="p1_t72" reading_order_no="63" segment_no="5" tag_type="text">multitude of possible future video trajectories that are natural</text>
<text top="333" left="312" width="251" height="9" font="font5" id="p1_t73" reading_order_no="64" segment_no="5" tag_type="text">looking. It would be unfair to compare such predicted videos</text>
<text top="345" left="312" width="137" height="9" font="font5" id="p1_t74" reading_order_no="65" segment_no="5" tag_type="text">against a given future realization.</text>
<text top="357" left="322" width="241" height="9" font="font5" id="p1_t75" reading_order_no="66" segment_no="6" tag_type="text">The quality assessment of predicted videos needs to capture</text>
<text top="369" left="312" width="251" height="9" font="font5" id="p1_t76" reading_order_no="67" segment_no="6" tag_type="text">multiple notions. Indeed, video prediction researchers have</text>
<text top="380" left="312" width="251" height="9" font="font5" id="p1_t77" reading_order_no="68" segment_no="6" tag_type="text">identified the sharpness of predicted frames as an important</text>
<text top="392" left="312" width="251" height="9" font="font5" id="p1_t78" reading_order_no="69" segment_no="6" tag_type="text">evaluation tool <a href="deeplearning_paper9.html#12">[7]. </a>The spatial quality of predicted video</text>
<text top="404" left="312" width="251" height="9" font="font5" id="p1_t79" reading_order_no="70" segment_no="6" tag_type="text">frames is also influenced by the realism of object shapes and</text>
<text top="416" left="312" width="251" height="9" font="font5" id="p1_t80" reading_order_no="71" segment_no="6" tag_type="text">texture. Object motion and temporal consistency are important</text>
<text top="428" left="312" width="251" height="9" font="font5" id="p1_t81" reading_order_no="72" segment_no="6" tag_type="text">elements of the quality of predicted videos. Further, the events</text>
<text top="440" left="312" width="251" height="9" font="font5" id="p1_t82" reading_order_no="73" segment_no="6" tag_type="text">unfolding in a video need to make logical sense. Thus, the</text>
<text top="452" left="312" width="251" height="9" font="font5" id="p1_t83" reading_order_no="74" segment_no="6" tag_type="text">quality assessment of predicted videos appears to involve</text>
<text top="464" left="312" width="251" height="9" font="font5" id="p1_t84" reading_order_no="75" segment_no="6" tag_type="text">elements of both early and later stages of human vision</text>
<text top="476" left="312" width="251" height="9" font="font5" id="p1_t85" reading_order_no="76" segment_no="6" tag_type="text">systems. In this work, we particularly focus on predicted</text>
<text top="488" left="312" width="251" height="9" font="font5" id="p1_t86" reading_order_no="77" segment_no="6" tag_type="text">videos obtained using generative prediction models given the</text>
<text top="500" left="312" width="112" height="9" font="font5" id="p1_t87" reading_order_no="78" segment_no="6" tag_type="text">rich literature on this topic.</text>
<text top="512" left="322" width="241" height="9" font="font5" id="p1_t88" reading_order_no="79" segment_no="10" tag_type="text">The quality assessment of predicted videos presents its own</text>
<text top="524" left="312" width="251" height="9" font="font5" id="p1_t89" reading_order_no="80" segment_no="10" tag_type="text">challenges when compared to generic video quality assessment</text>
<text top="536" left="312" width="251" height="9" font="font5" id="p1_t90" reading_order_no="81" segment_no="10" tag_type="text">(VQA) as described above. However, the problem formulation</text>
<text top="548" left="312" width="251" height="9" font="font5" id="p1_t91" reading_order_no="82" segment_no="10" tag_type="text">also allows one to exploit more information when compared</text>
<text top="560" left="312" width="251" height="9" font="font5" id="p1_t92" reading_order_no="83" segment_no="10" tag_type="text">to VQA without using a reference video. In particular, video</text>
<text top="572" left="312" width="251" height="9" font="font5" id="p1_t93" reading_order_no="84" segment_no="10" tag_type="text">prediction methods are typically applied given a few context</text>
<text top="584" left="312" width="251" height="9" font="font5" id="p1_t94" reading_order_no="85" segment_no="10" tag_type="text">frames which are of high quality. Thus, the quality of predicted</text>
<text top="596" left="312" width="251" height="9" font="font5" id="p1_t95" reading_order_no="86" segment_no="10" tag_type="text">videos could be assessed by exploiting information available</text>
<text top="608" left="312" width="251" height="9" font="font5" id="p1_t96" reading_order_no="87" segment_no="10" tag_type="text">in the context frames. We believe that assessment of aspects</text>
<text top="620" left="312" width="251" height="9" font="font5" id="p1_t97" reading_order_no="88" segment_no="10" tag_type="text">such as object shapes, their evolution, texture and blur in the</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p1_t98" reading_order_no="89" segment_no="10" tag_type="text">predicted frames can be achieved more reliably by making use</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p1_t99" reading_order_no="90" segment_no="10" tag_type="text">of such information. This makes the problem of assessing the</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p1_t100" reading_order_no="91" segment_no="10" tag_type="text">quality of predicted videos quite different from that of classical</text>
<text top="667" left="312" width="24" height="9" font="font5" id="p1_t101" reading_order_no="92" segment_no="10" tag_type="text">VQA.</text>
<text top="679" left="322" width="241" height="9" font="font5" id="p1_t102" reading_order_no="93" segment_no="13" tag_type="text">The main focus of our work is in the subjective and</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p1_t103" reading_order_no="94" segment_no="13" tag_type="text">objective study of the quality of predicted videos. Recently,</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p1_t104" reading_order_no="95" segment_no="13" tag_type="text">small scale subjective studies based on pairwise comparisons</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p1_t105" reading_order_no="96" segment_no="13" tag_type="text">have been carried out to prove the effectiveness of specific</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p1_t106" reading_order_no="97" segment_no="13" tag_type="text">video prediction models <a href="deeplearning_paper9.html#12">[9]. </a>While human opinion might be</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p1_t107" reading_order_no="98" segment_no="13" tag_type="text">the best subjective measure of quality, collecting such human</text>
<text top="556" left="32" width="0" height="18" font="font7" id="p1_t108" reading_order_no="0" segment_no="11" tag_type="title">arXiv:2005.00356v4  [eess.IV]  22 Jun 2021</text>
</page>
<page number="2" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font8" size="10" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p2_t1" reading_order_no="0" segment_no="0" tag_type="text">2</text>
<text top="232" left="49" width="514" height="9" font="font5" id="p2_t2" reading_order_no="1" segment_no="1" tag_type="text">Fig. 1: Example distortions observed in video frames in our database. The sequence of images in each row corresponds to</text>
<text top="244" left="49" width="514" height="9" font="font5" id="p2_t3" reading_order_no="2" segment_no="1" tag_type="text">the frames of a video. Starting from the first frame, we show every second frame. The first two frames correspond to the</text>
<text top="256" left="49" width="514" height="9" font="font5" id="p2_t4" reading_order_no="3" segment_no="1" tag_type="text">context frames, and the next eight frames correspond to the predicted frames. The mean opinion score (MOS) obtained from</text>
<text top="268" left="49" width="514" height="9" font="font5" id="p2_t5" reading_order_no="4" segment_no="1" tag_type="text">the subjective study is also shown below for each video. (a) In the first video, we observe the gradual increase in blur with</text>
<text top="280" left="49" width="514" height="9" font="font5" id="p2_t6" reading_order_no="5" segment_no="1" tag_type="text">deeper prediction in time. MOS: 40.01. (b) In the video in the second row, the shape of the bow gets distorted over time.</text>
<text top="292" left="49" width="514" height="9" font="font5" id="p2_t7" reading_order_no="6" segment_no="1" tag_type="text">While blur is global, shape distortion is highly localized. MOS: 45.41. (c) The video in the third row shows the disappearance</text>
<text top="304" left="49" width="514" height="9" font="font5" id="p2_t8" reading_order_no="7" segment_no="1" tag_type="text">of the robotic arm. MOS: 41.01. (d) In the video in the fourth row, as the person runs from right towards left, the color of his</text>
<text top="316" left="49" width="405" height="9" font="font5" id="p2_t9" reading_order_no="8" segment_no="1" tag_type="text">shirt changes from white to black. MOS: 55.24. The videos can be viewed on our project website.</text>
<text top="350" left="49" width="251" height="9" font="font5" id="p2_t10" reading_order_no="9" segment_no="2" tag_type="text">data is cumbersome, and it is desirable to have an objective,</text>
<text top="362" left="49" width="251" height="9" font="font5" id="p2_t11" reading_order_no="10" segment_no="2" tag_type="text">automatic measure of quality that can be evaluated on any</text>
<text top="374" left="49" width="251" height="9" font="font5" id="p2_t12" reading_order_no="11" segment_no="2" tag_type="text">video. We believe that a continuous valued measure that can be</text>
<text top="386" left="49" width="251" height="9" font="font5" id="p2_t13" reading_order_no="12" segment_no="2" tag_type="text">evaluated on any predicted video will be useful in comparing</text>
<text top="398" left="49" width="113" height="9" font="font5" id="p2_t14" reading_order_no="13" segment_no="2" tag_type="text">various prediction methods.</text>
<text top="411" left="59" width="241" height="9" font="font5" id="p2_t15" reading_order_no="14" segment_no="4" tag_type="text">Very recently, the Fr´echet video distance (FVD) was in-</text>
<text top="423" left="49" width="251" height="9" font="font5" id="p2_t16" reading_order_no="15" segment_no="4" tag_type="text">troduced to evaluate generative models and validated using a</text>
<text top="435" left="49" width="251" height="9" font="font5" id="p2_t17" reading_order_no="16" segment_no="4" tag_type="text">subjective study <a href="deeplearning_paper9.html#12">[10]. </a>The distance is meant to be applied on a</text>
<text top="446" left="49" width="251" height="9" font="font5" id="p2_t18" reading_order_no="17" segment_no="4" tag_type="text">collection of generated videos instead of individual videos and</text>
<text top="458" left="49" width="251" height="9" font="font5" id="p2_t19" reading_order_no="18" segment_no="4" tag_type="text">is thus different from our goal to measure quality. Further, the</text>
<text top="470" left="49" width="251" height="9" font="font5" id="p2_t20" reading_order_no="19" segment_no="4" tag_type="text">study is designed primarily to prove the effectiveness of FVD</text>
<text top="482" left="49" width="251" height="9" font="font5" id="p2_t21" reading_order_no="20" segment_no="4" tag_type="text">while we seek to design a study that can help benchmark and</text>
<text top="494" left="49" width="251" height="9" font="font5" id="p2_t22" reading_order_no="21" segment_no="4" tag_type="text">advance research in measuring the quality of any predicted</text>
<text top="506" left="49" width="251" height="9" font="font5" id="p2_t23" reading_order_no="22" segment_no="4" tag_type="text">video. To the best of our knowledge, there exists no human</text>
<text top="518" left="49" width="197" height="9" font="font5" id="p2_t24" reading_order_no="23" segment_no="4" tag_type="text">study on predicted videos that measures quality.</text>
<text top="554" left="49" width="121" height="9" font="font8" id="p2_t25" reading_order_no="24" segment_no="6" tag_type="title">A. Overview of Contributions</text>
<text top="572" left="59" width="241" height="9" font="font5" id="p2_t26" reading_order_no="25" segment_no="8" tag_type="text">Our main contributions in this work are in the creation of</text>
<text top="584" left="49" width="251" height="9" font="font5" id="p2_t27" reading_order_no="26" segment_no="8" tag_type="text">a database of predicted videos, design of a subjective study,</text>
<text top="596" left="49" width="251" height="9" font="font5" id="p2_t28" reading_order_no="27" segment_no="8" tag_type="text">benchmarking of existing objective methods used to evaluate</text>
<text top="608" left="49" width="251" height="9" font="font5" id="p2_t29" reading_order_no="28" segment_no="8" tag_type="text">quality, and introduction of mechanisms leading to improved</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p2_t30" reading_order_no="29" segment_no="8" tag_type="text">prediction of video quality. We create the Indian Institute of</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p2_t31" reading_order_no="30" segment_no="8" tag_type="text">Science Predicted Videos Quality Assessment (IISc PVQA)</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p2_t32" reading_order_no="31" segment_no="8" tag_type="text">Database consisting of 300 videos, each consisting of 20</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p2_t33" reading_order_no="32" segment_no="8" tag_type="text">frames, obtained from a variety of different prediction models.</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p2_t34" reading_order_no="33" segment_no="8" tag_type="text">The videos are generated by applying video prediction models</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p2_t35" reading_order_no="34" segment_no="8" tag_type="text">on databases typically used to evaluate them. Our database</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p2_t36" reading_order_no="35" segment_no="8" tag_type="text">contains a variety of sources of distortions such as blurred</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p2_t37" reading_order_no="36" segment_no="8" tag_type="text">frames, frames with distorted object shapes, temporal color</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p2_t38" reading_order_no="37" segment_no="8" tag_type="text">variations, and sudden appearance or disappearance of objects,</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p2_t39" reading_order_no="38" segment_no="8" tag_type="text">as shown in Fig. <a href="deeplearning_paper9.html#2">1. </a>Thus our database is diverse in terms of</text>
<text top="739" left="49" width="95" height="9" font="font5" id="p2_t40" reading_order_no="39" segment_no="8" tag_type="text">content and distortions.</text>
<text top="350" left="322" width="241" height="9" font="font5" id="p2_t41" reading_order_no="40" segment_no="3" tag_type="text">We conduct a subjective study involving 50 human subjects</text>
<text top="362" left="312" width="251" height="9" font="font5" id="p2_t42" reading_order_no="41" segment_no="3" tag_type="text">resulting in a total of 6000 video ratings under calibrated</text>
<text top="374" left="312" width="251" height="9" font="font5" id="p2_t43" reading_order_no="42" segment_no="3" tag_type="text">conditions. Since the videos from different databases are avail-</text>
<text top="386" left="312" width="251" height="9" font="font5" id="p2_t44" reading_order_no="43" segment_no="3" tag_type="text">able at different resolutions and might bias the quality scores,</text>
<text top="398" left="312" width="251" height="9" font="font5" id="p2_t45" reading_order_no="44" segment_no="3" tag_type="text">we adopt a double stimulus continuous quality assessment</text>
<text top="410" left="312" width="251" height="9" font="font5" id="p2_t46" reading_order_no="45" segment_no="3" tag_type="text">method. In our study, a pair of videos is shown, one being</text>
<text top="422" left="312" width="251" height="9" font="font5" id="p2_t47" reading_order_no="46" segment_no="3" tag_type="text">the test video and the other, a different natural video from the</text>
<text top="434" left="312" width="105" height="9" font="font5" id="p2_t48" reading_order_no="47" segment_no="3" tag_type="text">same or a similar dataset.</text>
<text top="446" left="322" width="241" height="9" font="font5" id="p2_t49" reading_order_no="48" segment_no="5" tag_type="text">We benchmark several popular measures to evaluate pre-</text>
<text top="458" left="312" width="251" height="9" font="font5" id="p2_t50" reading_order_no="49" segment_no="5" tag_type="text">dicted videos, such as MSE, SSIM and deep network based</text>
<text top="470" left="312" width="251" height="9" font="font5" id="p2_t51" reading_order_no="50" segment_no="5" tag_type="text">loss functions, against the subjective scores of video quality.</text>
<text top="482" left="312" width="251" height="9" font="font5" id="p2_t52" reading_order_no="51" segment_no="5" tag_type="text">We show that these measures do not correlate well with the</text>
<text top="494" left="312" width="251" height="9" font="font5" id="p2_t53" reading_order_no="52" segment_no="5" tag_type="text">subjective scores since they evaluate the predicted videos by</text>
<text top="506" left="312" width="251" height="9" font="font5" id="p2_t54" reading_order_no="53" segment_no="5" tag_type="text">assuming a fixed trajectory of the reference. We also show that</text>
<text top="518" left="312" width="251" height="9" font="font5" id="p2_t55" reading_order_no="54" segment_no="5" tag_type="text">popular no-reference video QA algorithms do not match well</text>
<text top="530" left="312" width="251" height="9" font="font5" id="p2_t56" reading_order_no="55" segment_no="5" tag_type="text">with subjective judgements of video quality since they are not</text>
<text top="542" left="312" width="251" height="9" font="font5" id="p2_t57" reading_order_no="56" segment_no="5" tag_type="text">designed to capture the artifacts that arise in video prediction.</text>
<text top="555" left="322" width="241" height="9" font="font5" id="p2_t58" reading_order_no="57" segment_no="7" tag_type="text">Finally, we introduce two novel sets of features to ef-</text>
<text top="567" left="312" width="251" height="9" font="font5" id="p2_t59" reading_order_no="58" segment_no="7" tag_type="text">fectively predict the quality of predicted videos. The first</text>
<text top="579" left="312" width="251" height="9" font="font5" id="p2_t60" reading_order_no="59" segment_no="7" tag_type="text">set of features is based on computing cosine similarities</text>
<text top="591" left="312" width="251" height="9" font="font5" id="p2_t61" reading_order_no="60" segment_no="7" tag_type="text">of deep features of past frames with corresponding motion-</text>
<text top="603" left="312" width="251" height="9" font="font5" id="p2_t62" reading_order_no="61" segment_no="7" tag_type="text">compensated features from the predicted frames. This helps</text>
<text top="615" left="312" width="251" height="9" font="font5" id="p2_t63" reading_order_no="62" segment_no="7" tag_type="text">capture object blur, shape, and color distortions in a robust</text>
<text top="627" left="312" width="251" height="9" font="font5" id="p2_t64" reading_order_no="63" segment_no="7" tag_type="text">fashion by comparing with the past frames. Secondly, we</text>
<text top="639" left="312" width="251" height="9" font="font5" id="p2_t65" reading_order_no="64" segment_no="7" tag_type="text">rescale frame differences of adjacent frames of the predicted</text>
<text top="651" left="312" width="251" height="9" font="font5" id="p2_t66" reading_order_no="65" segment_no="7" tag_type="text">video to appear like an image and extract corresponding deep</text>
<text top="663" left="312" width="251" height="9" font="font5" id="p2_t67" reading_order_no="66" segment_no="7" tag_type="text">features to capture object shape variations in regions contain-</text>
<text top="675" left="312" width="251" height="9" font="font5" id="p2_t68" reading_order_no="67" segment_no="7" tag_type="text">ing motion. We show that these features can effectively predict</text>
<text top="686" left="312" width="251" height="9" font="font5" id="p2_t69" reading_order_no="68" segment_no="7" tag_type="text">video quality by achieving the state of the art performance in</text>
<text top="698" left="312" width="192" height="9" font="font5" id="p2_t70" reading_order_no="69" segment_no="7" tag_type="text">terms of correlation with the subjective scores.</text>
<text top="711" left="322" width="241" height="9" font="font5" id="p2_t71" reading_order_no="70" segment_no="9" tag_type="text">We summarize the main contributions of our work as</text>
<text top="723" left="312" width="32" height="9" font="font5" id="p2_t72" reading_order_no="71" segment_no="9" tag_type="text">follows:</text>
<text top="739" left="322" width="241" height="9" font="font5" id="p2_t73" reading_order_no="72" segment_no="10" tag_type="text">1) We introduce the IISc PVQA database of 300 videos</text>
</page>
<page number="3" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="560" width="3" height="6" font="font0" id="p3_t1" reading_order_no="0" segment_no="0" tag_type="text">3</text>
<text top="58" left="73" width="227" height="9" font="font5" id="p3_t2" reading_order_no="1" segment_no="1" tag_type="list">predicted using a variety of models and based on mul-</text>
<text top="70" left="73" width="55" height="9" font="font5" id="p3_t3" reading_order_no="2" segment_no="1" tag_type="list">tiple datasets.</text>
<text top="82" left="59" width="241" height="9" font="font5" id="p3_t4" reading_order_no="3" segment_no="4" tag_type="list">2) We conduct a behavioral study with 50 subjects to</text>
<text top="94" left="73" width="227" height="9" font="font5" id="p3_t5" reading_order_no="4" segment_no="4" tag_type="list">measure the quality of the predicted videos through a</text>
<text top="106" left="73" width="149" height="9" font="font5" id="p3_t6" reading_order_no="5" segment_no="4" tag_type="list">double stimulus scoring mechanism.</text>
<text top="117" left="59" width="241" height="9" font="font5" id="p3_t7" reading_order_no="6" segment_no="5" tag_type="list">3) We benchmark several metrics popularly used in video</text>
<text top="129" left="73" width="227" height="9" font="font5" id="p3_t8" reading_order_no="7" segment_no="5" tag_type="list">prediction evaluation and show that existing metrics</text>
<text top="141" left="73" width="227" height="9" font="font5" id="p3_t9" reading_order_no="8" segment_no="5" tag_type="list">correlate poorly with human perception of video quality.</text>
<text top="153" left="59" width="27" height="9" font="font5" id="p3_t10" reading_order_no="9" segment_no="6" tag_type="list">4) We</text>
<text top="153" left="97" width="32" height="9" font="font5" id="p3_t11" reading_order_no="10" segment_no="6" tag_type="list">propose</text>
<text top="153" left="139" width="22" height="9" font="font5" id="p3_t12" reading_order_no="11" segment_no="6" tag_type="list">novel</text>
<text top="153" left="172" width="32" height="9" font="font5" id="p3_t13" reading_order_no="12" segment_no="6" tag_type="list">features</text>
<text top="153" left="214" width="23" height="9" font="font5" id="p3_t14" reading_order_no="13" segment_no="6" tag_type="list">based</text>
<text top="153" left="248" width="10" height="9" font="font5" id="p3_t15" reading_order_no="14" segment_no="6" tag_type="list">on</text>
<text top="153" left="268" width="32" height="9" font="font5" id="p3_t16" reading_order_no="15" segment_no="6" tag_type="list">motion-</text>
<text top="165" left="73" width="227" height="9" font="font5" id="p3_t17" reading_order_no="16" segment_no="6" tag_type="list">compensated cosine similarities and rescaled frame</text>
<text top="177" left="73" width="227" height="9" font="font5" id="p3_t18" reading_order_no="17" segment_no="6" tag_type="list">differences and show that they are useful in predicting</text>
<text top="189" left="73" width="227" height="9" font="font5" id="p3_t19" reading_order_no="18" segment_no="6" tag_type="list">quality in a manner that agrees very well with human</text>
<text top="201" left="73" width="45" height="9" font="font5" id="p3_t20" reading_order_no="19" segment_no="6" tag_type="list">perception.</text>
<text top="216" left="59" width="241" height="9" font="font5" id="p3_t21" reading_order_no="20" segment_no="7" tag_type="text">The rest of the paper is organized as follows. In Section <a href="deeplearning_paper9.html#3">II,</a></text>
<text top="228" left="49" width="251" height="9" font="font5" id="p3_t22" reading_order_no="21" segment_no="7" tag_type="text">we survey related work. We describe the predicted video qual-</text>
<text top="239" left="49" width="251" height="9" font="font5" id="p3_t23" reading_order_no="22" segment_no="7" tag_type="text">ity assessment database and the subjective study in Section <a href="deeplearning_paper9.html#3">III.</a></text>
<text top="251" left="49" width="251" height="9" font="font5" id="p3_t24" reading_order_no="23" segment_no="7" tag_type="text">We introduce our quality assessment features in Section <a href="deeplearning_paper9.html#6">IV.</a></text>
<text top="263" left="49" width="251" height="9" font="font5" id="p3_t25" reading_order_no="24" segment_no="7" tag_type="text">We present detailed experiments and ablation studies in Sec-</text>
<text top="275" left="49" width="215" height="9" font="font5" id="p3_t26" reading_order_no="25" segment_no="7" tag_type="text">tion <a href="deeplearning_paper9.html#8">V </a>and finally conclude the paper in Section <a href="deeplearning_paper9.html#11">VI.</a></text>
<text top="303" left="132" width="22" height="9" font="font5" id="p3_t27" reading_order_no="26" segment_no="8" tag_type="title">II. R</text>
<text top="304" left="154" width="33" height="7" font="font6" id="p3_t28" reading_order_no="27" segment_no="8" tag_type="title">ELATED</text>
<text top="303" left="190" width="9" height="9" font="font5" id="p3_t29" reading_order_no="28" segment_no="8" tag_type="title">W</text>
<text top="304" left="200" width="18" height="7" font="font6" id="p3_t30" reading_order_no="29" segment_no="8" tag_type="title">ORK</text>
<text top="319" left="49" width="81" height="9" font="font8" id="p3_t31" reading_order_no="30" segment_no="9" tag_type="title">A. Video Prediction</text>
<text top="335" left="59" width="241" height="9" font="font5" id="p3_t32" reading_order_no="31" segment_no="11" tag_type="text">Predicting video frames from past frames through classical</text>
<text top="347" left="49" width="251" height="9" font="font5" id="p3_t33" reading_order_no="32" segment_no="11" tag_type="text">methods and deep learning has been an important aspect of</text>
<text top="359" left="49" width="251" height="9" font="font5" id="p3_t34" reading_order_no="33" segment_no="11" tag_type="text">video compression <a href="deeplearning_paper9.html#12">[11], [12] </a>for long. However, a lot of</text>
<text top="371" left="49" width="251" height="9" font="font5" id="p3_t35" reading_order_no="34" segment_no="11" tag_type="text">progress has been made in video prediction with the advent of</text>
<text top="383" left="49" width="251" height="9" font="font5" id="p3_t36" reading_order_no="35" segment_no="11" tag_type="text">deep image generation models <a href="deeplearning_paper9.html#12">[15]. </a>One of the primary rea-</text>
<text top="395" left="49" width="251" height="9" font="font5" id="p3_t37" reading_order_no="36" segment_no="11" tag_type="text">sons for advancement in video prediction has been due to the</text>
<text top="407" left="49" width="251" height="9" font="font5" id="p3_t38" reading_order_no="37" segment_no="11" tag_type="text">use of adversarial loss functions <a href="deeplearning_paper9.html#12">[7], [16]. </a>Various researchers</text>
<text top="419" left="49" width="251" height="9" font="font5" id="p3_t39" reading_order_no="38" segment_no="11" tag_type="text">have approached video prediction through decomposition of</text>
<text top="431" left="49" width="251" height="9" font="font5" id="p3_t40" reading_order_no="39" segment_no="11" tag_type="text">motion and content <a href="deeplearning_paper9.html#12">[16], </a>motion modelling through filter</text>
<text top="443" left="49" width="251" height="9" font="font5" id="p3_t41" reading_order_no="40" segment_no="11" tag_type="text">kernels <a href="deeplearning_paper9.html#12">[4], </a>predictive coding <a href="deeplearning_paper9.html#12">[17] </a>and 3D Long Short Term</text>
<text top="455" left="49" width="251" height="9" font="font5" id="p3_t42" reading_order_no="41" segment_no="11" tag_type="text">Memory (3D LSTM) <a href="deeplearning_paper9.html#12">[18]. </a>Researchers have also developed</text>
<text top="467" left="49" width="251" height="9" font="font5" id="p3_t43" reading_order_no="42" segment_no="11" tag_type="text">stochastic generation models <a href="deeplearning_paper9.html#12">[19], [20] </a>to account for the</text>
<text top="478" left="49" width="251" height="9" font="font5" id="p3_t44" reading_order_no="43" segment_no="11" tag_type="text">uncertainty in prediction and the possibility of a multitude of</text>
<text top="490" left="49" width="251" height="9" font="font5" id="p3_t45" reading_order_no="44" segment_no="11" tag_type="text">future trajectories given the context frames. A detailed survey</text>
<text top="502" left="49" width="251" height="9" font="font5" id="p3_t46" reading_order_no="45" segment_no="11" tag_type="text">of deep learning based methods in video prediction can be</text>
<text top="514" left="49" width="57" height="9" font="font5" id="p3_t47" reading_order_no="46" segment_no="11" tag_type="text">found in <a href="deeplearning_paper9.html#12">[21].</a></text>
<text top="544" left="49" width="210" height="9" font="font8" id="p3_t48" reading_order_no="47" segment_no="14" tag_type="title">B. Evaluation methods for video prediction models</text>
<text top="560" left="59" width="241" height="9" font="font5" id="p3_t49" reading_order_no="48" segment_no="16" tag_type="text">The most popular method of evaluating predicted video</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p3_t50" reading_order_no="49" segment_no="16" tag_type="text">frames is using MSE or the SSIM index <a href="deeplearning_paper9.html#12">[8]. </a>In a variant of</text>
<text top="584" left="49" width="251" height="9" font="font5" id="p3_t51" reading_order_no="50" segment_no="16" tag_type="text">MSE, areas with higher motion are weighted preferentially</text>
<text top="596" left="49" width="251" height="9" font="font5" id="p3_t52" reading_order_no="51" segment_no="16" tag_type="text">using optical flow based weights <a href="deeplearning_paper9.html#12">[7]. </a>Other measures that</text>
<text top="608" left="49" width="251" height="9" font="font5" id="p3_t53" reading_order_no="52" segment_no="16" tag_type="text">involve comparison with a reference include squared error <a href="deeplearning_paper9.html#12">[5]</a></text>
<text top="620" left="49" width="251" height="9" font="font5" id="p3_t54" reading_order_no="53" segment_no="16" tag_type="text">and cosine similarity <a href="deeplearning_paper9.html#12">[9], [22] </a>in the pre-trained VGG net <a href="deeplearning_paper9.html#12">[23]</a></text>
<text top="632" left="49" width="251" height="9" font="font5" id="p3_t55" reading_order_no="54" segment_no="16" tag_type="text">feature space. The inception score for images <a href="deeplearning_paper9.html#12">[24] </a>has also</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p3_t56" reading_order_no="55" segment_no="16" tag_type="text">been applied to evaluate generated video frames <a href="deeplearning_paper9.html#12">[25], [26].</a></text>
<text top="655" left="49" width="251" height="9" font="font5" id="p3_t57" reading_order_no="56" segment_no="16" tag_type="text">The image inception distance has been extended to videos</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p3_t58" reading_order_no="57" segment_no="16" tag_type="text">through FVD <a href="deeplearning_paper9.html#12">[10]. </a>In particular, features based on Inflated</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p3_t59" reading_order_no="58" segment_no="16" tag_type="text">3D Convnet are used to compute a distance measure between</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p3_t60" reading_order_no="59" segment_no="16" tag_type="text">a set of generated videos and a database of pristine videos.</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p3_t61" reading_order_no="60" segment_no="16" tag_type="text">FVD was validated using a human study through pairwise</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p3_t62" reading_order_no="61" segment_no="16" tag_type="text">tests on the BAIR dataset <a href="deeplearning_paper9.html#12">[27]. </a>Further 2AFC experiments</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p3_t63" reading_order_no="62" segment_no="16" tag_type="text">were conducted to evaluate few video prediction models <a href="deeplearning_paper9.html#12">[9],</a></text>
<text top="739" left="49" width="19" height="9" font="font5" id="p3_t64" reading_order_no="63" segment_no="16" tag_type="text"><a href="deeplearning_paper9.html#12">[28].</a></text>
<text top="58" left="312" width="116" height="9" font="font8" id="p3_t65" reading_order_no="64" segment_no="2" tag_type="title">C. Video quality assessment</text>
<text top="73" left="322" width="241" height="9" font="font5" id="p3_t66" reading_order_no="65" segment_no="3" tag_type="text">Video quality assessment (VQA) has been studied quite</text>
<text top="85" left="312" width="251" height="9" font="font5" id="p3_t67" reading_order_no="66" segment_no="3" tag_type="text">extensively over the last decade or so with the conduct of</text>
<text top="97" left="312" width="251" height="9" font="font5" id="p3_t68" reading_order_no="67" segment_no="3" tag_type="text">several studies of subjective quality and the design of success-</text>
<text top="109" left="312" width="251" height="9" font="font5" id="p3_t69" reading_order_no="68" segment_no="3" tag_type="text">ful objective algorithms. Publicly available VQA databases</text>
<text top="121" left="312" width="251" height="9" font="font5" id="p3_t70" reading_order_no="69" segment_no="3" tag_type="text">include those containing synthetic distortions such as the</text>
<text top="132" left="312" width="251" height="9" font="font5" id="p3_t71" reading_order_no="70" segment_no="3" tag_type="text">LIVE VQA database <a href="deeplearning_paper9.html#12">[29] </a>or those containing authentic camera</text>
<text top="144" left="312" width="251" height="9" font="font5" id="p3_t72" reading_order_no="71" segment_no="3" tag_type="text">captured distortions such as the LIVE Video Quality Challenge</text>
<text top="156" left="312" width="251" height="9" font="font5" id="p3_t73" reading_order_no="72" segment_no="3" tag_type="text">(LIVE VQC) Database <a href="deeplearning_paper9.html#12">[30] </a>and the KoNViD-1k database <a href="deeplearning_paper9.html#12">[31].</a></text>
<text top="168" left="312" width="251" height="9" font="font5" id="p3_t74" reading_order_no="73" segment_no="3" tag_type="text">VQA algorithms are broadly divided into three categories,</text>
<text top="180" left="312" width="251" height="9" font="font5" id="p3_t75" reading_order_no="74" segment_no="3" tag_type="text">full reference (FR), reduced reference (RR) and no reference</text>
<text top="192" left="312" width="251" height="9" font="font5" id="p3_t76" reading_order_no="75" segment_no="3" tag_type="text">algorithms (NR). FR VQA algorithms utilize a reference video</text>
<text top="204" left="312" width="251" height="9" font="font5" id="p3_t77" reading_order_no="76" segment_no="3" tag_type="text">to predict the quality of a distorted video by exploiting both</text>
<text top="216" left="312" width="251" height="9" font="font5" id="p3_t78" reading_order_no="77" segment_no="3" tag_type="text">spatial and temporal similarity. RR VQA algorithms extract</text>
<text top="228" left="312" width="251" height="9" font="font5" id="p3_t79" reading_order_no="78" segment_no="3" tag_type="text">quality aware features from reference video and compare them</text>
<text top="240" left="312" width="251" height="9" font="font5" id="p3_t80" reading_order_no="79" segment_no="3" tag_type="text">with features of distorted video to predict its quality. Some</text>
<text top="252" left="312" width="251" height="9" font="font5" id="p3_t81" reading_order_no="80" segment_no="3" tag_type="text">examples of successful FR and RR VQA algorithms that</text>
<text top="264" left="312" width="251" height="9" font="font5" id="p3_t82" reading_order_no="81" segment_no="3" tag_type="text">exploit spatio-temporal information include MOVIE <a href="deeplearning_paper9.html#12">[32], </a>ST-</text>
<text top="276" left="312" width="251" height="9" font="font5" id="p3_t83" reading_order_no="82" segment_no="3" tag_type="text">MAD <a href="deeplearning_paper9.html#12">[33], </a>ST-RRED <a href="deeplearning_paper9.html#12">[34] </a>and VMAF <a href="deeplearning_paper9.html#12">[35]. </a>These algorithms</text>
<text top="288" left="312" width="251" height="9" font="font5" id="p3_t84" reading_order_no="83" segment_no="3" tag_type="text">operate either by computing spatio-temporal transformations</text>
<text top="300" left="312" width="251" height="9" font="font5" id="p3_t85" reading_order_no="84" segment_no="3" tag_type="text">or obtain quality features separately in the spatial and temporal</text>
<text top="312" left="312" width="115" height="9" font="font5" id="p3_t86" reading_order_no="85" segment_no="3" tag_type="text">domains and combine them.</text>
<text top="324" left="322" width="241" height="9" font="font5" id="p3_t87" reading_order_no="86" segment_no="10" tag_type="text">The lack of availability of a true reference in several</text>
<text top="336" left="312" width="251" height="9" font="font5" id="p3_t88" reading_order_no="87" segment_no="10" tag_type="text">scenarios motivates the design of NR algorithms. The NR</text>
<text top="348" left="312" width="251" height="9" font="font5" id="p3_t89" reading_order_no="88" segment_no="10" tag_type="text">VQA problem has been found to be much more challenging</text>
<text top="360" left="312" width="251" height="9" font="font5" id="p3_t90" reading_order_no="89" segment_no="10" tag_type="text">than the FR problem, and current NR algorithms are not yet</text>
<text top="372" left="312" width="251" height="9" font="font5" id="p3_t91" reading_order_no="90" segment_no="10" tag_type="text">as successful as the FR algorithms. Video BLIINDS <a href="deeplearning_paper9.html#12">[36],</a></text>
<text top="383" left="312" width="251" height="9" font="font5" id="p3_t92" reading_order_no="91" segment_no="10" tag_type="text">SACONVA <a href="deeplearning_paper9.html#12">[37] </a>and TLVQM <a href="deeplearning_paper9.html#12">[38] </a>are examples of methods</text>
<text top="395" left="312" width="251" height="9" font="font5" id="p3_t93" reading_order_no="92" segment_no="10" tag_type="text">that have been able to approach the performance of FR</text>
<text top="407" left="312" width="251" height="9" font="font5" id="p3_t94" reading_order_no="93" segment_no="10" tag_type="text">algorithms. Recently, deep neural networks have been used</text>
<text top="419" left="312" width="251" height="9" font="font5" id="p3_t95" reading_order_no="94" segment_no="10" tag_type="text">to obtain good performance on authentic distortions <a href="deeplearning_paper9.html#12">[39].</a></text>
<text top="431" left="312" width="251" height="9" font="font5" id="p3_t96" reading_order_no="95" segment_no="10" tag_type="text">Nevertheless, the use of convolutional neural networks to</text>
<text top="443" left="312" width="251" height="9" font="font5" id="p3_t97" reading_order_no="96" segment_no="10" tag_type="text">design successful NR VQA algorithms is still a nascent and</text>
<text top="455" left="312" width="94" height="9" font="font5" id="p3_t98" reading_order_no="97" segment_no="10" tag_type="text">active area of research.</text>
<text top="479" left="334" width="25" height="9" font="font5" id="p3_t99" reading_order_no="98" segment_no="12" tag_type="title">III. P</text>
<text top="481" left="360" width="43" height="7" font="font6" id="p3_t100" reading_order_no="99" segment_no="12" tag_type="title">REDICTED</text>
<text top="479" left="406" width="7" height="9" font="font5" id="p3_t101" reading_order_no="100" segment_no="12" tag_type="title">V</text>
<text top="481" left="413" width="25" height="7" font="font6" id="p3_t102" reading_order_no="101" segment_no="12" tag_type="title">IDEOS</text>
<text top="479" left="442" width="7" height="9" font="font5" id="p3_t103" reading_order_no="102" segment_no="12" tag_type="title">Q</text>
<text top="481" left="449" width="32" height="7" font="font6" id="p3_t104" reading_order_no="103" segment_no="12" tag_type="title">UALITY</text>
<text top="479" left="484" width="7" height="9" font="font5" id="p3_t105" reading_order_no="104" segment_no="12" tag_type="title">A</text>
<text top="481" left="492" width="49" height="7" font="font6" id="p3_t106" reading_order_no="105" segment_no="12" tag_type="title">SSESSMENT</text>
<text top="491" left="415" width="7" height="9" font="font5" id="p3_t107" reading_order_no="106" segment_no="12" tag_type="title">D</text>
<text top="493" left="422" width="38" height="7" font="font6" id="p3_t108" reading_order_no="107" segment_no="12" tag_type="title">ATABASE</text>
<text top="506" left="322" width="241" height="9" font="font5" id="p3_t109" reading_order_no="108" segment_no="13" tag_type="text">We now describe in detail, the IISc Predicted Videos Quality</text>
<text top="518" left="312" width="251" height="9" font="font5" id="p3_t110" reading_order_no="109" segment_no="13" tag_type="text">Assessment (IISc PVQA) database, our subjective study and</text>
<text top="530" left="312" width="158" height="9" font="font5" id="p3_t111" reading_order_no="110" segment_no="13" tag_type="text">important observations from the study.</text>
<text top="557" left="312" width="52" height="9" font="font8" id="p3_t112" reading_order_no="111" segment_no="15" tag_type="title">A. Database</text>
<text top="572" left="322" width="241" height="9" font="font5" id="p3_t113" reading_order_no="112" segment_no="17" tag_type="text">The videos in our database are generated by various video</text>
<text top="584" left="312" width="251" height="9" font="font5" id="p3_t114" reading_order_no="113" segment_no="17" tag_type="text">prediction algorithms. These video prediction algorithms are</text>
<text top="596" left="312" width="251" height="9" font="font5" id="p3_t115" reading_order_no="114" segment_no="17" tag_type="text">trained on a variety of datasets containing human actions,</text>
<text top="608" left="312" width="251" height="9" font="font5" id="p3_t116" reading_order_no="115" segment_no="17" tag_type="text">sports videos, vehicle driving, and robot pushing videos. In</text>
<text top="620" left="312" width="251" height="9" font="font5" id="p3_t117" reading_order_no="116" segment_no="17" tag_type="text">our database, we use a combination of publicly available pre-</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p3_t118" reading_order_no="117" segment_no="17" tag_type="text">trained models of different prediction algorithms and also</text>
<text top="644" left="312" width="159" height="9" font="font5" id="p3_t119" reading_order_no="118" segment_no="17" tag_type="text">models that we train on other datasets.</text>
<text top="656" left="322" width="35" height="9" font="font8" id="p3_t120" reading_order_no="119" segment_no="18" tag_type="text">Datasets</text>
<text top="655" left="357" width="206" height="9" font="font5" id="p3_t121" reading_order_no="120" segment_no="18" tag_type="text">: We apply the video prediction models on nine</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p3_t122" reading_order_no="121" segment_no="18" tag_type="text">different datasets typically used in their evaluation. These</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p3_t123" reading_order_no="122" segment_no="18" tag_type="text">include BAIR <a href="deeplearning_paper9.html#12">[27], </a>PUSH <a href="deeplearning_paper9.html#12">[4], </a>KTH <a href="deeplearning_paper9.html#12">[42], </a>MSR <a href="deeplearning_paper9.html#12">[43], </a>UCF-</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p3_t124" reading_order_no="123" segment_no="18" tag_type="text">101 <a href="deeplearning_paper9.html#12">[44], </a>PENN <a href="deeplearning_paper9.html#12">[45], </a>KITTI <a href="deeplearning_paper9.html#12">[46], </a>Caltech Pedestrian <a href="deeplearning_paper9.html#13">[47]</a></text>
<text top="703" left="312" width="251" height="9" font="font5" id="p3_t125" reading_order_no="124" segment_no="18" tag_type="text">and BDD100K <a href="deeplearning_paper9.html#13">[48]. </a>Among the above datasets, the BAIR</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p3_t126" reading_order_no="125" segment_no="18" tag_type="text">robot push dataset is highly stochastic i.e. the movement</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p3_t127" reading_order_no="126" segment_no="18" tag_type="text">of the robotic arm given the current frame is random. The</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p3_t128" reading_order_no="127" segment_no="18" tag_type="text">other datasets have relatively lower stochasticity, as argued</text>
</page>
<page number="4" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="560" width="3" height="6" font="font0" id="p4_t1" reading_order_no="0" segment_no="0" tag_type="text">4</text>
<text top="54" left="199" width="214" height="9" font="font5" id="p4_t2" reading_order_no="1" segment_no="1" tag_type="title">TABLE I: Number of videos from different datasets</text>
<text top="75" left="154" width="19" height="7" font="font6" id="p4_t3" reading_order_no="2" segment_no="2" tag_type="table">BAIR</text>
<text top="75" left="185" width="35" height="7" font="font6" id="p4_t4" reading_order_no="3" segment_no="2" tag_type="table">BDD100K</text>
<text top="75" left="232" width="24" height="7" font="font6" id="p4_t5" reading_order_no="4" segment_no="2" tag_type="table">Caltech</text>
<text top="75" left="268" width="21" height="7" font="font6" id="p4_t6" reading_order_no="5" segment_no="2" tag_type="table">KITTI</text>
<text top="75" left="302" width="16" height="7" font="font6" id="p4_t7" reading_order_no="6" segment_no="2" tag_type="table">KTH</text>
<text top="75" left="330" width="17" height="7" font="font6" id="p4_t8" reading_order_no="7" segment_no="2" tag_type="table">MSR</text>
<text top="75" left="359" width="21" height="7" font="font6" id="p4_t9" reading_order_no="8" segment_no="2" tag_type="table">PENN</text>
<text top="75" left="393" width="20" height="7" font="font6" id="p4_t10" reading_order_no="9" segment_no="2" tag_type="table">PUSH</text>
<text top="75" left="425" width="30" height="7" font="font6" id="p4_t11" reading_order_no="10" segment_no="2" tag_type="table">UCF-101</text>
<text top="84" left="159" width="8" height="7" font="font6" id="p4_t12" reading_order_no="11" segment_no="2" tag_type="table">40</text>
<text top="84" left="198" width="8" height="7" font="font6" id="p4_t13" reading_order_no="12" segment_no="2" tag_type="table">40</text>
<text top="84" left="240" width="8" height="7" font="font6" id="p4_t14" reading_order_no="13" segment_no="2" tag_type="table">14</text>
<text top="84" left="275" width="8" height="7" font="font6" id="p4_t15" reading_order_no="14" segment_no="2" tag_type="table">46</text>
<text top="84" left="306" width="8" height="7" font="font6" id="p4_t16" reading_order_no="15" segment_no="2" tag_type="table">33</text>
<text top="84" left="335" width="8" height="7" font="font6" id="p4_t17" reading_order_no="16" segment_no="2" tag_type="table">17</text>
<text top="84" left="366" width="8" height="7" font="font6" id="p4_t18" reading_order_no="17" segment_no="2" tag_type="table">50</text>
<text top="84" left="399" width="8" height="7" font="font6" id="p4_t19" reading_order_no="18" segment_no="2" tag_type="table">10</text>
<text top="84" left="436" width="8" height="7" font="font6" id="p4_t20" reading_order_no="19" segment_no="2" tag_type="table">50</text>
<text top="122" left="49" width="251" height="9" font="font5" id="p4_t21" reading_order_no="20" segment_no="3" tag_type="text">in <a href="deeplearning_paper9.html#12">[9]. </a>For the sake of simplicity, we refer to these datasets</text>
<text top="134" left="49" width="251" height="9" font="font5" id="p4_t22" reading_order_no="21" segment_no="3" tag_type="text">as deterministic datasets. Table <a href="deeplearning_paper9.html#4">I </a>shows the number of videos</text>
<text top="146" left="49" width="100" height="9" font="font5" id="p4_t23" reading_order_no="22" segment_no="3" tag_type="text">taken from each dataset.</text>
<text top="160" left="59" width="103" height="9" font="font8" id="p4_t24" reading_order_no="23" segment_no="4" tag_type="text">Video Prediction Models</text>
<text top="160" left="162" width="138" height="9" font="font5" id="p4_t25" reading_order_no="24" segment_no="4" tag_type="text">: We use a total of seven video</text>
<text top="172" left="49" width="251" height="9" font="font5" id="p4_t26" reading_order_no="25" segment_no="4" tag_type="text">prediction models. The models can be broadly classified as</text>
<text top="184" left="49" width="251" height="9" font="font5" id="p4_t27" reading_order_no="26" segment_no="4" tag_type="text">deterministic and stochastic. The deterministic models are</text>
<text top="196" left="49" width="251" height="9" font="font5" id="p4_t28" reading_order_no="27" segment_no="4" tag_type="text">trained to predict the future frames, exactly as in the ground</text>
<text top="208" left="49" width="251" height="9" font="font5" id="p4_t29" reading_order_no="28" segment_no="4" tag_type="text">truth video. The deterministic models we use are PredNet <a href="deeplearning_paper9.html#12">[17],</a></text>
<text top="220" left="49" width="251" height="9" font="font5" id="p4_t30" reading_order_no="29" segment_no="4" tag_type="text">MCnet <a href="deeplearning_paper9.html#12">[16], </a>Future GAN <a href="deeplearning_paper9.html#13">[49] </a>and DYAN <a href="deeplearning_paper9.html#13">[50]. </a>On the other</text>
<text top="232" left="49" width="251" height="9" font="font5" id="p4_t31" reading_order_no="30" segment_no="4" tag_type="text">hand, the stochastic models model uncertainty by being trained</text>
<text top="244" left="49" width="251" height="9" font="font5" id="p4_t32" reading_order_no="31" segment_no="4" tag_type="text">to predict a distribution of possible futures using noise as</text>
<text top="256" left="49" width="251" height="9" font="font5" id="p4_t33" reading_order_no="32" segment_no="4" tag_type="text">an additional input. We use videos generated by SAVP <a href="deeplearning_paper9.html#12">[9],</a></text>
<text top="267" left="49" width="251" height="9" font="font5" id="p4_t34" reading_order_no="33" segment_no="4" tag_type="text">SV2P <a href="deeplearning_paper9.html#13">[51], </a>SVG-LP <a href="deeplearning_paper9.html#12">[19] </a>and some of their ablation models in</text>
<text top="279" left="49" width="251" height="9" font="font5" id="p4_t35" reading_order_no="34" segment_no="4" tag_type="text">our database. Along with the videos predicted by these models,</text>
<text top="291" left="49" width="251" height="9" font="font5" id="p4_t36" reading_order_no="35" segment_no="4" tag_type="text">we also include ground truth or natural videos (videos that are</text>
<text top="303" left="49" width="251" height="9" font="font5" id="p4_t37" reading_order_no="36" segment_no="4" tag_type="text">not predicted) from these datasets in our database. This helps</text>
<text top="315" left="49" width="251" height="9" font="font5" id="p4_t38" reading_order_no="37" segment_no="4" tag_type="text">us validate various aspects of the study, such as biases due</text>
<text top="327" left="49" width="251" height="9" font="font5" id="p4_t39" reading_order_no="38" segment_no="4" tag_type="text">to different resolutions and whether the subjects are able to</text>
<text top="339" left="49" width="251" height="9" font="font5" id="p4_t40" reading_order_no="39" segment_no="4" tag_type="text">perceive the distortions and accordingly judge the quality of</text>
<text top="351" left="49" width="85" height="9" font="font5" id="p4_t41" reading_order_no="40" segment_no="4" tag_type="text">the predicted videos.</text>
<text top="365" left="59" width="45" height="9" font="font8" id="p4_t42" reading_order_no="41" segment_no="6" tag_type="text">Distortions</text>
<text top="365" left="104" width="196" height="9" font="font5" id="p4_t43" reading_order_no="42" segment_no="6" tag_type="text">: While loss of quality can occur in multiple</text>
<text top="377" left="49" width="251" height="9" font="font5" id="p4_t44" reading_order_no="43" segment_no="6" tag_type="text">ways, we broadly observe four different types of distortions</text>
<text top="389" left="49" width="251" height="9" font="font5" id="p4_t45" reading_order_no="44" segment_no="6" tag_type="text">in the predicted videos. The loss of quality is primarily seen</text>
<text top="401" left="49" width="251" height="9" font="font5" id="p4_t46" reading_order_no="45" segment_no="6" tag_type="text">in the form of blurred frames or distorted object shapes. The</text>
<text top="413" left="49" width="251" height="9" font="font5" id="p4_t47" reading_order_no="46" segment_no="6" tag_type="text">use of pixel level loss measures such as mean squared error</text>
<text top="425" left="49" width="251" height="9" font="font5" id="p4_t48" reading_order_no="47" segment_no="6" tag_type="text">in training video prediction algorithms can lead to blurred</text>
<text top="437" left="49" width="251" height="9" font="font5" id="p4_t49" reading_order_no="48" segment_no="6" tag_type="text">frames <a href="deeplearning_paper9.html#12">[17], </a>as shown in Fig. <a href="deeplearning_paper9.html#2">1a. </a>We observe that algorithms</text>
<text top="449" left="49" width="251" height="9" font="font5" id="p4_t50" reading_order_no="49" segment_no="6" tag_type="text">trained using adversarial loss functions <a href="deeplearning_paper9.html#12">[16], </a><a href="deeplearning_paper9.html#13">[49], </a>result in</text>
<text top="461" left="49" width="251" height="9" font="font5" id="p4_t51" reading_order_no="50" segment_no="6" tag_type="text">distortions of object shapes in frames further into the future,</text>
<text top="473" left="49" width="251" height="9" font="font5" id="p4_t52" reading_order_no="51" segment_no="6" tag_type="text">as shown in Fig. <a href="deeplearning_paper9.html#2">1b</a>. This primarily occurs in objects with</text>
<text top="484" left="49" width="251" height="9" font="font5" id="p4_t53" reading_order_no="52" segment_no="6" tag_type="text">reasonable motion. We also notice the sudden appearance or</text>
<text top="496" left="49" width="251" height="9" font="font5" id="p4_t54" reading_order_no="53" segment_no="6" tag_type="text">disappearance of objects, as shown in Fig. <a href="deeplearning_paper9.html#2">1c. </a>Occasionally,</text>
<text top="508" left="49" width="251" height="9" font="font5" id="p4_t55" reading_order_no="54" segment_no="6" tag_type="text">we observe inexplicable color variations during the video</text>
<text top="520" left="49" width="209" height="9" font="font5" id="p4_t56" reading_order_no="55" segment_no="6" tag_type="text">trajectory that look unnatural, as shown in Fig. <a href="deeplearning_paper9.html#2">1d.</a></text>
<text top="534" left="59" width="241" height="9" font="font5" id="p4_t57" reading_order_no="56" segment_no="9" tag_type="text">Further, we see different kinds of shape distortions such as</text>
<text top="546" left="49" width="251" height="9" font="font5" id="p4_t58" reading_order_no="57" segment_no="9" tag_type="text">deformations (Fig. <a href="deeplearning_paper9.html#4">2a), </a>splitting (Fig. <a href="deeplearning_paper9.html#4">2b) </a>and elongations of</text>
<text top="558" left="49" width="251" height="9" font="font5" id="p4_t59" reading_order_no="58" segment_no="9" tag_type="text">objects (Fig. <a href="deeplearning_paper9.html#4">2d). </a>In some videos, we witness a combination</text>
<text top="570" left="49" width="251" height="9" font="font5" id="p4_t60" reading_order_no="59" segment_no="9" tag_type="text">of shape distortions with object disappearance (Fig. <a href="deeplearning_paper9.html#4">2c). </a>We</text>
<text top="582" left="49" width="251" height="9" font="font5" id="p4_t61" reading_order_no="60" segment_no="9" tag_type="text">note that shape distortions are highly localized, while the rest</text>
<text top="594" left="49" width="183" height="9" font="font5" id="p4_t62" reading_order_no="61" segment_no="9" tag_type="text">of the video frame looks completely natural.</text>
<text top="608" left="59" width="81" height="9" font="font8" id="p4_t63" reading_order_no="62" segment_no="10" tag_type="text">Selection of Videos</text>
<text top="608" left="140" width="160" height="9" font="font5" id="p4_t64" reading_order_no="63" segment_no="10" tag_type="text">: The videos in our database include</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p4_t65" reading_order_no="64" segment_no="10" tag_type="text">those generated by applying stochastic models on stochastic</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p4_t66" reading_order_no="65" segment_no="10" tag_type="text">datasets, stochastic models on deterministic datasets, and de-</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p4_t67" reading_order_no="66" segment_no="10" tag_type="text">terministic models on deterministic datasets. Using the above</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p4_t68" reading_order_no="67" segment_no="10" tag_type="text">combinations, we generate a large number of videos. We group</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p4_t69" reading_order_no="68" segment_no="10" tag_type="text">these predicted videos based on the type of distortions, such</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p4_t70" reading_order_no="69" segment_no="10" tag_type="text">as blur, shape distortion, disappearance and color change, as</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p4_t71" reading_order_no="70" segment_no="10" tag_type="text">discussed earlier. Among videos that suffer from blur and</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p4_t72" reading_order_no="71" segment_no="10" tag_type="text">shape distortions, we manually tag them as low, medium</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p4_t73" reading_order_no="72" segment_no="10" tag_type="text">and high quality videos. We then roughly select an equal</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p4_t74" reading_order_no="73" segment_no="10" tag_type="text">number of blur and shape distorted videos at varying levels of</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p4_t75" reading_order_no="74" segment_no="10" tag_type="text">quality, ensuring diversity among the different datasets used to</text>
<text top="294" left="312" width="251" height="9" font="font5" id="p4_t76" reading_order_no="75" segment_no="5" tag_type="text">Fig. 2: Different kinds of shape distortions observed in pre-</text>
<text top="306" left="312" width="251" height="9" font="font5" id="p4_t77" reading_order_no="76" segment_no="5" tag_type="text">dicted videos. The sequence of images in each row correspond</text>
<text top="318" left="312" width="251" height="9" font="font5" id="p4_t78" reading_order_no="77" segment_no="5" tag_type="text">to the frames of a video. Starting from the first frame, we show</text>
<text top="330" left="312" width="251" height="9" font="font5" id="p4_t79" reading_order_no="78" segment_no="5" tag_type="text">every fifth frame. The first frame is a context frame and the</text>
<text top="342" left="312" width="251" height="9" font="font5" id="p4_t80" reading_order_no="79" segment_no="5" tag_type="text">next 3 frames are predicted frames. The videos can be viewed</text>
<text top="354" left="312" width="94" height="9" font="font5" id="p4_t81" reading_order_no="80" segment_no="5" tag_type="text">on our project website.</text>
<text top="387" left="312" width="251" height="9" font="font5" id="p4_t82" reading_order_no="81" segment_no="7" tag_type="text">generate the predicted videos. We observed that blur and shape</text>
<text top="399" left="312" width="251" height="9" font="font5" id="p4_t83" reading_order_no="82" segment_no="7" tag_type="text">distortions were most commonly seen in the predicted videos</text>
<text top="411" left="312" width="251" height="9" font="font5" id="p4_t84" reading_order_no="83" segment_no="7" tag_type="text">and they form a significant part of our database. Nonetheless,</text>
<text top="423" left="312" width="251" height="9" font="font5" id="p4_t85" reading_order_no="84" segment_no="7" tag_type="text">we also select videos with all the other distortions that we</text>
<text top="435" left="312" width="251" height="9" font="font5" id="p4_t86" reading_order_no="85" segment_no="7" tag_type="text">observed such as disappearance of objects and color changes,</text>
<text top="447" left="312" width="251" height="9" font="font5" id="p4_t87" reading_order_no="86" segment_no="7" tag_type="text">but these videos are fewer in number. Thus, we arrive at a total</text>
<text top="459" left="312" width="251" height="9" font="font5" id="p4_t88" reading_order_no="87" segment_no="7" tag_type="text">of 300 videos that represent most of the distortions observed</text>
<text top="471" left="312" width="96" height="9" font="font5" id="p4_t89" reading_order_no="88" segment_no="7" tag_type="text">in the predicted videos.</text>
<text top="483" left="322" width="128" height="9" font="font8" id="p4_t90" reading_order_no="89" segment_no="8" tag_type="text">Video Resolution and Duration</text>
<text top="483" left="450" width="113" height="9" font="font5" id="p4_t91" reading_order_no="90" segment_no="8" tag_type="text">: Since different video pre-</text>
<text top="495" left="312" width="251" height="9" font="font5" id="p4_t92" reading_order_no="91" segment_no="8" tag_type="text">diction models available in the literature are trained to generate</text>
<text top="507" left="312" width="251" height="9" font="font5" id="p4_t93" reading_order_no="92" segment_no="8" tag_type="text">videos at different resolutions, the videos in our database are of</text>
<text top="519" left="312" width="251" height="9" font="font5" id="p4_t94" reading_order_no="93" segment_no="8" tag_type="text">varying resolutions. The resolutions include 64x64, 128x128,</text>
<text top="531" left="312" width="251" height="9" font="font5" id="p4_t95" reading_order_no="94" segment_no="8" tag_type="text">160x128, and 320x240. We discuss the implications of this</text>
<text top="543" left="312" width="251" height="9" font="font5" id="p4_t96" reading_order_no="95" segment_no="8" tag_type="text">aspect of the database and the normalization required while</text>
<text top="554" left="312" width="251" height="9" font="font5" id="p4_t97" reading_order_no="96" segment_no="8" tag_type="text">conducting the subjective study in Section <a href="deeplearning_paper9.html#4">III-B. </a>All videos</text>
<text top="566" left="312" width="251" height="9" font="font5" id="p4_t98" reading_order_no="97" segment_no="8" tag_type="text">generated by the prediction algorithms have 4 context frames</text>
<text top="578" left="312" width="251" height="9" font="font5" id="p4_t99" reading_order_no="98" segment_no="8" tag_type="text">and 16 predicted frames. Following <a href="deeplearning_paper9.html#12">[9], </a>where a small scale</text>
<text top="590" left="312" width="251" height="9" font="font5" id="p4_t100" reading_order_no="99" segment_no="8" tag_type="text">subjective evaluation was conducted, we use a frame rate of</text>
<text top="602" left="312" width="251" height="9" font="font5" id="p4_t101" reading_order_no="100" segment_no="8" tag_type="text">4fps for all the videos. Thus, each video is of duration 5</text>
<text top="614" left="312" width="103" height="9" font="font5" id="p4_t102" reading_order_no="101" segment_no="8" tag_type="text">seconds during playback.</text>
<text top="641" left="312" width="80" height="9" font="font8" id="p4_t103" reading_order_no="102" segment_no="11" tag_type="title">B. Subjective Study</text>
<text top="655" left="322" width="241" height="9" font="font5" id="p4_t104" reading_order_no="103" segment_no="12" tag_type="text">We conduct a subjective study to assess the quality of the</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p4_t105" reading_order_no="104" segment_no="12" tag_type="text">predicted videos. Since the subjective evaluation of quality of</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p4_t106" reading_order_no="105" segment_no="12" tag_type="text">predicted videos has not been studied before and it is not clear</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p4_t107" reading_order_no="106" segment_no="12" tag_type="text">apriori how humans would respond to the task of assessing</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p4_t108" reading_order_no="107" segment_no="12" tag_type="text">distortions in predicted videos which are often localized, we</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p4_t109" reading_order_no="108" segment_no="12" tag_type="text">conduct the study in a controlled lab environment. In our study,</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p4_t110" reading_order_no="109" segment_no="12" tag_type="text">50 subjects participated under calibrated viewing conditions,</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p4_t111" reading_order_no="110" segment_no="12" tag_type="text">and all the subjects viewed the videos on a 24 inch LED</text>
</page>
<page number="5" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="560" width="3" height="6" font="font0" id="p5_t1" reading_order_no="0" segment_no="0" tag_type="text">5</text>
<text top="256" left="67" width="214" height="9" font="font5" id="p5_t2" reading_order_no="1" segment_no="2" tag_type="text">Fig. 3: Distribution of Mean Opinion Scores (MOS)</text>
<text top="293" left="49" width="251" height="9" font="font5" id="p5_t3" reading_order_no="2" segment_no="3" tag_type="text">monitor. Most of the subjects were students studying at IISc</text>
<text top="305" left="49" width="251" height="9" font="font5" id="p5_t4" reading_order_no="3" segment_no="3" tag_type="text">and hence in the age range 20–30. Of the 50 subjects, 35 were</text>
<text top="317" left="49" width="251" height="9" font="font5" id="p5_t5" reading_order_no="4" segment_no="3" tag_type="text">male and 15 were female. Each subject rated a total of 120</text>
<text top="329" left="49" width="251" height="9" font="font5" id="p5_t6" reading_order_no="5" segment_no="3" tag_type="text">videos, 60 each in two sessions, each session lasting around</text>
<text top="341" left="49" width="251" height="9" font="font5" id="p5_t7" reading_order_no="6" segment_no="3" tag_type="text">half an hour and separated by a minimum of 24 hours. For</text>
<text top="353" left="49" width="251" height="9" font="font5" id="p5_t8" reading_order_no="7" segment_no="3" tag_type="text">each subject, the videos were presented in a random sequence.</text>
<text top="365" left="49" width="251" height="9" font="font5" id="p5_t9" reading_order_no="8" segment_no="3" tag_type="text">Each video is rated by an equal number of subjects. Since there</text>
<text top="377" left="49" width="251" height="9" font="font5" id="p5_t10" reading_order_no="9" segment_no="3" tag_type="text">are 300 videos in our database, we obtain a total of 20 human</text>
<text top="389" left="49" width="90" height="9" font="font5" id="p5_t11" reading_order_no="10" segment_no="3" tag_type="text">scores for each video.</text>
<text top="402" left="59" width="241" height="9" font="font5" id="p5_t12" reading_order_no="11" segment_no="5" tag_type="text">Since it is difficult to perceptually understand the lower</text>
<text top="414" left="49" width="251" height="9" font="font5" id="p5_t13" reading_order_no="12" segment_no="5" tag_type="text">resolution videos in our database, such videos are upsampled</text>
<text top="426" left="49" width="251" height="9" font="font5" id="p5_t14" reading_order_no="13" segment_no="5" tag_type="text">using bicubic interpolation and shown during the subjective</text>
<text top="438" left="49" width="251" height="9" font="font5" id="p5_t15" reading_order_no="14" segment_no="5" tag_type="text">study. In order to remove any biases in the scoring of such</text>
<text top="450" left="49" width="251" height="9" font="font5" id="p5_t16" reading_order_no="15" segment_no="5" tag_type="text">upsampled videos, we employ a double stimulus continuous</text>
<text top="462" left="49" width="251" height="9" font="font5" id="p5_t17" reading_order_no="16" segment_no="5" tag_type="text">quality assessment scoring mechanism. Here, a natural video</text>
<text top="474" left="49" width="251" height="9" font="font5" id="p5_t18" reading_order_no="17" segment_no="5" tag_type="text">with similar content at the same resolution as the evaluation</text>
<text top="486" left="49" width="251" height="9" font="font5" id="p5_t19" reading_order_no="18" segment_no="5" tag_type="text">video is also upsampled and shown on the left while the</text>
<text top="498" left="49" width="251" height="9" font="font5" id="p5_t20" reading_order_no="19" segment_no="5" tag_type="text">evaluation video is shown on the right. The subjects are</text>
<text top="510" left="49" width="251" height="9" font="font5" id="p5_t21" reading_order_no="20" segment_no="5" tag_type="text">asked to rate the quality of the evaluation video on a scale</text>
<text top="522" left="49" width="251" height="9" font="font5" id="p5_t22" reading_order_no="21" segment_no="5" tag_type="text">between 0 and 100, assuming that the natural video shown</text>
<text top="534" left="49" width="251" height="9" font="font5" id="p5_t23" reading_order_no="22" segment_no="5" tag_type="text">would correspond to a score of 100. We show in Section <a href="deeplearning_paper9.html#5">III-C</a></text>
<text top="546" left="49" width="251" height="9" font="font5" id="p5_t24" reading_order_no="23" segment_no="5" tag_type="text">that such upsampling does not bias the quality scores of the</text>
<text top="558" left="49" width="75" height="9" font="font5" id="p5_t25" reading_order_no="24" segment_no="5" tag_type="text">upsampled videos.</text>
<text top="571" left="59" width="241" height="9" font="font5" id="p5_t26" reading_order_no="25" segment_no="8" tag_type="text">Since most of the videos in the database show a degradation</text>
<text top="583" left="49" width="251" height="9" font="font5" id="p5_t27" reading_order_no="26" segment_no="8" tag_type="text">of quality with time, we asked the subjects to take into account</text>
<text top="595" left="49" width="251" height="9" font="font5" id="p5_t28" reading_order_no="27" segment_no="8" tag_type="text">the entire 5s duration video and provide a single holistic score</text>
<text top="606" left="49" width="251" height="9" font="font5" id="p5_t29" reading_order_no="28" segment_no="8" tag_type="text">of the video quality. The videos are looped continuously and</text>
<text top="618" left="49" width="251" height="9" font="font5" id="p5_t30" reading_order_no="29" segment_no="8" tag_type="text">the subjects can view them as long as desired before providing</text>
<text top="630" left="49" width="251" height="9" font="font5" id="p5_t31" reading_order_no="30" segment_no="8" tag_type="text">a rating on a continuous scale that appears at the bottom of</text>
<text top="642" left="49" width="251" height="9" font="font5" id="p5_t32" reading_order_no="31" segment_no="8" tag_type="text">the screen. Every subject is shown 6 videos prior to the start</text>
<text top="654" left="49" width="251" height="9" font="font5" id="p5_t33" reading_order_no="32" segment_no="8" tag_type="text">of the study in each session. This allows the subject to get</text>
<text top="666" left="49" width="251" height="9" font="font5" id="p5_t34" reading_order_no="33" segment_no="8" tag_type="text">a sense of the range of quality levels and different kinds of</text>
<text top="678" left="49" width="109" height="9" font="font5" id="p5_t35" reading_order_no="34" segment_no="8" tag_type="text">distortions in the database.</text>
<text top="691" left="59" width="131" height="9" font="font8" id="p5_t36" reading_order_no="35" segment_no="10" tag_type="text">Processing of Subjective Scores</text>
<text top="691" left="190" width="110" height="9" font="font5" id="p5_t37" reading_order_no="36" segment_no="10" tag_type="text">: We process the collected</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p5_t38" reading_order_no="37" segment_no="10" tag_type="text">subjective scores to obtain a mean opinion score (MOS) of</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p5_t39" reading_order_no="38" segment_no="10" tag_type="text">quality for every video following well established procedures</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p5_t40" reading_order_no="39" segment_no="10" tag_type="text">in VQA <a href="deeplearning_paper9.html#12">[29]. </a>In particular, we subtract the mean and standard</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p5_t41" reading_order_no="40" segment_no="10" tag_type="text">deviation of the scores of each subject in each viewing session</text>
<text top="252" left="312" width="251" height="9" font="font5" id="p5_t42" reading_order_no="41" segment_no="1" tag_type="text">Fig. 4: Scatter plot of MOS obtained from two random halves</text>
<text top="264" left="312" width="73" height="9" font="font5" id="p5_t43" reading_order_no="42" segment_no="1" tag_type="text">of the population.</text>
<text top="299" left="312" width="251" height="9" font="font5" id="p5_t44" reading_order_no="43" segment_no="4" tag_type="text">to obtain ‘Z-scores’. We then apply the subject rejection</text>
<text top="311" left="312" width="251" height="9" font="font5" id="p5_t45" reading_order_no="44" segment_no="4" tag_type="text">procedure outlined in ITU-R BT 500.11 recommendation <a href="deeplearning_paper9.html#13">[52]</a></text>
<text top="322" left="312" width="251" height="9" font="font5" id="p5_t46" reading_order_no="45" segment_no="4" tag_type="text">to remove the outlier subjects. In our study, we found 7 out</text>
<text top="334" left="312" width="251" height="9" font="font5" id="p5_t47" reading_order_no="46" segment_no="4" tag_type="text">of 50 subjects to be outliers. We note that, 6 out of 7 outliers</text>
<text top="346" left="312" width="251" height="9" font="font5" id="p5_t48" reading_order_no="47" segment_no="4" tag_type="text">marginally satisfied the rejection criteria. The scores from the</text>
<text top="358" left="312" width="251" height="9" font="font5" id="p5_t49" reading_order_no="48" segment_no="4" tag_type="text">inlier subjects are then rescaled linearly to lie between 0 and</text>
<text top="370" left="312" width="251" height="9" font="font5" id="p5_t50" reading_order_no="49" segment_no="4" tag_type="text">100, and the MOS for every video is computed as the average</text>
<text top="382" left="312" width="251" height="9" font="font5" id="p5_t51" reading_order_no="50" segment_no="4" tag_type="text">Z-score (after rescaling) of every video across all subjects who</text>
<text top="394" left="312" width="251" height="9" font="font5" id="p5_t52" reading_order_no="51" segment_no="4" tag_type="text">rated that video. Fig. <a href="deeplearning_paper9.html#5">3 </a>shows the distribution of MOS, where</text>
<text top="406" left="312" width="251" height="9" font="font5" id="p5_t53" reading_order_no="52" segment_no="4" tag_type="text">we see that more than 90% of the scores lie in the range</text>
<text top="418" left="312" width="251" height="9" font="font5" id="p5_t54" reading_order_no="53" segment_no="4" tag_type="text">[30, 80]. Such a distribution of scores presents a challenging</text>
<text top="430" left="312" width="251" height="9" font="font5" id="p5_t55" reading_order_no="54" segment_no="4" tag_type="text">test condition for quality assessment methods. In Fig. <a href="deeplearning_paper9.html#5">3, </a>we</text>
<text top="442" left="312" width="251" height="9" font="font5" id="p5_t56" reading_order_no="55" segment_no="4" tag_type="text">observe a small peak around MOS value of 75. This peak is</text>
<text top="454" left="312" width="220" height="9" font="font5" id="p5_t57" reading_order_no="56" segment_no="4" tag_type="text">due to the presence of natural videos in our database.</text>
<text top="484" left="312" width="175" height="9" font="font8" id="p5_t58" reading_order_no="57" segment_no="6" tag_type="title">C. Observations from the Subjective Study</text>
<text top="500" left="322" width="114" height="9" font="font8" id="p5_t59" reading_order_no="58" segment_no="7" tag_type="text">1) Consistency of subjects:</text>
<text top="499" left="442" width="121" height="9" font="font5" id="p5_t60" reading_order_no="59" segment_no="7" tag_type="text">We randomly split the inlier</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p5_t61" reading_order_no="60" segment_no="7" tag_type="text">subjects into two halves and compute MOS for each video</text>
<text top="523" left="312" width="251" height="9" font="font5" id="p5_t62" reading_order_no="61" segment_no="7" tag_type="text">in each half of the population. We then compute the Pear-</text>
<text top="535" left="312" width="251" height="9" font="font5" id="p5_t63" reading_order_no="62" segment_no="7" tag_type="text">son’s linear correlation coefficient (PLCC) between the MOS</text>
<text top="547" left="312" width="251" height="9" font="font5" id="p5_t64" reading_order_no="63" segment_no="7" tag_type="text">coming from each half. Fig. <a href="deeplearning_paper9.html#5">4 </a>shows scatter plot of MOS</text>
<text top="559" left="312" width="251" height="9" font="font5" id="p5_t65" reading_order_no="64" segment_no="7" tag_type="text">obtained from each half for one such split, where we observe</text>
<text top="571" left="312" width="251" height="9" font="font5" id="p5_t66" reading_order_no="65" segment_no="7" tag_type="text">high correlation between MOS from the two halves. Further,</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p5_t67" reading_order_no="66" segment_no="7" tag_type="text">we compute median PLCC across 100 random splits of the</text>
<text top="595" left="312" width="251" height="9" font="font5" id="p5_t68" reading_order_no="67" segment_no="7" tag_type="text">population, which works out to 0.94. This shows that the</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p5_t69" reading_order_no="68" segment_no="7" tag_type="text">subjects are fairly consistent in assessing the quality of the</text>
<text top="619" left="312" width="251" height="9" font="font5" id="p5_t70" reading_order_no="69" segment_no="7" tag_type="text">videos. This also provides a reasonable upper bound on the</text>
<text top="631" left="312" width="251" height="9" font="font5" id="p5_t71" reading_order_no="70" segment_no="7" tag_type="text">correlation with the subjective scores, which we can expect</text>
<text top="643" left="312" width="144" height="9" font="font5" id="p5_t72" reading_order_no="71" segment_no="7" tag_type="text">from objective measures of quality.</text>
<text top="655" left="322" width="158" height="9" font="font8" id="p5_t73" reading_order_no="72" segment_no="9" tag_type="text">2) Validation of our subjective study:</text>
<text top="655" left="486" width="77" height="9" font="font5" id="p5_t74" reading_order_no="73" segment_no="9" tag_type="text">We now study the</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p5_t75" reading_order_no="74" segment_no="9" tag_type="text">average MOS of the natural videos and predicted videos in</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p5_t76" reading_order_no="75" segment_no="9" tag_type="text">Table <a href="deeplearning_paper9.html#6">II. </a>We clearly see that the average MOS for natural</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p5_t77" reading_order_no="76" segment_no="9" tag_type="text">videos is higher than that of predicted videos. This shows that</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p5_t78" reading_order_no="77" segment_no="9" tag_type="text">the subjects are able to perceive distortions and rate natural</text>
<text top="715" left="312" width="107" height="9" font="font5" id="p5_t79" reading_order_no="78" segment_no="9" tag_type="text">videos with higher scores.</text>
<text top="727" left="322" width="241" height="9" font="font5" id="p5_t80" reading_order_no="79" segment_no="11" tag_type="text">In order to study the impact of upsampling low resolution</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p5_t81" reading_order_no="80" segment_no="11" tag_type="text">videos on the subjective scores, we compare the average</text>
</page>
<page number="6" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font9" size="8" family="CMSY8" color="#000000"/>
	<fontspec id="font10" size="10" family="CMR10" color="#000000"/>
	<fontspec id="font11" size="10" family="CMSY10" color="#000000"/>
	<fontspec id="font12" size="10" family="CMMI10" color="#000000"/>
	<fontspec id="font13" size="7" family="CMSY7" color="#000000"/>
	<fontspec id="font14" size="7" family="CMR7" color="#000000"/>
	<fontspec id="font15" size="7" family="CMMI7" color="#000000"/>
	<fontspec id="font16" size="10" family="CMBX10" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p6_t1" reading_order_no="0" segment_no="0" tag_type="text">6</text>
<text top="54" left="49" width="251" height="9" font="font5" id="p6_t2" reading_order_no="1" segment_no="1" tag_type="text">TABLE II: Average MOS for different subsets of videos.</text>
<text top="66" left="49" width="251" height="9" font="font5" id="p6_t3" reading_order_no="2" segment_no="1" tag_type="text">Standard deviation of the scores and the number of videos</text>
<text top="78" left="49" width="251" height="9" font="font5" id="p6_t4" reading_order_no="3" segment_no="1" tag_type="text">in both categories is also shown. Note that some videos have</text>
<text top="90" left="49" width="251" height="9" font="font5" id="p6_t5" reading_order_no="4" segment_no="1" tag_type="text">both blur and shape distortions and such videos are marked</text>
<text top="102" left="49" width="90" height="9" font="font5" id="p6_t6" reading_order_no="5" segment_no="1" tag_type="text">under both categories.</text>
<text top="124" left="86" width="56" height="7" font="font6" id="p6_t7" reading_order_no="6" segment_no="3" tag_type="table">Experiment Type</text>
<text top="124" left="179" width="46" height="7" font="font6" id="p6_t8" reading_order_no="7" segment_no="3" tag_type="table">No. of Videos</text>
<text top="124" left="238" width="46" height="7" font="font6" id="p6_t9" reading_order_no="8" segment_no="3" tag_type="table">Average MOS</text>
<text top="152" left="88" width="55" height="7" font="font6" id="p6_t10" reading_order_no="11" segment_no="3" tag_type="table">Predicted Videos</text>
<text top="146" left="112" width="7" height="7" font="font6" id="p6_t11" reading_order_no="10" segment_no="3" tag_type="table">vs</text>
<text top="139" left="91" width="49" height="7" font="font6" id="p6_t12" reading_order_no="9" segment_no="3" tag_type="table">Natural Videos</text>
<text top="139" left="198" width="8" height="7" font="font6" id="p6_t13" reading_order_no="12" segment_no="3" tag_type="table">30</text>
<text top="139" left="237" width="18" height="7" font="font6" id="p6_t14" reading_order_no="14" segment_no="3" tag_type="table">76.68</text>
<text top="139" left="258" width="7" height="9" font="font9" id="p6_t15" reading_order_no="15" segment_no="3" tag_type="table">±</text>
<text top="139" left="267" width="18" height="7" font="font6" id="p6_t16" reading_order_no="16" segment_no="3" tag_type="table">03.79</text>
<text top="151" left="196" width="12" height="7" font="font6" id="p6_t17" reading_order_no="13" segment_no="3" tag_type="table">270</text>
<text top="151" left="237" width="18" height="7" font="font6" id="p6_t18" reading_order_no="17" segment_no="3" tag_type="table">46.97</text>
<text top="150" left="258" width="7" height="9" font="font9" id="p6_t19" reading_order_no="18" segment_no="3" tag_type="table">±</text>
<text top="151" left="267" width="18" height="7" font="font6" id="p6_t20" reading_order_no="19" segment_no="3" tag_type="table">10.88</text>
<text top="177" left="64" width="102" height="7" font="font6" id="p6_t21" reading_order_no="22" segment_no="3" tag_type="table">Non-upsampled Natural Videos</text>
<text top="171" left="112" width="7" height="7" font="font6" id="p6_t22" reading_order_no="21" segment_no="3" tag_type="table">vs</text>
<text top="165" left="71" width="88" height="7" font="font6" id="p6_t23" reading_order_no="20" segment_no="3" tag_type="table">Upsampled Natural Videos</text>
<text top="165" left="198" width="8" height="7" font="font6" id="p6_t24" reading_order_no="23" segment_no="3" tag_type="table">16</text>
<text top="165" left="239" width="18" height="7" font="font6" id="p6_t25" reading_order_no="25" segment_no="3" tag_type="table">75.50</text>
<text top="164" left="260" width="7" height="9" font="font9" id="p6_t26" reading_order_no="26" segment_no="3" tag_type="table">±</text>
<text top="165" left="269" width="14" height="7" font="font6" id="p6_t27" reading_order_no="27" segment_no="3" tag_type="table">3.46</text>
<text top="176" left="198" width="8" height="7" font="font6" id="p6_t28" reading_order_no="24" segment_no="3" tag_type="table">14</text>
<text top="176" left="239" width="18" height="7" font="font6" id="p6_t29" reading_order_no="28" segment_no="3" tag_type="table">78.03</text>
<text top="175" left="260" width="7" height="9" font="font9" id="p6_t30" reading_order_no="29" segment_no="3" tag_type="table">±</text>
<text top="176" left="269" width="14" height="7" font="font6" id="p6_t31" reading_order_no="30" segment_no="3" tag_type="table">3.68</text>
<text top="202" left="88" width="55" height="7" font="font6" id="p6_t32" reading_order_no="33" segment_no="3" tag_type="table">Shape Distortion</text>
<text top="196" left="112" width="7" height="7" font="font6" id="p6_t33" reading_order_no="32" segment_no="3" tag_type="table">vs</text>
<text top="190" left="108" width="14" height="7" font="font6" id="p6_t34" reading_order_no="31" segment_no="3" tag_type="table">Blur</text>
<text top="190" left="196" width="12" height="7" font="font6" id="p6_t35" reading_order_no="34" segment_no="3" tag_type="table">163</text>
<text top="190" left="237" width="18" height="7" font="font6" id="p6_t36" reading_order_no="36" segment_no="3" tag_type="table">45.57</text>
<text top="189" left="258" width="7" height="9" font="font9" id="p6_t37" reading_order_no="37" segment_no="3" tag_type="table">±</text>
<text top="190" left="267" width="18" height="7" font="font6" id="p6_t38" reading_order_no="38" segment_no="3" tag_type="table">08.52</text>
<text top="201" left="196" width="12" height="7" font="font6" id="p6_t39" reading_order_no="35" segment_no="3" tag_type="table">200</text>
<text top="201" left="237" width="18" height="7" font="font6" id="p6_t40" reading_order_no="39" segment_no="3" tag_type="table">45.32</text>
<text top="200" left="258" width="7" height="9" font="font9" id="p6_t41" reading_order_no="40" segment_no="3" tag_type="table">±</text>
<text top="201" left="267" width="18" height="7" font="font6" id="p6_t42" reading_order_no="41" segment_no="3" tag_type="table">10.80</text>
<text top="227" left="76" width="79" height="7" font="font6" id="p6_t43" reading_order_no="44" segment_no="3" tag_type="table">Deterministic Prediction</text>
<text top="221" left="112" width="7" height="7" font="font6" id="p6_t44" reading_order_no="43" segment_no="3" tag_type="table">vs</text>
<text top="215" left="81" width="68" height="7" font="font6" id="p6_t45" reading_order_no="42" segment_no="3" tag_type="table">Stochastic Prediction</text>
<text top="215" left="198" width="8" height="7" font="font6" id="p6_t46" reading_order_no="45" segment_no="3" tag_type="table">73</text>
<text top="215" left="237" width="18" height="7" font="font6" id="p6_t47" reading_order_no="47" segment_no="3" tag_type="table">54.26</text>
<text top="214" left="258" width="7" height="9" font="font9" id="p6_t48" reading_order_no="48" segment_no="3" tag_type="table">±</text>
<text top="215" left="267" width="18" height="7" font="font6" id="p6_t49" reading_order_no="49" segment_no="3" tag_type="table">12.52</text>
<text top="226" left="196" width="12" height="7" font="font6" id="p6_t50" reading_order_no="46" segment_no="3" tag_type="table">197</text>
<text top="226" left="237" width="18" height="7" font="font6" id="p6_t51" reading_order_no="50" segment_no="3" tag_type="table">44.27</text>
<text top="225" left="258" width="7" height="9" font="font9" id="p6_t52" reading_order_no="51" segment_no="3" tag_type="table">±</text>
<text top="226" left="267" width="18" height="7" font="font6" id="p6_t53" reading_order_no="52" segment_no="3" tag_type="table">08.78</text>
<text top="261" left="49" width="223" height="9" font="font5" id="p6_t54" reading_order_no="53" segment_no="6" tag_type="text">MOS of upsampled (for lower resolutions such as</text>
<text top="260" left="278" width="10" height="10" font="font10" id="p6_t55" reading_order_no="54" segment_no="6" tag_type="text">64</text>
<text top="260" left="292" width="8" height="12" font="font11" id="p6_t56" reading_order_no="55" segment_no="6" tag_type="text">×</text>
<text top="272" left="49" width="10" height="10" font="font10" id="p6_t57" reading_order_no="56" segment_no="6" tag_type="text">64</text>
<text top="272" left="59" width="3" height="10" font="font12" id="p6_t58" reading_order_no="57" segment_no="6" tag_type="text">,</text>
<text top="272" left="63" width="15" height="10" font="font10" id="p6_t59" reading_order_no="58" segment_no="6" tag_type="text">128</text>
<text top="272" left="81" width="8" height="12" font="font11" id="p6_t60" reading_order_no="59" segment_no="6" tag_type="text">×</text>
<text top="272" left="92" width="15" height="10" font="font10" id="p6_t61" reading_order_no="60" segment_no="6" tag_type="text">128</text>
<text top="272" left="107" width="3" height="10" font="font12" id="p6_t62" reading_order_no="61" segment_no="6" tag_type="text">,</text>
<text top="272" left="112" width="15" height="10" font="font10" id="p6_t63" reading_order_no="62" segment_no="6" tag_type="text">160</text>
<text top="272" left="130" width="8" height="12" font="font11" id="p6_t64" reading_order_no="63" segment_no="6" tag_type="text">×</text>
<text top="272" left="140" width="15" height="10" font="font10" id="p6_t65" reading_order_no="64" segment_no="6" tag_type="text">120</text>
<text top="273" left="155" width="145" height="9" font="font5" id="p6_t66" reading_order_no="65" segment_no="6" tag_type="text">) and non-upsampled videos (with</text>
<text top="285" left="49" width="69" height="9" font="font5" id="p6_t67" reading_order_no="66" segment_no="6" tag_type="text">higher resolution</text>
<text top="284" left="121" width="15" height="10" font="font10" id="p6_t68" reading_order_no="67" segment_no="6" tag_type="text">320</text>
<text top="284" left="138" width="8" height="12" font="font11" id="p6_t69" reading_order_no="68" segment_no="6" tag_type="text">×</text>
<text top="284" left="147" width="15" height="10" font="font10" id="p6_t70" reading_order_no="69" segment_no="6" tag_type="text">240</text>
<text top="285" left="162" width="138" height="9" font="font5" id="p6_t71" reading_order_no="70" segment_no="6" tag_type="text">) in Table <a href="deeplearning_paper9.html#6">II. </a>We conduct this test</text>
<text top="297" left="49" width="251" height="9" font="font5" id="p6_t72" reading_order_no="71" segment_no="6" tag_type="text">on natural videos presented as test videos to avoid any bias due</text>
<text top="309" left="49" width="251" height="9" font="font5" id="p6_t73" reading_order_no="72" segment_no="6" tag_type="text">to the distortions contained in the predicted videos. We observe</text>
<text top="321" left="49" width="251" height="9" font="font5" id="p6_t74" reading_order_no="73" segment_no="6" tag_type="text">that the average MOS for the upsampled videos is comparable</text>
<text top="333" left="49" width="251" height="9" font="font5" id="p6_t75" reading_order_no="74" segment_no="6" tag_type="text">to that of the videos at their original higher resolutions. In</text>
<text top="344" left="49" width="251" height="9" font="font5" id="p6_t76" reading_order_no="75" segment_no="6" tag_type="text">order to verify the statistical indistinguishability of the MOS</text>
<text top="356" left="49" width="251" height="9" font="font5" id="p6_t77" reading_order_no="76" segment_no="6" tag_type="text">in each case, we also conduct t-test <a href="deeplearning_paper9.html#13">[53] </a>at 99% significance</text>
<text top="368" left="49" width="251" height="9" font="font5" id="p6_t78" reading_order_no="77" segment_no="6" tag_type="text">level. The null hypothesis is that the mean of the MOS values</text>
<text top="380" left="49" width="251" height="9" font="font5" id="p6_t79" reading_order_no="78" segment_no="6" tag_type="text">for both groups are equal, and the alternate hypothesis is that</text>
<text top="392" left="49" width="121" height="9" font="font5" id="p6_t80" reading_order_no="79" segment_no="6" tag_type="text">the means are different. The</text>
<text top="392" left="174" width="5" height="10" font="font12" id="p6_t81" reading_order_no="80" segment_no="6" tag_type="text">p</text>
<text top="392" left="179" width="121" height="9" font="font5" id="p6_t82" reading_order_no="81" segment_no="6" tag_type="text">-value of the t-test evaluates</text>
<text top="404" left="49" width="8" height="9" font="font5" id="p6_t83" reading_order_no="82" segment_no="6" tag_type="text">to</text>
<text top="404" left="62" width="5" height="10" font="font10" id="p6_t84" reading_order_no="83" segment_no="6" tag_type="text">0</text>
<text top="404" left="67" width="3" height="10" font="font12" id="p6_t85" reading_order_no="84" segment_no="6" tag_type="text">.</text>
<text top="404" left="70" width="14" height="10" font="font10" id="p6_t86" reading_order_no="85" segment_no="6" tag_type="text">07(</text>
<text top="404" left="83" width="8" height="10" font="font12" id="p6_t87" reading_order_no="86" segment_no="6" tag_type="text">&gt;</text>
<text top="404" left="97" width="5" height="10" font="font10" id="p6_t88" reading_order_no="87" segment_no="6" tag_type="text">0</text>
<text top="404" left="102" width="3" height="10" font="font12" id="p6_t89" reading_order_no="88" segment_no="6" tag_type="text">.</text>
<text top="404" left="105" width="14" height="10" font="font10" id="p6_t90" reading_order_no="89" segment_no="6" tag_type="text">01)</text>
<text top="404" left="118" width="182" height="9" font="font5" id="p6_t91" reading_order_no="90" segment_no="6" tag_type="text">, and hence the null hypothesis cannot be</text>
<text top="416" left="49" width="251" height="9" font="font5" id="p6_t92" reading_order_no="91" segment_no="6" tag_type="text">rejected. Thus we conclude that the upsampled videos do not</text>
<text top="428" left="49" width="201" height="9" font="font5" id="p6_t93" reading_order_no="92" segment_no="6" tag_type="text">suffer from any biases in their subjective ratings.</text>
<text top="440" left="59" width="206" height="9" font="font8" id="p6_t94" reading_order_no="93" segment_no="7" tag_type="text">3) How does MOS vary for different distortions?:</text>
<text top="440" left="270" width="30" height="9" font="font5" id="p6_t95" reading_order_no="94" segment_no="7" tag_type="text">We ob-</text>
<text top="452" left="49" width="251" height="9" font="font5" id="p6_t96" reading_order_no="95" segment_no="7" tag_type="text">serve that shape distortions and blur are the two predominant</text>
<text top="464" left="49" width="251" height="9" font="font5" id="p6_t97" reading_order_no="96" segment_no="7" tag_type="text">classes of distortions in the predicted videos. We roughly</text>
<text top="476" left="49" width="251" height="9" font="font5" id="p6_t98" reading_order_no="97" segment_no="7" tag_type="text">classify the videos into those that contain shape distortions</text>
<text top="488" left="49" width="251" height="9" font="font5" id="p6_t99" reading_order_no="98" segment_no="7" tag_type="text">and those that contain blur. Some videos have both distortions,</text>
<text top="500" left="49" width="251" height="9" font="font5" id="p6_t100" reading_order_no="99" segment_no="7" tag_type="text">in which case they are marked under both categories. The</text>
<text top="512" left="49" width="251" height="9" font="font5" id="p6_t101" reading_order_no="100" segment_no="7" tag_type="text">resulting MOS for the two classes of videos is shown in</text>
<text top="524" left="49" width="251" height="9" font="font5" id="p6_t102" reading_order_no="101" segment_no="7" tag_type="text">Table <a href="deeplearning_paper9.html#6">II. </a>We find that the average MOS for videos with blur</text>
<text top="536" left="49" width="251" height="9" font="font5" id="p6_t103" reading_order_no="102" segment_no="7" tag_type="text">is roughly equal to the average MOS for videos with shape</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p6_t104" reading_order_no="103" segment_no="7" tag_type="text">distortions. Among other distortions such as disappearance and</text>
<text top="560" left="49" width="251" height="9" font="font5" id="p6_t105" reading_order_no="104" segment_no="7" tag_type="text">color changes of objects, our database has 30 videos with</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p6_t106" reading_order_no="105" segment_no="7" tag_type="text">such distortions and their average MOS is 47.41. Thus, these</text>
<text top="584" left="49" width="251" height="9" font="font5" id="p6_t107" reading_order_no="106" segment_no="7" tag_type="text">distortions appear to be as annoying as the blur and shape</text>
<text top="596" left="49" width="115" height="9" font="font5" id="p6_t108" reading_order_no="107" segment_no="7" tag_type="text">distortions discussed earlier.</text>
<text top="608" left="59" width="241" height="9" font="font8" id="p6_t109" reading_order_no="108" segment_no="11" tag_type="text">4) Do stochastic models perform better than deterministic</text>
<text top="620" left="49" width="37" height="9" font="font8" id="p6_t110" reading_order_no="109" segment_no="11" tag_type="text">models?:</text>
<text top="620" left="90" width="210" height="9" font="font5" id="p6_t111" reading_order_no="110" segment_no="11" tag_type="text">As we pointed out earlier, deterministic methods <a href="deeplearning_paper9.html#12">[7],</a></text>
<text top="632" left="49" width="251" height="9" font="font5" id="p6_t112" reading_order_no="111" segment_no="11" tag_type="text"><a href="deeplearning_paper9.html#12">[16] </a>pick only one of the multiple plausible trajectories. On the</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p6_t113" reading_order_no="112" segment_no="11" tag_type="text">other hand, stochastic approaches train the model to predict</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p6_t114" reading_order_no="113" segment_no="11" tag_type="text">multiple future trajectories <a href="deeplearning_paper9.html#12">[9], </a><a href="deeplearning_paper9.html#13">[51], </a><a href="deeplearning_paper9.html#12">[19]. </a>Table <a href="deeplearning_paper9.html#6">II </a>shows the</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p6_t115" reading_order_no="114" segment_no="11" tag_type="text">average MOS and standard deviation with respect to the two</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p6_t116" reading_order_no="115" segment_no="11" tag_type="text">methods described above. We see that the average MOS is</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p6_t117" reading_order_no="116" segment_no="11" tag_type="text">lower for deterministic methods when compared to stochastic</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p6_t118" reading_order_no="117" segment_no="11" tag_type="text">models. We also verify the statistical significance of this</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p6_t119" reading_order_no="118" segment_no="11" tag_type="text">observation using t-test <a href="deeplearning_paper9.html#13">[53] </a>at 99% significance level. The</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p6_t120" reading_order_no="119" segment_no="11" tag_type="text">null hypothesis is that the mean MOS scores of the two groups</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p6_t121" reading_order_no="120" segment_no="11" tag_type="text">are equal, and the alternate hypothesis is that the mean MOS</text>
<text top="58" left="312" width="251" height="9" font="font5" id="p6_t122" reading_order_no="121" segment_no="2" tag_type="text">score of stochastically predicted videos is higher than that of</text>
<text top="70" left="312" width="162" height="9" font="font5" id="p6_t123" reading_order_no="122" segment_no="2" tag_type="text">deterministically predicted videos. The</text>
<text top="69" left="479" width="5" height="10" font="font12" id="p6_t124" reading_order_no="123" segment_no="2" tag_type="text">p</text>
<text top="70" left="484" width="79" height="9" font="font5" id="p6_t125" reading_order_no="124" segment_no="2" tag_type="text">-value of the t-test</text>
<text top="82" left="312" width="48" height="9" font="font5" id="p6_t126" reading_order_no="125" segment_no="2" tag_type="text">evaluates to</text>
<text top="81" left="363" width="5" height="10" font="font10" id="p6_t127" reading_order_no="126" segment_no="2" tag_type="text">6</text>
<text top="81" left="371" width="8" height="12" font="font11" id="p6_t128" reading_order_no="127" segment_no="2" tag_type="text">×</text>
<text top="81" left="381" width="10" height="10" font="font10" id="p6_t129" reading_order_no="128" segment_no="2" tag_type="text">10</text>
<text top="80" left="391" width="6" height="7" font="font13" id="p6_t130" reading_order_no="129" segment_no="2" tag_type="text">−</text>
<text top="80" left="397" width="4" height="6" font="font14" id="p6_t131" reading_order_no="130" segment_no="2" tag_type="text">9</text>
<text top="81" left="401" width="4" height="10" font="font10" id="p6_t132" reading_order_no="131" segment_no="2" tag_type="text">(</text>
<text top="81" left="405" width="8" height="10" font="font12" id="p6_t133" reading_order_no="132" segment_no="2" tag_type="text">&lt;</text>
<text top="81" left="416" width="5" height="10" font="font10" id="p6_t134" reading_order_no="133" segment_no="2" tag_type="text">0</text>
<text top="81" left="421" width="3" height="10" font="font12" id="p6_t135" reading_order_no="134" segment_no="2" tag_type="text">.</text>
<text top="81" left="424" width="14" height="10" font="font10" id="p6_t136" reading_order_no="135" segment_no="2" tag_type="text">01)</text>
<text top="82" left="441" width="122" height="9" font="font5" id="p6_t137" reading_order_no="136" segment_no="2" tag_type="text">and hence the null hypothesis</text>
<text top="94" left="312" width="251" height="9" font="font5" id="p6_t138" reading_order_no="137" segment_no="2" tag_type="text">can be rejected. Thus, we can conclude that the ability of</text>
<text top="106" left="312" width="251" height="9" font="font5" id="p6_t139" reading_order_no="138" segment_no="2" tag_type="text">stochastic models to better capture the uncertainty in the future</text>
<text top="117" left="312" width="251" height="9" font="font5" id="p6_t140" reading_order_no="139" segment_no="2" tag_type="text">trajectories, allows them to generate videos of higher quality.</text>
<text top="141" left="333" width="24" height="9" font="font5" id="p6_t141" reading_order_no="140" segment_no="4" tag_type="title">IV. F</text>
<text top="142" left="357" width="33" height="7" font="font6" id="p6_t142" reading_order_no="141" segment_no="4" tag_type="title">EATURE</text>
<text top="141" left="393" width="6" height="9" font="font5" id="p6_t143" reading_order_no="142" segment_no="4" tag_type="title">E</text>
<text top="142" left="400" width="69" height="7" font="font6" id="p6_t144" reading_order_no="143" segment_no="4" tag_type="title">XTRACTION FOR</text>
<text top="141" left="472" width="7" height="9" font="font5" id="p6_t145" reading_order_no="144" segment_no="4" tag_type="title">V</text>
<text top="142" left="479" width="21" height="7" font="font6" id="p6_t146" reading_order_no="145" segment_no="4" tag_type="title">IDEO</text>
<text top="141" left="503" width="7" height="9" font="font5" id="p6_t147" reading_order_no="146" segment_no="4" tag_type="title">Q</text>
<text top="142" left="510" width="32" height="7" font="font6" id="p6_t148" reading_order_no="147" segment_no="4" tag_type="title">UALITY</text>
<text top="153" left="409" width="7" height="9" font="font5" id="p6_t149" reading_order_no="148" segment_no="4" tag_type="title">A</text>
<text top="154" left="417" width="49" height="7" font="font6" id="p6_t150" reading_order_no="149" segment_no="4" tag_type="title">SSESSMENT</text>
<text top="168" left="322" width="241" height="9" font="font5" id="p6_t151" reading_order_no="150" segment_no="5" tag_type="text">We now present two sets of features that are particularly</text>
<text top="180" left="312" width="251" height="9" font="font5" id="p6_t152" reading_order_no="151" segment_no="5" tag_type="text">relevant in reliably predicting the quality of predicted videos.</text>
<text top="192" left="312" width="251" height="9" font="font5" id="p6_t153" reading_order_no="152" segment_no="5" tag_type="text">The first set of features is motivated by the observation that</text>
<text top="204" left="312" width="251" height="9" font="font5" id="p6_t154" reading_order_no="153" segment_no="5" tag_type="text">objects in a scene are well represented in the past frames</text>
<text top="216" left="312" width="251" height="9" font="font5" id="p6_t155" reading_order_no="154" segment_no="5" tag_type="text">and can be used to measure how representations evolve in</text>
<text top="228" left="312" width="251" height="9" font="font5" id="p6_t156" reading_order_no="155" segment_no="5" tag_type="text">future predicted frames. Thus we exploit the rich information</text>
<text top="240" left="312" width="251" height="9" font="font5" id="p6_t157" reading_order_no="156" segment_no="5" tag_type="text">available in the deep features of objects in the past frames</text>
<text top="251" left="312" width="251" height="9" font="font5" id="p6_t158" reading_order_no="157" segment_no="5" tag_type="text">and make motion-compensated comparisons of deep features</text>
<text top="263" left="312" width="251" height="9" font="font5" id="p6_t159" reading_order_no="158" segment_no="5" tag_type="text">in predicted frames. We capture this idea through motion-</text>
<text top="275" left="312" width="251" height="9" font="font5" id="p6_t160" reading_order_no="159" segment_no="5" tag_type="text">compensated cosine similarity based features. This feature</text>
<text top="287" left="312" width="251" height="9" font="font5" id="p6_t161" reading_order_no="160" segment_no="5" tag_type="text">also helps identify the disappearance or vanishing of objects</text>
<text top="299" left="312" width="251" height="9" font="font5" id="p6_t162" reading_order_no="161" segment_no="5" tag_type="text">from the middle of a scene. Secondly, we observe that most</text>
<text top="311" left="312" width="251" height="9" font="font5" id="p6_t163" reading_order_no="162" segment_no="5" tag_type="text">of the abnormalities in predicted videos occur in regions of</text>
<text top="323" left="312" width="251" height="9" font="font5" id="p6_t164" reading_order_no="163" segment_no="5" tag_type="text">motion. In order to capture variations in representations in</text>
<text top="335" left="312" width="251" height="9" font="font5" id="p6_t165" reading_order_no="164" segment_no="5" tag_type="text">moving regions and also more carefully measure distortions</text>
<text top="347" left="312" width="251" height="9" font="font5" id="p6_t166" reading_order_no="165" segment_no="5" tag_type="text">in object shapes, we introduce the notion of rescaled frame</text>
<text top="359" left="312" width="251" height="9" font="font5" id="p6_t167" reading_order_no="166" segment_no="5" tag_type="text">differences and compute deep features from such images.</text>
<text top="371" left="312" width="251" height="9" font="font5" id="p6_t168" reading_order_no="167" segment_no="5" tag_type="text">We note that deep features based on motion compensation</text>
<text top="383" left="312" width="251" height="9" font="font5" id="p6_t169" reading_order_no="168" segment_no="5" tag_type="text">and frame differences contain information about higher order</text>
<text top="395" left="312" width="251" height="9" font="font5" id="p6_t170" reading_order_no="169" segment_no="5" tag_type="text">concepts such as shape. However, they also contain low level</text>
<text top="407" left="312" width="251" height="9" font="font5" id="p6_t171" reading_order_no="170" segment_no="5" tag_type="text">vision information related to blur and sharpness as shown</text>
<text top="419" left="312" width="251" height="9" font="font5" id="p6_t172" reading_order_no="171" segment_no="5" tag_type="text">through their application in picture quality assessment <a href="deeplearning_paper9.html#13">[54].</a></text>
<text top="431" left="312" width="251" height="9" font="font5" id="p6_t173" reading_order_no="172" segment_no="5" tag_type="text">We provide further details of both features in the following</text>
<text top="443" left="312" width="48" height="9" font="font5" id="p6_t174" reading_order_no="173" segment_no="5" tag_type="text">subsections.</text>
<text top="469" left="312" width="237" height="9" font="font8" id="p6_t175" reading_order_no="174" segment_no="8" tag_type="title">A. Motion-compensated Cosine Similarity (MCS) features</text>
<text top="483" left="322" width="241" height="9" font="font5" id="p6_t176" reading_order_no="175" segment_no="9" tag_type="text">We illustrate the computation of MCS features in Fig. <a href="deeplearning_paper9.html#7">5b</a>.</text>
<text top="495" left="312" width="251" height="9" font="font5" id="p6_t177" reading_order_no="176" segment_no="9" tag_type="text">We experiment with different networks to obtain deep features</text>
<text top="507" left="312" width="251" height="9" font="font5" id="p6_t178" reading_order_no="177" segment_no="9" tag_type="text">such as VGG-19, ResNet-50, and Inception-v3 and refer to one</text>
<text top="519" left="312" width="143" height="9" font="font5" id="p6_t179" reading_order_no="178" segment_no="9" tag_type="text">such network in the following. Let</text>
<text top="519" left="458" width="8" height="10" font="font12" id="p6_t180" reading_order_no="179" segment_no="9" tag_type="text">N</text>
<text top="519" left="471" width="92" height="9" font="font5" id="p6_t181" reading_order_no="180" segment_no="9" tag_type="text">be the total number of</text>
<text top="531" left="312" width="30" height="9" font="font5" id="p6_t182" reading_order_no="181" segment_no="9" tag_type="text">frames,</text>
<text top="531" left="345" width="8" height="10" font="font12" id="p6_t183" reading_order_no="182" segment_no="9" tag_type="text">N</text>
<text top="535" left="353" width="4" height="6" font="font15" id="p6_t184" reading_order_no="183" segment_no="9" tag_type="text">c</text>
<text top="531" left="361" width="156" height="9" font="font5" id="p6_t185" reading_order_no="184" segment_no="9" tag_type="text">be the number of context frames, and</text>
<text top="531" left="521" width="8" height="10" font="font12" id="p6_t186" reading_order_no="185" segment_no="9" tag_type="text">N</text>
<text top="535" left="529" width="4" height="6" font="font15" id="p6_t187" reading_order_no="186" segment_no="9" tag_type="text">p</text>
<text top="531" left="538" width="25" height="9" font="font5" id="p6_t188" reading_order_no="187" segment_no="9" tag_type="text">be the</text>
<text top="543" left="312" width="141" height="9" font="font5" id="p6_t189" reading_order_no="188" segment_no="9" tag_type="text">number of predicted frames. Thus</text>
<text top="542" left="456" width="8" height="10" font="font12" id="p6_t190" reading_order_no="189" segment_no="9" tag_type="text">N</text>
<text top="542" left="469" width="8" height="10" font="font10" id="p6_t191" reading_order_no="190" segment_no="9" tag_type="text">=</text>
<text top="542" left="480" width="8" height="10" font="font12" id="p6_t192" reading_order_no="191" segment_no="9" tag_type="text">N</text>
<text top="547" left="488" width="4" height="6" font="font15" id="p6_t193" reading_order_no="192" segment_no="9" tag_type="text">c</text>
<text top="542" left="494" width="8" height="10" font="font10" id="p6_t194" reading_order_no="193" segment_no="9" tag_type="text">+</text>
<text top="542" left="505" width="8" height="10" font="font12" id="p6_t195" reading_order_no="194" segment_no="9" tag_type="text">N</text>
<text top="547" left="513" width="4" height="6" font="font15" id="p6_t196" reading_order_no="195" segment_no="9" tag_type="text">p</text>
<text top="543" left="517" width="20" height="9" font="font5" id="p6_t197" reading_order_no="196" segment_no="9" tag_type="text">. Let</text>
<text top="542" left="541" width="8" height="10" font="font12" id="p6_t198" reading_order_no="197" segment_no="9" tag_type="text">K</text>
<text top="543" left="554" width="9" height="9" font="font5" id="p6_t199" reading_order_no="198" segment_no="9" tag_type="text">be</text>
<text top="555" left="312" width="251" height="9" font="font5" id="p6_t200" reading_order_no="199" segment_no="9" tag_type="text">the number of channels in the pre-trained model, at the layer</text>
<text top="567" left="312" width="131" height="9" font="font5" id="p6_t201" reading_order_no="200" segment_no="9" tag_type="text">where we tap the features. Let</text>
<text top="566" left="447" width="6" height="10" font="font12" id="p6_t202" reading_order_no="201" segment_no="9" tag_type="text">h</text>
<text top="567" left="457" width="14" height="9" font="font5" id="p6_t203" reading_order_no="202" segment_no="9" tag_type="text">and</text>
<text top="566" left="476" width="7" height="10" font="font12" id="p6_t204" reading_order_no="203" segment_no="9" tag_type="text">w</text>
<text top="567" left="488" width="75" height="9" font="font5" id="p6_t205" reading_order_no="204" segment_no="9" tag_type="text">be the height and</text>
<text top="579" left="312" width="165" height="9" font="font5" id="p6_t206" reading_order_no="205" segment_no="9" tag_type="text">width of the corresponding feature map.</text>
<text top="591" left="322" width="13" height="9" font="font5" id="p6_t207" reading_order_no="206" segment_no="10" tag_type="text">Let</text>
<text top="590" left="340" width="5" height="10" font="font12" id="p6_t208" reading_order_no="207" segment_no="10" tag_type="text">f</text>
<text top="590" left="346" width="4" height="10" font="font10" id="p6_t209" reading_order_no="208" segment_no="10" tag_type="text">(</text>
<text top="590" left="350" width="32" height="10" font="font12" id="p6_t210" reading_order_no="209" segment_no="10" tag_type="text">i, j, k, n</text>
<text top="590" left="382" width="4" height="10" font="font10" id="p6_t211" reading_order_no="210" segment_no="10" tag_type="text">)</text>
<text top="591" left="390" width="148" height="9" font="font5" id="p6_t212" reading_order_no="211" segment_no="10" tag_type="text">denote the deep feature at location</text>
<text top="590" left="543" width="4" height="10" font="font10" id="p6_t213" reading_order_no="212" segment_no="10" tag_type="text">(</text>
<text top="590" left="547" width="12" height="10" font="font12" id="p6_t214" reading_order_no="213" segment_no="10" tag_type="text">i, j</text>
<text top="590" left="559" width="4" height="10" font="font10" id="p6_t215" reading_order_no="214" segment_no="10" tag_type="text">)</text>
<text top="603" left="312" width="46" height="9" font="font5" id="p6_t216" reading_order_no="215" segment_no="10" tag_type="text">in Channel</text>
<text top="602" left="364" width="5" height="10" font="font12" id="p6_t217" reading_order_no="216" segment_no="10" tag_type="text">k</text>
<text top="603" left="375" width="39" height="9" font="font5" id="p6_t218" reading_order_no="217" segment_no="10" tag_type="text">in Frame</text>
<text top="602" left="419" width="6" height="10" font="font12" id="p6_t219" reading_order_no="218" segment_no="10" tag_type="text">n</text>
<text top="603" left="425" width="32" height="9" font="font5" id="p6_t220" reading_order_no="219" segment_no="10" tag_type="text">, where</text>
<text top="602" left="462" width="3" height="10" font="font12" id="p6_t221" reading_order_no="220" segment_no="10" tag_type="text">i</text>
<text top="602" left="472" width="18" height="12" font="font11" id="p6_t222" reading_order_no="221" segment_no="10" tag_type="text">∈ {</text>
<text top="602" left="490" width="5" height="10" font="font10" id="p6_t223" reading_order_no="222" segment_no="10" tag_type="text">1</text>
<text top="602" left="495" width="3" height="10" font="font12" id="p6_t224" reading_order_no="223" segment_no="10" tag_type="text">,</text>
<text top="602" left="500" width="5" height="10" font="font10" id="p6_t225" reading_order_no="224" segment_no="10" tag_type="text">2</text>
<text top="602" left="505" width="28" height="10" font="font12" id="p6_t226" reading_order_no="225" segment_no="10" tag_type="text">, . . . , h</text>
<text top="602" left="532" width="5" height="12" font="font11" id="p6_t227" reading_order_no="226" segment_no="10" tag_type="text">}</text>
<text top="603" left="537" width="2" height="9" font="font5" id="p6_t228" reading_order_no="227" segment_no="10" tag_type="text">,</text>
<text top="602" left="545" width="4" height="10" font="font12" id="p6_t229" reading_order_no="228" segment_no="10" tag_type="text">j</text>
<text top="602" left="556" width="7" height="12" font="font11" id="p6_t230" reading_order_no="229" segment_no="10" tag_type="text">∈</text>
<text top="614" left="312" width="5" height="12" font="font11" id="p6_t231" reading_order_no="230" segment_no="10" tag_type="text">{</text>
<text top="614" left="317" width="5" height="10" font="font10" id="p6_t232" reading_order_no="231" segment_no="10" tag_type="text">1</text>
<text top="614" left="322" width="3" height="10" font="font12" id="p6_t233" reading_order_no="232" segment_no="10" tag_type="text">,</text>
<text top="614" left="326" width="5" height="10" font="font10" id="p6_t234" reading_order_no="233" segment_no="10" tag_type="text">2</text>
<text top="614" left="331" width="29" height="10" font="font12" id="p6_t235" reading_order_no="234" segment_no="10" tag_type="text">, . . . , w</text>
<text top="614" left="361" width="5" height="12" font="font11" id="p6_t236" reading_order_no="235" segment_no="10" tag_type="text">}</text>
<text top="615" left="366" width="2" height="9" font="font5" id="p6_t237" reading_order_no="236" segment_no="10" tag_type="text">,</text>
<text top="614" left="372" width="5" height="10" font="font12" id="p6_t238" reading_order_no="237" segment_no="10" tag_type="text">k</text>
<text top="614" left="380" width="15" height="12" font="font11" id="p6_t239" reading_order_no="238" segment_no="10" tag_type="text">∈ {</text>
<text top="614" left="395" width="5" height="10" font="font10" id="p6_t240" reading_order_no="239" segment_no="10" tag_type="text">1</text>
<text top="614" left="400" width="3" height="10" font="font12" id="p6_t241" reading_order_no="240" segment_no="10" tag_type="text">,</text>
<text top="614" left="404" width="5" height="10" font="font10" id="p6_t242" reading_order_no="241" segment_no="10" tag_type="text">2</text>
<text top="614" left="409" width="31" height="10" font="font12" id="p6_t243" reading_order_no="242" segment_no="10" tag_type="text">, . . . , K</text>
<text top="614" left="441" width="5" height="12" font="font11" id="p6_t244" reading_order_no="243" segment_no="10" tag_type="text">}</text>
<text top="615" left="449" width="14" height="9" font="font5" id="p6_t245" reading_order_no="244" segment_no="10" tag_type="text">and</text>
<text top="614" left="467" width="6" height="10" font="font12" id="p6_t246" reading_order_no="245" segment_no="10" tag_type="text">n</text>
<text top="614" left="476" width="15" height="12" font="font11" id="p6_t247" reading_order_no="246" segment_no="10" tag_type="text">∈ {</text>
<text top="614" left="491" width="5" height="10" font="font10" id="p6_t248" reading_order_no="247" segment_no="10" tag_type="text">1</text>
<text top="614" left="496" width="3" height="10" font="font12" id="p6_t249" reading_order_no="248" segment_no="10" tag_type="text">,</text>
<text top="614" left="500" width="5" height="10" font="font10" id="p6_t250" reading_order_no="249" segment_no="10" tag_type="text">2</text>
<text top="614" left="505" width="30" height="10" font="font12" id="p6_t251" reading_order_no="250" segment_no="10" tag_type="text">, . . . , N</text>
<text top="614" left="536" width="5" height="12" font="font11" id="p6_t252" reading_order_no="251" segment_no="10" tag_type="text">}</text>
<text top="615" left="541" width="22" height="9" font="font5" id="p6_t253" reading_order_no="252" segment_no="10" tag_type="text">. The</text>
<text top="627" left="312" width="154" height="9" font="font5" id="p6_t254" reading_order_no="253" segment_no="10" tag_type="text">cosine similarity between two vectors</text>
<text top="627" left="470" width="6" height="9" font="font16" id="p6_t255" reading_order_no="254" segment_no="10" tag_type="text">p</text>
<text top="627" left="480" width="14" height="9" font="font5" id="p6_t256" reading_order_no="255" segment_no="10" tag_type="text">and</text>
<text top="627" left="497" width="6" height="9" font="font16" id="p6_t257" reading_order_no="256" segment_no="10" tag_type="text">q</text>
<text top="627" left="507" width="54" height="9" font="font5" id="p6_t258" reading_order_no="257" segment_no="10" tag_type="text">be defined as</text>
<text top="649" left="397" width="5" height="10" font="font12" id="p6_t259" reading_order_no="258" segment_no="12" tag_type="formula">s</text>
<text top="649" left="402" width="4" height="10" font="font10" id="p6_t260" reading_order_no="259" segment_no="12" tag_type="formula">(</text>
<text top="650" left="406" width="6" height="9" font="font16" id="p6_t261" reading_order_no="260" segment_no="12" tag_type="formula">p</text>
<text top="649" left="412" width="3" height="10" font="font12" id="p6_t262" reading_order_no="261" segment_no="12" tag_type="formula">,</text>
<text top="650" left="417" width="6" height="9" font="font16" id="p6_t263" reading_order_no="262" segment_no="12" tag_type="formula">q</text>
<text top="649" left="423" width="14" height="10" font="font10" id="p6_t264" reading_order_no="263" segment_no="12" tag_type="formula">) =</text>
<text top="643" left="448" width="6" height="9" font="font16" id="p6_t265" reading_order_no="264" segment_no="12" tag_type="formula">p</text>
<text top="642" left="454" width="5" height="6" font="font15" id="p6_t266" reading_order_no="265" segment_no="12" tag_type="formula">T</text>
<text top="643" left="461" width="6" height="9" font="font16" id="p6_t267" reading_order_no="266" segment_no="12" tag_type="formula">q</text>
<text top="656" left="441" width="5" height="12" font="font11" id="p6_t268" reading_order_no="267" segment_no="12" tag_type="formula">k</text>
<text top="657" left="446" width="6" height="9" font="font16" id="p6_t269" reading_order_no="268" segment_no="12" tag_type="formula">p</text>
<text top="656" left="453" width="10" height="12" font="font11" id="p6_t270" reading_order_no="269" segment_no="12" tag_type="formula">kk</text>
<text top="657" left="463" width="6" height="9" font="font16" id="p6_t271" reading_order_no="270" segment_no="12" tag_type="formula">q</text>
<text top="656" left="469" width="5" height="12" font="font11" id="p6_t272" reading_order_no="271" segment_no="12" tag_type="formula">k</text>
<text top="649" left="475" width="3" height="10" font="font12" id="p6_t273" reading_order_no="272" segment_no="12" tag_type="formula">.</text>
<text top="673" left="312" width="24" height="9" font="font5" id="p6_t274" reading_order_no="273" segment_no="13" tag_type="text">where</text>
<text top="672" left="339" width="13" height="12" font="font11" id="p6_t275" reading_order_no="274" segment_no="13" tag_type="text">k·k</text>
<text top="673" left="355" width="161" height="9" font="font5" id="p6_t276" reading_order_no="275" segment_no="13" tag_type="text">denotes the two-norm of the vector. Let</text>
<text top="672" left="520" width="5" height="10" font="font12" id="p6_t277" reading_order_no="276" segment_no="13" tag_type="text">f</text>
<text top="672" left="526" width="4" height="10" font="font10" id="p6_t278" reading_order_no="277" segment_no="13" tag_type="text">(</text>
<text top="672" left="530" width="15" height="10" font="font12" id="p6_t279" reading_order_no="278" segment_no="13" tag_type="text">i, j,</text>
<text top="672" left="546" width="3" height="12" font="font11" id="p6_t280" reading_order_no="279" segment_no="13" tag_type="text">·</text>
<text top="672" left="549" width="10" height="10" font="font12" id="p6_t281" reading_order_no="280" segment_no="13" tag_type="text">, n</text>
<text top="672" left="559" width="4" height="10" font="font10" id="p6_t282" reading_order_no="281" segment_no="13" tag_type="text">)</text>
<text top="685" left="312" width="251" height="9" font="font5" id="p6_t283" reading_order_no="282" segment_no="13" tag_type="text">denote a vector of deep features across channels at location</text>
<text top="696" left="312" width="4" height="10" font="font10" id="p6_t284" reading_order_no="283" segment_no="13" tag_type="text">(</text>
<text top="696" left="316" width="12" height="10" font="font12" id="p6_t285" reading_order_no="284" segment_no="13" tag_type="text">i, j</text>
<text top="696" left="328" width="4" height="10" font="font10" id="p6_t286" reading_order_no="285" segment_no="13" tag_type="text">)</text>
<text top="697" left="336" width="37" height="9" font="font5" id="p6_t287" reading_order_no="286" segment_no="13" tag_type="text">in Frame</text>
<text top="696" left="376" width="6" height="10" font="font12" id="p6_t288" reading_order_no="287" segment_no="13" tag_type="text">n</text>
<text top="697" left="382" width="85" height="9" font="font5" id="p6_t289" reading_order_no="288" segment_no="13" tag_type="text">. For a given feature</text>
<text top="696" left="470" width="5" height="10" font="font12" id="p6_t290" reading_order_no="289" segment_no="13" tag_type="text">f</text>
<text top="696" left="476" width="4" height="10" font="font10" id="p6_t291" reading_order_no="290" segment_no="13" tag_type="text">(</text>
<text top="696" left="480" width="34" height="10" font="font12" id="p6_t292" reading_order_no="291" segment_no="13" tag_type="text">i, j, k, N</text>
<text top="700" left="515" width="4" height="6" font="font15" id="p6_t293" reading_order_no="292" segment_no="13" tag_type="text">c</text>
<text top="696" left="519" width="4" height="10" font="font10" id="p6_t294" reading_order_no="293" segment_no="13" tag_type="text">)</text>
<text top="697" left="526" width="37" height="9" font="font5" id="p6_t295" reading_order_no="294" segment_no="13" tag_type="text">in Frame</text>
<text top="708" left="312" width="8" height="10" font="font12" id="p6_t296" reading_order_no="295" segment_no="13" tag_type="text">N</text>
<text top="712" left="320" width="4" height="6" font="font15" id="p6_t297" reading_order_no="296" segment_no="13" tag_type="text">c</text>
<text top="709" left="329" width="235" height="9" font="font5" id="p6_t298" reading_order_no="297" segment_no="13" tag_type="text">(last context frame), the corresponding motion compen-</text>
<text top="721" left="312" width="92" height="9" font="font5" id="p6_t299" reading_order_no="298" segment_no="13" tag_type="text">sated feature in Frame</text>
<text top="720" left="407" width="6" height="10" font="font12" id="p6_t300" reading_order_no="299" segment_no="13" tag_type="text">n</text>
<text top="721" left="417" width="18" height="9" font="font5" id="p6_t301" reading_order_no="300" segment_no="13" tag_type="text">with</text>
<text top="720" left="438" width="27" height="10" font="font12" id="p6_t302" reading_order_no="301" segment_no="13" tag_type="text">n &gt; N</text>
<text top="724" left="465" width="4" height="6" font="font15" id="p6_t303" reading_order_no="302" segment_no="13" tag_type="text">c</text>
<text top="721" left="473" width="56" height="9" font="font5" id="p6_t304" reading_order_no="303" segment_no="13" tag_type="text">is obtained as</text>
<text top="738" left="377" width="5" height="10" font="font12" id="p6_t305" reading_order_no="304" segment_no="14" tag_type="formula">f</text>
<text top="743" left="382" width="7" height="6" font="font15" id="p6_t306" reading_order_no="305" segment_no="14" tag_type="formula">m</text>
<text top="738" left="390" width="4" height="10" font="font10" id="p6_t307" reading_order_no="306" segment_no="14" tag_type="formula">(</text>
<text top="738" left="393" width="32" height="10" font="font12" id="p6_t308" reading_order_no="307" segment_no="14" tag_type="formula">i, j, k, n</text>
<text top="738" left="426" width="14" height="10" font="font10" id="p6_t309" reading_order_no="308" segment_no="14" tag_type="formula">) =</text>
<text top="738" left="443" width="5" height="10" font="font12" id="p6_t310" reading_order_no="309" segment_no="14" tag_type="formula">f</text>
<text top="738" left="449" width="4" height="10" font="font10" id="p6_t311" reading_order_no="310" segment_no="14" tag_type="formula">(</text>
<text top="738" left="453" width="3" height="10" font="font12" id="p6_t312" reading_order_no="311" segment_no="14" tag_type="formula">i</text>
<text top="737" left="456" width="2" height="7" font="font13" id="p6_t313" reading_order_no="312" segment_no="14" tag_type="formula">0</text>
<text top="738" left="459" width="9" height="10" font="font12" id="p6_t314" reading_order_no="313" segment_no="14" tag_type="formula">, j</text>
<text top="737" left="468" width="2" height="7" font="font13" id="p6_t315" reading_order_no="314" segment_no="14" tag_type="formula">0</text>
<text top="738" left="471" width="20" height="10" font="font12" id="p6_t316" reading_order_no="315" segment_no="14" tag_type="formula">, k, n</text>
<text top="738" left="491" width="4" height="10" font="font10" id="p6_t317" reading_order_no="316" segment_no="14" tag_type="formula">)</text>
<text top="738" left="495" width="3" height="10" font="font12" id="p6_t318" reading_order_no="317" segment_no="14" tag_type="formula">,</text>
</page>
<page number="7" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font17" size="5" family="CMR10" color="#000000"/>
	<fontspec id="font18" size="5" family="CMMI10" color="#000000"/>
	<fontspec id="font19" size="3" family="CMMI8" color="#000000"/>
	<fontspec id="font20" size="5" family="CMSY10" color="#000000"/>
	<fontspec id="font21" size="3" family="CMSY8" color="#000000"/>
	<fontspec id="font22" size="9" family="NimbusRomNo9L-Regu" color="#000000"/>
	<fontspec id="font23" size="4" family="CMMI8" color="#000000"/>
	<fontspec id="font24" size="9" family="CMR17" color="#000000"/>
	<fontspec id="font25" size="5" family="CMBSY10" color="#000000"/>
	<fontspec id="font26" size="5" family="CMBX12" color="#000000"/>
	<fontspec id="font27" size="5" family="CMSY5" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p7_t1" reading_order_no="0" segment_no="0" tag_type="text">7</text>
<text top="59" left="96" width="28" height="5" font="font17" id="p7_t2" reading_order_no="1" segment_no="2" tag_type="figure">MCS Feature</text>
<text top="65" left="99" width="22" height="5" font="font17" id="p7_t3" reading_order_no="2" segment_no="2" tag_type="figure">Extraction</text>
<text top="108" left="96" width="27" height="5" font="font17" id="p7_t4" reading_order_no="3" segment_no="2" tag_type="figure">RFD Feature</text>
<text top="114" left="99" width="22" height="5" font="font17" id="p7_t5" reading_order_no="4" segment_no="2" tag_type="figure">Extraction</text>
<text top="87" left="139" width="26" height="5" font="font17" id="p7_t6" reading_order_no="5" segment_no="2" tag_type="figure">Concatenate</text>
<text top="83" left="183" width="31" height="5" font="font17" id="p7_t7" reading_order_no="6" segment_no="2" tag_type="figure">Dimensionality</text>
<text top="89" left="188" width="21" height="5" font="font17" id="p7_t8" reading_order_no="7" segment_no="2" tag_type="figure">Reduction</text>
<text top="84" left="232" width="13" height="5" font="font17" id="p7_t9" reading_order_no="8" segment_no="2" tag_type="figure">Linear</text>
<text top="90" left="227" width="22" height="5" font="font17" id="p7_t10" reading_order_no="9" segment_no="2" tag_type="figure">Regression</text>
<text top="81" left="271" width="20" height="5" font="font17" id="p7_t11" reading_order_no="10" segment_no="2" tag_type="figure">Predicted</text>
<text top="87" left="274" width="16" height="5" font="font17" id="p7_t12" reading_order_no="11" segment_no="2" tag_type="figure">Quality</text>
<text top="93" left="276" width="11" height="5" font="font17" id="p7_t13" reading_order_no="12" segment_no="2" tag_type="figure">Score</text>
<text top="106" left="51" width="33" height="5" font="font17" id="p7_t14" reading_order_no="13" segment_no="2" tag_type="figure">Predicted Video</text>
<text top="86" left="108" width="8" height="5" font="font18" id="p7_t15" reading_order_no="14" segment_no="2" tag_type="figure">KN</text>
<text top="88" left="116" width="2" height="3" font="font19" id="p7_t16" reading_order_no="15" segment_no="2" tag_type="figure">p</text>
<text top="86" left="119" width="4" height="6" font="font20" id="p7_t17" reading_order_no="16" segment_no="2" tag_type="figure">×</text>
<text top="86" left="124" width="2" height="5" font="font17" id="p7_t18" reading_order_no="17" segment_no="2" tag_type="figure">1</text>
<text top="136" left="98" width="4" height="5" font="font18" id="p7_t19" reading_order_no="18" segment_no="2" tag_type="figure">K</text>
<text top="136" left="102" width="2" height="5" font="font17" id="p7_t20" reading_order_no="19" segment_no="2" tag_type="figure">(</text>
<text top="136" left="104" width="4" height="5" font="font18" id="p7_t21" reading_order_no="20" segment_no="2" tag_type="figure">N</text>
<text top="136" left="110" width="4" height="6" font="font20" id="p7_t22" reading_order_no="21" segment_no="2" tag_type="figure">−</text>
<text top="136" left="114" width="4" height="5" font="font17" id="p7_t23" reading_order_no="22" segment_no="2" tag_type="figure">1)</text>
<text top="136" left="120" width="4" height="6" font="font20" id="p7_t24" reading_order_no="23" segment_no="2" tag_type="figure">×</text>
<text top="136" left="124" width="2" height="5" font="font17" id="p7_t25" reading_order_no="24" segment_no="2" tag_type="figure">1</text>
<text top="124" left="154" width="4" height="5" font="font18" id="p7_t26" reading_order_no="25" segment_no="2" tag_type="figure">K</text>
<text top="124" left="158" width="2" height="5" font="font17" id="p7_t27" reading_order_no="26" segment_no="2" tag_type="figure">(</text>
<text top="124" left="160" width="4" height="5" font="font18" id="p7_t28" reading_order_no="27" segment_no="2" tag_type="figure">N</text>
<text top="124" left="165" width="4" height="5" font="font17" id="p7_t29" reading_order_no="28" segment_no="2" tag_type="figure">+</text>
<text top="124" left="170" width="4" height="5" font="font18" id="p7_t30" reading_order_no="29" segment_no="2" tag_type="figure">N</text>
<text top="125" left="174" width="2" height="3" font="font19" id="p7_t31" reading_order_no="30" segment_no="2" tag_type="figure">p</text>
<text top="123" left="177" width="4" height="6" font="font20" id="p7_t32" reading_order_no="31" segment_no="2" tag_type="figure">−</text>
<text top="124" left="182" width="4" height="5" font="font17" id="p7_t33" reading_order_no="32" segment_no="2" tag_type="figure">1)</text>
<text top="123" left="187" width="4" height="6" font="font20" id="p7_t34" reading_order_no="33" segment_no="2" tag_type="figure">×</text>
<text top="124" left="192" width="2" height="5" font="font17" id="p7_t35" reading_order_no="34" segment_no="2" tag_type="figure">1</text>
<text top="124" left="210" width="4" height="5" font="font18" id="p7_t36" reading_order_no="35" segment_no="2" tag_type="figure">K</text>
<text top="123" left="215" width="1" height="4" font="font21" id="p7_t37" reading_order_no="36" segment_no="2" tag_type="figure">0</text>
<text top="124" left="217" width="4" height="6" font="font20" id="p7_t38" reading_order_no="37" segment_no="2" tag_type="figure">×</text>
<text top="124" left="222" width="2" height="5" font="font17" id="p7_t39" reading_order_no="38" segment_no="2" tag_type="figure">1</text>
<text top="155" left="49" width="251" height="8" font="font22" id="p7_t40" reading_order_no="39" segment_no="4" tag_type="text">(a) High level architecture of our model. Note that MCS and RFD</text>
<text top="165" left="49" width="251" height="8" font="font22" id="p7_t41" reading_order_no="40" segment_no="4" tag_type="text">features are computed on frames and the features across frames are</text>
<text top="175" left="49" width="139" height="8" font="font22" id="p7_t42" reading_order_no="41" segment_no="4" tag_type="text">concatenated to obtain video features.</text>
<text top="228" left="105" width="27" height="5" font="font17" id="p7_t43" reading_order_no="42" segment_no="6" tag_type="figure">Pre-trained</text>
<text top="235" left="112" width="12" height="5" font="font17" id="p7_t44" reading_order_no="43" segment_no="6" tag_type="figure">CNN</text>
<text top="270" left="105" width="27" height="5" font="font17" id="p7_t45" reading_order_no="44" segment_no="6" tag_type="figure">Pre-trained</text>
<text top="277" left="112" width="12" height="5" font="font17" id="p7_t46" reading_order_no="45" segment_no="6" tag_type="figure">CNN</text>
<text top="270" left="181" width="17" height="5" font="font17" id="p7_t47" reading_order_no="46" segment_no="6" tag_type="figure">Motion</text>
<text top="277" left="172" width="34" height="5" font="font17" id="p7_t48" reading_order_no="47" segment_no="6" tag_type="figure">Compensation</text>
<text top="224" left="238" width="17" height="5" font="font17" id="p7_t49" reading_order_no="48" segment_no="6" tag_type="figure">Spatial</text>
<text top="231" left="238" width="16" height="5" font="font17" id="p7_t50" reading_order_no="49" segment_no="6" tag_type="figure">Cosine</text>
<text top="237" left="235" width="23" height="5" font="font17" id="p7_t51" reading_order_no="50" segment_no="6" tag_type="figure">Similarity</text>
<text top="213" left="51" width="40" height="5" font="font17" id="p7_t52" reading_order_no="51" segment_no="6" tag_type="figure">Context Frame (</text>
<text top="213" left="91" width="4" height="5" font="font18" id="p7_t53" reading_order_no="52" segment_no="6" tag_type="figure">N</text>
<text top="215" left="95" width="2" height="4" font="font23" id="p7_t54" reading_order_no="53" segment_no="6" tag_type="figure">c</text>
<text top="213" left="98" width="2" height="5" font="font17" id="p7_t55" reading_order_no="54" segment_no="6" tag_type="figure">)</text>
<text top="256" left="51" width="44" height="5" font="font17" id="p7_t56" reading_order_no="55" segment_no="6" tag_type="figure">Predicted Frame (</text>
<text top="256" left="95" width="3" height="5" font="font18" id="p7_t57" reading_order_no="56" segment_no="6" tag_type="figure">n</text>
<text top="256" left="98" width="2" height="5" font="font17" id="p7_t58" reading_order_no="57" segment_no="6" tag_type="figure">)</text>
<text top="195" left="130" width="39" height="5" font="font17" id="p7_t59" reading_order_no="58" segment_no="6" tag_type="figure">Deep features of</text>
<text top="202" left="133" width="33" height="5" font="font17" id="p7_t60" reading_order_no="59" segment_no="6" tag_type="figure">context frame</text>
<text top="216" left="162" width="3" height="5" font="font18" id="p7_t61" reading_order_no="60" segment_no="6" tag_type="figure">f</text>
<text top="216" left="166" width="2" height="5" font="font17" id="p7_t62" reading_order_no="61" segment_no="6" tag_type="figure">(</text>
<text top="216" left="168" width="19" height="5" font="font18" id="p7_t63" reading_order_no="62" segment_no="6" tag_type="figure">i, j, k, N</text>
<text top="218" left="187" width="2" height="4" font="font23" id="p7_t64" reading_order_no="63" segment_no="6" tag_type="figure">c</text>
<text top="216" left="189" width="2" height="5" font="font17" id="p7_t65" reading_order_no="64" segment_no="6" tag_type="figure">)</text>
<text top="302" left="130" width="39" height="5" font="font17" id="p7_t66" reading_order_no="65" segment_no="6" tag_type="figure">Deep features of</text>
<text top="309" left="131" width="38" height="5" font="font17" id="p7_t67" reading_order_no="66" segment_no="6" tag_type="figure">predicted frame</text>
<text top="290" left="162" width="3" height="5" font="font18" id="p7_t68" reading_order_no="67" segment_no="6" tag_type="figure">f</text>
<text top="290" left="166" width="2" height="5" font="font17" id="p7_t69" reading_order_no="68" segment_no="6" tag_type="figure">(</text>
<text top="290" left="168" width="18" height="5" font="font18" id="p7_t70" reading_order_no="69" segment_no="6" tag_type="figure">i, j, k, n</text>
<text top="290" left="186" width="2" height="5" font="font17" id="p7_t71" reading_order_no="70" segment_no="6" tag_type="figure">)</text>
<text top="302" left="196" width="50" height="5" font="font17" id="p7_t72" reading_order_no="71" segment_no="6" tag_type="figure">Motion compensated</text>
<text top="309" left="192" width="58" height="5" font="font17" id="p7_t73" reading_order_no="72" segment_no="6" tag_type="figure">predicted frame features</text>
<text top="290" left="233" width="3" height="5" font="font18" id="p7_t74" reading_order_no="73" segment_no="6" tag_type="figure">f</text>
<text top="292" left="236" width="4" height="4" font="font23" id="p7_t75" reading_order_no="74" segment_no="6" tag_type="figure">m</text>
<text top="290" left="240" width="2" height="5" font="font17" id="p7_t76" reading_order_no="75" segment_no="6" tag_type="figure">(</text>
<text top="290" left="242" width="18" height="5" font="font18" id="p7_t77" reading_order_no="76" segment_no="6" tag_type="figure">i, j, k, n</text>
<text top="290" left="260" width="2" height="5" font="font17" id="p7_t78" reading_order_no="77" segment_no="6" tag_type="figure">)</text>
<text top="200" left="258" width="34" height="5" font="font17" id="p7_t79" reading_order_no="78" segment_no="6" tag_type="figure">MCS Features</text>
<text top="231" left="284" width="5" height="5" font="font18" id="p7_t80" reading_order_no="79" segment_no="6" tag_type="figure">K</text>
<text top="231" left="290" width="4" height="6" font="font20" id="p7_t81" reading_order_no="80" segment_no="6" tag_type="figure">×</text>
<text top="231" left="295" width="3" height="5" font="font17" id="p7_t82" reading_order_no="81" segment_no="6" tag_type="figure">1</text>
<text top="324" left="49" width="251" height="8" font="font22" id="p7_t83" reading_order_no="82" segment_no="7" tag_type="text">(b) Architecture of the Motion-compensated Cosine Similarity (MCS)</text>
<text top="334" left="49" width="66" height="8" font="font22" id="p7_t84" reading_order_no="83" segment_no="7" tag_type="text">feature extraction.</text>
<text top="385" left="88" width="6" height="9" font="font24" id="p7_t85" reading_order_no="84" segment_no="9" tag_type="figure">Σ</text>
<text top="383" left="112" width="22" height="5" font="font17" id="p7_t86" reading_order_no="85" segment_no="9" tag_type="figure">Rescale to</text>
<text top="389" left="115" width="4" height="5" font="font17" id="p7_t87" reading_order_no="86" segment_no="9" tag_type="figure">[0</text>
<text top="389" left="119" width="1" height="5" font="font18" id="p7_t88" reading_order_no="87" segment_no="9" tag_type="figure">,</text>
<text top="389" left="122" width="9" height="5" font="font17" id="p7_t89" reading_order_no="88" segment_no="9" tag_type="figure">255]</text>
<text top="383" left="175" width="25" height="5" font="font17" id="p7_t90" reading_order_no="89" segment_no="9" tag_type="figure">Pre-trained</text>
<text top="389" left="182" width="11" height="5" font="font17" id="p7_t91" reading_order_no="90" segment_no="9" tag_type="figure">CNN</text>
<text top="383" left="244" width="15" height="5" font="font17" id="p7_t92" reading_order_no="91" segment_no="9" tag_type="figure">Spatial</text>
<text top="389" left="243" width="17" height="5" font="font17" id="p7_t93" reading_order_no="92" segment_no="9" tag_type="figure">Average</text>
<text top="354" left="55" width="17" height="5" font="font17" id="p7_t94" reading_order_no="93" segment_no="9" tag_type="figure">Frame -</text>
<text top="354" left="73" width="2" height="5" font="font18" id="p7_t95" reading_order_no="94" segment_no="9" tag_type="figure">i</text>
<text top="393" left="51" width="17" height="5" font="font17" id="p7_t96" reading_order_no="95" segment_no="9" tag_type="figure">Frame -</text>
<text top="393" left="69" width="2" height="5" font="font18" id="p7_t97" reading_order_no="96" segment_no="9" tag_type="figure">i</text>
<text top="393" left="72" width="7" height="5" font="font17" id="p7_t98" reading_order_no="97" segment_no="9" tag_type="figure">+ 1</text>
<text top="374" left="84" width="5" height="7" font="font25" id="p7_t99" reading_order_no="98" segment_no="9" tag_type="figure">−</text>
<text top="397" left="85" width="5" height="5" font="font26" id="p7_t100" reading_order_no="99" segment_no="9" tag_type="figure">+</text>
<text top="354" left="139" width="33" height="5" font="font17" id="p7_t101" reading_order_no="100" segment_no="9" tag_type="figure">Rescaled frame</text>
<text top="360" left="138" width="35" height="5" font="font17" id="p7_t102" reading_order_no="101" segment_no="9" tag_type="figure">difference image</text>
<text top="354" left="198" width="35" height="5" font="font17" id="p7_t103" reading_order_no="102" segment_no="9" tag_type="figure">Deep features of</text>
<text top="360" left="198" width="34" height="5" font="font17" id="p7_t104" reading_order_no="103" segment_no="9" tag_type="figure">frame difference</text>
<text top="358" left="262" width="31" height="5" font="font17" id="p7_t105" reading_order_no="104" segment_no="9" tag_type="figure">RFD Features</text>
<text top="386" left="285" width="4" height="5" font="font18" id="p7_t106" reading_order_no="105" segment_no="9" tag_type="figure">K</text>
<text top="386" left="291" width="4" height="6" font="font20" id="p7_t107" reading_order_no="106" segment_no="9" tag_type="figure">×</text>
<text top="386" left="296" width="2" height="5" font="font17" id="p7_t108" reading_order_no="107" segment_no="9" tag_type="figure">1</text>
<text top="424" left="49" width="251" height="8" font="font22" id="p7_t109" reading_order_no="108" segment_no="10" tag_type="text">(c) Architecture of the Rescaled Frame Difference (RFD) feature</text>
<text top="434" left="49" width="38" height="8" font="font22" id="p7_t110" reading_order_no="109" segment_no="10" tag_type="text">extraction.</text>
<text top="454" left="120" width="110" height="9" font="font5" id="p7_t111" reading_order_no="110" segment_no="11" tag_type="text">Fig. 5: Model architecture.</text>
<text top="491" left="49" width="24" height="9" font="font5" id="p7_t112" reading_order_no="111" segment_no="12" tag_type="text">where</text>
<text top="509" left="79" width="3" height="10" font="font12" id="p7_t113" reading_order_no="112" segment_no="13" tag_type="formula">i</text>
<text top="507" left="83" width="2" height="7" font="font13" id="p7_t114" reading_order_no="113" segment_no="13" tag_type="formula">0</text>
<text top="509" left="86" width="9" height="10" font="font12" id="p7_t115" reading_order_no="114" segment_no="13" tag_type="formula">, j</text>
<text top="507" left="95" width="2" height="7" font="font13" id="p7_t116" reading_order_no="115" segment_no="13" tag_type="formula">0</text>
<text top="509" left="100" width="45" height="10" font="font10" id="p7_t117" reading_order_no="116" segment_no="13" tag_type="formula">= arg max</text>
<text top="520" left="119" width="3" height="6" font="font15" id="p7_t118" reading_order_no="117" segment_no="13" tag_type="formula">i</text>
<text top="519" left="121" width="4" height="5" font="font27" id="p7_t119" reading_order_no="118" segment_no="13" tag_type="formula">00</text>
<text top="520" left="126" width="6" height="6" font="font15" id="p7_t120" reading_order_no="119" segment_no="13" tag_type="formula">,j</text>
<text top="519" left="132" width="4" height="5" font="font27" id="p7_t121" reading_order_no="120" segment_no="13" tag_type="formula">00</text>
<text top="509" left="147" width="5" height="10" font="font12" id="p7_t122" reading_order_no="121" segment_no="13" tag_type="formula">s</text>
<text top="509" left="151" width="4" height="10" font="font10" id="p7_t123" reading_order_no="122" segment_no="13" tag_type="formula">(</text>
<text top="509" left="155" width="5" height="10" font="font12" id="p7_t124" reading_order_no="123" segment_no="13" tag_type="formula">f</text>
<text top="509" left="161" width="4" height="10" font="font10" id="p7_t125" reading_order_no="124" segment_no="13" tag_type="formula">(</text>
<text top="509" left="165" width="15" height="10" font="font12" id="p7_t126" reading_order_no="125" segment_no="13" tag_type="formula">i, j,</text>
<text top="509" left="181" width="3" height="12" font="font11" id="p7_t127" reading_order_no="126" segment_no="13" tag_type="formula">·</text>
<text top="509" left="184" width="12" height="10" font="font12" id="p7_t128" reading_order_no="127" segment_no="13" tag_type="formula">, N</text>
<text top="513" left="197" width="4" height="6" font="font15" id="p7_t129" reading_order_no="128" segment_no="13" tag_type="formula">c</text>
<text top="509" left="201" width="4" height="10" font="font10" id="p7_t130" reading_order_no="129" segment_no="13" tag_type="formula">)</text>
<text top="509" left="205" width="9" height="10" font="font12" id="p7_t131" reading_order_no="130" segment_no="13" tag_type="formula">, f</text>
<text top="509" left="215" width="4" height="10" font="font10" id="p7_t132" reading_order_no="131" segment_no="13" tag_type="formula">(</text>
<text top="509" left="219" width="3" height="10" font="font12" id="p7_t133" reading_order_no="132" segment_no="13" tag_type="formula">i</text>
<text top="507" left="222" width="5" height="7" font="font13" id="p7_t134" reading_order_no="133" segment_no="13" tag_type="formula">00</text>
<text top="509" left="227" width="9" height="10" font="font12" id="p7_t135" reading_order_no="134" segment_no="13" tag_type="formula">, j</text>
<text top="507" left="236" width="5" height="7" font="font13" id="p7_t136" reading_order_no="135" segment_no="13" tag_type="formula">00</text>
<text top="509" left="242" width="3" height="10" font="font12" id="p7_t137" reading_order_no="136" segment_no="13" tag_type="formula">,</text>
<text top="509" left="246" width="3" height="12" font="font11" id="p7_t138" reading_order_no="137" segment_no="13" tag_type="formula">·</text>
<text top="509" left="249" width="10" height="10" font="font12" id="p7_t139" reading_order_no="138" segment_no="13" tag_type="formula">, n</text>
<text top="509" left="259" width="8" height="10" font="font10" id="p7_t140" reading_order_no="139" segment_no="13" tag_type="formula">))</text>
<text top="509" left="267" width="3" height="10" font="font12" id="p7_t141" reading_order_no="140" segment_no="13" tag_type="formula">.</text>
<text top="534" left="49" width="251" height="9" font="font5" id="p7_t142" reading_order_no="141" segment_no="14" tag_type="text">In other words, for every location in the context frame, we</text>
<text top="546" left="49" width="251" height="9" font="font5" id="p7_t143" reading_order_no="142" segment_no="14" tag_type="text">determine the location in the predicted frame with the best</text>
<text top="558" left="49" width="251" height="9" font="font5" id="p7_t144" reading_order_no="143" segment_no="14" tag_type="text">cosine similarity in the feature space. Thus we obtain the</text>
<text top="570" left="49" width="251" height="9" font="font5" id="p7_t145" reading_order_no="144" segment_no="14" tag_type="text">motion compensated features in each predicted frame and</text>
<text top="582" left="49" width="146" height="9" font="font5" id="p7_t146" reading_order_no="145" segment_no="14" tag_type="text">compute the MCS feature in Frame</text>
<text top="581" left="198" width="6" height="10" font="font12" id="p7_t147" reading_order_no="146" segment_no="14" tag_type="text">n</text>
<text top="582" left="208" width="51" height="9" font="font5" id="p7_t148" reading_order_no="147" segment_no="14" tag_type="text">and Channel</text>
<text top="581" left="262" width="5" height="10" font="font12" id="p7_t149" reading_order_no="148" segment_no="14" tag_type="text">k</text>
<text top="582" left="271" width="8" height="9" font="font5" id="p7_t150" reading_order_no="149" segment_no="14" tag_type="text">as</text>
<text top="600" left="86" width="5" height="10" font="font12" id="p7_t151" reading_order_no="150" segment_no="16" tag_type="formula">f</text>
<text top="604" left="91" width="15" height="6" font="font0" id="p7_t152" reading_order_no="151" segment_no="16" tag_type="formula">MCS</text>
<text top="600" left="106" width="4" height="10" font="font10" id="p7_t153" reading_order_no="152" segment_no="16" tag_type="formula">(</text>
<text top="600" left="110" width="16" height="10" font="font12" id="p7_t154" reading_order_no="153" segment_no="16" tag_type="formula">k, n</text>
<text top="600" left="126" width="14" height="10" font="font10" id="p7_t155" reading_order_no="154" segment_no="16" tag_type="formula">) =</text>
<text top="600" left="143" width="5" height="10" font="font12" id="p7_t156" reading_order_no="155" segment_no="16" tag_type="formula">s</text>
<text top="600" left="148" width="4" height="10" font="font10" id="p7_t157" reading_order_no="156" segment_no="16" tag_type="formula">(</text>
<text top="600" left="151" width="5" height="10" font="font12" id="p7_t158" reading_order_no="157" segment_no="16" tag_type="formula">f</text>
<text top="600" left="157" width="4" height="10" font="font10" id="p7_t159" reading_order_no="158" segment_no="16" tag_type="formula">(</text>
<text top="600" left="161" width="3" height="12" font="font11" id="p7_t160" reading_order_no="159" segment_no="16" tag_type="formula">·</text>
<text top="600" left="164" width="3" height="10" font="font12" id="p7_t161" reading_order_no="160" segment_no="16" tag_type="formula">,</text>
<text top="600" left="168" width="3" height="12" font="font11" id="p7_t162" reading_order_no="161" segment_no="16" tag_type="formula">·</text>
<text top="600" left="171" width="22" height="10" font="font12" id="p7_t163" reading_order_no="162" segment_no="16" tag_type="formula">, k, N</text>
<text top="604" left="194" width="4" height="6" font="font15" id="p7_t164" reading_order_no="163" segment_no="16" tag_type="formula">c</text>
<text top="600" left="198" width="4" height="10" font="font10" id="p7_t165" reading_order_no="164" segment_no="16" tag_type="formula">)</text>
<text top="600" left="202" width="9" height="10" font="font12" id="p7_t166" reading_order_no="165" segment_no="16" tag_type="formula">, f</text>
<text top="604" left="211" width="7" height="6" font="font15" id="p7_t167" reading_order_no="166" segment_no="16" tag_type="formula">m</text>
<text top="600" left="218" width="4" height="10" font="font10" id="p7_t168" reading_order_no="167" segment_no="16" tag_type="formula">(</text>
<text top="600" left="222" width="3" height="12" font="font11" id="p7_t169" reading_order_no="168" segment_no="16" tag_type="formula">·</text>
<text top="600" left="225" width="3" height="10" font="font12" id="p7_t170" reading_order_no="169" segment_no="16" tag_type="formula">,</text>
<text top="600" left="229" width="3" height="12" font="font11" id="p7_t171" reading_order_no="170" segment_no="16" tag_type="formula">·</text>
<text top="600" left="232" width="20" height="10" font="font12" id="p7_t172" reading_order_no="171" segment_no="16" tag_type="formula">, k, n</text>
<text top="600" left="253" width="8" height="10" font="font10" id="p7_t173" reading_order_no="172" segment_no="16" tag_type="formula">))</text>
<text top="600" left="260" width="3" height="10" font="font12" id="p7_t174" reading_order_no="173" segment_no="16" tag_type="formula">,</text>
<text top="620" left="49" width="24" height="9" font="font5" id="p7_t175" reading_order_no="174" segment_no="17" tag_type="text">where</text>
<text top="619" left="76" width="5" height="10" font="font12" id="p7_t176" reading_order_no="175" segment_no="17" tag_type="text">f</text>
<text top="619" left="82" width="4" height="10" font="font10" id="p7_t177" reading_order_no="176" segment_no="17" tag_type="text">(</text>
<text top="619" left="86" width="3" height="12" font="font11" id="p7_t178" reading_order_no="177" segment_no="17" tag_type="text">·</text>
<text top="619" left="88" width="3" height="10" font="font12" id="p7_t179" reading_order_no="178" segment_no="17" tag_type="text">,</text>
<text top="619" left="93" width="3" height="12" font="font11" id="p7_t180" reading_order_no="179" segment_no="17" tag_type="text">·</text>
<text top="619" left="96" width="22" height="10" font="font12" id="p7_t181" reading_order_no="180" segment_no="17" tag_type="text">, k, N</text>
<text top="623" left="118" width="4" height="6" font="font15" id="p7_t182" reading_order_no="181" segment_no="17" tag_type="text">c</text>
<text top="619" left="122" width="4" height="10" font="font10" id="p7_t183" reading_order_no="182" segment_no="17" tag_type="text">)</text>
<text top="620" left="128" width="172" height="9" font="font5" id="p7_t184" reading_order_no="183" segment_no="17" tag_type="text">denotes the vectorized deep features across</text>
<text top="632" left="49" width="104" height="9" font="font5" id="p7_t185" reading_order_no="184" segment_no="17" tag_type="text">spatial locations in Frame</text>
<text top="631" left="156" width="8" height="10" font="font12" id="p7_t186" reading_order_no="185" segment_no="17" tag_type="text">N</text>
<text top="635" left="164" width="4" height="6" font="font15" id="p7_t187" reading_order_no="186" segment_no="17" tag_type="text">c</text>
<text top="632" left="171" width="50" height="9" font="font5" id="p7_t188" reading_order_no="187" segment_no="17" tag_type="text">and Channel</text>
<text top="631" left="224" width="5" height="10" font="font12" id="p7_t189" reading_order_no="188" segment_no="17" tag_type="text">k</text>
<text top="632" left="232" width="14" height="9" font="font5" id="p7_t190" reading_order_no="189" segment_no="17" tag_type="text">and</text>
<text top="631" left="250" width="5" height="10" font="font12" id="p7_t191" reading_order_no="190" segment_no="17" tag_type="text">f</text>
<text top="635" left="254" width="7" height="6" font="font15" id="p7_t192" reading_order_no="191" segment_no="17" tag_type="text">m</text>
<text top="631" left="262" width="4" height="10" font="font10" id="p7_t193" reading_order_no="192" segment_no="17" tag_type="text">(</text>
<text top="631" left="266" width="3" height="12" font="font11" id="p7_t194" reading_order_no="193" segment_no="17" tag_type="text">·</text>
<text top="631" left="269" width="3" height="10" font="font12" id="p7_t195" reading_order_no="194" segment_no="17" tag_type="text">,</text>
<text top="631" left="273" width="3" height="12" font="font11" id="p7_t196" reading_order_no="195" segment_no="17" tag_type="text">·</text>
<text top="631" left="276" width="20" height="10" font="font12" id="p7_t197" reading_order_no="196" segment_no="17" tag_type="text">, k, n</text>
<text top="631" left="296" width="4" height="10" font="font10" id="p7_t198" reading_order_no="197" segment_no="17" tag_type="text">)</text>
<text top="643" left="49" width="163" height="9" font="font5" id="p7_t199" reading_order_no="198" segment_no="17" tag_type="text">is also defined similarly. This gives us a</text>
<text top="643" left="215" width="8" height="10" font="font12" id="p7_t200" reading_order_no="199" segment_no="17" tag_type="text">K</text>
<text top="643" left="227" width="73" height="9" font="font5" id="p7_t201" reading_order_no="200" segment_no="17" tag_type="text">dimensional MCS</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p7_t202" reading_order_no="201" segment_no="17" tag_type="text">feature vector per frame. We concatenate the MCS features</text>
<text top="667" left="49" width="137" height="9" font="font5" id="p7_t203" reading_order_no="202" segment_no="17" tag_type="text">from all predicted frames to get a</text>
<text top="667" left="189" width="8" height="10" font="font12" id="p7_t204" reading_order_no="203" segment_no="17" tag_type="text">K</text>
<text top="666" left="200" width="3" height="12" font="font11" id="p7_t205" reading_order_no="204" segment_no="17" tag_type="text">·</text>
<text top="667" left="205" width="8" height="10" font="font12" id="p7_t206" reading_order_no="205" segment_no="17" tag_type="text">N</text>
<text top="671" left="213" width="4" height="6" font="font15" id="p7_t207" reading_order_no="206" segment_no="17" tag_type="text">p</text>
<text top="667" left="220" width="80" height="9" font="font5" id="p7_t208" reading_order_no="207" segment_no="17" tag_type="text">dimensional feature</text>
<text top="679" left="49" width="69" height="9" font="font5" id="p7_t209" reading_order_no="208" segment_no="17" tag_type="text">vector per video.</text>
<text top="691" left="59" width="241" height="9" font="font5" id="p7_t210" reading_order_no="209" segment_no="18" tag_type="text">The MCS features are important in capturing several aspects</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p7_t211" reading_order_no="210" segment_no="18" tag_type="text">such as object blur, distortion of shapes, abnormal disappear-</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p7_t212" reading_order_no="211" segment_no="18" tag_type="text">ance of objects from the middle of a scene, and change in</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p7_t213" reading_order_no="212" segment_no="18" tag_type="text">object color. We believe that the natural disappearance of</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p7_t214" reading_order_no="213" segment_no="18" tag_type="text">objects from scenes (such as objects moving out of the field of</text>
<text top="58" left="312" width="251" height="9" font="font5" id="p7_t215" reading_order_no="214" segment_no="1" tag_type="text">view) can be distinguished from unnatural ones by observing</text>
<text top="70" left="312" width="251" height="9" font="font5" id="p7_t216" reading_order_no="215" segment_no="1" tag_type="text">the trajectory of MCS features across frames. However, we</text>
<text top="82" left="312" width="251" height="9" font="font5" id="p7_t217" reading_order_no="216" segment_no="1" tag_type="text">observe that the occurrence of such events is relatively less</text>
<text top="94" left="312" width="251" height="9" font="font5" id="p7_t218" reading_order_no="217" segment_no="1" tag_type="text">likely owing to the limited future duration over which video</text>
<text top="106" left="312" width="72" height="9" font="font5" id="p7_t219" reading_order_no="218" segment_no="1" tag_type="text">prediction occurs.</text>
<text top="144" left="312" width="189" height="9" font="font8" id="p7_t220" reading_order_no="219" segment_no="3" tag_type="title">B. Rescaled Frame Difference (RFD) features</text>
<text top="163" left="322" width="241" height="9" font="font5" id="p7_t221" reading_order_no="220" segment_no="5" tag_type="text">The second set of features we design is based on our</text>
<text top="175" left="312" width="251" height="9" font="font5" id="p7_t222" reading_order_no="221" segment_no="5" tag_type="text">observation that shape distortions are highly localized in</text>
<text top="187" left="312" width="251" height="9" font="font5" id="p7_t223" reading_order_no="222" segment_no="5" tag_type="text">regions containing motion. While optical flow may be used to</text>
<text top="199" left="312" width="251" height="9" font="font5" id="p7_t224" reading_order_no="223" segment_no="5" tag_type="text">determine motion masked frames as in <a href="deeplearning_paper9.html#12">[7], </a>the flow estimates</text>
<text top="211" left="312" width="251" height="9" font="font5" id="p7_t225" reading_order_no="224" segment_no="5" tag_type="text">tend to be noisy in predicted videos that contain a variety</text>
<text top="223" left="312" width="251" height="9" font="font5" id="p7_t226" reading_order_no="225" segment_no="5" tag_type="text">of artifacts. In order to overcome this challenge, we resort</text>
<text top="235" left="312" width="251" height="9" font="font5" id="p7_t227" reading_order_no="226" segment_no="5" tag_type="text">to measuring frame differences between adjacent frames to</text>
<text top="247" left="312" width="251" height="9" font="font5" id="p7_t228" reading_order_no="227" segment_no="5" tag_type="text">capture moving regions. However, instead of using such in-</text>
<text top="258" left="312" width="251" height="9" font="font5" id="p7_t229" reading_order_no="228" segment_no="5" tag_type="text">formation to mask frames, we rescale the frame differences</text>
<text top="270" left="312" width="251" height="9" font="font5" id="p7_t230" reading_order_no="229" segment_no="5" tag_type="text">in the intensity range [0,255] for each color channel and</text>
<text top="282" left="312" width="251" height="9" font="font5" id="p7_t231" reading_order_no="230" segment_no="5" tag_type="text">extract deep features from such images. The deep features</text>
<text top="294" left="312" width="251" height="9" font="font5" id="p7_t232" reading_order_no="231" segment_no="5" tag_type="text">(from VGG-19, ResNet-50, or Inception-v3) of rescaled frame</text>
<text top="306" left="312" width="251" height="9" font="font5" id="p7_t233" reading_order_no="232" segment_no="5" tag_type="text">differences enable robust measurement of shape distortions as</text>
<text top="318" left="312" width="56" height="9" font="font5" id="p7_t234" reading_order_no="233" segment_no="5" tag_type="text">argued below.</text>
<text top="331" left="322" width="241" height="9" font="font5" id="p7_t235" reading_order_no="234" segment_no="8" tag_type="text">In Fig. <a href="deeplearning_paper9.html#8">6, </a>we show examples of rescaled frame differences</text>
<text top="343" left="312" width="251" height="9" font="font5" id="p7_t236" reading_order_no="235" segment_no="8" tag_type="text">of two predicted videos from our database. We observe that</text>
<text top="355" left="312" width="251" height="9" font="font5" id="p7_t237" reading_order_no="236" segment_no="8" tag_type="text">the rescaled frame differences, simultaneously capture both the</text>
<text top="367" left="312" width="251" height="9" font="font5" id="p7_t238" reading_order_no="237" segment_no="8" tag_type="text">moving regions of frames as well as the changing contours of</text>
<text top="379" left="312" width="251" height="9" font="font5" id="p7_t239" reading_order_no="238" segment_no="8" tag_type="text">moving objects. We believe that the visualization of changing</text>
<text top="391" left="312" width="251" height="9" font="font5" id="p7_t240" reading_order_no="239" segment_no="8" tag_type="text">contours of moving objects in RFD adds robustness in the</text>
<text top="403" left="312" width="251" height="9" font="font5" id="p7_t241" reading_order_no="240" segment_no="8" tag_type="text">design of features along with MCS. We note that RFD</text>
<text top="415" left="312" width="251" height="9" font="font5" id="p7_t242" reading_order_no="241" segment_no="8" tag_type="text">resemble sketch images <a href="deeplearning_paper9.html#13">[55] </a>in the manner in which object</text>
<text top="427" left="312" width="251" height="9" font="font5" id="p7_t243" reading_order_no="242" segment_no="8" tag_type="text">outlines are visible. Motivated by the success of deep ResNet</text>
<text top="439" left="312" width="251" height="9" font="font5" id="p7_t244" reading_order_no="243" segment_no="8" tag_type="text">features in sketch recognition applications <a href="deeplearning_paper9.html#13">[56], </a>we extract</text>
<text top="451" left="312" width="251" height="9" font="font5" id="p7_t245" reading_order_no="244" segment_no="8" tag_type="text">similar features from RFD. We also note that deep features pre-</text>
<text top="463" left="312" width="251" height="9" font="font5" id="p7_t246" reading_order_no="245" segment_no="8" tag_type="text">trained on natural images have been processed by adding a few</text>
<text top="475" left="312" width="251" height="9" font="font5" id="p7_t247" reading_order_no="246" segment_no="8" tag_type="text">more layers and successfully applied in other applications for</text>
<text top="487" left="312" width="251" height="9" font="font5" id="p7_t248" reading_order_no="247" segment_no="8" tag_type="text">medical and satellite image processing <a href="deeplearning_paper9.html#13">[57], [58]. </a>We spatially</text>
<text top="499" left="312" width="251" height="9" font="font5" id="p7_t249" reading_order_no="248" segment_no="8" tag_type="text">average the deep features from each RFD to get a single</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p7_t250" reading_order_no="249" segment_no="8" tag_type="text">feature per channel, and then we concatenate the features</text>
<text top="523" left="312" width="200" height="9" font="font5" id="p7_t251" reading_order_no="250" segment_no="8" tag_type="text">across all frame differences and channels to get a</text>
<text top="522" left="516" width="8" height="10" font="font12" id="p7_t252" reading_order_no="251" segment_no="8" tag_type="text">K</text>
<text top="522" left="526" width="3" height="12" font="font11" id="p7_t253" reading_order_no="252" segment_no="8" tag_type="text">·</text>
<text top="522" left="530" width="4" height="10" font="font10" id="p7_t254" reading_order_no="253" segment_no="8" tag_type="text">(</text>
<text top="522" left="534" width="8" height="10" font="font12" id="p7_t255" reading_order_no="254" segment_no="8" tag_type="text">N</text>
<text top="522" left="545" width="8" height="12" font="font11" id="p7_t256" reading_order_no="255" segment_no="8" tag_type="text">−</text>
<text top="522" left="554" width="9" height="10" font="font10" id="p7_t257" reading_order_no="256" segment_no="8" tag_type="text">1)</text>
<text top="535" left="312" width="86" height="9" font="font5" id="p7_t258" reading_order_no="257" segment_no="8" tag_type="text">length feature vector.</text>
<text top="548" left="322" width="241" height="9" font="font5" id="p7_t259" reading_order_no="258" segment_no="15" tag_type="text">In order to further understand the relevance of deep features</text>
<text top="560" left="312" width="251" height="9" font="font5" id="p7_t260" reading_order_no="259" segment_no="15" tag_type="text">of RFD, we compare them with deep features of frames. Note</text>
<text top="572" left="312" width="251" height="9" font="font5" id="p7_t261" reading_order_no="260" segment_no="15" tag_type="text">that deep features of frames typically capture aspects such</text>
<text top="584" left="312" width="251" height="9" font="font5" id="p7_t262" reading_order_no="261" segment_no="15" tag_type="text">as object texture, shape, color, and so on <a href="deeplearning_paper9.html#13">[59]. </a>However, we</text>
<text top="596" left="312" width="251" height="9" font="font5" id="p7_t263" reading_order_no="262" segment_no="15" tag_type="text">observe that in RFD in Fig. <a href="deeplearning_paper9.html#8">6, </a>color, and other local properties</text>
<text top="608" left="312" width="251" height="9" font="font5" id="p7_t264" reading_order_no="263" segment_no="15" tag_type="text">tend to get suppressed. Thus, the corresponding deep features</text>
<text top="620" left="312" width="251" height="9" font="font5" id="p7_t265" reading_order_no="264" segment_no="15" tag_type="text">are primarily sensitive to the shape of the moving objects. In</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p7_t266" reading_order_no="265" segment_no="15" tag_type="text">order to study this more carefully, for the videos in Fig. <a href="deeplearning_paper9.html#8">6, </a>we</text>
<text top="643" left="312" width="251" height="9" font="font5" id="p7_t267" reading_order_no="266" segment_no="15" tag_type="text">compare the dissimilarity of spatially averaged deep features</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p7_t268" reading_order_no="267" segment_no="15" tag_type="text">of frames and RFD between the first context frame and the last</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p7_t269" reading_order_no="268" segment_no="15" tag_type="text">predicted frame. For Video 1, we observe that the dissimilarity</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p7_t270" reading_order_no="269" segment_no="15" tag_type="text">score (1 - cosine similarity) for RFD features is 0.34, while</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p7_t271" reading_order_no="270" segment_no="15" tag_type="text">that of frame features is 0.16. For Video 2, the corresponding</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p7_t272" reading_order_no="271" segment_no="15" tag_type="text">scores are 0.43 and 0.27 respectively. This illustrates that</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p7_t273" reading_order_no="272" segment_no="15" tag_type="text">the deep features of RFD are more sensitive to variations in</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p7_t274" reading_order_no="273" segment_no="15" tag_type="text">object shapes when compared with the features of the frames</text>
<text top="739" left="312" width="47" height="9" font="font5" id="p7_t275" reading_order_no="274" segment_no="15" tag_type="text">themselves.</text>
</page>
<page number="8" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="560" width="3" height="6" font="font0" id="p8_t1" reading_order_no="0" segment_no="0" tag_type="text">8</text>
<text top="206" left="49" width="514" height="9" font="font5" id="p8_t2" reading_order_no="1" segment_no="1" tag_type="text">Fig. 6: Examples of frame differences for various distortions. In the first video, we see the disappearance of the upper torso of</text>
<text top="218" left="49" width="514" height="9" font="font5" id="p8_t3" reading_order_no="2" segment_no="1" tag_type="text">the girl. In the second video, we observe the movement of the baseline of the tennis court. While the first and the last frame</text>
<text top="230" left="49" width="514" height="9" font="font5" id="p8_t4" reading_order_no="3" segment_no="1" tag_type="text">may appear largely similar, the movement of the object boundaries is clearly visible in the frame differences. The videos can</text>
<text top="242" left="49" width="139" height="9" font="font5" id="p8_t5" reading_order_no="4" segment_no="1" tag_type="text">be viewed on our project website.</text>
<text top="276" left="49" width="139" height="9" font="font8" id="p8_t6" reading_order_no="5" segment_no="2" tag_type="title">C. Learning quality from features</text>
<text top="291" left="59" width="241" height="9" font="font5" id="p8_t7" reading_order_no="6" segment_no="4" tag_type="text">We concatenate the MCS and RFD features, which gives a</text>
<text top="302" left="49" width="8" height="10" font="font12" id="p8_t8" reading_order_no="7" segment_no="4" tag_type="text">K</text>
<text top="302" left="61" width="3" height="12" font="font11" id="p8_t9" reading_order_no="8" segment_no="4" tag_type="text">·</text>
<text top="302" left="66" width="4" height="10" font="font10" id="p8_t10" reading_order_no="9" segment_no="4" tag_type="text">(</text>
<text top="302" left="70" width="8" height="10" font="font12" id="p8_t11" reading_order_no="10" segment_no="4" tag_type="text">N</text>
<text top="302" left="82" width="8" height="10" font="font10" id="p8_t12" reading_order_no="11" segment_no="4" tag_type="text">+</text>
<text top="302" left="92" width="8" height="10" font="font12" id="p8_t13" reading_order_no="12" segment_no="4" tag_type="text">N</text>
<text top="306" left="100" width="4" height="6" font="font15" id="p8_t14" reading_order_no="13" segment_no="4" tag_type="text">p</text>
<text top="302" left="108" width="8" height="12" font="font11" id="p8_t15" reading_order_no="14" segment_no="4" tag_type="text">−</text>
<text top="302" left="118" width="9" height="10" font="font10" id="p8_t16" reading_order_no="15" segment_no="4" tag_type="text">1)</text>
<text top="303" left="131" width="169" height="9" font="font5" id="p8_t17" reading_order_no="16" segment_no="4" tag_type="text">length vector. Since the resulting feature</text>
<text top="315" left="49" width="251" height="9" font="font5" id="p8_t18" reading_order_no="17" segment_no="4" tag_type="text">dimension is much higher than the number of videos in our</text>
<text top="327" left="49" width="251" height="9" font="font5" id="p8_t19" reading_order_no="18" segment_no="4" tag_type="text">database, we reduce the dimensionality of the feature vector</text>
<text top="338" left="49" width="251" height="9" font="font5" id="p8_t20" reading_order_no="19" segment_no="4" tag_type="text">through principal component analysis. We select a subset</text>
<text top="350" left="49" width="54" height="9" font="font5" id="p8_t21" reading_order_no="20" segment_no="4" tag_type="text">consisting of</text>
<text top="350" left="109" width="8" height="10" font="font12" id="p8_t22" reading_order_no="21" segment_no="4" tag_type="text">K</text>
<text top="348" left="118" width="2" height="7" font="font13" id="p8_t23" reading_order_no="22" segment_no="4" tag_type="text">0</text>
<text top="350" left="126" width="174" height="9" font="font5" id="p8_t24" reading_order_no="23" segment_no="4" tag_type="text">principal components. We employ linear</text>
<text top="362" left="49" width="202" height="9" font="font5" id="p8_t25" reading_order_no="24" segment_no="4" tag_type="text">regression to predict the quality scores from the</text>
<text top="362" left="256" width="8" height="10" font="font12" id="p8_t26" reading_order_no="25" segment_no="4" tag_type="text">K</text>
<text top="360" left="265" width="2" height="7" font="font13" id="p8_t27" reading_order_no="26" segment_no="4" tag_type="text">0</text>
<text top="362" left="272" width="28" height="9" font="font5" id="p8_t28" reading_order_no="27" segment_no="4" tag_type="text">dimen-</text>
<text top="374" left="49" width="251" height="9" font="font5" id="p8_t29" reading_order_no="28" segment_no="4" tag_type="text">sional feature vector. We also experimented with regression</text>
<text top="386" left="49" width="251" height="9" font="font5" id="p8_t30" reading_order_no="29" segment_no="4" tag_type="text">models that are directly trained on the high dimensional feature</text>
<text top="398" left="49" width="251" height="9" font="font5" id="p8_t31" reading_order_no="30" segment_no="4" tag_type="text">vector. Since we observed similar performance, we present our</text>
<text top="410" left="49" width="251" height="9" font="font5" id="p8_t32" reading_order_no="31" segment_no="4" tag_type="text">simpler approach involving the feature vector obtained through</text>
<text top="422" left="49" width="114" height="9" font="font5" id="p8_t33" reading_order_no="32" segment_no="4" tag_type="text">PCA with linear regression.</text>
<text top="446" left="137" width="20" height="9" font="font5" id="p8_t34" reading_order_no="33" segment_no="7" tag_type="title">V. E</text>
<text top="448" left="158" width="54" height="7" font="font6" id="p8_t35" reading_order_no="34" segment_no="7" tag_type="title">XPERIMENTS</text>
<text top="461" left="49" width="185" height="9" font="font8" id="p8_t36" reading_order_no="35" segment_no="8" tag_type="title">A. Evaluation of Objective Quality Measures</text>
<text top="476" left="59" width="241" height="9" font="font5" id="p8_t37" reading_order_no="36" segment_no="9" tag_type="text">We present the evaluation of various measures of quality,</text>
<text top="488" left="49" width="251" height="9" font="font5" id="p8_t38" reading_order_no="37" segment_no="9" tag_type="text">spanning FR and NR image and video QA indices including</text>
<text top="500" left="49" width="251" height="9" font="font5" id="p8_t39" reading_order_no="38" segment_no="9" tag_type="text">those that are currently used to evaluate predicted videos, deep</text>
<text top="512" left="49" width="251" height="9" font="font5" id="p8_t40" reading_order_no="39" segment_no="9" tag_type="text">features of spatial and spatio-temporal networks, and finally</text>
<text top="524" left="49" width="251" height="9" font="font5" id="p8_t41" reading_order_no="40" segment_no="9" tag_type="text">our feature design contributions. To the best of our knowledge,</text>
<text top="536" left="49" width="251" height="9" font="font5" id="p8_t42" reading_order_no="41" segment_no="9" tag_type="text">there exists no other publicly available database to assess the</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p8_t43" reading_order_no="42" segment_no="9" tag_type="text">quality of predicted videos. Also, the conditions for application</text>
<text top="560" left="49" width="251" height="9" font="font5" id="p8_t44" reading_order_no="43" segment_no="9" tag_type="text">of our model such as the need for context frames for video</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p8_t45" reading_order_no="44" segment_no="9" tag_type="text">prediction, are quite different from classical VQA. Thus, we</text>
<text top="584" left="49" width="234" height="9" font="font5" id="p8_t46" reading_order_no="45" segment_no="9" tag_type="text">perform all our experiments on the IISc-PVQA database.</text>
<text top="596" left="59" width="92" height="9" font="font8" id="p8_t47" reading_order_no="46" segment_no="11" tag_type="text">1) Existing measures:</text>
<text top="596" left="157" width="143" height="9" font="font5" id="p8_t48" reading_order_no="47" segment_no="11" tag_type="text">Several QA indices are popularly</text>
<text top="608" left="49" width="251" height="9" font="font5" id="p8_t49" reading_order_no="48" segment_no="11" tag_type="text">used to evaluate predicted videos. Among FR image QA</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p8_t50" reading_order_no="49" segment_no="11" tag_type="text">metrics, we evaluate MSE, SSIM <a href="deeplearning_paper9.html#12">[8], </a>MS-SSIM <a href="deeplearning_paper9.html#13">[60], </a>gradient</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p8_t51" reading_order_no="50" segment_no="11" tag_type="text">difference <a href="deeplearning_paper9.html#12">[7], </a>LPIPS <a href="deeplearning_paper9.html#13">[62], </a>PieApp <a href="deeplearning_paper9.html#13">[63] </a>and DISTS <a href="deeplearning_paper9.html#13">[64].</a></text>
<text top="643" left="49" width="251" height="9" font="font5" id="p8_t52" reading_order_no="51" segment_no="11" tag_type="text">We note that the gradient difference measure is related to a</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p8_t53" reading_order_no="52" segment_no="11" tag_type="text">sharpness measure as shown in <a href="deeplearning_paper9.html#12">[7]. </a>We also evaluate MSE</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p8_t54" reading_order_no="53" segment_no="11" tag_type="text">and cosine similarity in the VGG feature space <a href="deeplearning_paper9.html#12">[9], [22] </a>by</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p8_t55" reading_order_no="54" segment_no="11" tag_type="text">tapping the features from the fourth convolutional layer of the</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p8_t56" reading_order_no="55" segment_no="11" tag_type="text">fifth block of the VGG-19 network <a href="deeplearning_paper9.html#13">[61]. </a>Among NR image</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p8_t57" reading_order_no="56" segment_no="11" tag_type="text">QA indices, we evaluate BRISQUE <a href="deeplearning_paper9.html#13">[66] </a>and NIQE <a href="deeplearning_paper9.html#13">[67] </a>by</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p8_t58" reading_order_no="57" segment_no="11" tag_type="text">computing them on each frame and taking their average. We</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p8_t59" reading_order_no="58" segment_no="11" tag_type="text">also evaluate a modified version of Inception Score <a href="deeplearning_paper9.html#12">[24] </a>on</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p8_t60" reading_order_no="59" segment_no="11" tag_type="text">individual frames by computing the entropy of the conditional</text>
<text top="276" left="312" width="251" height="9" font="font5" id="p8_t61" reading_order_no="60" segment_no="3" tag_type="text">distribution alone as a measure of the quality. Among video</text>
<text top="288" left="312" width="251" height="9" font="font5" id="p8_t62" reading_order_no="61" segment_no="3" tag_type="text">QA measures, we evaluate FR measures such as ST-MAD <a href="deeplearning_paper9.html#12">[33]</a></text>
<text top="300" left="312" width="251" height="9" font="font5" id="p8_t63" reading_order_no="62" segment_no="3" tag_type="text">and VMAF v1.5.1 <a href="deeplearning_paper9.html#13">[65] </a>and NR indices such as Video BLI-</text>
<text top="311" left="312" width="251" height="9" font="font5" id="p8_t64" reading_order_no="63" segment_no="3" tag_type="text">INDS <a href="deeplearning_paper9.html#12">[36], </a>NSTSS <a href="deeplearning_paper9.html#13">[68], </a>TLVQM <a href="deeplearning_paper9.html#12">[38] </a>and VSFA <a href="deeplearning_paper9.html#12">[39]. </a>We</text>
<text top="323" left="312" width="251" height="9" font="font5" id="p8_t65" reading_order_no="64" segment_no="3" tag_type="text">train VMAF and the NR measures on our database for a fair</text>
<text top="335" left="312" width="49" height="9" font="font5" id="p8_t66" reading_order_no="65" segment_no="3" tag_type="text">comparison.</text>
<text top="347" left="322" width="180" height="9" font="font8" id="p8_t67" reading_order_no="66" segment_no="5" tag_type="text">2) Quality assessment using deep features:</text>
<text top="347" left="508" width="55" height="9" font="font5" id="p8_t68" reading_order_no="67" segment_no="5" tag_type="text">We present a</text>
<text top="359" left="312" width="251" height="9" font="font5" id="p8_t69" reading_order_no="68" segment_no="5" tag_type="text">simple baseline by processing the features extracted from</text>
<text top="370" left="312" width="251" height="9" font="font5" id="p8_t70" reading_order_no="69" segment_no="5" tag_type="text">ResNet-50 <a href="deeplearning_paper9.html#13">[69] </a>model, pre-trained on the ImageNet-1k <a href="deeplearning_paper9.html#13">[70]</a></text>
<text top="382" left="312" width="251" height="9" font="font5" id="p8_t71" reading_order_no="70" segment_no="5" tag_type="text">image classification database. We tap the features before the</text>
<text top="394" left="312" width="251" height="9" font="font5" id="p8_t72" reading_order_no="71" segment_no="5" tag_type="text">global pooling operation, apply simple spatial averaging (SSA)</text>
<text top="406" left="312" width="145" height="9" font="font5" id="p8_t73" reading_order_no="72" segment_no="5" tag_type="text">to get a feature vector of dimension</text>
<text top="406" left="460" width="8" height="10" font="font12" id="p8_t74" reading_order_no="73" segment_no="5" tag_type="text">K</text>
<text top="406" left="472" width="30" height="10" font="font10" id="p8_t75" reading_order_no="74" segment_no="5" tag_type="text">= 2048</text>
<text top="406" left="505" width="58" height="9" font="font5" id="p8_t76" reading_order_no="75" segment_no="5" tag_type="text">per frame. We</text>
<text top="418" left="312" width="251" height="9" font="font5" id="p8_t77" reading_order_no="76" segment_no="5" tag_type="text">then concatenate the features from each frame and feed them</text>
<text top="430" left="312" width="224" height="9" font="font5" id="p8_t78" reading_order_no="77" segment_no="5" tag_type="text">to a regression model, similar to ours in Section <a href="deeplearning_paper9.html#8">IV-C.</a></text>
<text top="442" left="322" width="241" height="9" font="font5" id="p8_t79" reading_order_no="78" segment_no="6" tag_type="text">Additionally, we present another baseline, using features</text>
<text top="454" left="312" width="251" height="9" font="font5" id="p8_t80" reading_order_no="79" segment_no="6" tag_type="text">from the pre-trained 3D ConvNet (C3D) model <a href="deeplearning_paper9.html#13">[71], </a>suc-</text>
<text top="466" left="312" width="251" height="9" font="font5" id="p8_t81" reading_order_no="80" segment_no="6" tag_type="text">cessfully used in action recognition on videos. We resize the</text>
<text top="478" left="312" width="124" height="9" font="font5" id="p8_t82" reading_order_no="81" segment_no="6" tag_type="text">input frames to a resolution of</text>
<text top="477" left="439" width="15" height="10" font="font10" id="p8_t83" reading_order_no="82" segment_no="6" tag_type="text">112</text>
<text top="477" left="455" width="8" height="12" font="font11" id="p8_t84" reading_order_no="83" segment_no="6" tag_type="text">×</text>
<text top="477" left="465" width="15" height="10" font="font10" id="p8_t85" reading_order_no="84" segment_no="6" tag_type="text">112</text>
<text top="478" left="480" width="83" height="9" font="font5" id="p8_t86" reading_order_no="85" segment_no="6" tag_type="text">, tap spatio-temporal</text>
<text top="490" left="312" width="251" height="9" font="font5" id="p8_t87" reading_order_no="86" segment_no="6" tag_type="text">features before the last pooling layer, and process them using</text>
<text top="502" left="312" width="251" height="9" font="font5" id="p8_t88" reading_order_no="87" segment_no="6" tag_type="text">PCA and linear regression as the other models. The number</text>
<text top="514" left="312" width="251" height="9" font="font5" id="p8_t89" reading_order_no="88" segment_no="6" tag_type="text">of principal components is set to 40, based on a simple grid</text>
<text top="526" left="312" width="251" height="9" font="font5" id="p8_t90" reading_order_no="89" segment_no="6" tag_type="text">search. While ResNet-50 is trained on images, C3D is directly</text>
<text top="537" left="312" width="73" height="9" font="font5" id="p8_t91" reading_order_no="90" segment_no="6" tag_type="text">trained on videos.</text>
<text top="549" left="322" width="61" height="9" font="font8" id="p8_t92" reading_order_no="91" segment_no="10" tag_type="text">3) Our model:</text>
<text top="549" left="388" width="175" height="9" font="font5" id="p8_t93" reading_order_no="92" segment_no="10" tag_type="text">We evaluate our model for quality assess-</text>
<text top="561" left="312" width="251" height="9" font="font5" id="p8_t94" reading_order_no="93" segment_no="10" tag_type="text">ment of predicted videos based on MCS and RFD features</text>
<text top="573" left="312" width="251" height="9" font="font5" id="p8_t95" reading_order_no="94" segment_no="10" tag_type="text">using different networks such as VGG-19 <a href="deeplearning_paper9.html#12">[23], </a>ResNet-50 <a href="deeplearning_paper9.html#13">[69]</a></text>
<text top="584" left="312" width="251" height="9" font="font5" id="p8_t96" reading_order_no="95" segment_no="10" tag_type="text">and Inception-v3 <a href="deeplearning_paper9.html#13">[72], </a>all of which are pre-trained on the</text>
<text top="596" left="312" width="251" height="9" font="font5" id="p8_t97" reading_order_no="96" segment_no="10" tag_type="text">ImageNet-1k <a href="deeplearning_paper9.html#13">[70] </a>image classification database. We tap fea-</text>
<text top="608" left="312" width="251" height="9" font="font5" id="p8_t98" reading_order_no="97" segment_no="10" tag_type="text">tures from the last convolutional layer before the FC layers.</text>
<text top="620" left="312" width="112" height="9" font="font5" id="p8_t99" reading_order_no="98" segment_no="10" tag_type="text">This results in a choice of</text>
<text top="620" left="428" width="8" height="10" font="font12" id="p8_t100" reading_order_no="99" segment_no="10" tag_type="text">K</text>
<text top="620" left="442" width="27" height="10" font="font10" id="p8_t101" reading_order_no="100" segment_no="10" tag_type="text">= 512</text>
<text top="620" left="469" width="3" height="10" font="font12" id="p8_t102" reading_order_no="101" segment_no="10" tag_type="text">,</text>
<text top="620" left="474" width="20" height="10" font="font10" id="p8_t103" reading_order_no="102" segment_no="10" tag_type="text">2048</text>
<text top="620" left="494" width="3" height="10" font="font12" id="p8_t104" reading_order_no="103" segment_no="10" tag_type="text">,</text>
<text top="620" left="498" width="20" height="10" font="font10" id="p8_t105" reading_order_no="104" segment_no="10" tag_type="text">2048</text>
<text top="620" left="522" width="41" height="9" font="font5" id="p8_t106" reading_order_no="105" segment_no="10" tag_type="text">for VGG-</text>
<text top="632" left="312" width="251" height="9" font="font5" id="p8_t107" reading_order_no="106" segment_no="10" tag_type="text">19, ResNet-50 and Inception-v3 networks respectively. For</text>
<text top="644" left="312" width="205" height="9" font="font5" id="p8_t108" reading_order_no="107" segment_no="10" tag_type="text">PCA, we set the number of principal components,</text>
<text top="644" left="520" width="8" height="10" font="font12" id="p8_t109" reading_order_no="108" segment_no="10" tag_type="text">K</text>
<text top="642" left="530" width="2" height="7" font="font13" id="p8_t110" reading_order_no="109" segment_no="10" tag_type="text">0</text>
<text top="644" left="535" width="25" height="10" font="font10" id="p8_t111" reading_order_no="110" segment_no="10" tag_type="text">= 240</text>
<text top="644" left="561" width="2" height="9" font="font5" id="p8_t112" reading_order_no="111" segment_no="10" tag_type="text">,</text>
<text top="656" left="312" width="251" height="9" font="font5" id="p8_t113" reading_order_no="112" segment_no="10" tag_type="text">based on the number of videos in the training set. We also</text>
<text top="668" left="312" width="235" height="9" font="font5" id="p8_t114" reading_order_no="113" segment_no="10" tag_type="text">demonstrate the variation in performance with respect to</text>
<text top="667" left="551" width="8" height="10" font="font12" id="p8_t115" reading_order_no="114" segment_no="10" tag_type="text">K</text>
<text top="666" left="560" width="2" height="7" font="font13" id="p8_t116" reading_order_no="115" segment_no="10" tag_type="text">0</text>
<text top="680" left="312" width="68" height="9" font="font5" id="p8_t117" reading_order_no="116" segment_no="10" tag_type="text">in Section <a href="deeplearning_paper9.html#11">V-B4.</a></text>
<text top="691" left="322" width="114" height="9" font="font8" id="p8_t118" reading_order_no="117" segment_no="12" tag_type="text">4) Performance Evaluation:</text>
<text top="691" left="440" width="123" height="9" font="font5" id="p8_t119" reading_order_no="118" segment_no="12" tag_type="text">We evaluate the different qual-</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p8_t120" reading_order_no="119" segment_no="12" tag_type="text">ity assessment indices using Spearman Rank Order Correlation</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p8_t121" reading_order_no="120" segment_no="12" tag_type="text">Coefficient (SROCC), Pearson linear correlation coefficient</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p8_t122" reading_order_no="121" segment_no="12" tag_type="text">(PLCC), and root mean squared error (RMSE) popularly used</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p8_t123" reading_order_no="122" segment_no="12" tag_type="text">in the QA literature <a href="deeplearning_paper9.html#12">[29]. </a>In order to evaluate PLCC and</text>
</page>
<page number="9" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font28" size="8" family="NimbusRomNo9L-Medi" color="#000000"/>
	<fontspec id="font29" size="7" family="Helvetica" color="#252525"/>
	<fontspec id="font30" size="8" family="Helvetica" color="#252525"/>
	<fontspec id="font31" size="8" family="Helvetica" color="#252525"/>
	<fontspec id="font32" size="7" family="Helvetica" color="#000000"/>
<text top="26" left="560" width="3" height="6" font="font0" id="p9_t1" reading_order_no="0" segment_no="0" tag_type="text">9</text>
<text top="54" left="49" width="514" height="9" font="font5" id="p9_t2" reading_order_no="1" segment_no="1" tag_type="text">TABLE III: Evaluation of Objective Measures of Quality in terms of SROCC, PLCC and RMSE. We show the median</text>
<text top="66" left="49" width="514" height="9" font="font5" id="p9_t3" reading_order_no="2" segment_no="1" tag_type="text">performance over 100 trials of train-test split of the database. Also shown are the standard deviations in the performance</text>
<text top="78" left="49" width="66" height="9" font="font5" id="p9_t4" reading_order_no="3" segment_no="1" tag_type="text">across the trials.</text>
<text top="96" left="136" width="21" height="7" font="font6" id="p9_t5" reading_order_no="4" segment_no="2" tag_type="table">Metric</text>
<text top="96" left="311" width="26" height="7" font="font6" id="p9_t6" reading_order_no="5" segment_no="2" tag_type="table">SROCC</text>
<text top="96" left="375" width="20" height="7" font="font6" id="p9_t7" reading_order_no="6" segment_no="2" tag_type="table">PLCC</text>
<text top="96" left="436" width="22" height="7" font="font6" id="p9_t8" reading_order_no="7" segment_no="2" tag_type="table">RMSE</text>
<text top="105" left="136" width="16" height="7" font="font6" id="p9_t9" reading_order_no="8" segment_no="2" tag_type="table">MSE</text>
<text top="105" left="300" width="22" height="7" font="font6" id="p9_t10" reading_order_no="9" segment_no="2" tag_type="table">0.4044</text>
<text top="104" left="324" width="7" height="9" font="font9" id="p9_t11" reading_order_no="10" segment_no="2" tag_type="table">±</text>
<text top="105" left="334" width="14" height="7" font="font6" id="p9_t12" reading_order_no="11" segment_no="2" tag_type="table">0.11</text>
<text top="105" left="361" width="22" height="7" font="font6" id="p9_t13" reading_order_no="12" segment_no="2" tag_type="table">0.6578</text>
<text top="104" left="385" width="7" height="9" font="font9" id="p9_t14" reading_order_no="13" segment_no="2" tag_type="table">±</text>
<text top="105" left="395" width="14" height="7" font="font6" id="p9_t15" reading_order_no="14" segment_no="2" tag_type="table">0.08</text>
<text top="105" left="421" width="26" height="7" font="font6" id="p9_t16" reading_order_no="15" segment_no="2" tag_type="table">10.2556</text>
<text top="104" left="450" width="7" height="9" font="font9" id="p9_t17" reading_order_no="16" segment_no="2" tag_type="table">±</text>
<text top="105" left="459" width="14" height="7" font="font6" id="p9_t18" reading_order_no="17" segment_no="2" tag_type="table">0.86</text>
<text top="114" left="136" width="31" height="7" font="font6" id="p9_t19" reading_order_no="18" segment_no="2" tag_type="table">SSIM <a href="deeplearning_paper9.html#12">[8]</a></text>
<text top="114" left="300" width="22" height="7" font="font6" id="p9_t20" reading_order_no="19" segment_no="2" tag_type="table">0.5274</text>
<text top="113" left="324" width="7" height="9" font="font9" id="p9_t21" reading_order_no="20" segment_no="2" tag_type="table">±</text>
<text top="114" left="334" width="14" height="7" font="font6" id="p9_t22" reading_order_no="21" segment_no="2" tag_type="table">0.09</text>
<text top="114" left="361" width="22" height="7" font="font6" id="p9_t23" reading_order_no="22" segment_no="2" tag_type="table">0.6828</text>
<text top="113" left="385" width="7" height="9" font="font9" id="p9_t24" reading_order_no="23" segment_no="2" tag_type="table">±</text>
<text top="114" left="395" width="14" height="7" font="font6" id="p9_t25" reading_order_no="24" segment_no="2" tag_type="table">0.07</text>
<text top="114" left="421" width="26" height="7" font="font6" id="p9_t26" reading_order_no="25" segment_no="2" tag_type="table">09.9311</text>
<text top="113" left="450" width="7" height="9" font="font9" id="p9_t27" reading_order_no="26" segment_no="2" tag_type="table">±</text>
<text top="114" left="459" width="14" height="7" font="font6" id="p9_t28" reading_order_no="27" segment_no="2" tag_type="table">0.89</text>
<text top="123" left="136" width="49" height="7" font="font6" id="p9_t29" reading_order_no="28" segment_no="2" tag_type="table">MS-SSIM <a href="deeplearning_paper9.html#13">[60]</a></text>
<text top="123" left="300" width="22" height="7" font="font6" id="p9_t30" reading_order_no="29" segment_no="2" tag_type="table">0.5174</text>
<text top="122" left="324" width="7" height="9" font="font9" id="p9_t31" reading_order_no="30" segment_no="2" tag_type="table">±</text>
<text top="123" left="334" width="14" height="7" font="font6" id="p9_t32" reading_order_no="31" segment_no="2" tag_type="table">0.09</text>
<text top="123" left="361" width="22" height="7" font="font6" id="p9_t33" reading_order_no="32" segment_no="2" tag_type="table">0.6548</text>
<text top="122" left="385" width="7" height="9" font="font9" id="p9_t34" reading_order_no="33" segment_no="2" tag_type="table">±</text>
<text top="123" left="395" width="14" height="7" font="font6" id="p9_t35" reading_order_no="34" segment_no="2" tag_type="table">0.08</text>
<text top="123" left="421" width="26" height="7" font="font6" id="p9_t36" reading_order_no="35" segment_no="2" tag_type="table">10.2474</text>
<text top="122" left="450" width="7" height="9" font="font9" id="p9_t37" reading_order_no="36" segment_no="2" tag_type="table">±</text>
<text top="123" left="459" width="14" height="7" font="font6" id="p9_t38" reading_order_no="37" segment_no="2" tag_type="table">0.88</text>
<text top="132" left="136" width="77" height="7" font="font6" id="p9_t39" reading_order_no="38" segment_no="2" tag_type="table">Gradient Difference <a href="deeplearning_paper9.html#12">[7]</a></text>
<text top="132" left="300" width="22" height="7" font="font6" id="p9_t40" reading_order_no="39" segment_no="2" tag_type="table">0.4908</text>
<text top="131" left="324" width="7" height="9" font="font9" id="p9_t41" reading_order_no="40" segment_no="2" tag_type="table">±</text>
<text top="132" left="334" width="14" height="7" font="font6" id="p9_t42" reading_order_no="41" segment_no="2" tag_type="table">0.10</text>
<text top="132" left="361" width="22" height="7" font="font6" id="p9_t43" reading_order_no="42" segment_no="2" tag_type="table">0.6838</text>
<text top="131" left="385" width="7" height="9" font="font9" id="p9_t44" reading_order_no="43" segment_no="2" tag_type="table">±</text>
<text top="132" left="395" width="14" height="7" font="font6" id="p9_t45" reading_order_no="44" segment_no="2" tag_type="table">0.07</text>
<text top="132" left="421" width="26" height="7" font="font6" id="p9_t46" reading_order_no="45" segment_no="2" tag_type="table">10.8074</text>
<text top="131" left="450" width="7" height="9" font="font9" id="p9_t47" reading_order_no="46" segment_no="2" tag_type="table">±</text>
<text top="132" left="459" width="14" height="7" font="font6" id="p9_t48" reading_order_no="47" segment_no="2" tag_type="table">1.04</text>
<text top="141" left="136" width="63" height="7" font="font6" id="p9_t49" reading_order_no="48" segment_no="2" tag_type="table">VGG-19 MSE <a href="deeplearning_paper9.html#13">[61]</a></text>
<text top="141" left="300" width="22" height="7" font="font6" id="p9_t50" reading_order_no="49" segment_no="2" tag_type="table">0.5364</text>
<text top="140" left="324" width="7" height="9" font="font9" id="p9_t51" reading_order_no="50" segment_no="2" tag_type="table">±</text>
<text top="141" left="334" width="14" height="7" font="font6" id="p9_t52" reading_order_no="51" segment_no="2" tag_type="table">0.08</text>
<text top="141" left="361" width="22" height="7" font="font6" id="p9_t53" reading_order_no="52" segment_no="2" tag_type="table">0.6403</text>
<text top="140" left="385" width="7" height="9" font="font9" id="p9_t54" reading_order_no="53" segment_no="2" tag_type="table">±</text>
<text top="141" left="395" width="14" height="7" font="font6" id="p9_t55" reading_order_no="54" segment_no="2" tag_type="table">0.07</text>
<text top="141" left="421" width="26" height="7" font="font6" id="p9_t56" reading_order_no="55" segment_no="2" tag_type="table">11.4350</text>
<text top="140" left="450" width="7" height="9" font="font9" id="p9_t57" reading_order_no="56" segment_no="2" tag_type="table">±</text>
<text top="141" left="459" width="14" height="7" font="font6" id="p9_t58" reading_order_no="57" segment_no="2" tag_type="table">0.97</text>
<text top="150" left="136" width="89" height="7" font="font6" id="p9_t59" reading_order_no="58" segment_no="2" tag_type="table">LPIPS v0.1 (VGG-16) <a href="deeplearning_paper9.html#13">[62]</a></text>
<text top="150" left="300" width="22" height="7" font="font6" id="p9_t60" reading_order_no="59" segment_no="2" tag_type="table">0.6053</text>
<text top="149" left="324" width="7" height="9" font="font9" id="p9_t61" reading_order_no="60" segment_no="2" tag_type="table">±</text>
<text top="150" left="334" width="14" height="7" font="font6" id="p9_t62" reading_order_no="61" segment_no="2" tag_type="table">0.09</text>
<text top="150" left="361" width="22" height="7" font="font6" id="p9_t63" reading_order_no="62" segment_no="2" tag_type="table">0.7566</text>
<text top="149" left="385" width="7" height="9" font="font9" id="p9_t64" reading_order_no="63" segment_no="2" tag_type="table">±</text>
<text top="150" left="395" width="14" height="7" font="font6" id="p9_t65" reading_order_no="64" segment_no="2" tag_type="table">0.06</text>
<text top="150" left="421" width="26" height="7" font="font6" id="p9_t66" reading_order_no="65" segment_no="2" tag_type="table">08.9861</text>
<text top="149" left="450" width="7" height="9" font="font9" id="p9_t67" reading_order_no="66" segment_no="2" tag_type="table">±</text>
<text top="150" left="459" width="14" height="7" font="font6" id="p9_t68" reading_order_no="67" segment_no="2" tag_type="table">0.72</text>
<text top="159" left="136" width="40" height="7" font="font6" id="p9_t69" reading_order_no="68" segment_no="2" tag_type="table">PieApp <a href="deeplearning_paper9.html#13">[63]</a></text>
<text top="159" left="300" width="22" height="7" font="font6" id="p9_t70" reading_order_no="69" segment_no="2" tag_type="table">0.6112</text>
<text top="158" left="324" width="7" height="9" font="font9" id="p9_t71" reading_order_no="70" segment_no="2" tag_type="table">±</text>
<text top="159" left="334" width="14" height="7" font="font6" id="p9_t72" reading_order_no="71" segment_no="2" tag_type="table">0.08</text>
<text top="159" left="361" width="22" height="7" font="font6" id="p9_t73" reading_order_no="72" segment_no="2" tag_type="table">0.7513</text>
<text top="158" left="385" width="7" height="9" font="font9" id="p9_t74" reading_order_no="73" segment_no="2" tag_type="table">±</text>
<text top="159" left="395" width="14" height="7" font="font6" id="p9_t75" reading_order_no="74" segment_no="2" tag_type="table">0.05</text>
<text top="159" left="421" width="26" height="7" font="font6" id="p9_t76" reading_order_no="75" segment_no="2" tag_type="table">08.8769</text>
<text top="158" left="450" width="7" height="9" font="font9" id="p9_t77" reading_order_no="76" segment_no="2" tag_type="table">±</text>
<text top="159" left="459" width="14" height="7" font="font6" id="p9_t78" reading_order_no="77" segment_no="2" tag_type="table">0.77</text>
<text top="168" left="136" width="38" height="7" font="font6" id="p9_t79" reading_order_no="78" segment_no="2" tag_type="table">DISTS <a href="deeplearning_paper9.html#13">[64]</a></text>
<text top="168" left="300" width="22" height="7" font="font6" id="p9_t80" reading_order_no="79" segment_no="2" tag_type="table">0.6272</text>
<text top="167" left="324" width="7" height="9" font="font9" id="p9_t81" reading_order_no="80" segment_no="2" tag_type="table">±</text>
<text top="168" left="334" width="14" height="7" font="font6" id="p9_t82" reading_order_no="81" segment_no="2" tag_type="table">0.09</text>
<text top="168" left="361" width="22" height="7" font="font6" id="p9_t83" reading_order_no="82" segment_no="2" tag_type="table">0.7592</text>
<text top="167" left="385" width="7" height="9" font="font9" id="p9_t84" reading_order_no="83" segment_no="2" tag_type="table">±</text>
<text top="168" left="395" width="14" height="7" font="font6" id="p9_t85" reading_order_no="84" segment_no="2" tag_type="table">0.06</text>
<text top="168" left="421" width="26" height="7" font="font6" id="p9_t86" reading_order_no="85" segment_no="2" tag_type="table">08.8064</text>
<text top="167" left="450" width="7" height="9" font="font9" id="p9_t87" reading_order_no="86" segment_no="2" tag_type="table">±</text>
<text top="168" left="459" width="14" height="7" font="font6" id="p9_t88" reading_order_no="87" segment_no="2" tag_type="table">0.67</text>
<text top="177" left="136" width="84" height="7" font="font6" id="p9_t89" reading_order_no="88" segment_no="2" tag_type="table">VGG-19 cosine similarity</text>
<text top="177" left="300" width="22" height="7" font="font6" id="p9_t90" reading_order_no="89" segment_no="2" tag_type="table">0.6404</text>
<text top="176" left="324" width="7" height="9" font="font9" id="p9_t91" reading_order_no="90" segment_no="2" tag_type="table">±</text>
<text top="177" left="334" width="14" height="7" font="font6" id="p9_t92" reading_order_no="91" segment_no="2" tag_type="table">0.08</text>
<text top="177" left="361" width="22" height="7" font="font6" id="p9_t93" reading_order_no="92" segment_no="2" tag_type="table">0.7506</text>
<text top="176" left="385" width="7" height="9" font="font9" id="p9_t94" reading_order_no="93" segment_no="2" tag_type="table">±</text>
<text top="177" left="395" width="14" height="7" font="font6" id="p9_t95" reading_order_no="94" segment_no="2" tag_type="table">0.06</text>
<text top="177" left="421" width="26" height="7" font="font6" id="p9_t96" reading_order_no="95" segment_no="2" tag_type="table">08.9538</text>
<text top="176" left="450" width="7" height="9" font="font9" id="p9_t97" reading_order_no="96" segment_no="2" tag_type="table">±</text>
<text top="177" left="459" width="14" height="7" font="font6" id="p9_t98" reading_order_no="97" segment_no="2" tag_type="table">0.72</text>
<text top="186" left="136" width="46" height="7" font="font6" id="p9_t99" reading_order_no="98" segment_no="2" tag_type="table">ST-MAD <a href="deeplearning_paper9.html#12">[33]</a></text>
<text top="186" left="300" width="22" height="7" font="font6" id="p9_t100" reading_order_no="99" segment_no="2" tag_type="table">0.3730</text>
<text top="185" left="324" width="7" height="9" font="font9" id="p9_t101" reading_order_no="100" segment_no="2" tag_type="table">±</text>
<text top="186" left="334" width="14" height="7" font="font6" id="p9_t102" reading_order_no="101" segment_no="2" tag_type="table">0.12</text>
<text top="186" left="361" width="22" height="7" font="font6" id="p9_t103" reading_order_no="102" segment_no="2" tag_type="table">0.6516</text>
<text top="185" left="385" width="7" height="9" font="font9" id="p9_t104" reading_order_no="103" segment_no="2" tag_type="table">±</text>
<text top="186" left="395" width="14" height="7" font="font6" id="p9_t105" reading_order_no="104" segment_no="2" tag_type="table">0.08</text>
<text top="186" left="421" width="26" height="7" font="font6" id="p9_t106" reading_order_no="105" segment_no="2" tag_type="table">10.3446</text>
<text top="185" left="450" width="7" height="9" font="font9" id="p9_t107" reading_order_no="106" segment_no="2" tag_type="table">±</text>
<text top="186" left="459" width="14" height="7" font="font6" id="p9_t108" reading_order_no="107" segment_no="2" tag_type="table">0.88</text>
<text top="195" left="136" width="39" height="7" font="font6" id="p9_t109" reading_order_no="108" segment_no="2" tag_type="table">VMAF <a href="deeplearning_paper9.html#13">[65]</a></text>
<text top="195" left="300" width="22" height="7" font="font6" id="p9_t110" reading_order_no="109" segment_no="2" tag_type="table">0.6003</text>
<text top="194" left="324" width="7" height="9" font="font9" id="p9_t111" reading_order_no="110" segment_no="2" tag_type="table">±</text>
<text top="195" left="334" width="14" height="7" font="font6" id="p9_t112" reading_order_no="111" segment_no="2" tag_type="table">0.09</text>
<text top="195" left="361" width="22" height="7" font="font6" id="p9_t113" reading_order_no="112" segment_no="2" tag_type="table">0.7462</text>
<text top="194" left="385" width="7" height="9" font="font9" id="p9_t114" reading_order_no="113" segment_no="2" tag_type="table">±</text>
<text top="195" left="395" width="14" height="7" font="font6" id="p9_t115" reading_order_no="114" segment_no="2" tag_type="table">0.06</text>
<text top="195" left="421" width="26" height="7" font="font6" id="p9_t116" reading_order_no="115" segment_no="2" tag_type="table">09.3609</text>
<text top="194" left="450" width="7" height="9" font="font9" id="p9_t117" reading_order_no="116" segment_no="2" tag_type="table">±</text>
<text top="195" left="459" width="14" height="7" font="font6" id="p9_t118" reading_order_no="117" segment_no="2" tag_type="table">0.73</text>
<text top="204" left="136" width="50" height="7" font="font6" id="p9_t119" reading_order_no="118" segment_no="2" tag_type="table">BRISQUE <a href="deeplearning_paper9.html#13">[66]</a></text>
<text top="204" left="300" width="22" height="7" font="font6" id="p9_t120" reading_order_no="119" segment_no="2" tag_type="table">0.0905</text>
<text top="203" left="324" width="7" height="9" font="font9" id="p9_t121" reading_order_no="120" segment_no="2" tag_type="table">±</text>
<text top="204" left="334" width="14" height="7" font="font6" id="p9_t122" reading_order_no="121" segment_no="2" tag_type="table">0.11</text>
<text top="204" left="361" width="22" height="7" font="font6" id="p9_t123" reading_order_no="122" segment_no="2" tag_type="table">0.0942</text>
<text top="203" left="385" width="7" height="9" font="font9" id="p9_t124" reading_order_no="123" segment_no="2" tag_type="table">±</text>
<text top="204" left="395" width="14" height="7" font="font6" id="p9_t125" reading_order_no="124" segment_no="2" tag_type="table">0.11</text>
<text top="204" left="421" width="26" height="7" font="font6" id="p9_t126" reading_order_no="125" segment_no="2" tag_type="table">13.8893</text>
<text top="203" left="450" width="7" height="9" font="font9" id="p9_t127" reading_order_no="126" segment_no="2" tag_type="table">±</text>
<text top="204" left="459" width="14" height="7" font="font6" id="p9_t128" reading_order_no="127" segment_no="2" tag_type="table">1.27</text>
<text top="213" left="136" width="35" height="7" font="font6" id="p9_t129" reading_order_no="128" segment_no="2" tag_type="table">NIQE <a href="deeplearning_paper9.html#13">[67]</a></text>
<text top="213" left="300" width="22" height="7" font="font6" id="p9_t130" reading_order_no="129" segment_no="2" tag_type="table">0.0819</text>
<text top="212" left="324" width="7" height="9" font="font9" id="p9_t131" reading_order_no="130" segment_no="2" tag_type="table">±</text>
<text top="213" left="334" width="14" height="7" font="font6" id="p9_t132" reading_order_no="131" segment_no="2" tag_type="table">0.12</text>
<text top="213" left="361" width="22" height="7" font="font6" id="p9_t133" reading_order_no="132" segment_no="2" tag_type="table">0.0698</text>
<text top="212" left="385" width="7" height="9" font="font9" id="p9_t134" reading_order_no="133" segment_no="2" tag_type="table">±</text>
<text top="213" left="395" width="14" height="7" font="font6" id="p9_t135" reading_order_no="134" segment_no="2" tag_type="table">0.12</text>
<text top="213" left="421" width="26" height="7" font="font6" id="p9_t136" reading_order_no="135" segment_no="2" tag_type="table">15.6844</text>
<text top="212" left="450" width="7" height="9" font="font9" id="p9_t137" reading_order_no="136" segment_no="2" tag_type="table">±</text>
<text top="213" left="459" width="14" height="7" font="font6" id="p9_t138" reading_order_no="137" segment_no="2" tag_type="table">1.09</text>
<text top="222" left="136" width="152" height="7" font="font6" id="p9_t139" reading_order_no="138" segment_no="2" tag_type="table">Inception Score (Entropy of Conditional only)</text>
<text top="222" left="300" width="22" height="7" font="font6" id="p9_t140" reading_order_no="139" segment_no="2" tag_type="table">0.0828</text>
<text top="221" left="324" width="7" height="9" font="font9" id="p9_t141" reading_order_no="140" segment_no="2" tag_type="table">±</text>
<text top="222" left="334" width="14" height="7" font="font6" id="p9_t142" reading_order_no="141" segment_no="2" tag_type="table">0.11</text>
<text top="222" left="361" width="22" height="7" font="font6" id="p9_t143" reading_order_no="142" segment_no="2" tag_type="table">0.0458</text>
<text top="221" left="385" width="7" height="9" font="font9" id="p9_t144" reading_order_no="143" segment_no="2" tag_type="table">±</text>
<text top="222" left="395" width="14" height="7" font="font6" id="p9_t145" reading_order_no="144" segment_no="2" tag_type="table">0.10</text>
<text top="222" left="421" width="26" height="7" font="font6" id="p9_t146" reading_order_no="145" segment_no="2" tag_type="table">15.4043</text>
<text top="221" left="450" width="7" height="9" font="font9" id="p9_t147" reading_order_no="146" segment_no="2" tag_type="table">±</text>
<text top="222" left="459" width="14" height="7" font="font6" id="p9_t148" reading_order_no="147" segment_no="2" tag_type="table">1.22</text>
<text top="231" left="136" width="69" height="7" font="font6" id="p9_t149" reading_order_no="148" segment_no="2" tag_type="table">Video BLIINDS <a href="deeplearning_paper9.html#12">[36]</a></text>
<text top="231" left="300" width="22" height="7" font="font6" id="p9_t150" reading_order_no="149" segment_no="2" tag_type="table">0.4072</text>
<text top="230" left="324" width="7" height="9" font="font9" id="p9_t151" reading_order_no="150" segment_no="2" tag_type="table">±</text>
<text top="231" left="334" width="14" height="7" font="font6" id="p9_t152" reading_order_no="151" segment_no="2" tag_type="table">0.10</text>
<text top="231" left="361" width="22" height="7" font="font6" id="p9_t153" reading_order_no="152" segment_no="2" tag_type="table">0.6200</text>
<text top="230" left="385" width="7" height="9" font="font9" id="p9_t154" reading_order_no="153" segment_no="2" tag_type="table">±</text>
<text top="231" left="395" width="14" height="7" font="font6" id="p9_t155" reading_order_no="154" segment_no="2" tag_type="table">0.10</text>
<text top="231" left="421" width="26" height="7" font="font6" id="p9_t156" reading_order_no="155" segment_no="2" tag_type="table">12.4202</text>
<text top="230" left="450" width="7" height="9" font="font9" id="p9_t157" reading_order_no="156" segment_no="2" tag_type="table">±</text>
<text top="231" left="459" width="14" height="7" font="font6" id="p9_t158" reading_order_no="157" segment_no="2" tag_type="table">1.14</text>
<text top="240" left="136" width="40" height="7" font="font6" id="p9_t159" reading_order_no="158" segment_no="2" tag_type="table">NSTSS <a href="deeplearning_paper9.html#13">[68]</a></text>
<text top="240" left="300" width="22" height="7" font="font6" id="p9_t160" reading_order_no="159" segment_no="2" tag_type="table">0.5798</text>
<text top="239" left="324" width="7" height="9" font="font9" id="p9_t161" reading_order_no="160" segment_no="2" tag_type="table">±</text>
<text top="240" left="334" width="14" height="7" font="font6" id="p9_t162" reading_order_no="161" segment_no="2" tag_type="table">0.09</text>
<text top="240" left="361" width="22" height="7" font="font6" id="p9_t163" reading_order_no="162" segment_no="2" tag_type="table">0.5900</text>
<text top="239" left="385" width="7" height="9" font="font9" id="p9_t164" reading_order_no="163" segment_no="2" tag_type="table">±</text>
<text top="240" left="395" width="14" height="7" font="font6" id="p9_t165" reading_order_no="164" segment_no="2" tag_type="table">0.09</text>
<text top="240" left="421" width="26" height="7" font="font6" id="p9_t166" reading_order_no="165" segment_no="2" tag_type="table">11.3086</text>
<text top="239" left="450" width="7" height="9" font="font9" id="p9_t167" reading_order_no="166" segment_no="2" tag_type="table">±</text>
<text top="240" left="459" width="14" height="7" font="font6" id="p9_t168" reading_order_no="167" segment_no="2" tag_type="table">1.31</text>
<text top="249" left="136" width="44" height="7" font="font6" id="p9_t169" reading_order_no="168" segment_no="2" tag_type="table">TLVQM <a href="deeplearning_paper9.html#12">[38]</a></text>
<text top="249" left="300" width="22" height="7" font="font6" id="p9_t170" reading_order_no="169" segment_no="2" tag_type="table">0.6028</text>
<text top="248" left="324" width="7" height="9" font="font9" id="p9_t171" reading_order_no="170" segment_no="2" tag_type="table">±</text>
<text top="249" left="334" width="14" height="7" font="font6" id="p9_t172" reading_order_no="171" segment_no="2" tag_type="table">0.08</text>
<text top="249" left="361" width="22" height="7" font="font6" id="p9_t173" reading_order_no="172" segment_no="2" tag_type="table">0.6442</text>
<text top="248" left="385" width="7" height="9" font="font9" id="p9_t174" reading_order_no="173" segment_no="2" tag_type="table">±</text>
<text top="249" left="395" width="14" height="7" font="font6" id="p9_t175" reading_order_no="174" segment_no="2" tag_type="table">0.07</text>
<text top="249" left="421" width="26" height="7" font="font6" id="p9_t176" reading_order_no="175" segment_no="2" tag_type="table">10.3313</text>
<text top="248" left="450" width="7" height="9" font="font9" id="p9_t177" reading_order_no="176" segment_no="2" tag_type="table">±</text>
<text top="249" left="459" width="14" height="7" font="font6" id="p9_t178" reading_order_no="177" segment_no="2" tag_type="table">0.97</text>
<text top="258" left="136" width="36" height="7" font="font6" id="p9_t179" reading_order_no="178" segment_no="2" tag_type="table">VSFA <a href="deeplearning_paper9.html#12">[39]</a></text>
<text top="258" left="300" width="22" height="7" font="font6" id="p9_t180" reading_order_no="179" segment_no="2" tag_type="table">0.6371</text>
<text top="257" left="324" width="7" height="9" font="font9" id="p9_t181" reading_order_no="180" segment_no="2" tag_type="table">±</text>
<text top="258" left="334" width="14" height="7" font="font6" id="p9_t182" reading_order_no="181" segment_no="2" tag_type="table">0.09</text>
<text top="258" left="361" width="22" height="7" font="font6" id="p9_t183" reading_order_no="182" segment_no="2" tag_type="table">0.6504</text>
<text top="257" left="385" width="7" height="9" font="font9" id="p9_t184" reading_order_no="183" segment_no="2" tag_type="table">±</text>
<text top="258" left="395" width="14" height="7" font="font6" id="p9_t185" reading_order_no="184" segment_no="2" tag_type="table">0.08</text>
<text top="258" left="421" width="26" height="7" font="font6" id="p9_t186" reading_order_no="185" segment_no="2" tag_type="table">10.7497</text>
<text top="257" left="450" width="7" height="9" font="font9" id="p9_t187" reading_order_no="186" segment_no="2" tag_type="table">±</text>
<text top="258" left="459" width="14" height="7" font="font6" id="p9_t188" reading_order_no="187" segment_no="2" tag_type="table">1.12</text>
<text top="267" left="136" width="128" height="7" font="font6" id="p9_t189" reading_order_no="188" segment_no="2" tag_type="table">Baseline - SSA features - 3D ConvNet</text>
<text top="267" left="300" width="22" height="7" font="font6" id="p9_t190" reading_order_no="189" segment_no="2" tag_type="table">0.4180</text>
<text top="266" left="324" width="7" height="9" font="font9" id="p9_t191" reading_order_no="190" segment_no="2" tag_type="table">±</text>
<text top="267" left="334" width="14" height="7" font="font6" id="p9_t192" reading_order_no="191" segment_no="2" tag_type="table">0.10</text>
<text top="267" left="361" width="22" height="7" font="font6" id="p9_t193" reading_order_no="192" segment_no="2" tag_type="table">0.4570</text>
<text top="266" left="385" width="7" height="9" font="font9" id="p9_t194" reading_order_no="193" segment_no="2" tag_type="table">±</text>
<text top="267" left="395" width="14" height="7" font="font6" id="p9_t195" reading_order_no="194" segment_no="2" tag_type="table">0.09</text>
<text top="267" left="421" width="26" height="7" font="font6" id="p9_t196" reading_order_no="195" segment_no="2" tag_type="table">12.4736</text>
<text top="266" left="450" width="7" height="9" font="font9" id="p9_t197" reading_order_no="196" segment_no="2" tag_type="table">±</text>
<text top="267" left="459" width="14" height="7" font="font6" id="p9_t198" reading_order_no="197" segment_no="2" tag_type="table">1.07</text>
<text top="276" left="136" width="121" height="7" font="font6" id="p9_t199" reading_order_no="198" segment_no="2" tag_type="table">Baseline - SSA features - ResNet-50</text>
<text top="276" left="300" width="22" height="7" font="font6" id="p9_t200" reading_order_no="199" segment_no="2" tag_type="table">0.7272</text>
<text top="275" left="324" width="7" height="9" font="font9" id="p9_t201" reading_order_no="200" segment_no="2" tag_type="table">±</text>
<text top="276" left="334" width="14" height="7" font="font6" id="p9_t202" reading_order_no="201" segment_no="2" tag_type="table">0.06</text>
<text top="276" left="361" width="22" height="7" font="font6" id="p9_t203" reading_order_no="202" segment_no="2" tag_type="table">0.7496</text>
<text top="275" left="385" width="7" height="9" font="font9" id="p9_t204" reading_order_no="203" segment_no="2" tag_type="table">±</text>
<text top="276" left="395" width="14" height="7" font="font6" id="p9_t205" reading_order_no="204" segment_no="2" tag_type="table">0.06</text>
<text top="276" left="421" width="26" height="7" font="font6" id="p9_t206" reading_order_no="205" segment_no="2" tag_type="table">09.1168</text>
<text top="275" left="450" width="7" height="9" font="font9" id="p9_t207" reading_order_no="206" segment_no="2" tag_type="table">±</text>
<text top="276" left="459" width="14" height="7" font="font6" id="p9_t208" reading_order_no="207" segment_no="2" tag_type="table">0.76</text>
<text top="285" left="136" width="72" height="7" font="font6" id="p9_t209" reading_order_no="208" segment_no="2" tag_type="table">Our Model - VGG-19</text>
<text top="285" left="300" width="22" height="7" font="font6" id="p9_t210" reading_order_no="209" segment_no="2" tag_type="table">0.7352</text>
<text top="284" left="324" width="7" height="9" font="font9" id="p9_t211" reading_order_no="210" segment_no="2" tag_type="table">±</text>
<text top="285" left="334" width="14" height="7" font="font6" id="p9_t212" reading_order_no="211" segment_no="2" tag_type="table">0.07</text>
<text top="285" left="361" width="22" height="7" font="font6" id="p9_t213" reading_order_no="212" segment_no="2" tag_type="table">0.8023</text>
<text top="284" left="385" width="7" height="9" font="font9" id="p9_t214" reading_order_no="213" segment_no="2" tag_type="table">±</text>
<text top="285" left="395" width="14" height="7" font="font6" id="p9_t215" reading_order_no="214" segment_no="2" tag_type="table">0.05</text>
<text top="285" left="421" width="26" height="7" font="font6" id="p9_t216" reading_order_no="215" segment_no="2" tag_type="table">08.1906</text>
<text top="284" left="450" width="7" height="9" font="font9" id="p9_t217" reading_order_no="216" segment_no="2" tag_type="table">±</text>
<text top="285" left="459" width="14" height="7" font="font6" id="p9_t218" reading_order_no="217" segment_no="2" tag_type="table">0.85</text>
<text top="294" left="136" width="85" height="7" font="font6" id="p9_t219" reading_order_no="218" segment_no="2" tag_type="table">Our Model - Inception-v3</text>
<text top="294" left="300" width="22" height="7" font="font6" id="p9_t220" reading_order_no="219" segment_no="2" tag_type="table">0.7976</text>
<text top="293" left="324" width="7" height="9" font="font9" id="p9_t221" reading_order_no="220" segment_no="2" tag_type="table">±</text>
<text top="294" left="334" width="14" height="7" font="font6" id="p9_t222" reading_order_no="221" segment_no="2" tag_type="table">0.06</text>
<text top="294" left="361" width="22" height="7" font="font6" id="p9_t223" reading_order_no="222" segment_no="2" tag_type="table">0.8363</text>
<text top="293" left="385" width="7" height="9" font="font9" id="p9_t224" reading_order_no="223" segment_no="2" tag_type="table">±</text>
<text top="294" left="395" width="14" height="7" font="font6" id="p9_t225" reading_order_no="224" segment_no="2" tag_type="table">0.04</text>
<text top="294" left="421" width="26" height="7" font="font6" id="p9_t226" reading_order_no="225" segment_no="2" tag_type="table">07.5792</text>
<text top="293" left="450" width="7" height="9" font="font9" id="p9_t227" reading_order_no="226" segment_no="2" tag_type="table">±</text>
<text top="294" left="459" width="14" height="7" font="font6" id="p9_t228" reading_order_no="227" segment_no="2" tag_type="table">0.87</text>
<text top="303" left="136" width="78" height="7" font="font6" id="p9_t229" reading_order_no="228" segment_no="2" tag_type="table">Our Model - ResNet-50</text>
<text top="303" left="300" width="22" height="7" font="font28" id="p9_t230" reading_order_no="229" segment_no="2" tag_type="table">0.8268</text>
<text top="302" left="324" width="7" height="9" font="font9" id="p9_t231" reading_order_no="230" segment_no="2" tag_type="table">±</text>
<text top="303" left="334" width="14" height="7" font="font28" id="p9_t232" reading_order_no="231" segment_no="2" tag_type="table">0.04</text>
<text top="303" left="360" width="22" height="7" font="font28" id="p9_t233" reading_order_no="232" segment_no="2" tag_type="table">0.8626</text>
<text top="302" left="385" width="7" height="9" font="font9" id="p9_t234" reading_order_no="233" segment_no="2" tag_type="table">±</text>
<text top="303" left="395" width="14" height="7" font="font28" id="p9_t235" reading_order_no="234" segment_no="2" tag_type="table">0.03</text>
<text top="303" left="421" width="26" height="7" font="font28" id="p9_t236" reading_order_no="235" segment_no="2" tag_type="table">06.9854</text>
<text top="302" left="450" width="7" height="9" font="font9" id="p9_t237" reading_order_no="236" segment_no="2" tag_type="table">±</text>
<text top="303" left="460" width="14" height="7" font="font28" id="p9_t238" reading_order_no="237" segment_no="2" tag_type="table">0.68</text>
<text top="335" left="49" width="251" height="9" font="font5" id="p9_t239" reading_order_no="238" segment_no="3" tag_type="text">RMSE, a non-linear function is fitted to predict the MOS</text>
<text top="347" left="49" width="251" height="9" font="font5" id="p9_t240" reading_order_no="239" segment_no="3" tag_type="text">from the objective scores for objective measures that are not</text>
<text top="359" left="49" width="251" height="9" font="font5" id="p9_t241" reading_order_no="240" segment_no="3" tag_type="text">trained on our database. All the results are obtained by splitting</text>
<text top="371" left="49" width="251" height="9" font="font5" id="p9_t242" reading_order_no="241" segment_no="3" tag_type="text">the dataset into training and testing in the ratio 80:20 over</text>
<text top="383" left="49" width="251" height="9" font="font5" id="p9_t243" reading_order_no="242" segment_no="3" tag_type="text">100 iterations and computing the median performance. For</text>
<text top="395" left="49" width="251" height="9" font="font5" id="p9_t244" reading_order_no="243" segment_no="3" tag_type="text">models that require dimensionality reduction, the principal</text>
<text top="407" left="49" width="251" height="9" font="font5" id="p9_t245" reading_order_no="244" segment_no="3" tag_type="text">components are determined on the respective training sets.</text>
<text top="419" left="49" width="251" height="9" font="font5" id="p9_t246" reading_order_no="245" segment_no="3" tag_type="text">For a fair comparison of measures that require no training</text>
<text top="431" left="49" width="251" height="9" font="font5" id="p9_t247" reading_order_no="246" segment_no="3" tag_type="text">on our database, we evaluate the performance measures in the</text>
<text top="443" left="49" width="165" height="9" font="font5" id="p9_t248" reading_order_no="247" segment_no="3" tag_type="text">corresponding test sets of each iteration.</text>
<text top="461" left="59" width="45" height="9" font="font8" id="p9_t249" reading_order_no="248" segment_no="5" tag_type="text">5) Results:</text>
<text top="460" left="111" width="189" height="9" font="font5" id="p9_t250" reading_order_no="249" segment_no="5" tag_type="text">The results of our experiments are presented</text>
<text top="472" left="49" width="251" height="9" font="font5" id="p9_t251" reading_order_no="250" segment_no="5" tag_type="text">in Table <a href="deeplearning_paper9.html#9">III. </a>We only show the magnitude of PLCC and</text>
<text top="484" left="49" width="251" height="9" font="font5" id="p9_t252" reading_order_no="251" segment_no="5" tag_type="text">SROCC in the table. We see that among the FR measures,</text>
<text top="496" left="49" width="251" height="9" font="font5" id="p9_t253" reading_order_no="252" segment_no="5" tag_type="text">VGG-19 cosine similarity achieves the best performance in</text>
<text top="508" left="49" width="251" height="9" font="font5" id="p9_t254" reading_order_no="253" segment_no="5" tag_type="text">terms of correlation with the subjective scores. We believe that</text>
<text top="520" left="49" width="251" height="9" font="font5" id="p9_t255" reading_order_no="254" segment_no="5" tag_type="text">the normalization implicit in the computation of the cosine</text>
<text top="532" left="49" width="251" height="9" font="font5" id="p9_t256" reading_order_no="255" segment_no="5" tag_type="text">similarity makes it perform better than VGG-19 MSE. We</text>
<text top="544" left="49" width="251" height="9" font="font5" id="p9_t257" reading_order_no="256" segment_no="5" tag_type="text">notice similar performance of SSIM and MS-SSIM measures,</text>
<text top="556" left="49" width="251" height="9" font="font5" id="p9_t258" reading_order_no="257" segment_no="5" tag_type="text">perhaps due to the lower resolution of videos in our database.</text>
<text top="568" left="49" width="251" height="9" font="font5" id="p9_t259" reading_order_no="258" segment_no="5" tag_type="text">NR image QA indices and Inception Score seem to correlate</text>
<text top="580" left="49" width="251" height="9" font="font5" id="p9_t260" reading_order_no="259" segment_no="5" tag_type="text">poorly with human perception while video QA indices perform</text>
<text top="592" left="49" width="101" height="9" font="font5" id="p9_t261" reading_order_no="260" segment_no="5" tag_type="text">better than these indices.</text>
<text top="608" left="59" width="241" height="9" font="font5" id="p9_t262" reading_order_no="261" segment_no="8" tag_type="text">On the other hand, deep features of pre-trained networks ex-</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p9_t263" reading_order_no="262" segment_no="8" tag_type="text">tracted from video frames tend to achieve better performance.</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p9_t264" reading_order_no="263" segment_no="8" tag_type="text">In particular, they outperform NR video QA indices such as</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p9_t265" reading_order_no="264" segment_no="8" tag_type="text">Video BLIINDS, NSTSS, and TLVQM, which are also trained</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p9_t266" reading_order_no="265" segment_no="8" tag_type="text">on our database. We believe that the superior performance of</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p9_t267" reading_order_no="266" segment_no="8" tag_type="text">deep features over other QA methods is due to their ability to</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p9_t268" reading_order_no="267" segment_no="8" tag_type="text">extract high level features. We note that the poor performance</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p9_t269" reading_order_no="268" segment_no="8" tag_type="text">of the Conv 3D model may be attributed to the training of</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p9_t270" reading_order_no="269" segment_no="8" tag_type="text">this model on action recognition. Thus, the resulting features</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p9_t271" reading_order_no="270" segment_no="8" tag_type="text">may not capture the spatial distortions in video frames. Finally,</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p9_t272" reading_order_no="271" segment_no="8" tag_type="text">we observe that our model based on MCS and RFD features</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p9_t273" reading_order_no="272" segment_no="8" tag_type="text">performs significantly better than all other measures. We see</text>
<text top="517" left="352" width="14" height="7" font="font29" id="p9_t274" reading_order_no="280" segment_no="4" tag_type="figure">SSA</text>
<text top="517" left="397" width="15" height="7" font="font29" id="p9_t275" reading_order_no="281" segment_no="4" tag_type="figure">MCS</text>
<text top="517" left="442" width="14" height="7" font="font29" id="p9_t276" reading_order_no="282" segment_no="4" tag_type="figure">RFD</text>
<text top="517" left="478" width="32" height="7" font="font29" id="p9_t277" reading_order_no="283" segment_no="4" tag_type="figure">SSA+RFD</text>
<text top="517" left="522" width="34" height="7" font="font29" id="p9_t278" reading_order_no="284" segment_no="4" tag_type="figure">MCS+RFD</text>
<text top="527" left="422" width="49" height="7" font="font30" id="p9_t279" reading_order_no="285" segment_no="4" tag_type="figure">Features used</text>
<text top="510" left="323" width="10" height="7" font="font29" id="p9_t280" reading_order_no="279" segment_no="4" tag_type="figure">0.5</text>
<text top="478" left="323" width="10" height="7" font="font29" id="p9_t281" reading_order_no="273" segment_no="4" tag_type="figure">0.6</text>
<text top="445" left="323" width="10" height="7" font="font29" id="p9_t282" reading_order_no="274" segment_no="4" tag_type="figure">0.7</text>
<text top="413" left="323" width="10" height="7" font="font29" id="p9_t283" reading_order_no="275" segment_no="4" tag_type="figure">0.8</text>
<text top="380" left="323" width="10" height="7" font="font29" id="p9_t284" reading_order_no="276" segment_no="4" tag_type="figure">0.9</text>
<text top="348" left="329" width="4" height="7" font="font29" id="p9_t285" reading_order_no="277" segment_no="4" tag_type="figure">1</text>
<text top="432" left="318" width="0" height="7" font="font31" id="p9_t286" reading_order_no="278" segment_no="4" tag_type="figure">SROCC</text>
<text top="344" left="346" width="150" height="7" font="font32" id="p9_t287" reading_order_no="286" segment_no="4" tag_type="figure">SSA: Simple Spatial Averaging of frame features</text>
<text top="353" left="346" width="165" height="7" font="font32" id="p9_t288" reading_order_no="287" segment_no="4" tag_type="figure">MCS: Motion-compensated Cosine Similarity features</text>
<text top="362" left="346" width="129" height="7" font="font32" id="p9_t289" reading_order_no="288" segment_no="4" tag_type="figure">RFD: Rescaled Frame Difference features</text>
<text top="540" left="312" width="251" height="9" font="font5" id="p9_t290" reading_order_no="289" segment_no="6" tag_type="text">Fig. 7: Evaluation of ablation models. ResNet-50 features are</text>
<text top="552" left="312" width="84" height="9" font="font5" id="p9_t291" reading_order_no="290" segment_no="6" tag_type="text">used for all variants.</text>
<text top="587" left="312" width="251" height="9" font="font5" id="p9_t292" reading_order_no="291" segment_no="7" tag_type="text">an improved performance in terms of all evaluation measures.</text>
<text top="599" left="312" width="251" height="9" font="font5" id="p9_t293" reading_order_no="292" segment_no="7" tag_type="text">The lower standard deviation across splits in the performance</text>
<text top="611" left="312" width="251" height="9" font="font5" id="p9_t294" reading_order_no="293" segment_no="7" tag_type="text">numbers when compared to other methods also suggests that</text>
<text top="623" left="312" width="251" height="9" font="font5" id="p9_t295" reading_order_no="294" segment_no="7" tag_type="text">our model consistently achieves excellent performance across</text>
<text top="635" left="312" width="24" height="9" font="font5" id="p9_t296" reading_order_no="295" segment_no="7" tag_type="text">splits.</text>
<text top="664" left="312" width="164" height="9" font="font8" id="p9_t297" reading_order_no="296" segment_no="9" tag_type="title">B. Ablations and Extended Experiments</text>
<text top="679" left="322" width="171" height="9" font="font8" id="p9_t298" reading_order_no="297" segment_no="10" tag_type="text">1) Contribution of individual components:</text>
<text top="679" left="498" width="65" height="9" font="font5" id="p9_t299" reading_order_no="298" segment_no="10" tag_type="text">Since our model</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p9_t300" reading_order_no="299" segment_no="10" tag_type="text">involves two components, the MCS and RFD features, we</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p9_t301" reading_order_no="300" segment_no="10" tag_type="text">study the impact of each of the components in Fig. <a href="deeplearning_paper9.html#9">7. </a>We</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p9_t302" reading_order_no="301" segment_no="10" tag_type="text">perform this experiment on our model trained on ResNet-50</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p9_t303" reading_order_no="302" segment_no="10" tag_type="text">features, which achieved the best performance. We note that</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p9_t304" reading_order_no="303" segment_no="10" tag_type="text">RFD features perform better than frame features. Further, we</text>
</page>
<page number="10" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p10_t1" reading_order_no="0" segment_no="0" tag_type="text">10</text>
<text top="252" left="49" width="514" height="9" font="font5" id="p10_t2" reading_order_no="1" segment_no="1" tag_type="text">Fig. 8: This figure highlights the shortcomings of full reference measures (Section <a href="deeplearning_paper9.html#10">V-B2). </a>Two examples of ground truth and</text>
<text top="264" left="49" width="514" height="9" font="font5" id="p10_t3" reading_order_no="2" segment_no="1" tag_type="text">predicted videos are shown. We show every second frame in the video sequence. The first 2 frames correspond to the context</text>
<text top="276" left="49" width="514" height="9" font="font5" id="p10_t4" reading_order_no="3" segment_no="1" tag_type="text">and the next 8 frames are predicted. The scores of (Predicted Video 1, Predicted Video 2) according to different measures of</text>
<text top="288" left="49" width="514" height="9" font="font5" id="p10_t5" reading_order_no="4" segment_no="1" tag_type="text">quality are MSE: (344, 4731), MS-SSIM: (0.9435,0.5586), Cosine Similarity: (0.8860,0.5028), Our Model: (57.40,62.61). The</text>
<text top="300" left="49" width="361" height="9" font="font5" id="p10_t6" reading_order_no="5" segment_no="1" tag_type="text">corresponding MOS is (46.54,71.65). The videos can be viewed on our project website.</text>
<text top="331" left="49" width="251" height="9" font="font5" id="p10_t7" reading_order_no="6" segment_no="2" tag_type="text">TABLE IV: Evaluation of Objective Measures of Quality</text>
<text top="343" left="49" width="251" height="9" font="font5" id="p10_t8" reading_order_no="7" segment_no="2" tag_type="text">on stochastically predicted videos. Only SROCC values are</text>
<text top="355" left="49" width="30" height="9" font="font5" id="p10_t9" reading_order_no="8" segment_no="2" tag_type="text">quoted.</text>
<text top="372" left="108" width="21" height="7" font="font6" id="p10_t10" reading_order_no="9" segment_no="4" tag_type="table">Metric</text>
<text top="372" left="213" width="26" height="7" font="font6" id="p10_t11" reading_order_no="10" segment_no="4" tag_type="table">SROCC</text>
<text top="382" left="108" width="84" height="7" font="font6" id="p10_t12" reading_order_no="11" segment_no="4" tag_type="table">VGG-19 cosine similarity</text>
<text top="382" left="215" width="22" height="7" font="font6" id="p10_t13" reading_order_no="12" segment_no="4" tag_type="table">0.4549</text>
<text top="391" left="108" width="23" height="7" font="font6" id="p10_t14" reading_order_no="13" segment_no="4" tag_type="table">VMAF</text>
<text top="391" left="215" width="22" height="7" font="font6" id="p10_t15" reading_order_no="14" segment_no="4" tag_type="table">0.3758</text>
<text top="400" left="108" width="53" height="7" font="font6" id="p10_t16" reading_order_no="15" segment_no="4" tag_type="table">Video BLIINDS</text>
<text top="400" left="215" width="22" height="7" font="font6" id="p10_t17" reading_order_no="16" segment_no="4" tag_type="table">0.6484</text>
<text top="409" left="108" width="36" height="7" font="font6" id="p10_t18" reading_order_no="17" segment_no="4" tag_type="table">VSFA <a href="deeplearning_paper9.html#12">[39]</a></text>
<text top="409" left="215" width="22" height="7" font="font6" id="p10_t19" reading_order_no="18" segment_no="4" tag_type="table">0.7165</text>
<text top="418" left="108" width="92" height="7" font="font6" id="p10_t20" reading_order_no="19" segment_no="4" tag_type="table">Baseline (SSA) - ResNet-50</text>
<text top="418" left="215" width="22" height="7" font="font6" id="p10_t21" reading_order_no="20" segment_no="4" tag_type="table">0.7011</text>
<text top="427" left="108" width="78" height="7" font="font6" id="p10_t22" reading_order_no="21" segment_no="4" tag_type="table">Our Model - ResNet-50</text>
<text top="427" left="215" width="22" height="7" font="font28" id="p10_t23" reading_order_no="22" segment_no="4" tag_type="table">0.7714</text>
<text top="462" left="49" width="251" height="9" font="font5" id="p10_t24" reading_order_no="23" segment_no="7" tag_type="text">see that the combination of the MCS and RFD features leads</text>
<text top="474" left="49" width="251" height="9" font="font5" id="p10_t25" reading_order_no="24" segment_no="7" tag_type="text">to a significant improvement in the performance. Even though</text>
<text top="486" left="49" width="251" height="9" font="font5" id="p10_t26" reading_order_no="25" segment_no="7" tag_type="text">the performance of SSA features is comparable to that of MCS</text>
<text top="498" left="49" width="251" height="9" font="font5" id="p10_t27" reading_order_no="26" segment_no="7" tag_type="text">features, MCS features in combination with RFD features</text>
<text top="510" left="49" width="251" height="9" font="font5" id="p10_t28" reading_order_no="27" segment_no="7" tag_type="text">perform better than SSA features combined with RFD features.</text>
<text top="522" left="49" width="251" height="9" font="font5" id="p10_t29" reading_order_no="28" segment_no="7" tag_type="text">Thus, to understand the complementary nature of different</text>
<text top="534" left="49" width="251" height="9" font="font5" id="p10_t30" reading_order_no="29" segment_no="7" tag_type="text">sets of features, we quantitatively analyze their dependence.</text>
<text top="546" left="49" width="251" height="9" font="font5" id="p10_t31" reading_order_no="30" segment_no="7" tag_type="text">In particular, we compute the distance correlation <a href="deeplearning_paper9.html#13">[73] </a>of SSA</text>
<text top="558" left="49" width="251" height="9" font="font5" id="p10_t32" reading_order_no="31" segment_no="7" tag_type="text">features with RFD features and compare it with the distance</text>
<text top="569" left="49" width="251" height="9" font="font5" id="p10_t33" reading_order_no="32" segment_no="7" tag_type="text">correlation of MCS features with RFD features. Distance</text>
<text top="581" left="49" width="251" height="9" font="font5" id="p10_t34" reading_order_no="33" segment_no="7" tag_type="text">correlation measures the dependence between random vectors</text>
<text top="593" left="49" width="251" height="9" font="font5" id="p10_t35" reading_order_no="34" segment_no="7" tag_type="text">(or features in our case). This is obtained by first computing</text>
<text top="605" left="49" width="251" height="9" font="font5" id="p10_t36" reading_order_no="35" segment_no="7" tag_type="text">pairwise Euclidean distances of samples belonging to each</text>
<text top="617" left="49" width="251" height="9" font="font5" id="p10_t37" reading_order_no="36" segment_no="7" tag_type="text">of the two feature types. A correlation coefficient is then</text>
<text top="629" left="49" width="221" height="9" font="font5" id="p10_t38" reading_order_no="37" segment_no="7" tag_type="text">computed between the resulting two sets of distances.</text>
<text top="642" left="59" width="241" height="9" font="font5" id="p10_t39" reading_order_no="38" segment_no="8" tag_type="text">We observed a distance correlation of 0.71 between the</text>
<text top="654" left="49" width="251" height="9" font="font5" id="p10_t40" reading_order_no="39" segment_no="8" tag_type="text">SSA and RFD features, while the correlation reduces to 0.57</text>
<text top="666" left="49" width="251" height="9" font="font5" id="p10_t41" reading_order_no="40" segment_no="8" tag_type="text">for MCS and RFD features. This probably explains why the</text>
<text top="678" left="49" width="251" height="9" font="font5" id="p10_t42" reading_order_no="41" segment_no="8" tag_type="text">combination of MCS and RFD features performs better than</text>
<text top="690" left="49" width="177" height="9" font="font5" id="p10_t43" reading_order_no="42" segment_no="8" tag_type="text">the combination of SSA and RFD features.</text>
<text top="703" left="59" width="157" height="9" font="font8" id="p10_t44" reading_order_no="43" segment_no="10" tag_type="text">2) Performance on stochastic videos:</text>
<text top="703" left="223" width="77" height="9" font="font5" id="p10_t45" reading_order_no="44" segment_no="10" tag_type="text">We now present a</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p10_t46" reading_order_no="45" segment_no="10" tag_type="text">couple of examples to support our argument in Section <a href="deeplearning_paper9.html#1">I</a></text>
<text top="727" left="49" width="251" height="9" font="font5" id="p10_t47" reading_order_no="46" segment_no="10" tag_type="text">that the inherent stochasticity of the future may reduce the</text>
<text top="739" left="49" width="251" height="9" font="font5" id="p10_t48" reading_order_no="47" segment_no="10" tag_type="text">efficiency of full reference measures. In Fig. <a href="deeplearning_paper9.html#10">8, </a>we show two</text>
<text top="331" left="312" width="251" height="9" font="font5" id="p10_t49" reading_order_no="48" segment_no="3" tag_type="text">TABLE V: Performance of our features with different regres-</text>
<text top="343" left="312" width="251" height="9" font="font5" id="p10_t50" reading_order_no="49" segment_no="3" tag_type="text">sion models. ResNet-50 features and 240 principal components</text>
<text top="355" left="312" width="100" height="9" font="font5" id="p10_t51" reading_order_no="50" segment_no="3" tag_type="text">are used for all variants.</text>
<text top="372" left="318" width="47" height="7" font="font6" id="p10_t52" reading_order_no="51" segment_no="5" tag_type="table">Model Variant</text>
<text top="372" left="464" width="26" height="7" font="font6" id="p10_t53" reading_order_no="52" segment_no="5" tag_type="table">SROCC</text>
<text top="372" left="501" width="20" height="7" font="font6" id="p10_t54" reading_order_no="53" segment_no="5" tag_type="table">PLCC</text>
<text top="372" left="533" width="22" height="7" font="font6" id="p10_t55" reading_order_no="54" segment_no="5" tag_type="table">RMSE</text>
<text top="382" left="318" width="59" height="7" font="font6" id="p10_t56" reading_order_no="55" segment_no="5" tag_type="table">Linear Regression</text>
<text top="381" left="466" width="22" height="7" font="font28" id="p10_t57" reading_order_no="56" segment_no="5" tag_type="table">0.8268</text>
<text top="381" left="500" width="22" height="7" font="font28" id="p10_t58" reading_order_no="57" segment_no="5" tag_type="table">0.8626</text>
<text top="381" left="532" width="22" height="7" font="font28" id="p10_t59" reading_order_no="58" segment_no="5" tag_type="table">6.9854</text>
<text top="391" left="318" width="118" height="7" font="font6" id="p10_t60" reading_order_no="59" segment_no="5" tag_type="table">Polynomial Regression (degree = 2)</text>
<text top="391" left="466" width="22" height="7" font="font6" id="p10_t61" reading_order_no="60" segment_no="5" tag_type="table">0.7502</text>
<text top="391" left="500" width="22" height="7" font="font6" id="p10_t62" reading_order_no="61" segment_no="5" tag_type="table">0.7599</text>
<text top="391" left="532" width="22" height="7" font="font6" id="p10_t63" reading_order_no="62" segment_no="5" tag_type="table">9.5022</text>
<text top="400" left="318" width="136" height="7" font="font6" id="p10_t64" reading_order_no="63" segment_no="5" tag_type="table">Support Vector Regression (linear kernel)</text>
<text top="399" left="466" width="22" height="7" font="font28" id="p10_t65" reading_order_no="64" segment_no="5" tag_type="table">0.8268</text>
<text top="400" left="500" width="22" height="7" font="font6" id="p10_t66" reading_order_no="65" segment_no="5" tag_type="table">0.8622</text>
<text top="400" left="532" width="22" height="7" font="font6" id="p10_t67" reading_order_no="66" segment_no="5" tag_type="table">6.9906</text>
<text top="408" left="318" width="127" height="7" font="font6" id="p10_t68" reading_order_no="67" segment_no="5" tag_type="table">Support Vector Regression (rbf kernel)</text>
<text top="408" left="466" width="22" height="7" font="font6" id="p10_t69" reading_order_no="68" segment_no="5" tag_type="table">0.7135</text>
<text top="408" left="500" width="22" height="7" font="font6" id="p10_t70" reading_order_no="69" segment_no="5" tag_type="table">0.7264</text>
<text top="408" left="532" width="22" height="7" font="font6" id="p10_t71" reading_order_no="70" segment_no="5" tag_type="table">11.969</text>
<text top="417" left="318" width="52" height="7" font="font6" id="p10_t72" reading_order_no="71" segment_no="5" tag_type="table">Neural Network</text>
<text top="417" left="466" width="22" height="7" font="font6" id="p10_t73" reading_order_no="72" segment_no="5" tag_type="table">0.7961</text>
<text top="417" left="500" width="22" height="7" font="font6" id="p10_t74" reading_order_no="73" segment_no="5" tag_type="table">0.8547</text>
<text top="417" left="532" width="22" height="7" font="font6" id="p10_t75" reading_order_no="74" segment_no="5" tag_type="table">8.3034</text>
<text top="451" left="312" width="251" height="9" font="font5" id="p10_t76" reading_order_no="75" segment_no="6" tag_type="text">examples of ground truth and predicted videos, along with the</text>
<text top="463" left="312" width="251" height="9" font="font5" id="p10_t77" reading_order_no="76" segment_no="6" tag_type="text">scores of various full reference measures and our model. In</text>
<text top="475" left="312" width="251" height="9" font="font5" id="p10_t78" reading_order_no="77" segment_no="6" tag_type="text">Predicted Video 1, we see the disappearance of the robotic</text>
<text top="487" left="312" width="251" height="9" font="font5" id="p10_t79" reading_order_no="78" segment_no="6" tag_type="text">arm, which is highly unnatural. The movement of the robotic</text>
<text top="499" left="312" width="251" height="9" font="font5" id="p10_t80" reading_order_no="79" segment_no="6" tag_type="text">arm in Predicted Video 2 is completely natural and of high</text>
<text top="511" left="312" width="251" height="9" font="font5" id="p10_t81" reading_order_no="80" segment_no="6" tag_type="text">quality, just that it is different from Ground Truth 2. From</text>
<text top="523" left="312" width="251" height="9" font="font5" id="p10_t82" reading_order_no="81" segment_no="6" tag_type="text">the scores shown, we see that all full reference measures fail</text>
<text top="535" left="312" width="251" height="9" font="font5" id="p10_t83" reading_order_no="82" segment_no="6" tag_type="text">to capture the quality of videos by indicating that Predicted</text>
<text top="547" left="312" width="251" height="9" font="font5" id="p10_t84" reading_order_no="83" segment_no="6" tag_type="text">Video 1 is of higher quality than Predicted Video 2, whereas</text>
<text top="559" left="312" width="251" height="9" font="font5" id="p10_t85" reading_order_no="84" segment_no="6" tag_type="text">our model is consistent with human opinion. Further, we</text>
<text top="571" left="312" width="251" height="9" font="font5" id="p10_t86" reading_order_no="85" segment_no="6" tag_type="text">evaluate various quality measures on stochastically predicted</text>
<text top="583" left="312" width="251" height="9" font="font5" id="p10_t87" reading_order_no="86" segment_no="6" tag_type="text">videos of our database in Table <a href="deeplearning_paper9.html#10">IV. </a>We observe that the</text>
<text top="595" left="312" width="251" height="9" font="font5" id="p10_t88" reading_order_no="87" segment_no="6" tag_type="text">performance of the full reference measures is much poorer</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p10_t89" reading_order_no="88" segment_no="6" tag_type="text">than the no reference measures. Thus we conclude that no</text>
<text top="619" left="312" width="251" height="9" font="font5" id="p10_t90" reading_order_no="89" segment_no="6" tag_type="text">reference measures are better equipped to measure quality of</text>
<text top="631" left="312" width="188" height="9" font="font5" id="p10_t91" reading_order_no="90" segment_no="6" tag_type="text">predicted videos than full reference measures.</text>
<text top="644" left="322" width="130" height="9" font="font8" id="p10_t92" reading_order_no="91" segment_no="9" tag_type="text">3) Different regression models:</text>
<text top="643" left="458" width="105" height="9" font="font5" id="p10_t93" reading_order_no="92" segment_no="9" tag_type="text">We also experiment with</text>
<text top="655" left="312" width="251" height="9" font="font5" id="p10_t94" reading_order_no="93" segment_no="9" tag_type="text">different regression models such as polynomial regression,</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p10_t95" reading_order_no="94" segment_no="9" tag_type="text">Support Vector Regression (SVR) and neural networks. For</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p10_t96" reading_order_no="95" segment_no="9" tag_type="text">SVR, we found that the linear kernel performs better than</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p10_t97" reading_order_no="96" segment_no="9" tag_type="text">radial basis function (rbf) or any other kernel. For the neural</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p10_t98" reading_order_no="97" segment_no="9" tag_type="text">network, we use two hidden layers with 150 and 50 units</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p10_t99" reading_order_no="98" segment_no="9" tag_type="text">respectively. We train the model with mean squared error</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p10_t100" reading_order_no="99" segment_no="9" tag_type="text">(MSE) loss and Adam optimizer with a learning rate of</text>
<text top="738" left="312" width="5" height="10" font="font10" id="p10_t101" reading_order_no="100" segment_no="9" tag_type="text">0</text>
<text top="738" left="317" width="3" height="10" font="font12" id="p10_t102" reading_order_no="101" segment_no="9" tag_type="text">.</text>
<text top="738" left="320" width="10" height="10" font="font10" id="p10_t103" reading_order_no="102" segment_no="9" tag_type="text">02</text>
<text top="739" left="330" width="233" height="9" font="font5" id="p10_t104" reading_order_no="103" segment_no="9" tag_type="text">. From the results shown in Table <a href="deeplearning_paper9.html#10">V, </a>we note that the</text>
</page>
<page number="11" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font33" size="10" family="Times" color="#000000"/>
	<fontspec id="font34" size="10" family="Times" color="#000000"/>
	<fontspec id="font35" size="6" family="Times" color="#000000"/>
<text top="26" left="556" width="7" height="6" font="font0" id="p11_t1" reading_order_no="0" segment_no="0" tag_type="text">11</text>
<text top="54" left="49" width="251" height="9" font="font5" id="p11_t2" reading_order_no="1" segment_no="1" tag_type="text">TABLE VI: Performance of our model with different number</text>
<text top="66" left="49" width="222" height="9" font="font5" id="p11_t3" reading_order_no="2" segment_no="1" tag_type="text">of principal components. ResNet-50 features are used.</text>
<text top="84" left="56" width="95" height="7" font="font6" id="p11_t4" reading_order_no="3" segment_no="3" tag_type="table">Number of PCA components</text>
<text top="84" left="163" width="8" height="7" font="font6" id="p11_t5" reading_order_no="4" segment_no="3" tag_type="table">40</text>
<text top="84" left="186" width="8" height="7" font="font6" id="p11_t6" reading_order_no="5" segment_no="3" tag_type="table">80</text>
<text top="84" left="208" width="12" height="7" font="font6" id="p11_t7" reading_order_no="6" segment_no="3" tag_type="table">120</text>
<text top="84" left="231" width="12" height="7" font="font6" id="p11_t8" reading_order_no="7" segment_no="3" tag_type="table">160</text>
<text top="84" left="254" width="12" height="7" font="font6" id="p11_t9" reading_order_no="8" segment_no="3" tag_type="table">200</text>
<text top="84" left="278" width="12" height="7" font="font6" id="p11_t10" reading_order_no="9" segment_no="3" tag_type="table">240</text>
<text top="93" left="56" width="53" height="7" font="font6" id="p11_t11" reading_order_no="10" segment_no="3" tag_type="table">Median SROCC</text>
<text top="93" left="160" width="14" height="7" font="font6" id="p11_t12" reading_order_no="11" segment_no="3" tag_type="table">0.79</text>
<text top="93" left="183" width="14" height="7" font="font6" id="p11_t13" reading_order_no="12" segment_no="3" tag_type="table">0.81</text>
<text top="93" left="207" width="14" height="7" font="font6" id="p11_t14" reading_order_no="13" segment_no="3" tag_type="table">0.82</text>
<text top="93" left="230" width="14" height="7" font="font6" id="p11_t15" reading_order_no="14" segment_no="3" tag_type="table">0.82</text>
<text top="93" left="253" width="14" height="7" font="font6" id="p11_t16" reading_order_no="15" segment_no="3" tag_type="table">0.82</text>
<text top="93" left="277" width="14" height="7" font="font6" id="p11_t17" reading_order_no="16" segment_no="3" tag_type="table">0.83</text>
<text top="285" left="82" width="8" height="14" font="font33" id="p11_t18" reading_order_no="25" segment_no="4" tag_type="figure">10</text>
<text top="285" left="111" width="8" height="14" font="font33" id="p11_t19" reading_order_no="26" segment_no="4" tag_type="figure">20</text>
<text top="285" left="139" width="8" height="14" font="font33" id="p11_t20" reading_order_no="27" segment_no="4" tag_type="figure">30</text>
<text top="285" left="168" width="8" height="14" font="font33" id="p11_t21" reading_order_no="28" segment_no="4" tag_type="figure">40</text>
<text top="285" left="196" width="8" height="14" font="font33" id="p11_t22" reading_order_no="29" segment_no="4" tag_type="figure">50</text>
<text top="285" left="225" width="8" height="14" font="font33" id="p11_t23" reading_order_no="30" segment_no="4" tag_type="figure">60</text>
<text top="285" left="253" width="8" height="14" font="font33" id="p11_t24" reading_order_no="31" segment_no="4" tag_type="figure">70</text>
<text top="285" left="282" width="8" height="14" font="font33" id="p11_t25" reading_order_no="32" segment_no="4" tag_type="figure">80</text>
<text top="294" left="135" width="101" height="14" font="font33" id="p11_t26" reading_order_no="33" segment_no="4" tag_type="figure">Percentage of videos for training</text>
<text top="262" left="62" width="10" height="14" font="font33" id="p11_t27" reading_order_no="24" segment_no="4" tag_type="figure">0.2</text>
<text top="229" left="62" width="10" height="14" font="font33" id="p11_t28" reading_order_no="23" segment_no="4" tag_type="figure">0.4</text>
<text top="197" left="62" width="10" height="14" font="font33" id="p11_t29" reading_order_no="22" segment_no="4" tag_type="figure">0.6</text>
<text top="164" left="62" width="10" height="14" font="font33" id="p11_t30" reading_order_no="21" segment_no="4" tag_type="figure">0.8</text>
<text top="131" left="62" width="10" height="14" font="font33" id="p11_t31" reading_order_no="20" segment_no="4" tag_type="figure">1.0</text>
<text top="205" left="58" width="0" height="14" font="font34" id="p11_t32" reading_order_no="19" segment_no="4" tag_type="figure">SR</text>
<text top="197" left="58" width="0" height="14" font="font34" id="p11_t33" reading_order_no="18" segment_no="4" tag_type="figure">OC</text>
<text top="188" left="58" width="0" height="14" font="font34" id="p11_t34" reading_order_no="17" segment_no="4" tag_type="figure">C</text>
<text top="123" left="99" width="77" height="14" font="font33" id="p11_t35" reading_order_no="34" segment_no="4" tag_type="figure">VGG-19 Cosine Similarity</text>
<text top="132" left="99" width="17" height="14" font="font33" id="p11_t36" reading_order_no="35" segment_no="4" tag_type="figure">VMAF</text>
<text top="141" left="99" width="44" height="14" font="font33" id="p11_t37" reading_order_no="36" segment_no="4" tag_type="figure">Video BLIINDS</text>
<text top="123" left="206" width="15" height="14" font="font33" id="p11_t38" reading_order_no="37" segment_no="4" tag_type="figure">VSFA</text>
<text top="132" left="206" width="40" height="14" font="font33" id="p11_t39" reading_order_no="38" segment_no="4" tag_type="figure">SSA Baseline</text>
<text top="141" left="206" width="70" height="14" font="font33" id="p11_t40" reading_order_no="39" segment_no="4" tag_type="figure">Our Model (MCS+RFD)</text>
<text top="317" left="49" width="251" height="9" font="font5" id="p11_t41" reading_order_no="40" segment_no="6" tag_type="text">Fig. 9: Evaluation of different models for different training set</text>
<text top="329" left="49" width="251" height="9" font="font5" id="p11_t42" reading_order_no="41" segment_no="6" tag_type="text">size. ResNet-50 features are used for our model. Only SROCC</text>
<text top="341" left="49" width="251" height="9" font="font5" id="p11_t43" reading_order_no="42" segment_no="6" tag_type="text">values are quoted. We observe similar trend w.r.t. PLCC and</text>
<text top="353" left="49" width="30" height="9" font="font5" id="p11_t44" reading_order_no="43" segment_no="6" tag_type="text">RMSE.</text>
<text top="389" left="49" width="251" height="9" font="font5" id="p11_t45" reading_order_no="44" segment_no="9" tag_type="text">performance of linear regression model is slightly superior</text>
<text top="401" left="49" width="251" height="9" font="font5" id="p11_t46" reading_order_no="45" segment_no="9" tag_type="text">to that of SVR model, and better than that of polynomial</text>
<text top="413" left="49" width="154" height="9" font="font5" id="p11_t47" reading_order_no="46" segment_no="9" tag_type="text">regression model and neural network.</text>
<text top="427" left="59" width="241" height="9" font="font8" id="p11_t48" reading_order_no="47" segment_no="10" tag_type="text">4) Variation in performance with respect to number of</text>
<text top="439" left="49" width="94" height="9" font="font8" id="p11_t49" reading_order_no="48" segment_no="10" tag_type="text">principal components:</text>
<text top="439" left="151" width="149" height="9" font="font5" id="p11_t50" reading_order_no="49" segment_no="10" tag_type="text">We vary the number of principal</text>
<text top="451" left="49" width="251" height="9" font="font5" id="p11_t51" reading_order_no="50" segment_no="10" tag_type="text">components used in our PCA model and show the results</text>
<text top="463" left="49" width="251" height="9" font="font5" id="p11_t52" reading_order_no="51" segment_no="10" tag_type="text">of the resulting regression models in Table <a href="deeplearning_paper9.html#11">VI. </a>We note that</text>
<text top="475" left="49" width="251" height="9" font="font5" id="p11_t53" reading_order_no="52" segment_no="10" tag_type="text">the performance increases with the increase in the number of</text>
<text top="487" left="49" width="111" height="9" font="font5" id="p11_t54" reading_order_no="53" segment_no="10" tag_type="text">principal components used.</text>
<text top="500" left="59" width="163" height="9" font="font8" id="p11_t55" reading_order_no="54" segment_no="11" tag_type="text">5) Robustness with less training data:</text>
<text top="500" left="228" width="72" height="9" font="font5" id="p11_t56" reading_order_no="55" segment_no="11" tag_type="text">We also evaluate</text>
<text top="512" left="49" width="251" height="9" font="font5" id="p11_t57" reading_order_no="56" segment_no="11" tag_type="text">the robustness of our model with respect to the amount of</text>
<text top="524" left="49" width="251" height="9" font="font5" id="p11_t58" reading_order_no="57" segment_no="11" tag_type="text">training data. For a given split of the dataset into training</text>
<text top="536" left="49" width="251" height="9" font="font5" id="p11_t59" reading_order_no="58" segment_no="11" tag_type="text">and testing in the ratio 80:20, we build a series of training</text>
<text top="548" left="49" width="251" height="9" font="font5" id="p11_t60" reading_order_no="59" segment_no="11" tag_type="text">sets starting with 10% of the videos and adding 10% more</text>
<text top="560" left="49" width="251" height="9" font="font5" id="p11_t61" reading_order_no="60" segment_no="11" tag_type="text">videos in each step. We then evaluate the performance of our</text>
<text top="572" left="49" width="251" height="9" font="font5" id="p11_t62" reading_order_no="61" segment_no="11" tag_type="text">model when trained with these subsets as shown in Fig. <a href="deeplearning_paper9.html#11">9.</a></text>
<text top="584" left="49" width="251" height="9" font="font5" id="p11_t63" reading_order_no="62" segment_no="11" tag_type="text">Here, the number of principal components is set to be equal</text>
<text top="596" left="49" width="251" height="9" font="font5" id="p11_t64" reading_order_no="63" segment_no="11" tag_type="text">to the number of videos in the respective training set. We</text>
<text top="608" left="49" width="251" height="9" font="font5" id="p11_t65" reading_order_no="64" segment_no="11" tag_type="text">note that the test data is kept constant across all steps, and in</text>
<text top="620" left="49" width="251" height="9" font="font5" id="p11_t66" reading_order_no="65" segment_no="11" tag_type="text">each step, the scores are computed as the median performance</text>
<text top="632" left="49" width="251" height="9" font="font5" id="p11_t67" reading_order_no="66" segment_no="11" tag_type="text">across 100 splits. For comparison, we also show the perfor-</text>
<text top="643" left="49" width="251" height="9" font="font5" id="p11_t68" reading_order_no="67" segment_no="11" tag_type="text">mance of other benchmarks and baselines. We observe that</text>
<text top="655" left="49" width="251" height="9" font="font5" id="p11_t69" reading_order_no="68" segment_no="11" tag_type="text">our model trained with just 10% of videos in our database,</text>
<text top="667" left="49" width="251" height="9" font="font5" id="p11_t70" reading_order_no="69" segment_no="11" tag_type="text">outperforms all existing measures of quality. Note that the</text>
<text top="679" left="49" width="251" height="9" font="font5" id="p11_t71" reading_order_no="70" segment_no="11" tag_type="text">VGG-19 cosine similarity achieves a constant performance as</text>
<text top="691" left="49" width="251" height="9" font="font5" id="p11_t72" reading_order_no="71" segment_no="11" tag_type="text">it is not a training based algorithm. Further, we note that our</text>
<text top="703" left="49" width="251" height="9" font="font5" id="p11_t73" reading_order_no="72" segment_no="11" tag_type="text">model consistently performs better than other models as the</text>
<text top="715" left="49" width="251" height="9" font="font5" id="p11_t74" reading_order_no="73" segment_no="11" tag_type="text">amount of training data increases. Thus, we conclude that the</text>
<text top="727" left="49" width="251" height="9" font="font5" id="p11_t75" reading_order_no="74" segment_no="11" tag_type="text">robustness of our model with less training data allows for a</text>
<text top="739" left="49" width="201" height="9" font="font5" id="p11_t76" reading_order_no="75" segment_no="11" tag_type="text">reliable prediction of quality of predicted videos.</text>
<text top="227" left="375" width="2" height="7" font="font35" id="p11_t77" reading_order_no="88" segment_no="2" tag_type="figure">1</text>
<text top="227" left="397" width="2" height="7" font="font35" id="p11_t78" reading_order_no="89" segment_no="2" tag_type="figure">2</text>
<text top="227" left="409" width="2" height="7" font="font35" id="p11_t79" reading_order_no="90" segment_no="2" tag_type="figure">3</text>
<text top="227" left="425" width="2" height="7" font="font35" id="p11_t80" reading_order_no="91" segment_no="2" tag_type="figure">5</text>
<text top="227" left="445" width="4" height="7" font="font35" id="p11_t81" reading_order_no="92" segment_no="2" tag_type="figure">10</text>
<text top="227" left="467" width="4" height="7" font="font35" id="p11_t82" reading_order_no="93" segment_no="2" tag_type="figure">20</text>
<text top="227" left="479" width="4" height="7" font="font35" id="p11_t83" reading_order_no="94" segment_no="2" tag_type="figure">30</text>
<text top="227" left="495" width="4" height="7" font="font35" id="p11_t84" reading_order_no="95" segment_no="2" tag_type="figure">50</text>
<text top="227" left="515" width="7" height="7" font="font35" id="p11_t85" reading_order_no="96" segment_no="2" tag_type="figure">100</text>
<text top="227" left="536" width="19" height="7" font="font35" id="p11_t86" reading_order_no="97" segment_no="2" tag_type="figure">200 300</text>
<text top="230" left="401" width="88" height="13" font="font33" id="p11_t87" reading_order_no="98" segment_no="2" tag_type="figure">Average Execution Time (s)</text>
<text top="204" left="322" width="5" height="7" font="font35" id="p11_t88" reading_order_no="87" segment_no="2" tag_type="figure">0.0</text>
<text top="187" left="322" width="5" height="7" font="font35" id="p11_t89" reading_order_no="86" segment_no="2" tag_type="figure">0.1</text>
<text top="171" left="322" width="5" height="7" font="font35" id="p11_t90" reading_order_no="85" segment_no="2" tag_type="figure">0.2</text>
<text top="154" left="322" width="5" height="7" font="font35" id="p11_t91" reading_order_no="84" segment_no="2" tag_type="figure">0.3</text>
<text top="138" left="322" width="5" height="7" font="font35" id="p11_t92" reading_order_no="83" segment_no="2" tag_type="figure">0.4</text>
<text top="122" left="322" width="5" height="7" font="font35" id="p11_t93" reading_order_no="82" segment_no="2" tag_type="figure">0.5</text>
<text top="105" left="322" width="5" height="7" font="font35" id="p11_t94" reading_order_no="81" segment_no="2" tag_type="figure">0.6</text>
<text top="89" left="322" width="5" height="7" font="font35" id="p11_t95" reading_order_no="80" segment_no="2" tag_type="figure">0.7</text>
<text top="72" left="322" width="5" height="7" font="font35" id="p11_t96" reading_order_no="79" segment_no="2" tag_type="figure">0.8</text>
<text top="56" left="322" width="5" height="7" font="font35" id="p11_t97" reading_order_no="78" segment_no="2" tag_type="figure">0.9</text>
<text top="147" left="320" width="0" height="13" font="font34" id="p11_t98" reading_order_no="77" segment_no="2" tag_type="figure">SRO</text>
<text top="132" left="320" width="0" height="13" font="font34" id="p11_t99" reading_order_no="76" segment_no="2" tag_type="figure">CC</text>
<text top="137" left="344" width="9" height="7" font="font35" id="p11_t100" reading_order_no="110" segment_no="2" tag_type="figure">MSE</text>
<text top="113" left="365" width="10" height="7" font="font35" id="p11_t101" reading_order_no="108" segment_no="2" tag_type="figure">SSIM</text>
<text top="119" left="377" width="18" height="7" font="font35" id="p11_t102" reading_order_no="112" segment_no="2" tag_type="figure">MS-SSIM</text>
<text top="124" left="343" width="35" height="7" font="font35" id="p11_t103" reading_order_no="109" segment_no="2" tag_type="figure">Gradient Difference</text>
<text top="114" left="432" width="25" height="7" font="font35" id="p11_t104" reading_order_no="107" segment_no="2" tag_type="figure">VGG-19 MSE</text>
<text top="105" left="437" width="11" height="7" font="font35" id="p11_t105" reading_order_no="106" segment_no="2" tag_type="figure">LPIPS</text>
<text top="99" left="544" width="13" height="7" font="font35" id="p11_t106" reading_order_no="114" segment_no="2" tag_type="figure">PieApp</text>
<text top="101" left="435" width="12" height="7" font="font35" id="p11_t107" reading_order_no="105" segment_no="2" tag_type="figure">DISTS</text>
<text top="95" left="427" width="45" height="7" font="font35" id="p11_t108" reading_order_no="104" segment_no="2" tag_type="figure">VGG-19 cosine similarity</text>
<text top="143" left="428" width="16" height="7" font="font35" id="p11_t109" reading_order_no="117" segment_no="2" tag_type="figure">ST-MAD</text>
<text top="106" left="382" width="13" height="7" font="font35" id="p11_t110" reading_order_no="111" segment_no="2" tag_type="figure">VMAF</text>
<text top="187" left="360" width="19" height="7" font="font35" id="p11_t111" reading_order_no="118" segment_no="2" tag_type="figure">BRISQUE</text>
<text top="215" left="363" width="10" height="7" font="font35" id="p11_t112" reading_order_no="120" segment_no="2" tag_type="figure">NIQE</text>
<text top="188" left="401" width="27" height="7" font="font35" id="p11_t113" reading_order_no="119" segment_no="2" tag_type="figure">Inception Score</text>
<text top="137" left="428" width="29" height="7" font="font35" id="p11_t114" reading_order_no="116" segment_no="2" tag_type="figure">Video BLIINDS</text>
<text top="109" left="517" width="13" height="7" font="font35" id="p11_t115" reading_order_no="115" segment_no="2" tag_type="figure">NSTSS</text>
<text top="105" left="405" width="15" height="7" font="font35" id="p11_t116" reading_order_no="103" segment_no="2" tag_type="figure">TLVQM</text>
<text top="100" left="408" width="11" height="7" font="font35" id="p11_t117" reading_order_no="102" segment_no="2" tag_type="figure">VSFA</text>
<text top="131" left="379" width="36" height="7" font="font35" id="p11_t118" reading_order_no="113" segment_no="2" tag_type="figure">SSA Baseline - C3D</text>
<text top="80" left="393" width="84" height="11" font="font35" id="p11_t119" reading_order_no="101" segment_no="2" tag_type="figure">SSA Baseline - ResNet50 Our Model - VGG-19</text>
<text top="73" left="419" width="45" height="7" font="font35" id="p11_t120" reading_order_no="100" segment_no="2" tag_type="figure">Our Model - Inception-v3</text>
<text top="69" left="437" width="42" height="7" font="font35" id="p11_t121" reading_order_no="99" segment_no="2" tag_type="figure">Our Model - ResNet-50</text>
<text top="252" left="312" width="251" height="9" font="font5" id="p11_t122" reading_order_no="121" segment_no="5" tag_type="text">Fig. 10: Runtime per video (in seconds) of various QA models</text>
<text top="264" left="312" width="251" height="9" font="font5" id="p11_t123" reading_order_no="122" segment_no="5" tag_type="text">is shown against their SROCC scores. The markers in red</text>
<text top="276" left="312" width="251" height="9" font="font5" id="p11_t124" reading_order_no="123" segment_no="5" tag_type="text">denote FR measures while those in blue denote NR measures.</text>
<text top="288" left="312" width="251" height="9" font="font5" id="p11_t125" reading_order_no="124" segment_no="5" tag_type="text">Circles denote image measures while triangles denote video</text>
<text top="300" left="312" width="251" height="9" font="font5" id="p11_t126" reading_order_no="125" segment_no="5" tag_type="text">measures. Finally the markers in green represent our models.</text>
<text top="334" left="312" width="123" height="9" font="font8" id="p11_t127" reading_order_no="126" segment_no="7" tag_type="title">C. Computational Complexity</text>
<text top="350" left="322" width="241" height="9" font="font5" id="p11_t128" reading_order_no="127" segment_no="8" tag_type="text">We compare the computational complexity all models by</text>
<text top="362" left="312" width="251" height="9" font="font5" id="p11_t129" reading_order_no="128" segment_no="8" tag_type="text">measuring their testing time per video in the IISc-PVQA</text>
<text top="374" left="312" width="251" height="9" font="font5" id="p11_t130" reading_order_no="129" segment_no="8" tag_type="text">database. We run all the models on an Ubuntu 64-bit PC</text>
<text top="386" left="312" width="251" height="9" font="font5" id="p11_t131" reading_order_no="130" segment_no="8" tag_type="text">with 32 GB RAM and 4GHz octa-core Intel i7-6700K CPU.</text>
<text top="398" left="312" width="251" height="9" font="font5" id="p11_t132" reading_order_no="131" segment_no="8" tag_type="text">In Fig. <a href="deeplearning_paper9.html#11">10, </a>we observe that our models achieve the best</text>
<text top="410" left="312" width="251" height="9" font="font5" id="p11_t133" reading_order_no="132" segment_no="8" tag_type="text">SROCC at computational times that are comparable with other</text>
<text top="422" left="312" width="251" height="9" font="font5" id="p11_t134" reading_order_no="133" segment_no="8" tag_type="text">deep features based models. Although our models involve</text>
<text top="433" left="312" width="251" height="9" font="font5" id="p11_t135" reading_order_no="134" segment_no="8" tag_type="text">the computation of motion compensated similarities, since</text>
<text top="445" left="312" width="251" height="9" font="font5" id="p11_t136" reading_order_no="135" segment_no="8" tag_type="text">this is evaluated in a feature space at low spatial resolution,</text>
<text top="457" left="312" width="251" height="9" font="font5" id="p11_t137" reading_order_no="136" segment_no="8" tag_type="text">it does not appear to have a big impact on runtimes. The</text>
<text top="469" left="312" width="251" height="9" font="font5" id="p11_t138" reading_order_no="137" segment_no="8" tag_type="text">computational times of our two stream architecture involving</text>
<text top="481" left="312" width="251" height="9" font="font5" id="p11_t139" reading_order_no="138" segment_no="8" tag_type="text">MCS and RFD features are comparable to VGG-19 feature</text>
<text top="493" left="312" width="251" height="9" font="font5" id="p11_t140" reading_order_no="139" segment_no="8" tag_type="text">based FR measures, which need to compute deep features on</text>
<text top="505" left="312" width="162" height="9" font="font5" id="p11_t141" reading_order_no="140" segment_no="8" tag_type="text">both the reference and distorted videos.</text>
<text top="532" left="399" width="26" height="9" font="font5" id="p11_t142" reading_order_no="141" segment_no="12" tag_type="title">VI. C</text>
<text top="533" left="426" width="50" height="7" font="font6" id="p11_t143" reading_order_no="142" segment_no="12" tag_type="title">ONCLUSION</text>
<text top="548" left="322" width="241" height="9" font="font5" id="p11_t144" reading_order_no="143" segment_no="13" tag_type="text">We build a quality assessment database for video prediction</text>
<text top="560" left="312" width="251" height="9" font="font5" id="p11_t145" reading_order_no="144" segment_no="13" tag_type="text">models. Our subjective study and benchmarking experiments</text>
<text top="572" left="312" width="251" height="9" font="font5" id="p11_t146" reading_order_no="145" segment_no="13" tag_type="text">reveal that existing measures do not correlate well with human</text>
<text top="584" left="312" width="251" height="9" font="font5" id="p11_t147" reading_order_no="146" segment_no="13" tag_type="text">perception. We show that the MCS and RFD features proposed</text>
<text top="596" left="312" width="251" height="9" font="font5" id="p11_t148" reading_order_no="147" segment_no="13" tag_type="text">in this study can capture the quality of predicted videos and</text>
<text top="607" left="312" width="251" height="9" font="font5" id="p11_t149" reading_order_no="148" segment_no="13" tag_type="text">outperform all the existing measures of quality. We believe</text>
<text top="619" left="312" width="251" height="9" font="font5" id="p11_t150" reading_order_no="149" segment_no="13" tag_type="text">that our database will be particularly useful in further research</text>
<text top="631" left="312" width="251" height="9" font="font5" id="p11_t151" reading_order_no="150" segment_no="13" tag_type="text">in this area and help design improved models for video</text>
<text top="643" left="312" width="43" height="9" font="font5" id="p11_t152" reading_order_no="151" segment_no="13" tag_type="text">prediction.</text>
<text top="655" left="322" width="241" height="9" font="font5" id="p11_t153" reading_order_no="152" segment_no="14" tag_type="text">Our work in establishing that quality of predicted videos</text>
<text top="667" left="312" width="251" height="9" font="font5" id="p11_t154" reading_order_no="153" segment_no="14" tag_type="text">can be assessed reliably by human subjects sets the stage for</text>
<text top="679" left="312" width="251" height="9" font="font5" id="p11_t155" reading_order_no="154" segment_no="14" tag_type="text">much larger human studies on more videos, potentially using</text>
<text top="691" left="312" width="251" height="9" font="font5" id="p11_t156" reading_order_no="155" segment_no="14" tag_type="text">crowdsourcing. We largely focused on predicted videos based</text>
<text top="703" left="312" width="251" height="9" font="font5" id="p11_t157" reading_order_no="156" segment_no="14" tag_type="text">on generative models. It will be of interest to study the quality</text>
<text top="715" left="312" width="251" height="9" font="font5" id="p11_t158" reading_order_no="157" segment_no="14" tag_type="text">of other synthetically generated videos in gaming scenarios.</text>
<text top="727" left="312" width="251" height="9" font="font5" id="p11_t159" reading_order_no="158" segment_no="14" tag_type="text">Moreover, it will be interesting to understand the role of</text>
<text top="739" left="312" width="251" height="9" font="font5" id="p11_t160" reading_order_no="159" segment_no="14" tag_type="text">physics engines in video prediction and quality assessment</text>
</page>
<page number="12" position="absolute" top="0" left="0" height="792" width="612">
	<fontspec id="font36" size="8" family="NimbusRomNo9L-ReguItal" color="#000000"/>
<text top="26" left="556" width="7" height="6" font="font0" id="p12_t1" reading_order_no="0" segment_no="0" tag_type="text">12</text>
<text top="58" left="49" width="251" height="9" font="font5" id="p12_t2" reading_order_no="1" segment_no="1" tag_type="text"><a href="deeplearning_paper9.html#12">[5]. </a>Finally, we primarily looked at a supervised setting by</text>
<text top="70" left="49" width="251" height="9" font="font5" id="p12_t3" reading_order_no="2" segment_no="1" tag_type="text">learning quality from human scores. It will also be interesting</text>
<text top="82" left="49" width="251" height="9" font="font5" id="p12_t4" reading_order_no="3" segment_no="1" tag_type="text">to explore unsupervised measures of predicted video quality</text>
<text top="94" left="49" width="251" height="9" font="font5" id="p12_t5" reading_order_no="4" segment_no="1" tag_type="text">that can be designed by merely having access to a large corpus</text>
<text top="106" left="49" width="71" height="9" font="font5" id="p12_t6" reading_order_no="5" segment_no="1" tag_type="text">of natural videos.</text>
<text top="135" left="131" width="7" height="9" font="font5" id="p12_t7" reading_order_no="6" segment_no="5" tag_type="title">A</text>
<text top="137" left="138" width="80" height="7" font="font6" id="p12_t8" reading_order_no="7" segment_no="5" tag_type="title">CKNOWLEDGMENT</text>
<text top="154" left="59" width="241" height="9" font="font5" id="p12_t9" reading_order_no="8" segment_no="7" tag_type="text">The authors would like to thank all the volunteers who took</text>
<text top="166" left="49" width="113" height="9" font="font5" id="p12_t10" reading_order_no="9" segment_no="7" tag_type="text">part in the subjective study.</text>
<text top="196" left="147" width="7" height="9" font="font5" id="p12_t11" reading_order_no="10" segment_no="9" tag_type="title">R</text>
<text top="197" left="154" width="49" height="7" font="font6" id="p12_t12" reading_order_no="11" segment_no="9" tag_type="title">EFERENCES</text>
<text top="215" left="53" width="247" height="7" font="font6" id="p12_t13" reading_order_no="12" segment_no="11" tag_type="text">[1] W. Liu, W. Luo, D. Lian, and S. Gao, “Future frame prediction for</text>
<text top="224" left="67" width="126" height="7" font="font6" id="p12_t14" reading_order_no="13" segment_no="11" tag_type="text">anomaly detection - a new baseline,” in</text>
<text top="224" left="195" width="105" height="7" font="font36" id="p12_t15" reading_order_no="14" segment_no="11" tag_type="text">IEEE Conf. Comput. Vis. Pattern</text>
<text top="233" left="67" width="50" height="7" font="font36" id="p12_t16" reading_order_no="15" segment_no="11" tag_type="text">Recog. (CVPR)</text>
<text top="233" left="117" width="23" height="7" font="font6" id="p12_t17" reading_order_no="16" segment_no="11" tag_type="text">, 2018.</text>
<text top="242" left="53" width="247" height="7" font="font6" id="p12_t18" reading_order_no="17" segment_no="13" tag_type="text">[2] N. Srivastava, E. Mansimov, and R. Salakhudinov, “Unsupervised learn-</text>
<text top="251" left="67" width="152" height="7" font="font6" id="p12_t19" reading_order_no="18" segment_no="13" tag_type="text">ing of video representations using LSTMs,” in</text>
<text top="251" left="222" width="78" height="7" font="font36" id="p12_t20" reading_order_no="19" segment_no="13" tag_type="text">Int. Conf. Mach. Learn.</text>
<text top="260" left="67" width="24" height="7" font="font36" id="p12_t21" reading_order_no="20" segment_no="13" tag_type="text">(ICML)</text>
<text top="260" left="92" width="23" height="7" font="font6" id="p12_t22" reading_order_no="21" segment_no="13" tag_type="text">, 2015.</text>
<text top="269" left="53" width="247" height="7" font="font6" id="p12_t23" reading_order_no="22" segment_no="15" tag_type="text">[3] W. Byeon, Q. Wang, R. Kumar Srivastava, and P. Koumoutsakos,</text>
<text top="278" left="67" width="193" height="7" font="font6" id="p12_t24" reading_order_no="23" segment_no="15" tag_type="text">“ContextVP : Fully context-aware video prediction,” in</text>
<text top="278" left="265" width="35" height="7" font="font36" id="p12_t25" reading_order_no="24" segment_no="15" tag_type="text">Eur. Conf.</text>
<text top="287" left="67" width="70" height="7" font="font36" id="p12_t26" reading_order_no="25" segment_no="15" tag_type="text">Comput. Vis. (ECCV)</text>
<text top="287" left="137" width="23" height="7" font="font6" id="p12_t27" reading_order_no="26" segment_no="15" tag_type="text">, 2018.</text>
<text top="296" left="53" width="247" height="7" font="font6" id="p12_t28" reading_order_no="27" segment_no="17" tag_type="text">[4] C. Finn, I. Goodfellow, and S. Levine, “Unsupervised learning for</text>
<text top="305" left="67" width="171" height="7" font="font6" id="p12_t29" reading_order_no="28" segment_no="17" tag_type="text">physical interaction through video prediction,” in</text>
<text top="305" left="244" width="56" height="7" font="font36" id="p12_t30" reading_order_no="29" segment_no="17" tag_type="text">Adv. Neural Inf.</text>
<text top="314" left="67" width="80" height="7" font="font36" id="p12_t31" reading_order_no="30" segment_no="17" tag_type="text">Process. Syst. (NeurIPS)</text>
<text top="314" left="147" width="23" height="7" font="font6" id="p12_t32" reading_order_no="31" segment_no="17" tag_type="text">, 2016.</text>
<text top="324" left="53" width="247" height="7" font="font6" id="p12_t33" reading_order_no="32" segment_no="20" tag_type="text">[5] M. Janner, S. Levine, W. T. Freeman, J. B. Tenenbaum, C. Finn,</text>
<text top="333" left="67" width="233" height="7" font="font6" id="p12_t34" reading_order_no="33" segment_no="20" tag_type="text">and J. Wu, “Reasoning about physical interactions with object-centric</text>
<text top="342" left="67" width="37" height="7" font="font6" id="p12_t35" reading_order_no="34" segment_no="20" tag_type="text">models,” in</text>
<text top="342" left="107" width="117" height="7" font="font36" id="p12_t36" reading_order_no="35" segment_no="20" tag_type="text">Int. Conf. Learn. Represent. (ICLR)</text>
<text top="342" left="224" width="23" height="7" font="font6" id="p12_t37" reading_order_no="36" segment_no="20" tag_type="text">, 2019.</text>
<text top="351" left="53" width="247" height="7" font="font6" id="p12_t38" reading_order_no="37" segment_no="22" tag_type="text">[6] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine, “Learning</text>
<text top="360" left="67" width="216" height="7" font="font6" id="p12_t39" reading_order_no="38" segment_no="22" tag_type="text">to poke by poking: Experiential learning of intuitive physics,” in</text>
<text top="360" left="286" width="14" height="7" font="font36" id="p12_t40" reading_order_no="39" segment_no="22" tag_type="text">Adv.</text>
<text top="369" left="67" width="118" height="7" font="font36" id="p12_t41" reading_order_no="40" segment_no="22" tag_type="text">Neural Inf. Process. Syst. (NeurIPS)</text>
<text top="369" left="186" width="23" height="7" font="font6" id="p12_t42" reading_order_no="41" segment_no="22" tag_type="text">, 2016.</text>
<text top="378" left="53" width="247" height="7" font="font6" id="p12_t43" reading_order_no="42" segment_no="24" tag_type="text">[7] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video</text>
<text top="387" left="67" width="137" height="7" font="font6" id="p12_t44" reading_order_no="43" segment_no="24" tag_type="text">prediction beyond mean square error,” in</text>
<text top="387" left="207" width="93" height="7" font="font36" id="p12_t45" reading_order_no="44" segment_no="24" tag_type="text">Int. Conf. Learn. Represent.</text>
<text top="396" left="67" width="23" height="7" font="font36" id="p12_t46" reading_order_no="45" segment_no="24" tag_type="text">(ICLR)</text>
<text top="396" left="90" width="23" height="7" font="font6" id="p12_t47" reading_order_no="46" segment_no="24" tag_type="text">, 2016.</text>
<text top="405" left="53" width="247" height="7" font="font6" id="p12_t48" reading_order_no="47" segment_no="26" tag_type="text">[8] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image</text>
<text top="414" left="67" width="212" height="7" font="font6" id="p12_t49" reading_order_no="48" segment_no="26" tag_type="text">quality assessment: from error visibility to structural similarity,”</text>
<text top="414" left="283" width="17" height="7" font="font36" id="p12_t50" reading_order_no="49" segment_no="26" tag_type="text">IEEE</text>
<text top="423" left="67" width="72" height="7" font="font36" id="p12_t51" reading_order_no="50" segment_no="26" tag_type="text">Trans. Image Process.</text>
<text top="423" left="139" width="117" height="7" font="font6" id="p12_t52" reading_order_no="51" segment_no="26" tag_type="text">, vol. 13, no. 4, pp. 600–612, 2004.</text>
<text top="432" left="53" width="22" height="7" font="font6" id="p12_t53" reading_order_no="52" segment_no="27" tag_type="text">[9] A.</text>
<text top="432" left="82" width="8" height="7" font="font6" id="p12_t54" reading_order_no="53" segment_no="27" tag_type="text">X.</text>
<text top="432" left="98" width="14" height="7" font="font6" id="p12_t55" reading_order_no="54" segment_no="27" tag_type="text">Lee,</text>
<text top="432" left="119" width="7" height="7" font="font6" id="p12_t56" reading_order_no="55" segment_no="27" tag_type="text">R.</text>
<text top="432" left="134" width="22" height="7" font="font6" id="p12_t57" reading_order_no="56" segment_no="27" tag_type="text">Zhang,</text>
<text top="432" left="164" width="6" height="7" font="font6" id="p12_t58" reading_order_no="57" segment_no="27" tag_type="text">F.</text>
<text top="432" left="177" width="19" height="7" font="font6" id="p12_t59" reading_order_no="58" segment_no="27" tag_type="text">Ebert,</text>
<text top="432" left="204" width="6" height="7" font="font6" id="p12_t60" reading_order_no="59" segment_no="27" tag_type="text">P.</text>
<text top="432" left="217" width="25" height="7" font="font6" id="p12_t61" reading_order_no="60" segment_no="27" tag_type="text">Abbeel,</text>
<text top="432" left="250" width="7" height="7" font="font6" id="p12_t62" reading_order_no="61" segment_no="27" tag_type="text">C.</text>
<text top="432" left="264" width="17" height="7" font="font6" id="p12_t63" reading_order_no="62" segment_no="27" tag_type="text">Finn,</text>
<text top="432" left="289" width="12" height="7" font="font6" id="p12_t64" reading_order_no="63" segment_no="27" tag_type="text">and</text>
<text top="441" left="67" width="173" height="7" font="font6" id="p12_t65" reading_order_no="64" segment_no="27" tag_type="text">S. Levine, “Stochastic adversarial video prediction,”</text>
<text top="441" left="243" width="46" height="7" font="font36" id="p12_t66" reading_order_no="65" segment_no="27" tag_type="text">arXiv e-prints</text>
<text top="441" left="289" width="11" height="7" font="font6" id="p12_t67" reading_order_no="66" segment_no="27" tag_type="text">, p.</text>
<text top="450" left="67" width="81" height="7" font="font6" id="p12_t68" reading_order_no="67" segment_no="27" tag_type="text">arXiv:1804.01523, 2018.</text>
<text top="460" left="49" width="251" height="7" font="font6" id="p12_t69" reading_order_no="68" segment_no="30" tag_type="text">[10] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski,</text>
<text top="469" left="67" width="199" height="7" font="font6" id="p12_t70" reading_order_no="69" segment_no="30" tag_type="text">and S. Gelly, “FVD : A new metric for video generation,” in</text>
<text top="469" left="269" width="31" height="7" font="font36" id="p12_t71" reading_order_no="70" segment_no="30" tag_type="text">Int. Conf.</text>
<text top="478" left="67" width="233" height="7" font="font36" id="p12_t72" reading_order_no="71" segment_no="30" tag_type="text">Learn. Represent. (ICLR) workshop on Deep Generative Models for</text>
<text top="487" left="67" width="77" height="7" font="font36" id="p12_t73" reading_order_no="72" segment_no="30" tag_type="text">Highly Structured Data</text>
<text top="486" left="144" width="23" height="7" font="font6" id="p12_t74" reading_order_no="73" segment_no="30" tag_type="text">, 2019.</text>
<text top="496" left="49" width="251" height="7" font="font6" id="p12_t75" reading_order_no="74" segment_no="32" tag_type="text">[11] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview of</text>
<text top="505" left="67" width="123" height="7" font="font6" id="p12_t76" reading_order_no="75" segment_no="32" tag_type="text">the h.264/avc video coding standard,”</text>
<text top="505" left="193" width="107" height="7" font="font36" id="p12_t77" reading_order_no="76" segment_no="32" tag_type="text">IEEE Trans. Circuits Syst. Video</text>
<text top="514" left="67" width="27" height="7" font="font36" id="p12_t78" reading_order_no="77" segment_no="32" tag_type="text">Technol.</text>
<text top="514" left="94" width="117" height="7" font="font6" id="p12_t79" reading_order_no="78" segment_no="32" tag_type="text">, vol. 13, no. 7, pp. 560–576, 2003.</text>
<text top="523" left="49" width="232" height="7" font="font6" id="p12_t80" reading_order_no="79" segment_no="34" tag_type="text">[12] H. Choi and I. V. Baji´c, “Deep frame prediction for video coding,”</text>
<text top="523" left="283" width="17" height="7" font="font36" id="p12_t81" reading_order_no="80" segment_no="34" tag_type="text">IEEE</text>
<text top="532" left="67" width="113" height="7" font="font36" id="p12_t82" reading_order_no="81" segment_no="34" tag_type="text">Trans. Circuits Syst. Video Technol.</text>
<text top="532" left="180" width="120" height="7" font="font6" id="p12_t83" reading_order_no="82" segment_no="34" tag_type="text">, vol. 30, no. 7, pp. 1843–1855, 2020.</text>
<text top="541" left="49" width="251" height="7" font="font6" id="p12_t84" reading_order_no="83" segment_no="35" tag_type="text">[13] R. P. Rao and D. H. Ballard, “Predictive coding in the visual cortex: a</text>
<text top="550" left="67" width="233" height="7" font="font6" id="p12_t85" reading_order_no="84" segment_no="35" tag_type="text">functional interpretation of some extra-classical receptive-field effects,”</text>
<text top="559" left="67" width="66" height="7" font="font36" id="p12_t86" reading_order_no="85" segment_no="35" tag_type="text">Nature neuroscience</text>
<text top="559" left="134" width="105" height="7" font="font6" id="p12_t87" reading_order_no="86" segment_no="35" tag_type="text">, vol. 2, no. 1, pp. 79–87, 1999.</text>
<text top="568" left="49" width="251" height="7" font="font6" id="p12_t88" reading_order_no="87" segment_no="37" tag_type="text">[14] H. E. Den Ouden, P. Kok, and F. P. De Lange, “How prediction errors</text>
<text top="577" left="67" width="149" height="7" font="font6" id="p12_t89" reading_order_no="88" segment_no="37" tag_type="text">shape perception, attention, and motivation,”</text>
<text top="577" left="220" width="78" height="7" font="font36" id="p12_t90" reading_order_no="89" segment_no="37" tag_type="text">Frontiers in psychology</text>
<text top="577" left="298" width="2" height="7" font="font6" id="p12_t91" reading_order_no="90" segment_no="37" tag_type="text">,</text>
<text top="586" left="67" width="67" height="7" font="font6" id="p12_t92" reading_order_no="91" segment_no="37" tag_type="text">vol. 3, p. 548, 2012.</text>
<text top="596" left="49" width="251" height="7" font="font6" id="p12_t93" reading_order_no="92" segment_no="39" tag_type="text">[15] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,</text>
<text top="605" left="67" width="233" height="7" font="font6" id="p12_t94" reading_order_no="93" segment_no="39" tag_type="text">S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in</text>
<text top="614" left="67" width="135" height="7" font="font36" id="p12_t95" reading_order_no="94" segment_no="39" tag_type="text">Adv. Neural Inf. Process. Syst. (NeurIPS)</text>
<text top="613" left="202" width="23" height="7" font="font6" id="p12_t96" reading_order_no="95" segment_no="39" tag_type="text">, 2014.</text>
<text top="623" left="49" width="251" height="7" font="font6" id="p12_t97" reading_order_no="96" segment_no="41" tag_type="text">[16] R. Villegas, J. Yang, S. Hong, X. Lin, and H. Lee, “Decomposing motion</text>
<text top="632" left="67" width="176" height="7" font="font6" id="p12_t98" reading_order_no="97" segment_no="41" tag_type="text">and content for natural video sequence prediction,” in</text>
<text top="632" left="245" width="55" height="7" font="font36" id="p12_t99" reading_order_no="98" segment_no="41" tag_type="text">Int. Conf. Learn.</text>
<text top="641" left="67" width="59" height="7" font="font36" id="p12_t100" reading_order_no="99" segment_no="41" tag_type="text">Represent. (ICLR)</text>
<text top="641" left="126" width="23" height="7" font="font6" id="p12_t101" reading_order_no="100" segment_no="41" tag_type="text">, 2017.</text>
<text top="650" left="49" width="251" height="7" font="font6" id="p12_t102" reading_order_no="101" segment_no="43" tag_type="text">[17] W. Lotter, G. Kreiman, and D. Cox, “Deep predictive coding networks</text>
<text top="659" left="67" width="172" height="7" font="font6" id="p12_t103" reading_order_no="102" segment_no="43" tag_type="text">for video prediction and unsupervised learning,” in</text>
<text top="659" left="243" width="57" height="7" font="font36" id="p12_t104" reading_order_no="103" segment_no="43" tag_type="text">Int. Conf. Learn.</text>
<text top="668" left="67" width="59" height="7" font="font36" id="p12_t105" reading_order_no="104" segment_no="43" tag_type="text">Represent. (ICLR)</text>
<text top="668" left="126" width="23" height="7" font="font6" id="p12_t106" reading_order_no="105" segment_no="43" tag_type="text">, 2017.</text>
<text top="677" left="49" width="251" height="7" font="font6" id="p12_t107" reading_order_no="106" segment_no="46" tag_type="text">[18] Y. Wang, L. Jiang, M.-H. Yang, L.-J. Li, M. Long, and L. Fei-Fei,</text>
<text top="686" left="67" width="219" height="7" font="font6" id="p12_t108" reading_order_no="107" segment_no="46" tag_type="text">“Eidetic 3D LSTM: A model for video prediction and beyond,” in</text>
<text top="686" left="289" width="11" height="7" font="font36" id="p12_t109" reading_order_no="108" segment_no="46" tag_type="text">Int.</text>
<text top="695" left="67" width="103" height="7" font="font36" id="p12_t110" reading_order_no="109" segment_no="46" tag_type="text">Conf. Learn. Represent. (ICLR)</text>
<text top="695" left="170" width="23" height="7" font="font6" id="p12_t111" reading_order_no="110" segment_no="46" tag_type="text">, 2019.</text>
<text top="704" left="49" width="251" height="7" font="font6" id="p12_t112" reading_order_no="111" segment_no="48" tag_type="text">[19] E. Denton and R. Fergus, “Stochastic video generation with a learned</text>
<text top="713" left="67" width="29" height="7" font="font6" id="p12_t113" reading_order_no="112" segment_no="48" tag_type="text">prior,” in</text>
<text top="713" left="99" width="105" height="7" font="font36" id="p12_t114" reading_order_no="113" segment_no="48" tag_type="text">Int. Conf. Mach. Learn. (ICML)</text>
<text top="713" left="204" width="23" height="7" font="font6" id="p12_t115" reading_order_no="114" segment_no="48" tag_type="text">, 2018.</text>
<text top="723" left="49" width="251" height="7" font="font6" id="p12_t116" reading_order_no="115" segment_no="49" tag_type="text">[20] R. Villegas, A. Pathak, H. Kannan, D. Erhan, Q. V. Le, and H. Lee,</text>
<text top="732" left="67" width="233" height="7" font="font6" id="p12_t117" reading_order_no="116" segment_no="49" tag_type="text">“High fidelity video prediction with large stochastic recurrent neural</text>
<text top="740" left="67" width="43" height="7" font="font6" id="p12_t118" reading_order_no="117" segment_no="49" tag_type="text">networks,” in</text>
<text top="741" left="113" width="135" height="7" font="font36" id="p12_t119" reading_order_no="118" segment_no="49" tag_type="text">Adv. Neural Inf. Process. Syst. (NeurIPS)</text>
<text top="740" left="248" width="23" height="7" font="font6" id="p12_t120" reading_order_no="119" segment_no="49" tag_type="text">, 2019.</text>
<text top="59" left="312" width="251" height="7" font="font6" id="p12_t121" reading_order_no="120" segment_no="2" tag_type="text">[21] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,</text>
<text top="68" left="330" width="233" height="7" font="font6" id="p12_t122" reading_order_no="121" segment_no="2" tag_type="text">S. Orts-Escolano, J. Garcia-Rodriguez, and A. Argyros, “A review on</text>
<text top="77" left="330" width="160" height="7" font="font6" id="p12_t123" reading_order_no="122" segment_no="2" tag_type="text">deep learning techniques for video prediction,”</text>
<text top="77" left="494" width="69" height="7" font="font36" id="p12_t124" reading_order_no="123" segment_no="2" tag_type="text">IEEE Trans. Pattern</text>
<text top="86" left="330" width="61" height="7" font="font36" id="p12_t125" reading_order_no="124" segment_no="2" tag_type="text">Anal. Mach. Intell.</text>
<text top="86" left="392" width="23" height="7" font="font6" id="p12_t126" reading_order_no="125" segment_no="2" tag_type="text">, 2020.</text>
<text top="95" left="312" width="251" height="7" font="font6" id="p12_t127" reading_order_no="126" segment_no="3" tag_type="text">[22] M. Kumar, M. Babaeizadeh, D. Erhan, C. Finn, S. Levine, L. Dinh,</text>
<text top="104" left="330" width="233" height="7" font="font6" id="p12_t128" reading_order_no="127" segment_no="3" tag_type="text">and D. Kingma, “Videoflow: A flow-based generative model for video,”</text>
<text top="113" left="330" width="45" height="7" font="font36" id="p12_t129" reading_order_no="128" segment_no="3" tag_type="text">arXiv e-prints</text>
<text top="113" left="376" width="94" height="7" font="font6" id="p12_t130" reading_order_no="129" segment_no="3" tag_type="text">, p. arXiv:1903.01434, 2019.</text>
<text top="123" left="312" width="251" height="7" font="font6" id="p12_t131" reading_order_no="130" segment_no="4" tag_type="text">[23] K. Simonyan and A. Zisserman, “Very deep convolutional networks for</text>
<text top="131" left="330" width="110" height="7" font="font6" id="p12_t132" reading_order_no="131" segment_no="4" tag_type="text">large-scale image recognition,” in</text>
<text top="132" left="444" width="117" height="7" font="font36" id="p12_t133" reading_order_no="132" segment_no="4" tag_type="text">Int. Conf. Learn. Represent. (ICLR)</text>
<text top="131" left="561" width="2" height="7" font="font6" id="p12_t134" reading_order_no="133" segment_no="4" tag_type="text">,</text>
<text top="140" left="330" width="18" height="7" font="font6" id="p12_t135" reading_order_no="134" segment_no="4" tag_type="text">2015.</text>
<text top="150" left="312" width="251" height="7" font="font6" id="p12_t136" reading_order_no="135" segment_no="6" tag_type="text">[24] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and</text>
<text top="159" left="330" width="179" height="7" font="font6" id="p12_t137" reading_order_no="136" segment_no="6" tag_type="text">X. Chen, “Improved techniques for training GANs,” in</text>
<text top="159" left="511" width="52" height="7" font="font36" id="p12_t138" reading_order_no="137" segment_no="6" tag_type="text">Adv. Neural Inf.</text>
<text top="168" left="330" width="80" height="7" font="font36" id="p12_t139" reading_order_no="138" segment_no="6" tag_type="text">Process. Syst. (NeurIPS)</text>
<text top="168" left="410" width="23" height="7" font="font6" id="p12_t140" reading_order_no="139" segment_no="6" tag_type="text">, 2016.</text>
<text top="177" left="312" width="251" height="7" font="font6" id="p12_t141" reading_order_no="140" segment_no="8" tag_type="text">[25] J. He, A. Lehrmann, J. Marino, G. Mori, and L. Sigal, “Probabilistic</text>
<text top="186" left="330" width="167" height="7" font="font6" id="p12_t142" reading_order_no="141" segment_no="8" tag_type="text">video generation using holistic attribute control,” in</text>
<text top="186" left="500" width="63" height="7" font="font36" id="p12_t143" reading_order_no="142" segment_no="8" tag_type="text">Eur. Conf. Comput.</text>
<text top="195" left="330" width="40" height="7" font="font36" id="p12_t144" reading_order_no="143" segment_no="8" tag_type="text">Vis. (ECCV)</text>
<text top="195" left="370" width="23" height="7" font="font6" id="p12_t145" reading_order_no="144" segment_no="8" tag_type="text">, 2018.</text>
<text top="204" left="312" width="251" height="7" font="font6" id="p12_t146" reading_order_no="145" segment_no="10" tag_type="text">[26] J. Xu, B. Ni, and X. Yang, “Video prediction via selective sampling,”</text>
<text top="213" left="330" width="6" height="7" font="font6" id="p12_t147" reading_order_no="146" segment_no="10" tag_type="text">in</text>
<text top="213" left="339" width="135" height="7" font="font36" id="p12_t148" reading_order_no="147" segment_no="10" tag_type="text">Adv. Neural Inf. Process. Syst. (NeurIPS)</text>
<text top="213" left="474" width="23" height="7" font="font6" id="p12_t149" reading_order_no="148" segment_no="10" tag_type="text">, 2018.</text>
<text top="223" left="312" width="251" height="7" font="font6" id="p12_t150" reading_order_no="149" segment_no="12" tag_type="text">[27] F. Ebert, C. Finn, A. X. Lee, and S. Levine, “Self-supervised visual</text>
<text top="232" left="330" width="142" height="7" font="font6" id="p12_t151" reading_order_no="150" segment_no="12" tag_type="text">planning with temporal skip connections,” in</text>
<text top="232" left="474" width="87" height="7" font="font36" id="p12_t152" reading_order_no="151" segment_no="12" tag_type="text">Conf. Robot Learn. (CoRL)</text>
<text top="232" left="561" width="2" height="7" font="font6" id="p12_t153" reading_order_no="152" segment_no="12" tag_type="text">,</text>
<text top="241" left="330" width="18" height="7" font="font6" id="p12_t154" reading_order_no="153" segment_no="12" tag_type="text">2017.</text>
<text top="250" left="312" width="251" height="7" font="font6" id="p12_t155" reading_order_no="154" segment_no="14" tag_type="text">[28] N. Wichers, R. Villegas, D. Erhan, and H. Lee, “Hierarchical long-</text>
<text top="259" left="330" width="152" height="7" font="font6" id="p12_t156" reading_order_no="155" segment_no="14" tag_type="text">term video prediction without supervision,” in</text>
<text top="259" left="485" width="78" height="7" font="font36" id="p12_t157" reading_order_no="156" segment_no="14" tag_type="text">Int. Conf. Mach. Learn.</text>
<text top="268" left="330" width="24" height="7" font="font36" id="p12_t158" reading_order_no="157" segment_no="14" tag_type="text">(ICML)</text>
<text top="268" left="355" width="23" height="7" font="font6" id="p12_t159" reading_order_no="158" segment_no="14" tag_type="text">, 2018.</text>
<text top="277" left="312" width="251" height="7" font="font6" id="p12_t160" reading_order_no="159" segment_no="16" tag_type="text">[29] K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack,</text>
<text top="286" left="330" width="212" height="7" font="font6" id="p12_t161" reading_order_no="160" segment_no="16" tag_type="text">“Study of subjective and objective quality assessment of video,”</text>
<text top="286" left="546" width="17" height="7" font="font36" id="p12_t162" reading_order_no="161" segment_no="16" tag_type="text">IEEE</text>
<text top="295" left="330" width="72" height="7" font="font36" id="p12_t163" reading_order_no="162" segment_no="16" tag_type="text">Trans. Image Process.</text>
<text top="295" left="402" width="125" height="7" font="font6" id="p12_t164" reading_order_no="163" segment_no="16" tag_type="text">, vol. 19, no. 6, pp. 1427–1441, 2010.</text>
<text top="304" left="312" width="251" height="7" font="font6" id="p12_t165" reading_order_no="164" segment_no="18" tag_type="text">[30] Z. Sinno and A. C. Bovik, “Large-scale study of perceptual video</text>
<text top="313" left="330" width="27" height="7" font="font6" id="p12_t166" reading_order_no="165" segment_no="18" tag_type="text">quality,”</text>
<text top="313" left="359" width="91" height="7" font="font36" id="p12_t167" reading_order_no="166" segment_no="18" tag_type="text">IEEE Trans. Image Process.</text>
<text top="313" left="450" width="113" height="7" font="font6" id="p12_t168" reading_order_no="167" segment_no="18" tag_type="text">, vol. 28, no. 2, pp. 612–627, 2019.</text>
<text top="323" left="312" width="251" height="7" font="font6" id="p12_t169" reading_order_no="168" segment_no="19" tag_type="text">[31] V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Szir´anyi, S. Li, and</text>
<text top="332" left="330" width="219" height="7" font="font6" id="p12_t170" reading_order_no="169" segment_no="19" tag_type="text">D. Saupe, “The Konstanz natural video database (KoNViD-1k),” in</text>
<text top="332" left="552" width="11" height="7" font="font36" id="p12_t171" reading_order_no="170" segment_no="19" tag_type="text">Int.</text>
<text top="341" left="330" width="135" height="7" font="font36" id="p12_t172" reading_order_no="171" segment_no="19" tag_type="text">Conf. Qual. Multimedia Exper. (QoMEX)</text>
<text top="341" left="465" width="23" height="7" font="font6" id="p12_t173" reading_order_no="172" segment_no="19" tag_type="text">, 2017.</text>
<text top="350" left="312" width="251" height="7" font="font6" id="p12_t174" reading_order_no="173" segment_no="21" tag_type="text">[32] K. Seshadrinathan and A. C. Bovik, “Motion tuned spatio-temporal</text>
<text top="359" left="330" width="129" height="7" font="font6" id="p12_t175" reading_order_no="174" segment_no="21" tag_type="text">quality assessment of natural videos,”</text>
<text top="359" left="464" width="97" height="7" font="font36" id="p12_t176" reading_order_no="175" segment_no="21" tag_type="text">IEEE Trans. Image Process.</text>
<text top="359" left="561" width="2" height="7" font="font6" id="p12_t177" reading_order_no="176" segment_no="21" tag_type="text">,</text>
<text top="368" left="330" width="112" height="7" font="font6" id="p12_t178" reading_order_no="177" segment_no="21" tag_type="text">vol. 19, no. 2, pp. 335–350, 2009.</text>
<text top="377" left="312" width="251" height="7" font="font6" id="p12_t179" reading_order_no="178" segment_no="23" tag_type="text">[33] P. V. Vu, C. T. Vu, and D. M. Chandler, “A spatiotemporal most-</text>
<text top="386" left="330" width="198" height="7" font="font6" id="p12_t180" reading_order_no="179" segment_no="23" tag_type="text">apparent-distortion model for video quality assessment,” in</text>
<text top="386" left="531" width="32" height="7" font="font36" id="p12_t181" reading_order_no="180" segment_no="23" tag_type="text">IEEE Int.</text>
<text top="395" left="330" width="93" height="7" font="font36" id="p12_t182" reading_order_no="181" segment_no="23" tag_type="text">Conf. Image Process. (ICIP)</text>
<text top="395" left="423" width="23" height="7" font="font6" id="p12_t183" reading_order_no="182" segment_no="23" tag_type="text">, 2011.</text>
<text top="404" left="312" width="251" height="7" font="font6" id="p12_t184" reading_order_no="183" segment_no="25" tag_type="text">[34] R. Soundararajan and A. C. Bovik, “Video quality assessment by</text>
<text top="413" left="330" width="189" height="7" font="font6" id="p12_t185" reading_order_no="184" segment_no="25" tag_type="text">reduced reference spatio-temporal entropic differencing,”</text>
<text top="413" left="522" width="41" height="7" font="font36" id="p12_t186" reading_order_no="185" segment_no="25" tag_type="text">IEEE Trans.</text>
<text top="422" left="330" width="93" height="7" font="font36" id="p12_t187" reading_order_no="186" segment_no="25" tag_type="text">Circuits Syst. Video Technol.</text>
<text top="422" left="424" width="117" height="7" font="font6" id="p12_t188" reading_order_no="187" segment_no="25" tag_type="text">, vol. 23, no. 4, pp. 684–694, 2013.</text>
<text top="432" left="312" width="251" height="7" font="font6" id="p12_t189" reading_order_no="188" segment_no="28" tag_type="text">[35] Z. Li, C. Bampis, J. Novak, A. Aaron, K. Swanson, A. Moorthy, and</text>
<text top="441" left="330" width="146" height="7" font="font6" id="p12_t190" reading_order_no="189" segment_no="28" tag_type="text">J. Cock, “VMAF : The journey continues,”</text>
<text top="441" left="480" width="81" height="7" font="font36" id="p12_t191" reading_order_no="190" segment_no="28" tag_type="text">The NETFLIX tech blog</text>
<text top="441" left="561" width="2" height="7" font="font6" id="p12_t192" reading_order_no="191" segment_no="28" tag_type="text">,</text>
<text top="450" left="330" width="18" height="7" font="font6" id="p12_t193" reading_order_no="192" segment_no="28" tag_type="text">2018.</text>
<text top="459" left="312" width="251" height="7" font="font6" id="p12_t194" reading_order_no="193" segment_no="29" tag_type="text">[36] M. A. Saad, A. C. Bovik, and C. Charrier, “Blind prediction of natural</text>
<text top="468" left="330" width="47" height="7" font="font6" id="p12_t195" reading_order_no="194" segment_no="29" tag_type="text">video quality,”</text>
<text top="468" left="381" width="93" height="7" font="font36" id="p12_t196" reading_order_no="195" segment_no="29" tag_type="text">IEEE Trans. Image Process.</text>
<text top="468" left="474" width="89" height="7" font="font6" id="p12_t197" reading_order_no="196" segment_no="29" tag_type="text">, vol. 23, no. 3, pp. 1352–</text>
<text top="477" left="330" width="39" height="7" font="font6" id="p12_t198" reading_order_no="197" segment_no="29" tag_type="text">1365, 2014.</text>
<text top="486" left="312" width="251" height="7" font="font6" id="p12_t199" reading_order_no="198" segment_no="31" tag_type="text">[37] Y. Li, L.-M. Po, C.-H. Cheung, X. Xu, L. Feng, F. Yuan, and K.-</text>
<text top="495" left="330" width="233" height="7" font="font6" id="p12_t200" reading_order_no="199" segment_no="31" tag_type="text">W. Cheung, “No-reference video quality assessment with 3d shearlet</text>
<text top="504" left="330" width="148" height="7" font="font6" id="p12_t201" reading_order_no="200" segment_no="31" tag_type="text">transform and convolutional neural networks,”</text>
<text top="504" left="480" width="83" height="7" font="font36" id="p12_t202" reading_order_no="201" segment_no="31" tag_type="text">IEEE Trans. Circuits Syst.</text>
<text top="513" left="330" width="48" height="7" font="font36" id="p12_t203" reading_order_no="202" segment_no="31" tag_type="text">Video Technol.</text>
<text top="513" left="378" width="125" height="7" font="font6" id="p12_t204" reading_order_no="203" segment_no="31" tag_type="text">, vol. 26, no. 6, pp. 1044–1057, 2016.</text>
<text top="522" left="312" width="251" height="7" font="font6" id="p12_t205" reading_order_no="204" segment_no="33" tag_type="text">[38] J. Korhonen, “Two-level approach for no-reference consumer video</text>
<text top="531" left="330" width="66" height="7" font="font6" id="p12_t206" reading_order_no="205" segment_no="33" tag_type="text">quality assessment,”</text>
<text top="531" left="399" width="94" height="7" font="font36" id="p12_t207" reading_order_no="206" segment_no="33" tag_type="text">IEEE Trans. Image Process.</text>
<text top="531" left="493" width="70" height="7" font="font6" id="p12_t208" reading_order_no="207" segment_no="33" tag_type="text">, vol. 28, no. 12, pp.</text>
<text top="540" left="330" width="59" height="7" font="font6" id="p12_t209" reading_order_no="208" segment_no="33" tag_type="text">5923–5938, 2019.</text>
<text top="550" left="312" width="251" height="7" font="font6" id="p12_t210" reading_order_no="209" segment_no="36" tag_type="text">[39] D. Li, T. Jiang, and M. Jiang, “Quality assessment of in-the-wild videos,”</text>
<text top="559" left="330" width="6" height="7" font="font6" id="p12_t211" reading_order_no="210" segment_no="36" tag_type="text">in</text>
<text top="559" left="339" width="111" height="7" font="font36" id="p12_t212" reading_order_no="211" segment_no="36" tag_type="text">Int. Conf. Multimedia (ACM-MM)</text>
<text top="559" left="450" width="23" height="7" font="font6" id="p12_t213" reading_order_no="212" segment_no="36" tag_type="text">, 2019.</text>
<text top="568" left="312" width="251" height="7" font="font6" id="p12_t214" reading_order_no="213" segment_no="38" tag_type="text">[40] S. Fan, T.-T. Ng, B. L. Koenig, J. S. Herberg, M. Jiang, Z. Shen, and</text>
<text top="577" left="330" width="233" height="7" font="font6" id="p12_t215" reading_order_no="214" segment_no="38" tag_type="text">Q. Zhao, “Image visual realism: From human perception to machine</text>
<text top="586" left="330" width="45" height="7" font="font6" id="p12_t216" reading_order_no="215" segment_no="38" tag_type="text">computation,”</text>
<text top="586" left="378" width="132" height="7" font="font36" id="p12_t217" reading_order_no="216" segment_no="38" tag_type="text">IEEE Trans. Pattern Anal. Mach. Intell.</text>
<text top="586" left="510" width="53" height="7" font="font6" id="p12_t218" reading_order_no="217" segment_no="38" tag_type="text">, vol. 40, no. 9,</text>
<text top="595" left="330" width="71" height="7" font="font6" id="p12_t219" reading_order_no="218" segment_no="38" tag_type="text">pp. 2180–2193, 2017.</text>
<text top="604" left="312" width="251" height="7" font="font6" id="p12_t220" reading_order_no="219" segment_no="40" tag_type="text">[41] L. Ren, A. Patrick, A. A. Efros, J. K. Hodgins, and J. M. Rehg, “A</text>
<text top="613" left="330" width="192" height="7" font="font6" id="p12_t221" reading_order_no="220" segment_no="40" tag_type="text">data-driven approach to quantifying natural human motion,”</text>
<text top="613" left="524" width="39" height="7" font="font36" id="p12_t222" reading_order_no="221" segment_no="40" tag_type="text">ACM Trans.</text>
<text top="622" left="330" width="23" height="7" font="font36" id="p12_t223" reading_order_no="222" segment_no="40" tag_type="text">Graph.</text>
<text top="622" left="353" width="125" height="7" font="font6" id="p12_t224" reading_order_no="223" segment_no="40" tag_type="text">, vol. 24, no. 3, pp. 1090–1097, 2005.</text>
<text top="631" left="312" width="251" height="7" font="font6" id="p12_t225" reading_order_no="224" segment_no="42" tag_type="text">[42] C. Schuldt, I. Laptev, and B. Caputo, “Recognizing human actions: a</text>
<text top="640" left="330" width="82" height="7" font="font6" id="p12_t226" reading_order_no="225" segment_no="42" tag_type="text">local SVM approach,” in</text>
<text top="640" left="415" width="107" height="7" font="font36" id="p12_t227" reading_order_no="226" segment_no="42" tag_type="text">Int. Conf. Pattern Recog. (ICPR)</text>
<text top="640" left="522" width="23" height="7" font="font6" id="p12_t228" reading_order_no="227" segment_no="42" tag_type="text">, 2004.</text>
<text top="650" left="312" width="251" height="7" font="font6" id="p12_t229" reading_order_no="228" segment_no="44" tag_type="text">[43] Microsoft Research, “MSR action dataset,” 2016. [Online]. Available:</text>
<text top="659" left="330" width="214" height="7" font="font6" id="p12_t230" reading_order_no="229" segment_no="44" tag_type="text"><a href="https://www.microsoft.com/en-us/download/details.aspx?id=52315">https://www.microsoft.com/en-us/download/details.aspx?id=52315</a></text>
<text top="668" left="312" width="251" height="7" font="font6" id="p12_t231" reading_order_no="230" segment_no="45" tag_type="text">[44] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101</text>
<text top="677" left="330" width="169" height="7" font="font6" id="p12_t232" reading_order_no="231" segment_no="45" tag_type="text">human actions classes from videos in the wild,”</text>
<text top="677" left="504" width="47" height="7" font="font36" id="p12_t233" reading_order_no="232" segment_no="45" tag_type="text">arXiv e-prints</text>
<text top="677" left="551" width="12" height="7" font="font6" id="p12_t234" reading_order_no="233" segment_no="45" tag_type="text">, p.</text>
<text top="686" left="330" width="77" height="7" font="font6" id="p12_t235" reading_order_no="234" segment_no="45" tag_type="text">arXiv:1212.0402, 2012.</text>
<text top="695" left="312" width="251" height="7" font="font6" id="p12_t236" reading_order_no="235" segment_no="47" tag_type="text">[45] W. Zhang, M. Zhu, and K. G. Derpanis, “From actemes to action: A</text>
<text top="704" left="330" width="233" height="7" font="font6" id="p12_t237" reading_order_no="236" segment_no="47" tag_type="text">strongly-supervised representation for detailed action understanding,” in</text>
<text top="713" left="330" width="122" height="7" font="font36" id="p12_t238" reading_order_no="237" segment_no="47" tag_type="text">IEEE Int. Conf. Comput. Vis. (ICCV)</text>
<text top="713" left="452" width="23" height="7" font="font6" id="p12_t239" reading_order_no="238" segment_no="47" tag_type="text">, 2013.</text>
<text top="723" left="312" width="251" height="7" font="font6" id="p12_t240" reading_order_no="239" segment_no="50" tag_type="text">[46] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:</text>
<text top="732" left="330" width="65" height="7" font="font6" id="p12_t241" reading_order_no="240" segment_no="50" tag_type="text">The KITTI dataset,”</text>
<text top="732" left="398" width="58" height="7" font="font36" id="p12_t242" reading_order_no="241" segment_no="50" tag_type="text">Int. J. Robot. Res.</text>
<text top="732" left="456" width="107" height="7" font="font6" id="p12_t243" reading_order_no="242" segment_no="50" tag_type="text">, vol. 32, no. 11, pp. 1231–1237,</text>
<text top="740" left="330" width="18" height="7" font="font6" id="p12_t244" reading_order_no="243" segment_no="50" tag_type="text">2013.</text>
</page>
<page number="13" position="absolute" top="0" left="0" height="792" width="612">
<text top="26" left="556" width="7" height="6" font="font0" id="p13_t1" reading_order_no="0" segment_no="0" tag_type="text">13</text>
<text top="59" left="49" width="251" height="7" font="font6" id="p13_t2" reading_order_no="1" segment_no="1" tag_type="text">[47] P. Doll´ar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detection:</text>
<text top="68" left="67" width="122" height="7" font="font6" id="p13_t3" reading_order_no="2" segment_no="1" tag_type="text">An evaluation of the state of the art,”</text>
<text top="68" left="192" width="108" height="7" font="font36" id="p13_t4" reading_order_no="3" segment_no="1" tag_type="text">IEEE Trans. Pattern Anal. Mach.</text>
<text top="77" left="67" width="19" height="7" font="font36" id="p13_t5" reading_order_no="4" segment_no="1" tag_type="text">Intell.</text>
<text top="77" left="86" width="117" height="7" font="font6" id="p13_t6" reading_order_no="5" segment_no="1" tag_type="text">, vol. 34, no. 4, pp. 743–761, 2011.</text>
<text top="86" left="49" width="251" height="7" font="font6" id="p13_t7" reading_order_no="6" segment_no="3" tag_type="text">[48] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,</text>
<text top="95" left="67" width="233" height="7" font="font6" id="p13_t8" reading_order_no="7" segment_no="3" tag_type="text">“BDD100K : A diverse driving video database with scalable annotation</text>
<text top="104" left="67" width="28" height="7" font="font6" id="p13_t9" reading_order_no="8" segment_no="3" tag_type="text">tooling,”</text>
<text top="104" left="98" width="45" height="7" font="font36" id="p13_t10" reading_order_no="9" segment_no="3" tag_type="text">arXiv e-prints</text>
<text top="104" left="143" width="94" height="7" font="font6" id="p13_t11" reading_order_no="10" segment_no="3" tag_type="text">, p. arXiv:1805.04687, 2018.</text>
<text top="114" left="49" width="251" height="7" font="font6" id="p13_t12" reading_order_no="11" segment_no="4" tag_type="text">[49] S. Aigner and M. K¨orner, “FutureGAN : Anticipating the future frames</text>
<text top="123" left="67" width="233" height="7" font="font6" id="p13_t13" reading_order_no="12" segment_no="4" tag_type="text">of video sequences using spatio-temporal 3d convolutions in progres-</text>
<text top="131" left="67" width="77" height="7" font="font6" id="p13_t14" reading_order_no="13" segment_no="4" tag_type="text">sively growing GANs,”</text>
<text top="132" left="148" width="152" height="7" font="font36" id="p13_t15" reading_order_no="14" segment_no="4" tag_type="text">Int. Arch. Photogramm. Remote Sens. Spatial</text>
<text top="141" left="67" width="25" height="7" font="font36" id="p13_t16" reading_order_no="15" segment_no="4" tag_type="text">Inf. Sci.</text>
<text top="140" left="92" width="23" height="7" font="font6" id="p13_t17" reading_order_no="16" segment_no="4" tag_type="text">, 2019.</text>
<text top="150" left="49" width="251" height="7" font="font6" id="p13_t18" reading_order_no="17" segment_no="5" tag_type="text">[50] W. Liu, A. Sharma, O. Camps, and M. Sznaier, “DYAN : A dynamical</text>
<text top="159" left="67" width="151" height="7" font="font6" id="p13_t19" reading_order_no="18" segment_no="5" tag_type="text">atoms-based network for video prediction,” in</text>
<text top="159" left="222" width="78" height="7" font="font36" id="p13_t20" reading_order_no="19" segment_no="5" tag_type="text">Eur. Conf. Comput. Vis.</text>
<text top="168" left="67" width="26" height="7" font="font36" id="p13_t21" reading_order_no="20" segment_no="5" tag_type="text">(ECCV)</text>
<text top="168" left="93" width="23" height="7" font="font6" id="p13_t22" reading_order_no="21" segment_no="5" tag_type="text">, 2018.</text>
<text top="177" left="49" width="251" height="7" font="font6" id="p13_t23" reading_order_no="22" segment_no="6" tag_type="text">[51] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine,</text>
<text top="186" left="67" width="141" height="7" font="font6" id="p13_t24" reading_order_no="23" segment_no="6" tag_type="text">“Stochastic variational video prediction,” in</text>
<text top="186" left="210" width="90" height="7" font="font36" id="p13_t25" reading_order_no="24" segment_no="6" tag_type="text">Int. Conf. Learn. Represent.</text>
<text top="195" left="67" width="23" height="7" font="font36" id="p13_t26" reading_order_no="25" segment_no="6" tag_type="text">(ICLR)</text>
<text top="195" left="90" width="23" height="7" font="font6" id="p13_t27" reading_order_no="26" segment_no="6" tag_type="text">, 2018.</text>
<text top="204" left="49" width="251" height="7" font="font6" id="p13_t28" reading_order_no="27" segment_no="7" tag_type="text">[52] “Methodology for the subjective assessment of the quality of television</text>
<text top="213" left="67" width="233" height="7" font="font6" id="p13_t29" reading_order_no="28" segment_no="7" tag_type="text">pictures ITU-R Recommendation BT.500-11,” Int. Telecommun. Union,</text>
<text top="222" left="67" width="58" height="7" font="font6" id="p13_t30" reading_order_no="29" segment_no="7" tag_type="text">Tech. Rep., 2002.</text>
<text top="232" left="49" width="112" height="7" font="font6" id="p13_t31" reading_order_no="30" segment_no="8" tag_type="text">[53] G. Casella and R. L. Berger,</text>
<text top="232" left="164" width="64" height="7" font="font36" id="p13_t32" reading_order_no="31" segment_no="8" tag_type="text">Statistical Inference</text>
<text top="232" left="229" width="46" height="7" font="font6" id="p13_t33" reading_order_no="32" segment_no="8" tag_type="text">, 2002, vol. 2.</text>
<text top="241" left="49" width="251" height="7" font="font6" id="p13_t34" reading_order_no="33" segment_no="9" tag_type="text">[54] J. Kim, H. Zeng, D. Ghadiyaram, S. Lee, L. Zhang, and A. C.</text>
<text top="250" left="67" width="233" height="7" font="font6" id="p13_t35" reading_order_no="34" segment_no="9" tag_type="text">Bovik, “Deep convolutional neural models for picture-quality prediction:</text>
<text top="259" left="67" width="214" height="7" font="font6" id="p13_t36" reading_order_no="35" segment_no="9" tag_type="text">Challenges and solutions to data-driven image quality assessment,”</text>
<text top="259" left="283" width="17" height="7" font="font36" id="p13_t37" reading_order_no="36" segment_no="9" tag_type="text">IEEE</text>
<text top="268" left="67" width="69" height="7" font="font36" id="p13_t38" reading_order_no="37" segment_no="9" tag_type="text">Signal Process. Mag.</text>
<text top="268" left="136" width="117" height="7" font="font6" id="p13_t39" reading_order_no="38" segment_no="9" tag_type="text">, vol. 34, no. 6, pp. 130–141, 2017.</text>
<text top="277" left="49" width="232" height="7" font="font6" id="p13_t40" reading_order_no="39" segment_no="10" tag_type="text">[55] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?”</text>
<text top="277" left="283" width="17" height="7" font="font36" id="p13_t41" reading_order_no="40" segment_no="10" tag_type="text">ACM</text>
<text top="286" left="67" width="46" height="7" font="font36" id="p13_t42" reading_order_no="41" segment_no="10" tag_type="text">Trans. Graph.</text>
<text top="286" left="113" width="105" height="7" font="font6" id="p13_t43" reading_order_no="42" segment_no="10" tag_type="text">, vol. 31, no. 4, pp. 1–10, 2012.</text>
<text top="295" left="49" width="251" height="7" font="font6" id="p13_t44" reading_order_no="43" segment_no="11" tag_type="text">[56] C. Zou, Q. Yu, R. Du, H. Mo, Y.-Z. Song, T. Xiang, C. Gao, B. Chen,</text>
<text top="304" left="67" width="217" height="7" font="font6" id="p13_t45" reading_order_no="44" segment_no="11" tag_type="text">and H. Zhang, “Sketchyscene: Richly-annotated scene sketches,” in</text>
<text top="304" left="287" width="13" height="7" font="font36" id="p13_t46" reading_order_no="45" segment_no="11" tag_type="text">Eur.</text>
<text top="313" left="67" width="90" height="7" font="font36" id="p13_t47" reading_order_no="46" segment_no="11" tag_type="text">Conf. Comput. Vis. (ECCV)</text>
<text top="313" left="157" width="23" height="7" font="font6" id="p13_t48" reading_order_no="47" segment_no="11" tag_type="text">, 2018.</text>
<text top="323" left="49" width="251" height="7" font="font6" id="p13_t49" reading_order_no="48" segment_no="12" tag_type="text">[57] M. Raghu, C. Zhang, J. Kleinberg, and S. Bengio, “Transfusion:</text>
<text top="332" left="67" width="190" height="7" font="font6" id="p13_t50" reading_order_no="49" segment_no="12" tag_type="text">Understanding transfer learning for medical imaging,” in</text>
<text top="332" left="261" width="39" height="7" font="font36" id="p13_t51" reading_order_no="50" segment_no="12" tag_type="text">Adv. Neural</text>
<text top="341" left="67" width="94" height="7" font="font36" id="p13_t52" reading_order_no="51" segment_no="12" tag_type="text">Inf. Process. Syst. (NeurIPS)</text>
<text top="341" left="161" width="23" height="7" font="font6" id="p13_t53" reading_order_no="52" segment_no="12" tag_type="text">, 2019.</text>
<text top="350" left="49" width="251" height="7" font="font6" id="p13_t54" reading_order_no="53" segment_no="13" tag_type="text">[58] A. Buslaev, S. Seferbekov, V. Iglovikov, and A. Shvets, “Fully convo-</text>
<text top="359" left="67" width="233" height="7" font="font6" id="p13_t55" reading_order_no="54" segment_no="13" tag_type="text">lutional network for automatic road extraction from satellite imagery,”</text>
<text top="368" left="67" width="6" height="7" font="font6" id="p13_t56" reading_order_no="55" segment_no="13" tag_type="text">in</text>
<text top="368" left="76" width="198" height="7" font="font36" id="p13_t57" reading_order_no="56" segment_no="13" tag_type="text">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR) Workshops</text>
<text top="368" left="274" width="23" height="7" font="font6" id="p13_t58" reading_order_no="57" segment_no="13" tag_type="text">, 2018.</text>
<text top="377" left="49" width="251" height="7" font="font6" id="p13_t59" reading_order_no="58" segment_no="14" tag_type="text">[59] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolu-</text>
<text top="386" left="67" width="64" height="7" font="font6" id="p13_t60" reading_order_no="59" segment_no="14" tag_type="text">tional networks,” in</text>
<text top="386" left="134" width="106" height="7" font="font36" id="p13_t61" reading_order_no="60" segment_no="14" tag_type="text">Eur. Conf. Comput. Vis. (ECCV)</text>
<text top="386" left="240" width="23" height="7" font="font6" id="p13_t62" reading_order_no="61" segment_no="14" tag_type="text">, 2014.</text>
<text top="395" left="49" width="251" height="7" font="font6" id="p13_t63" reading_order_no="62" segment_no="15" tag_type="text">[60] Z. Wang, E. P. Simoncelli, and A. C. Bovik, “Multiscale structural</text>
<text top="404" left="67" width="138" height="7" font="font6" id="p13_t64" reading_order_no="63" segment_no="15" tag_type="text">similarity for image quality assessment,” in</text>
<text top="404" left="207" width="93" height="7" font="font36" id="p13_t65" reading_order_no="64" segment_no="15" tag_type="text">Asilomar Conf. Signals, Syst.</text>
<text top="413" left="67" width="27" height="7" font="font36" id="p13_t66" reading_order_no="65" segment_no="15" tag_type="text">Comput.</text>
<text top="413" left="94" width="23" height="7" font="font6" id="p13_t67" reading_order_no="66" segment_no="15" tag_type="text">, 2003.</text>
<text top="423" left="49" width="251" height="7" font="font6" id="p13_t68" reading_order_no="67" segment_no="16" tag_type="text">[61] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham, A. Acosta,</text>
<text top="432" left="67" width="233" height="7" font="font6" id="p13_t69" reading_order_no="68" segment_no="16" tag_type="text">A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic</text>
<text top="441" left="67" width="233" height="7" font="font6" id="p13_t70" reading_order_no="69" segment_no="16" tag_type="text">single image super-resolution using a generative adversarial network,”</text>
<text top="450" left="67" width="6" height="7" font="font6" id="p13_t71" reading_order_no="70" segment_no="16" tag_type="text">in</text>
<text top="450" left="76" width="160" height="7" font="font36" id="p13_t72" reading_order_no="71" segment_no="16" tag_type="text">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</text>
<text top="450" left="237" width="23" height="7" font="font6" id="p13_t73" reading_order_no="72" segment_no="16" tag_type="text">, 2017.</text>
<text top="459" left="49" width="251" height="7" font="font6" id="p13_t74" reading_order_no="73" segment_no="17" tag_type="text">[62] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The</text>
<text top="468" left="67" width="233" height="7" font="font6" id="p13_t75" reading_order_no="74" segment_no="17" tag_type="text">unreasonable effectiveness of deep features as a perceptual metric,” in</text>
<text top="477" left="67" width="160" height="7" font="font36" id="p13_t76" reading_order_no="75" segment_no="17" tag_type="text">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</text>
<text top="477" left="228" width="23" height="7" font="font6" id="p13_t77" reading_order_no="76" segment_no="17" tag_type="text">, 2018.</text>
<text top="486" left="49" width="251" height="7" font="font6" id="p13_t78" reading_order_no="77" segment_no="18" tag_type="text">[63] E. Prashnani, H. Cai, Y. Mostofi, and P. Sen, “PieAPP: Perceptual image-</text>
<text top="495" left="67" width="162" height="7" font="font6" id="p13_t79" reading_order_no="78" segment_no="18" tag_type="text">error assessment through pairwise preference,” in</text>
<text top="495" left="232" width="68" height="7" font="font36" id="p13_t80" reading_order_no="79" segment_no="18" tag_type="text">IEEE Conf. Comput.</text>
<text top="504" left="67" width="90" height="7" font="font36" id="p13_t81" reading_order_no="80" segment_no="18" tag_type="text">Vis. Pattern Recog. (CVPR)</text>
<text top="504" left="157" width="23" height="7" font="font6" id="p13_t82" reading_order_no="81" segment_no="18" tag_type="text">, 2018.</text>
<text top="513" left="49" width="251" height="7" font="font6" id="p13_t83" reading_order_no="82" segment_no="19" tag_type="text">[64] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, “Image quality</text>
<text top="522" left="67" width="186" height="7" font="font6" id="p13_t84" reading_order_no="83" segment_no="19" tag_type="text">assessment: Unifying structure and texture similarity,”</text>
<text top="522" left="258" width="42" height="7" font="font36" id="p13_t85" reading_order_no="84" segment_no="19" tag_type="text">IEEE Trans.</text>
<text top="531" left="67" width="88" height="7" font="font36" id="p13_t86" reading_order_no="85" segment_no="19" tag_type="text">Pattern Anal. Mach. Intell.</text>
<text top="531" left="155" width="23" height="7" font="font6" id="p13_t87" reading_order_no="86" segment_no="19" tag_type="text">, 2020.</text>
<text top="541" left="49" width="251" height="7" font="font6" id="p13_t88" reading_order_no="87" segment_no="20" tag_type="text">[65] Netflix, “VMAF - video multi-method assessment fusion,” 2020.</text>
<text top="550" left="67" width="229" height="7" font="font6" id="p13_t89" reading_order_no="88" segment_no="20" tag_type="text">[Online]. Available: <a href="https://github.com/Netflix/vmaf/releases/tag/v1.5.1">https://github.com/Netflix/vmaf/releases/tag/v1.5.1</a></text>
<text top="559" left="49" width="251" height="7" font="font6" id="p13_t90" reading_order_no="89" segment_no="21" tag_type="text">[66] A. Mittal, A. K. Moorthy, and A. C. Bovik, “No-reference image quality</text>
<text top="568" left="67" width="111" height="7" font="font6" id="p13_t91" reading_order_no="90" segment_no="21" tag_type="text">assessment in the spatial domain,”</text>
<text top="568" left="180" width="91" height="7" font="font36" id="p13_t92" reading_order_no="91" segment_no="21" tag_type="text">IEEE Trans. Image Process.</text>
<text top="568" left="271" width="29" height="7" font="font6" id="p13_t93" reading_order_no="92" segment_no="21" tag_type="text">, vol. 21,</text>
<text top="577" left="67" width="97" height="7" font="font6" id="p13_t94" reading_order_no="93" segment_no="21" tag_type="text">no. 12, pp. 4695–4708, 2012.</text>
<text top="586" left="49" width="251" height="7" font="font6" id="p13_t95" reading_order_no="94" segment_no="22" tag_type="text">[67] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely</text>
<text top="595" left="67" width="99" height="7" font="font6" id="p13_t96" reading_order_no="95" segment_no="22" tag_type="text">blind” image quality analyzer,”</text>
<text top="595" left="168" width="85" height="7" font="font36" id="p13_t97" reading_order_no="96" segment_no="22" tag_type="text">IEEE Signal Process. Lett.</text>
<text top="595" left="253" width="47" height="7" font="font6" id="p13_t98" reading_order_no="97" segment_no="22" tag_type="text">, vol. 20, no. 3,</text>
<text top="604" left="67" width="63" height="7" font="font6" id="p13_t99" reading_order_no="98" segment_no="22" tag_type="text">pp. 209–212, 2013.</text>
<text top="614" left="49" width="251" height="7" font="font6" id="p13_t100" reading_order_no="99" segment_no="23" tag_type="text">[68] S. V. R. Dendi and S. S. Channappayya, “No-reference video quality</text>
<text top="622" left="67" width="189" height="7" font="font6" id="p13_t101" reading_order_no="100" segment_no="23" tag_type="text">assessment using natural spatiotemporal scene statistics,”</text>
<text top="623" left="259" width="41" height="7" font="font36" id="p13_t102" reading_order_no="101" segment_no="23" tag_type="text">IEEE Trans.</text>
<text top="632" left="67" width="49" height="7" font="font36" id="p13_t103" reading_order_no="102" segment_no="23" tag_type="text">Image Process.</text>
<text top="631" left="117" width="104" height="7" font="font6" id="p13_t104" reading_order_no="103" segment_no="23" tag_type="text">, vol. 29, pp. 5612–5624, 2020.</text>
<text top="641" left="49" width="251" height="7" font="font6" id="p13_t105" reading_order_no="104" segment_no="24" tag_type="text">[69] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image</text>
<text top="650" left="67" width="50" height="7" font="font6" id="p13_t106" reading_order_no="105" segment_no="24" tag_type="text">recognition,” in</text>
<text top="650" left="120" width="158" height="7" font="font36" id="p13_t107" reading_order_no="106" segment_no="24" tag_type="text">IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)</text>
<text top="650" left="278" width="22" height="7" font="font6" id="p13_t108" reading_order_no="107" segment_no="24" tag_type="text">, 2016.</text>
<text top="659" left="49" width="251" height="7" font="font6" id="p13_t109" reading_order_no="108" segment_no="25" tag_type="text">[70] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,</text>
<text top="668" left="67" width="233" height="7" font="font6" id="p13_t110" reading_order_no="109" segment_no="25" tag_type="text">Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-</text>
<text top="677" left="67" width="183" height="7" font="font6" id="p13_t111" reading_order_no="110" segment_no="25" tag_type="text">Fei, “ImageNet large scale visual recognition challenge,”</text>
<text top="677" left="252" width="48" height="7" font="font36" id="p13_t112" reading_order_no="111" segment_no="25" tag_type="text">Int. J. Comput.</text>
<text top="686" left="67" width="12" height="7" font="font36" id="p13_t113" reading_order_no="112" segment_no="25" tag_type="text">Vis.</text>
<text top="686" left="79" width="121" height="7" font="font6" id="p13_t114" reading_order_no="113" segment_no="25" tag_type="text">, vol. 115, no. 3, pp. 211–252, 2015.</text>
<text top="695" left="49" width="251" height="7" font="font6" id="p13_t115" reading_order_no="114" segment_no="26" tag_type="text">[71] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning</text>
<text top="704" left="67" width="198" height="7" font="font6" id="p13_t116" reading_order_no="115" segment_no="26" tag_type="text">spatiotemporal features with 3d convolutional networks,” in</text>
<text top="704" left="269" width="31" height="7" font="font36" id="p13_t117" reading_order_no="116" segment_no="26" tag_type="text">IEEE Int.</text>
<text top="713" left="67" width="88" height="7" font="font36" id="p13_t118" reading_order_no="117" segment_no="26" tag_type="text">Conf. Comput. Vis. (ICCV)</text>
<text top="713" left="155" width="23" height="7" font="font6" id="p13_t119" reading_order_no="118" segment_no="26" tag_type="text">, 2015.</text>
<text top="723" left="49" width="251" height="7" font="font6" id="p13_t120" reading_order_no="119" segment_no="27" tag_type="text">[72] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking</text>
<text top="732" left="67" width="163" height="7" font="font6" id="p13_t121" reading_order_no="120" segment_no="27" tag_type="text">the inception architecture for computer vision,” in</text>
<text top="732" left="233" width="67" height="7" font="font36" id="p13_t122" reading_order_no="121" segment_no="27" tag_type="text">IEEE Conf. Comput.</text>
<text top="741" left="67" width="90" height="7" font="font36" id="p13_t123" reading_order_no="122" segment_no="27" tag_type="text">Vis. Pattern Recog. (CVPR)</text>
<text top="740" left="157" width="23" height="7" font="font6" id="p13_t124" reading_order_no="123" segment_no="27" tag_type="text">, 2016.</text>
<text top="59" left="312" width="251" height="7" font="font6" id="p13_t125" reading_order_no="124" segment_no="2" tag_type="text">[73] G. J. Sz´ekely, M. L. Rizzo, and N. K. Bakirov, “Measuring and testing</text>
<text top="68" left="330" width="130" height="7" font="font6" id="p13_t126" reading_order_no="125" segment_no="2" tag_type="text">dependence by correlation of distances,”</text>
<text top="68" left="463" width="39" height="7" font="font36" id="p13_t127" reading_order_no="126" segment_no="2" tag_type="text">Ann. Statist.</text>
<text top="68" left="502" width="61" height="7" font="font6" id="p13_t128" reading_order_no="127" segment_no="2" tag_type="text">, vol. 35, no. 6, pp.</text>
<text top="77" left="330" width="59" height="7" font="font6" id="p13_t129" reading_order_no="128" segment_no="2" tag_type="text">2769–2794, 2007.</text>
</page>
<outline>
<item page="1">I Introduction</item>
<outline>
<item page="2">I-A Overview of Contributions</item>
</outline>
<item page="3">II Related Work</item>
<outline>
<item page="3">II-A Video Prediction</item>
<item page="3">II-B Evaluation methods for video prediction models</item>
<item page="3">II-C Video quality assessment</item>
</outline>
<item page="3">III Predicted Videos Quality Assessment Database</item>
<outline>
<item page="3">III-A Database</item>
<item page="4">III-B Subjective Study</item>
<item page="5">III-C Observations from the Subjective Study</item>
<outline>
<item page="5">III-C1 Consistency of subjects</item>
<item page="5">III-C2 Validation of our subjective study</item>
<item page="6">III-C3 How does MOS vary for different distortions?</item>
<item page="6">III-C4 Do stochastic models perform better than deterministic models?</item>
</outline>
</outline>
<item page="6">IV Feature Extraction for Video Quality Assessment</item>
<outline>
<item page="6">IV-A Motion-compensated Cosine Similarity (MCS) features</item>
<item page="7">IV-B Rescaled Frame Difference (RFD) features</item>
<item page="8">IV-C Learning quality from features</item>
</outline>
<item page="8">V Experiments</item>
<outline>
<item page="8">V-A Evaluation of Objective Quality Measures</item>
<outline>
<item page="8">V-A1 Existing measures</item>
<item page="8">V-A2 Quality assessment using deep features</item>
<item page="8">V-A3 Our model</item>
<item page="8">V-A4 Performance Evaluation</item>
<item page="9">V-A5 Results</item>
</outline>
<item page="9">V-B Ablations and Extended Experiments</item>
<outline>
<item page="9">V-B1 Contribution of individual components</item>
<item page="10">V-B2 Performance on stochastic videos</item>
<item page="10">V-B3 Different regression models</item>
<item page="11">V-B4 Variation in performance with respect to number of principal components</item>
<item page="11">V-B5 Robustness with less training data</item>
</outline>
<item page="11">V-C Computational Complexity</item>
</outline>
<item page="11">VI Conclusion</item>
<item page="12">References</item>
</outline>
</pdf2xml>
